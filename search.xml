<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-17</title>
    <url>/2020/02/17/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-17/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Purifying Real Images with an Attention-guided Style Transfer Network  for Gaze Estimation <a href="https://arxiv.org/pdf/2002.06145" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Combining Visual and Textual Features for Semantic Segmentation of  Historical Newspapers <a href="https://arxiv.org/pdf/2002.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Layer-wise Pruning and Auto-tuning of Layer-wise Learning Rates in  Fine-tuning of Deep Networks <a href="https://arxiv.org/pdf/2002.06048" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Constrained Dominant sets and Its applications in computer vision <a href="https://arxiv.org/pdf/2002.06028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Building Networks for Image Segmentation using Particle Competition and  Cooperation <a href="https://arxiv.org/pdf/2002.06001" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> A Hybrid 3DCNN and 3DC-LSTM based model for 4D Spatio-temporal fMRI  data: An ABIDE Autism Classification study <a href="https://arxiv.org/pdf/2002.05981" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Context Conditional Variational Autoencoder for Predicting Multi-Path  Trajectories in Mixed Traffic <a href="https://arxiv.org/pdf/2002.05966" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Multi-Level Feature Fusion Mechanism for Single Image Super-Resolution <a href="https://arxiv.org/pdf/2002.05962" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Counting dense objects in remote sensing images <a href="https://arxiv.org/pdf/2002.05928" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> End-to-end Learning of Object Motion Estimation from Retinal Events for  Event-based Object Tracking <a href="https://arxiv.org/pdf/2002.05911" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> A Survey on 3D Skeleton-Based Action Recognition Using Learning Method <a href="https://arxiv.org/pdf/2002.05907" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Liver Segmentation in Abdominal CT Images via Auto-Context Neural  Network and Self-Supervised Contour Attention <a href="https://arxiv.org/pdf/2002.05895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset <a href="https://arxiv.org/pdf/2002.05878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Skip Connections Matter: On the Transferability of Adversarial Examples  Generated with ResNets <a href="https://arxiv.org/pdf/2002.05990" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> SemI2I: Semantically Consistent Image-to-Image Translation for Domain  Adaptation of Remote Sensing Data <a href="https://arxiv.org/pdf/2002.05925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Remove Appearance Shift for Ultrasound Image Segmentation via Fast and  Universal Style Transfer <a href="https://arxiv.org/pdf/2002.05844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Variational Conditional-Dependence Hidden Markov Models for Human Action  Recognition <a href="https://arxiv.org/pdf/2002.05809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> ACEnet: Anatomical Context-Encoding Network for Neuroanatomy  Segmentation <a href="https://arxiv.org/pdf/2002.05773" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Purifying Real Images with an Attention-guided Style Transfer Network  for Gaze Estimation</b>  <a href="https://arxiv.org/pdf/2002.06145" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxiao Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinjia Peng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huibing Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xianping Fu</a><br>
<font size="3">
Abstract: Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distribution of synthetic images compared to real images, the desired performance cannot be achieved. Real images consist of multiple forms of light orientation, while synthetic images consist of a uniform light orientation. These features are considered to be characteristic of outdoor and indoor scenes, respectively. To solve this problem, the previous method learned a model to improve the realism of the synthetic image. Different from the previous methods, this paper try to purify real image by extracting discriminative and robust features to convert outdoor real images to indoor synthetic images. In this paper, we first introduce the segmentation masks to construct RGB-mask pairs as inputs, then we design a attention-guided style transfer network to learn style features separately from the attention and bkgd(background) region , learn content features from full and attention region. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using mixed studies (qualitative and quantitative) methods to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on three public datasets, including LPW, COCO and MPIIGaze. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，边学边合成的进步提出了合成图像的人才培养模式，可有效降低人力和物力成本。然而，由于不同的分布相比，实际图像的合成图像，无法实现期望的性能。真实图像包括的光取向多种形式，而合成图像由一个均匀的光取向的。这些特性被认为是室内和室外场景的特点，分别。为了解决这个问题，以前的方法学到了模式，提高合成图像的真实感。从以前的方法不同，本文尝试通过提取歧视和强大的功能，真正的户外图像转换为室内合成图像净化真实图像。在本文中，我们首先介绍的分割掩码构建RGB-掩模对作为输入，然后我们设计了注意力引导式传送网络学习风格从关注和BKGD（背景）区分开的特征，从全学习内容的特征和关注区域。此外，我们提出了一个新的区域级任务引导损失抑制从风格和内容学习的特点。实验使用混合研究（定性和定量）方法来证明在复杂的方向净化真实图像的可能性进行。我们评估三个公共数据集，包括LPW，COCO和MPIIGaze所提出的方法。广泛的实验结果表明，所提出的方法是有效的和达到状态的最先进的结果。</font>
</div>


<hr>
<div id="paper2"> <b>2. Combining Visual and Textual Features for Semantic Segmentation of  Historical Newspapers</b>  <a href="https://arxiv.org/pdf/2002.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Barman%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raphaël Barman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ehrmann%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maud Ehrmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clematide%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Clematide</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oliveira%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Sofia Ares Oliveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaplan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Frédéric Kaplan</a><br>
<font size="3">
Abstract: The massive amounts of digitized historical documents acquired over the last decades naturally lend themselves to automatic processing and exploration. Research work seeking to automatically process facsimiles and extract information thereby are multiplying with, as a first essential step, document layout analysis. If the identification and categorization of segments of interest in document images have seen significant progress over the last years thanks to deep learning techniques, many challenges remain with, among others, the use of finer-grained segmentation typologies and the consideration of complex, heterogeneous documents such as historical newspapers. Besides, most approaches consider visual features only, ignoring textual signal. In this context, we introduce a multimodal approach for the semantic segmentation of historical newspapers that combines visual and textual features. Based on a series of experiments on diachronic Swiss and Luxembourgish newspapers, we investigate, among others, the predictive power of visual and textual features and their capacity to generalize across time and sources. Results show consistent improvement of multimodal models in comparison to a strong visual baseline, as well as better robustness to high material variance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在过去的十年中取得数字化的历史记录了大量的自然借给自己自动处理和探索。研究工作寻求自动处理传真和提取信息，从而与倍增，作为第一个重要步骤，文档布局分析。如果识别和文件图像的兴趣细分的分类已经看到过去几年中由于深学习技术了显著的进步，许多挑战仍然存在，等等，使用细粒度分割类型学和考虑复杂的异构文件如历史报纸。此外，大多数的方法只考虑视觉特征，忽略文本信号。在这方面，我们引入历史报纸的语义分割，结合视觉和文本特征的多模态的方法。基于一系列关于历时瑞士和卢森堡报纸实验，调查，除其他外，视觉和文字特征的预测能力和他们的能力，以跨越时间和来源一概而论。结果表明，比较多车型的持续改进，以强烈的视觉底线，以及更好的鲁棒性高的材料差异。</font>
</div>


<hr>
<div id="paper3"> <b>3. Layer-wise Pruning and Auto-tuning of Layer-wise Learning Rates in  Fine-tuning of Deep Networks</b>  <a href="https://arxiv.org/pdf/2002.06048" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ro%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youngmin Ro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Jin Young Choi</a><br>
<font size="3">
Abstract: Existing fine-tuning methods use a single learning rate over all layers. In this paper, first, we discuss that trends of layer-wise weight variations by fine-tuning using a single learning rate do not match the well-known notion that lower-level layers extract general features and higher-level layers extract specific features. Based on our discussion, we propose an algorithm that improves fine-tuning performance and reduces network complexity through layer-wise pruning and auto-tuning of layer-wise learning rates. Through in-depth experiments on image retrieval (CUB-200-2011, Stanford online products, and Inshop) and fine-grained classification (Stanford cars, Aircraft) datasets, the effectiveness of the proposed algorithm is verified. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有的微调方法使用在所有层的单个学习率。在本文中，第一，我们使用单个学习速率不匹配的公知的概念，即较低级的层提取一般特征和更高级别的层提取特定特征讨论逐层重量变化通过微调该趋势。根据我们的讨论，我们提出改进微调性能，并通过逐层修剪和逐层学习速率自动调整降低了网络复杂性的算法。通过对图像检索进行了深入的实验（CUB-200-2011，斯坦福在线产品和Inshop）和细粒度分类（斯坦福汽车，飞机）的数据集，该算法的有效性进行验证。</font>
</div>


<hr>
<div id="paper4"> <b>4. Constrained Dominant sets and Its applications in computer vision</b>  <a href="https://arxiv.org/pdf/2002.06028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tesfaye%2C+A+L" target="_blank" rel="noopener" style="color:#0000EE;">Alemu Leulseged Tesfaye</a><br>
<font size="3">
Abstract: In this thesis, we present new schemes which leverage a constrained clustering method to solve several computer vision tasks ranging from image retrieval, image segmentation and co-segmentation, to person re-identification. In the last decades clustering methods have played a vital role in computer vision applications; herein, we focus on the extension, reformulation, and integration of a well-known graph and game theoretic clustering method known as Dominant Sets. Thus, we have demonstrated the validity of the proposed methods with extensive experiments which are conducted on several benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出新的方案，其杠杆率约束的聚类方法来解决几个计算机视觉任务，从图像检索，图像分割和共同分割，以人重新鉴定。在聚类方法在过去几十年都起到了计算机视觉应用至关重要的作用;本文中，我们侧重于延伸，再形成，以及一个公知的图形和博弈论聚类方法称为显性集的整合。因此，我们已经证明了广泛的实验所提出的方法被几个基准数据集进行的有效性。</font>
</div>


<hr>
<div id="paper5"> <b>5. Building Networks for Image Segmentation using Particle Competition and  Cooperation</b>  <a href="https://arxiv.org/pdf/2002.06001" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Breve%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fabricio Breve</a><br>
<font size="3">
Abstract: Particle competition and cooperation (PCC) is a graph-based semi-supervised learning approach. When PCC is applied to interactive image segmentation tasks, pixels are converted into network nodes, and each node is connected to its k-nearest neighbors, according to the distance between a set of features extracted from the image. Building a proper network to feed PCC is crucial to achieve good segmentation results. However, some features may be more important than others to identify the segments, depending on the characteristics of the image to be segmented. In this paper, an index to evaluate candidate networks is proposed. Thus, building the network becomes a problem of optimizing some feature weights based on the proposed index. Computer simulations are performed on some real-world images from the Microsoft GrabCut database, and the segmentation results related in this paper show the effectiveness of the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：粒子竞争与合作（PCC）是一种基于图的半监督学习方法。当PCC被施加到交互式图像分割任务，像素被转换成的网络节点，并且每个节点被连接到它的k-最近邻，根据一组从图像中提取特征之间的距离。建立一个适当的网络饲料PCC是实现良好的分割效果的关键。然而，比其他人来识别段，根据图像的特性来被分段一些特征可能是更重要的。在本文中，来评估候选网络的指数建议。因此，建设网络成为优化基础上，提出了指数一些要素权重的问题。计算机模拟从微软GrabCut数据库中的一些真实世界的影像进行，在本文中涉及的分割结果证明了该方法的有效性。</font>
</div>


<hr>
<div id="paper6"> <b>6. A Hybrid 3DCNN and 3DC-LSTM based model for 4D Spatio-temporal fMRI  data: An ABIDE Autism Classification study</b>  <a href="https://arxiv.org/pdf/2002.05981" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=El-Gazzar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmed El-Gazzar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Quaak%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mirjam Quaak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cerliani%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leonardo Cerliani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bloem%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Bloem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=van+Wingen%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guido van Wingen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+R+M" target="_blank" rel="noopener" style="color:#0000EE;">Rajat Mani Thomas</a><br>
<font size="3">
Abstract: Functional Magnetic Resonance Imaging (fMRI) captures the temporal dynamics of neural activity as a function of spatial location in the brain. Thus, fMRI scans are represented as 4-Dimensional (3-space + 1-time) tensors. And it is widely believed that the spatio-temporal patterns in fMRI manifests as behaviour and clinical symptoms. Because of the high dimensionality ($\sim$ 1 Million) of fMRI, and the added constraints of limited cardinality of data sets, extracting such patterns are challenging. A standard approach to overcome these hurdles is to reduce the dimensionality of the data by either summarizing activation over time or space at the expense of possible loss of useful information. Here, we introduce an end-to-end algorithm capable of extracting spatiotemporal features from the full 4-D data using 3-D CNNs and 3-D Convolutional LSTMs. We evaluate our proposed model on the publicly available ABIDE dataset to demonstrate the capability of our model to classify Autism Spectrum Disorder (ASD) from resting-state fMRI data. Our results show that the proposed model achieves state of the art results on single sites with F1-scores of 0.78 and 0.7 on NYU and UM sites, respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：功能性磁共振成像（fMRI）技术捕获的空间位置在大脑功能的神经活动的时间动态。因此，功能磁共振成像扫描被表示为4维（3-空间+ 1时）张量。而人们普遍认为，在功能磁共振成像表现为行为和临床症状的时空格局。因为高维数（$ \ SIM $ 100万）的fMRI的，和数据集的基数有限的附加约束，提取这种图案是具有挑战性。克服这些障碍的标准方法是通过任一总结激活随时间或空间，以减少数据的维度在有用的信息可能丢失的费用。在这里，介绍一种能够使用3-d细胞神经网络和3-d卷积LSTMs全4-d数据提取时空特征的端至端的算法。我们评估的可公开获得的数据集遵守我们提出的模型，以展示我们的模型，以泛自闭症障碍（ASD），从静止状态fMRI数据进行分类的能力。我们的研究结果表明，该模型实现了对单个站点，分别为0.78和0.7的纽约大学和UM网站，F1分数艺术成果的状态。</font>
</div>


<hr>
<div id="paper7"> <b>7. Context Conditional Variational Autoencoder for Predicting Multi-Path  Trajectories in Mixed Traffic</b>  <a href="https://arxiv.org/pdf/2002.05966" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W+L+M+Y" target="_blank" rel="noopener" style="color:#0000EE;">Wentong Liao. Michael Ying Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sester%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Monica Sester</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosenhahn%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bodo Rosenhahn</a><br>
<font size="3">
Abstract: Trajectory prediction in urban mixed-traffic zones is critical for many AI systems, such as traffic management, social robots and autonomous driving.However, there are many challenges to predict the trajectories of heterogeneous road agents (pedestrians, cyclists and vehicles) at a microscopic-level. For example, an agent might be able to choose multiple plausible paths in complex interactions with other agents in varying environments. To this end, we propose an approach named Context Conditional Variational Autoencoder (CCVAE) that encodes both past and future scene context, interaction context and motion information to capture the variations of the future trajectories using a set of stochastic latent variables. We predict multi-path trajectories conditioned on past information of the target agent by sampling the latent variable multiple times. Through experiments on several datasets of varying scenes, our method outperforms the recent state-of-the-art methods for mixed traffic trajectory prediction by a large margin and more robust in a very challenging environment. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在城市混合交通区轨迹预测是许多AI系统，如交通管理，社会机器人和自主driving.However，有预测异质道路剂（行人，自行车和汽车）的的轨迹许多挑战的关键微观层面。例如，一个代理可能能够选择与在不同环境中的其他代理复杂交互的多个合理的路径。为此，我们提出了一个名为语境条件变自动编码器（CCVAE）编码过去和将来的景物情境，互动情境和运动信息采集使用一组随机潜在变量的未来轨迹的变化的方法。我们预测多路径轨迹通过潜变量多次取样空调以往的目标代理的信息。通过对不同场景的几个数据集实验，我们的方法优于近期的大幅度和一个非常具有挑战性的环境中更稳健的国家的最先进的方法混合交通轨迹预测。</font>
</div>


<hr>
<div id="paper8"> <b>8. Multi-Level Feature Fusion Mechanism for Single Image Super-Resolution</b>  <a href="https://arxiv.org/pdf/2002.05962" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lyn%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiawen Lyn</a><br>
<font size="3">
Abstract: Convolution neural network (CNN) has been widely used in Single Image Super Resolution (SISR) so that SISR has been a great success recently. As the network deepens, the learning ability of network becomes more and more powerful. However, most SISR methods based on CNN do not make full use of hierarchical feature and the learning ability of network. These features cannot be extracted directly by subsequent layers, so the previous layer hierarchical information has little impact on the output and performance of subsequent layers relatively poor. To solve above problem, a novel Multi-Level Feature Fusion network (MLRN) is proposed, which can take full use of global intermediate features. We also introduce Feature Skip Fusion Block (FSFblock) as basic module. Each block can be extracted directly to the raw multiscale feature and fusion multi-level feature, then learn feature spatial correlation. The correlation among the features of the holistic approach leads to a continuous global memory of information mechanism. Extensive experiments on public datasets show that the method proposed by MLRN can be implemented, which is favorable performance for the most advanced methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（CNN）已被广泛应用于单图像超分辨率（SISR），以便SISR取得了很大的成功最近。随着网络的深入，网络的学习能力变得越来越强大。然而，根据CNN最SISR方法没有充分利用分层特征和网络的学习能力。这些功能不能由随后的层直接提取，所以先前层的分层信息对输出和后续层相对较差的性能的影响很小。为了解决上述问题，一种新颖的多级特征融合网络（MLRN）提出了一种能充分利用全球中间特性。我们还引进功能跳过融合块（FSFblock）为基本模块。每个块可以被直接提取到的原始多尺度特征和融合多层次特征，然后学习特征空间相关性。整体性方法导致的功能当中的信息机制，连续全局内存的相关性。公共数据集大量的实验表明，MLRN提出的方法可以实现，这是最先进的方法良好的性能。</font>
</div>


<hr>
<div id="paper9"> <b>9. Counting dense objects in remote sensing images</b>  <a href="https://arxiv.org/pdf/2002.05928" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guangshuai Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingjie Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunhong Wang</a><br>
<font size="3">
Abstract: Estimating accurate number of interested objects from a given image is a challenging yet important task. Significant efforts have been made to address this problem and achieve great progress, yet counting number of ground objects from remote sensing images is barely studied. In this paper, we are interested in counting dense objects from remote sensing images. Compared with object counting in natural scene, this task is challenging in following factors: large scale variation, complex cluttered background and orientation arbitrariness. More importantly, the scarcity of data severely limits the development of research in this field. To address these issues, we first construct a large-scale object counting dataset based on remote sensing images, which contains four kinds of objects: buildings, crowded ships in harbor, large-vehicles and small-vehicles in parking lot. We then benchmark the dataset by designing a novel neural network which can generate density map of an input image. The proposed network consists of three parts namely convolution block attention module (CBAM), scale pyramid module (SPM) and deformable convolution module (DCM). Experiments on the proposed dataset and comparisons with state of the art methods demonstrate the challenging of the proposed dataset, and superiority and effectiveness of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从给定的图像估计感兴趣对象的准确数字是一个挑战而重要的任务。显著已作出努力来解决这个问题，并取得了长足的进步，但是从遥感影像地面物体的计数值几乎没有影响。在本文中，我们感兴趣的是从遥感图像计数密集的对象。大规模的变化，复杂的复杂背景和方向随意性：在自然场景对象计数相比，这个任务是在以下因素的挑战。更重要的是，数据的缺乏严重限制了该领域研究的发展。为了解决这些问题，我们首先建立基于遥感影像，其包含四个类型的对象的大型对象计数数据集：建筑，拥挤的船只在港口，大型车和小型车的停车场。然后，我们的基准通过设计其可产生输入图像的密度图的新的神经网络的数据集。所提出的网络由三个部分组成，即卷积块注意模块（CBAM），规模金字塔模块（SPM）和可变形卷积模块（DCM）的。上提出的数据集实验和比较用的现有技术的方法证明了该数据集的挑战，而我们的方法的优越性和有效性。</font>
</div>


<hr>
<div id="paper10"> <b>10. End-to-end Learning of Object Motion Estimation from Retinal Events for  Event-based Object Tracking</b>  <a href="https://arxiv.org/pdf/2002.05911" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haosheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Suter%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Suter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiangqiang Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanzi Wang</a><br>
<font size="3">
Abstract: Event cameras, which are asynchronous bio-inspired vision sensors, have shown great potential in computer vision and artificial intelligence. However, the application of event cameras to object-level motion estimation or tracking is still in its infancy. The main idea behind this work is to propose a novel deep neural network to learn and regress a parametric object-level motion/transform model for event-based object tracking. To achieve this goal, we propose a synchronous Time-Surface with Linear Time Decay (TSLTD) representation, which effectively encodes the spatio-temporal information of asynchronous retinal events into TSLTD frames with clear motion patterns. We feed the sequence of TSLTD frames to a novel Retinal Motion Regression Network (RMRNet) to perform an end-to-end 5-DoF object motion regression. Our method is compared with state-of-the-art object tracking methods, that are based on conventional cameras or event cameras. The experimental results show the superiority of our method in handling various challenging environments such as fast motion and low illumination conditions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事件相机，这是异步的仿生视觉传感器，显示在计算机视觉和人工智能的巨大潜力。然而，事件摄像机对象级运动估计或跟踪的应用仍处于初级阶段。这背后工作的主要思想是提出一种新颖的深层神经网络学习和退步的参数对象级运动/变换模型基于事件的对象跟踪。为了实现这个目标，提出了一种同步时间曲面的线性时间衰减（TSLTD）表示，这有效地编码异步事件视网膜与清晰的运动模式TSLTD帧的时空信息。我们从进料TSLTD帧序列，以一种新颖的视网膜运动回归网络（RMRNet）来执行一个终端到终端的五自由度对象运动消退。我们的方法是与国家的最先进的物体跟踪方法中，是基于传统摄像机或事件相机相比。实验结果表明我们在处理各种挑战性的环境，诸如快速运动和低光照条件的方法的优越性。</font>
</div>


<hr>
<div id="paper11"> <b>11. A Survey on 3D Skeleton-Based Action Recognition Using Learning Method</b>  <a href="https://arxiv.org/pdf/2002.05907" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengyuan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Runwei Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hong Liu</a><br>
<font size="3">
Abstract: 3D skeleton-based action recognition, owing to the latent advantages of skeleton, has been an active topic in computer vision. As a consequence, there are lots of impressive works including conventional handcraft feature based and learned feature based have been done over the years. However, previous surveys about action recognition mostly focus on the video or RGB data dominated methods, and the scanty existing reviews related to skeleton data mainly indicate the representation of skeleton data or performance of some classic techniques on a certain dataset. Besides, though deep learning methods has been applied to this field for years, there is no related reserach concern about an introduction or review from the perspective of deep learning architectures. To break those limitations, this survey firstly highlight the necessity of action recognition and the significance of 3D-skeleton data. Then a comprehensive introduction about Recurrent Neural Network(RNN)-based, Convolutional Neural Network(CNN)-based and Graph Convolutional Network(GCN)-based main stream action recognition techniques are illustrated in a data-driven manner. Finally, we give a brief talk about the biggest 3D skeleton dataset NTU-RGB+D and its new edition called NTU-RGB+D 120, accompanied with several existing top rank algorithms within those two datasets. To our best knowledge, this is the first research which give an overall discussion over deep learning-based action recognitin using 3D skeleton data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于骨架的3D动作识别，由于骨骼的潜在优势，一直是计算机视觉活跃的课题。因此，有很多令人印象深刻的作品，包括和基于学习的特征已多年来做传统手工功能。然而，关于动作识别之前的调查主要集中在视频和RGB数据为主的方法，和现有相关的骨架数据的评论寥寥无几，主要显示骨架数据或对某一数据集一些经典的技术性能的表现。此外，虽然深学习方法已应用到这个领域里，有来自深学习架构的角度介绍或审查没有相关启发式算法关注。为了打破这些限制，本次调查首先突出动作识别的必要性和3D骨架数据的意义。然后将约回归神经网络（RNN）为主，卷积神经网络（CNN）的全面介绍的基于图形和卷积网络（GCN）基主流动作识别技术在数据驱动的方式示出。最后，我们给出一个简单说说最大的三维骨骼数据集NTU-RGB + d和它的新版本称为NTU-RGB + d 120，伴随着这两个数据集内的多个现有热门排名算法。据我们所知，这是这给使用的三维骨骼数据在深基础的学习行动recognitin全面讨论第一个研究。</font>
</div>


<hr>
<div id="paper12"> <b>12. Liver Segmentation in Abdominal CT Images via Auto-Context Neural  Network and Self-Supervised Contour Attention</b>  <a href="https://arxiv.org/pdf/2002.05895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minyoung Chung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingyu Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeongjin Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yeong-Gil Shin</a><br>
<font size="3">
Abstract: Accurate image segmentation of the liver is a challenging problem owing to its large shape variability and unclear boundaries. Although the applications of fully convolutional neural networks (CNNs) have shown groundbreaking results, limited studies have focused on the performance of generalization. In this study, we introduce a CNN for liver segmentation on abdominal computed tomography (CT) images that shows high generalization performance and accuracy. To improve the generalization performance, we initially propose an auto-context algorithm in a single CNN. The proposed auto-context neural network exploits an effective high-level residual estimation to obtain the shape prior. Identical dual paths are effectively trained to represent mutual complementary features for an accurate posterior analysis of a liver. Further, we extend our network by employing a self-supervised contour scheme. We trained sparse contour features by penalizing the ground-truth contour to focus more contour attentions on the failures. The experimental results show that the proposed network results in better accuracy when compared to the state-of-the-art networks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CT images for training and validation. Two-fold cross-validation is presented for a comparison with the state-of-the-art neural networks. Novel multiple N-fold cross-validations are conducted to verify the performance of generalization. The proposed network showed the best generalization performance among the networks. Additionally, we present a series of ablation experiments that comprehensively support the importance of the underlying concepts. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：肝脏精确的图像分割是由于其较大的形状变化和界限不清一个具有挑战性的问题。虽然完全卷积神经网络（细胞神经网络）的应用已经显示开创性成果，有限的研究都集中在泛化的表现。在这项研究中，我们对腹部电脑断层扫描（CT）图像引入CNN肝分割昭示着高泛化性能和精度。为了提高推广能力，我们初步提出了在一个单一的CNN自动背景算法。所提出的自动上下文神经网络利用一个有效的高层次的剩余估计之前获得的形状。相同的双路径被有效地训练以表示用于肝脏的精确分析后相互互补的特征。此外，我们通过采用自监督轮廓方案我们的网络扩展。我们通过惩罚地面真轮廓专注于故障的详细轮廓重视培训的稀疏轮廓特征。实验结果表明，在更好的准确度所提出的网络的结果通过减少Hausdorff距离的10.31％，较先进的最先进的网络时。我们使用180幅腹部CT图像进行训练和验证。两折交叉验证呈现，用于与状态的最先进的神经网络的比较。新颖倍数N倍交叉验证是为了验证泛化的性能。所提出的网络显示网络中最好的泛化性能。此外，我们提出了一系列的消融实验证明，全面支持基本概念的重要性。</font>
</div>


<hr>
<div id="paper13"> <b>13. An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset</b>  <a href="https://arxiv.org/pdf/2002.05878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhicheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhihao Gu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Di%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuan Di</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rongye Shi</a><br>
<font size="3">
Abstract: The Waymo Open Dataset has been released recently, providing a platform to crowdsource some fundamental challenges for automated vehicles (AVs), such as 3D detection and tracking. While the dataset provides a large amount of high-quality and multi-source driving information, people in academia are more interested in the underlying driving policy programmed in Waymo self-driving cars, which is inaccessible due to AV manufacturers' proprietary protection. Accordingly, academic researchers have to make various assumptions to implement AV components in their models or simulations, which may not represent the realistic interactions in real-world traffic. Thus, this paper introduces an approach to learn an long short-term memory (LSTM)-based model for imitating the behavior of Waymo's self-driving model. The proposed model has been evaluated based on Mean Absolute Error (MAE). The experimental results show that our model outperforms several baseline models in driving action prediction. Also, a visualization tool is presented for verifying the performance of the model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：Waymo打开的数据集已经发布最近，提供一个平台，以众包的自动车（AVS），如3D检测和跟踪一些根本性的挑战。而该数据集提供了大量的高品质和多源驾驶信息，学术界人士更关心的是Waymo自动驾驶汽车编程的基本推动政策，这是人迹罕至，由于AV厂商的专有保护对策研究。因此，学术研究人员不得不做出各种假设以实现他们的模型或模拟AV设备，这可能不能代表现实世界的交通现实的互动。因此，本文介绍了学习的长短期记忆（LSTM）为基础的模型模仿Waymo的自驾车模型的行为的方法。基于平均绝对误差（MAE），该模型已被评估。实验结果表明，该模型优于几个基本模式，在驾驶行动的预测。此外，可视化工具提出了用于验证模型的性能。</font>
</div>


<hr>
<div id="paper14"> <b>14. Skip Connections Matter: On the Transferability of Adversarial Examples  Generated with ResNets</b>  <a href="https://arxiv.org/pdf/2002.05990" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongxian Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yisen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shu-Tao Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bailey%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Bailey</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xingjun Ma</a><br>
<font size="3">
Abstract: Skip connections are an essential component of current state-of-the-art deep neural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt. Despite their huge success in building deeper and more powerful DNNs, we identify a surprising security weakness of skip connections in this paper. Use of skip connections allows easier generation of highly transferable adversarial examples. Specifically, in ResNet-like (with skip connections) neural networks, gradients can backpropagate through either skip connections or residual modules. We find that using more gradients from the skip connections rather than the residual modules according to a decay factor, allows one to craft adversarial examples with high transferability. Our method is termed Skip Gradient Method(SGM). We conduct comprehensive transfer attacks against state-of-the-art DNNs including ResNets, DenseNets, Inceptions, Inception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trained DNNs. We show that employing SGM on the gradient flow can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, SGM can be easily combined with existing black-box attack techniques, and obtain high improvements over state-of-the-art transferability methods. Our findings not only motivate new research into the architectural vulnerability of DNNs, but also open up further challenges for the design of secure DNN architectures. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跳过连接是国家的最先进的电流深神经网络（DNNs）如RESNET，WideResNet，DenseNet，和ResNeXt的一个基本组成部分。尽管他们在建立更深入和更强大的DNNs巨大的成功，我们确定在本文中跳跃连接的一个令人惊讶的安全弱点。跳过连接的使用使得更容易产生高度对抗性转让的例子。具体而言，在RESNET状（具有跳跃连接）神经网络，梯度可backpropagate通过任跳过连接或残余模块。我们发现，使用更多的梯度从跳过连接，而不是根据衰减因子残留的模块，允许人们与手艺高转印对抗性例子。我们的方法被称为跳过梯度法（SGM）。我们开展反对国家的最先进的DNNs全面转让攻击，包括ResNets，DenseNets，Inceptions，成立之初，RESNET，挤压和激励网络（SENET）和稳健的培训DNNs。我们发现，采用SGM的梯度流动可以大大提高制作的攻击转让在几乎所有情况。此外，SGM能够容易地与现有的黑盒攻击技术组合，并且获得对国家的最先进的转印方法高的改进。我们的研究结果不仅激发新的研究DNNs的建筑的脆弱性，同时也开辟安全DNN架构的设计进一步的挑战。</font>
</div>


<hr>
<div id="paper15"> <b>15. SemI2I: Semantically Consistent Image-to-Image Translation for Domain  Adaptation of Remote Sensing Data</b>  <a href="https://arxiv.org/pdf/2002.05925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Tasar%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Onur Tasar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Happy%2C+S+L" target="_blank" rel="noopener" style="color:#0000EE;">S L Happy</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tarabalka%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuliya Tarabalka</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Alliez%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre Alliez</a><br>
<font size="3">
Abstract: Although convolutional neural networks have been proven to be an effective tool to generate high quality maps from remote sensing images, their performance significantly deteriorates when there exists a large domain shift between training and test data. To address this issue, we propose a new data augmentation approach that transfers the style of test data to training data using generative adversarial networks. Our semantic segmentation framework consists in first training a U-net from the real training data and then fine-tuning it on the test stylized fake training data generated by the proposed approach. Our experimental results prove that our framework outperforms the existing domain adaptation methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然卷积神经网络已经被证明是产生从遥感影像的高质量地图的有效工具，当存在训练和测试数据之间存在较大的领域转变他们的表现显著恶化。为了解决这个问题，我们提出了转会的测试数据的样式训练数据使用生成对抗性的网络新的数据增强方法。我们的语义分割框架由在第一次训练从真正的训练数据的U形网，然后进行精细调整，通过该方法生成的测试程式化假的训练数据。我们的实验结果证明我们的架构优于现有的域自适应方法。</font>
</div>


<hr>
<div id="paper16"> <b>16. Remove Appearance Shift for Ultrasound Image Segmentation via Fast and  Universal Style Transfer</b>  <a href="https://arxiv.org/pdf/2002.05844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhendong Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Yang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Gao%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Gao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shengfeng Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dou%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoran Dou</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=He%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuangchi He</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Huang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhao Huang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Huang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yankai Huang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Luo%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huanjia Luo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanji Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xiong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Xiong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ni%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dong Ni</a><br>
<font size="3">
Abstract: Deep Neural Networks (DNNs) suffer from the performance degradation when image appearance shift occurs, especially in ultrasound (US) image segmentation. In this paper, we propose a novel and intuitive framework to remove the appearance shift, and hence improve the generalization ability of DNNs. Our work has three highlights. First, we follow the spirit of universal style transfer to remove appearance shifts, which was not explored before for US images. Without sacrificing image structure details, it enables the arbitrary style-content transfer. Second, accelerated with Adaptive Instance Normalization block, our framework achieved real-time speed required in the clinical US scanning. Third, an efficient and effective style image selection strategy is proposed to ensure the target-style US image and testing content US image properly match each other. Experiments on two large US datasets demonstrate that our methods are superior to state-of-the-art methods on making DNNs robust against various appearance shifts. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络（DNNs）从性能下降时遭受图像外观发生偏移，尤其是在超声（US）图像分割。在本文中，我们提出了一种新颖的和直观的架构，消除外观移位，并因此提高DNNs的泛化能力。我们的工作有三大亮点。首先，我们按照通用的风格传递的精神，以除去外观的变化，这是以前没有的超声图像研究。在不牺牲图像结构的详细信息，它使任意的方式，内容传输。其次，与Adaptive实例标准化框加快，我们的框架实现在美国临床扫描所需的实时速度。第三，高效和有效的风格形象的选择策略，提出了确保目标式的美国形象和测试内容美国的形象正确地相互匹配。两个大型数据集美国实验证明我们的方法是优于国家的最先进的方法上做出DNNs对各种外观的变化稳健。</font>
</div>


<hr>
<div id="paper17"> <b>17. Variational Conditional-Dependence Hidden Markov Models for Human Action  Recognition</b>  <a href="https://arxiv.org/pdf/2002.05809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Panousis%2C+K+P" target="_blank" rel="noopener" style="color:#0000EE;">Konstantinos P. Panousis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chatzis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sotirios Chatzis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Theodoridis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergios Theodoridis</a><br>
<font size="3">
Abstract: Hidden Markov Models (HMMs) are a powerful generative approach for modeling sequential data and time-series in general. However, the commonly employed assumption of the dependence of the current time frame to a single or multiple immediately preceding frames is unrealistic; more complicated dynamics potentially exist in real world scenarios. Human Action Recognition constitutes such a scenario, and has attracted increased attention with the advent of low-cost 3D sensors. The naturally arising variations and complex temporal dependencies have established this task as a challenging problem in the community. This paper revisits conventional sequential modeling approaches, aiming to address the problem of capturing time-varying temporal dependency patterns. To this end, we propose a different formulation of HMMs, whereby the dependence on past frames is dynamically inferred from the data. Specifically, we introduce a hierarchical extension by postulating an additional latent variable layer; therein, the (time-varying) temporal dependence patterns are treated as latent variables over which inference is performed. We leverage solid arguments from the Variational Bayes framework and derive a tractable inference algorithm based on the forward-backward algorithm. As we experimentally show using benchmark datasets, our approach yields competitive recognition accuracy and can effectively handle data with missing values. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：隐马尔可夫模型（HMM）是用于一般模拟连续数据和时间序列的一个强大的生成方法。然而，当前时间帧的单个或多个紧接在前的帧的依赖性的通常使用的假设是不现实的;更复杂的动态潜在存在于真实世界的场景。人类行为识别构成这样的情景，并吸引具有低成本的3D传感器的出现越来越多的关注。自然产生的变化和复杂的时序依赖已经建立了这个任务，因为在社会上具有挑战性的问题。本文回访传统的顺序建模方法，旨在解决捕捉时间变化的时间依赖性模式的问题。为此，我们提出的HMM的不同的制剂，由此在过去的帧的依赖性被动态地从数据推断。具体来说，我们介绍通过假定一个附加潜变量层的分层扩展;在其中，所述（随时间变化）时间依赖性模式将被视为在其上执行推理潜变量。我们从变贝叶斯框架充分利用了坚实的论据并且基于向前向后的算法易处理推理算法。正如我们通过实验证明使用标准数据集，我们的方法产生有竞争力的识别精度和能有效地缺失值处理数据。</font>
</div>


<hr>
<div id="paper18"> <b>18. ACEnet: Anatomical Context-Encoding Network for Neuroanatomy  Segmentation</b>  <a href="https://arxiv.org/pdf/2002.05773" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuemeng Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongming Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong Fan</a><br>
<font size="3">
Abstract: Segmentation of brain structures from magnetic resonance (MR) scans plays an important role in the quantification of brain morphology. Since 3D deep learning models suffer from high computational cost, 2D deep learning methods are favored for their computational efficiency. However, existing 2D deep learning methods are not equipped to effectively capture 3D spatial contextual information that is needed to achieve accurate brain structure segmentation. In order to overcome this limitation, we develop an Anatomical Context-Encoding Network (ACEnet) to incorporate 3D spatial and anatomical contexts in 2D convolutional neural networks (CNNs) for efficient and accurate segmentation of brain structures from MR scans, consisting of 1) an anatomical context encoding module to incorporate anatomical information in 2D CNNs, 2) a spatial context encoding module to integrate 3D image information in 2D CNNs, and 3) a skull stripping module to guide 2D CNNs to attend to the brain. Extensive experiments on three benchmark datasets have demonstrated that our method outperforms state-of-the-art alternative methods for brain structure segmentation in terms of both computational efficiency and segmentation accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从分割磁共振脑结构（MR）扫描起着大脑形态的量化具有重要作用。由于3D深度学习模型从高计算成本受到影响，2D深学习方法有利于他们的计算效率。然而，现有的2D深学习方法不具备有效地捕捉所需要实现精确的大脑结构分割三维空间的上下文信息。为了克服这种限制，我们开发的解剖上下文编码网络（ACENET）掺入三维空间和解剖上下文在2D卷积神经网络（细胞神经网络），用于从MR扫描脑结构的有效和准确的分割，包括1）一个解剖上下文编码模块纳入在2D细胞神经网络，2）空间上下文编码模块到3D图像信息中的2D细胞神经网络整合，以及3）一个头骨汽提模块引导2D细胞神经网络参加到大脑的解剖信息。三个基准数据集大量的实验已经证明，我们的方法优于国家的最先进的脑结构分割的替代方法在两个计算效率和分割准确度方面。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-17</title>
    <url>/2020/02/17/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-17/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Transformer on a Diet <a href="https://arxiv.org/pdf/2002.06170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base <a href="https://arxiv.org/pdf/2002.06115" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><div id="title3">
<b>3.</b> FQuAD: French Question Answering Dataset <a href="https://arxiv.org/pdf/2002.06071" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>


<div id="title4">
<b>4.</b> Dialogue history integration into end-to-end signal-to-concept spoken  language understanding systems <a href="https://arxiv.org/pdf/2002.06012" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Integrating Discrete and Neural Features via Mixed-feature  Trans-dimensional Random Field Language Models <a href="https://arxiv.org/pdf/2002.05967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> A Data Efficient End-To-End Spoken Language Understanding Architecture <a href="https://arxiv.org/pdf/2002.05955" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Zero-Resource Cross-Domain Named Entity Recognition <a href="https://arxiv.org/pdf/2002.05923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Understanding patient complaint characteristics using contextual  clinical BERT embeddings <a href="https://arxiv.org/pdf/2002.05902" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Transformers as Soft Reasoners over Language <a href="https://arxiv.org/pdf/2002.05867" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> HULK: An Energy Efficiency Benchmark Platform for Responsible Natural  Language Processing <a href="https://arxiv.org/pdf/2002.05829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Unsupervised Speaker Adaptation using Attention-based Speaker Memory for  End-to-End ASR <a href="https://arxiv.org/pdf/2002.06165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Combining Visual and Textual Features for Semantic Segmentation of  Historical Newspapers <a href="https://arxiv.org/pdf/2002.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Exploring Chemical Space using Natural Language Processing Methodologies  for Drug Discovery <a href="https://arxiv.org/pdf/2002.06053" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Deep Speaker Embeddings for Far-Field Speaker Recognition on Short  Utterances <a href="https://arxiv.org/pdf/2002.06033" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Query2box: Reasoning over Knowledge Graphs in Vector Space using Box  Embeddings <a href="https://arxiv.org/pdf/2002.05969" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Transformer on a Diet</b>  <a href="https://arxiv.org/pdf/2002.06170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenguang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihao Ye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aston Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Smola%2C+A+J" target="_blank" rel="noopener" style="color:#0000EE;">Alexander J. Smola</a><br>
<font size="3">
Abstract: Transformer has been widely used thanks to its ability to capture sequence information in an efficient way. However, recent developments, such as BERT and GPT-2, deliver only heavy architectures with a focus on effectiveness. In this paper, we explore three carefully-designed light Transformer architectures to figure out whether the Transformer with less computations could produce competitive results. Experimental results on language model benchmark datasets hint that such trade-off is promising, and the light Transformer reduces 70% parameters at best, while obtains competitive perplexity compared to standard Transformer. The source code is publicly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：变压器已广泛应用于得益于其能否捕获序列信息的有效途径。然而，最近的事态发展，如BERT和GPT-2，重点放在有效性只提供重的体系结构。在本文中，我们将探讨3精心设计的灯变压器架构弄清楚用更少的计算，Transformer是否可以产生竞争的结果。在语言模型标准数据集实验结果提示，这种权衡是有希望的，虽然取得竞争的困惑与标准变压器，光变压器充其量减少70％的参数。源代码是公开的。</font>
</div>


<hr>
<div id="paper2"> <b>2. Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base</b>  <a href="https://arxiv.org/pdf/2002.06115" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+W+W" target="_blank" rel="noopener" style="color:#0000EE;">William W. Cohen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haitian Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hofer%2C+R+A" target="_blank" rel="noopener" style="color:#0000EE;">R. Alex Hofer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Siegler%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Siegler</a><br>
<font size="3">
Abstract: We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB. This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations. The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们描述代表称为稀疏矩阵物化KB一个象征性的知识基础（KB）的新方法。这表示使得神经模块完全区分的，忠实于KB，表现足够多跳推理模型，可扩展性足以与使用大现实KB的原始语义。稀疏矩阵物化KB可以跨多个GPU分布，可以扩展到数以千万计的实体和事实的，是数量级比幼稚稀疏矩阵实现更快。在物化的KB能够非常简单的终端到终端的架构就代表任务的两个家庭几个基准获得有竞争力的性能：KB完成，并从denotations学习语义解析器。</font>
</div>


<hr>
<div id="paper3"> <b>3. FQuAD: French Question Answering Dataset</b>  <a href="https://arxiv.org/pdf/2002.06071" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=d%27Hoffschmidt%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin d'Hoffschmidt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vidal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maxime Vidal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Belblidia%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wacim Belblidia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brendl%C3%A9%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Brendlé</a><br>
<font size="3">
Abstract: Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, the Machine Reading Comprehension task has made significant progress. However, most of the results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is French Native Reading Comprehension dataset that consists of 25,000+ questions on a set of Wikipedia articles. A baseline model is trained which achieves an F1 score of 88.0% and an exact match ratio of 77.9% on the test set. The dataset is made freely available at https://fquad.illuin.tech. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在语言建模领域的最新进展已改善许多自然语言处理任务的国家的最先进的成果。其中，机器阅读理解任务，取得了显著的进步。然而，大部分的结果都报道了英语，因为在其他语言，如法语标注的资源，仍然十分匮乏。在目前的工作中，我们引进法国问答集（FQuAD）。 FQuAD是法语为母语阅读理解数据集包括一组的维基百科文章的25000个问题。基线模型被训练其实现的88.0％的F1分数和在测试组的77.9％的精确匹配比。该数据集是在https://fquad.illuin.tech免费提供。</font>
</div>


<hr>
<div id="paper4"> <b>4. Dialogue history integration into end-to-end signal-to-concept spoken  language understanding systems</b>  <a href="https://arxiv.org/pdf/2002.06012" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tomashenko%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Natalia Tomashenko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Raymond%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Raymond</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caubriere%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antoine Caubriere</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=De+Mori%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Renato De Mori</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Esteve%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yannick Esteve</a><br>
<font size="3">
Abstract: This work investigates the embeddings for representing dialog history in spoken language understanding (SLU) systems. We focus on the scenario when the semantic information is extracted directly from the speech signal by means of a single end-to-end neural network model. We proposed to integrate dialogue history into an end-to-end signal-to-concept SLU system. The dialog history is represented in the form of dialog history embedding vectors (so-called h-vectors) and is provided as an additional information to end-to-end SLU models in order to improve the system performance. Three following types of h-vectors are proposed and experimentally evaluated in this paper: (1) supervised-all embeddings predicting bag-of-concepts expected in the answer of the user from the last dialog system response; (2) supervised-freq embeddings focusing on predicting only a selected set of semantic concept (corresponding to the most frequent errors in our experiments); and (3) unsupervised embeddings. Experiments on the MEDIA corpus for the semantic slot filling task demonstrate that the proposed h-vectors improve the model performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：该作品探讨了在口语理解（SLU）系统代表对话历史的嵌入物。我们专注于场景时，直接从语音信号通过一个单端至端的神经网络模型的装置所提取的语义信息。我们提出了对话的历史融入一个终端到终端的信号 - 概念SLU系统。对话历史被在对话历史嵌入矢量的形式表示（所谓的H-载体）和以提高系统的性能被设置为附加信息，以端 - 端SLU模型。三个以下类型的h-向量提出和实验本文评价：（1）监督-所有的嵌入预测袋的的概念预计在从最后一个对话系统响应所述用户的答案; （2）监督-FREQ的嵌入集中于预测只有选定的一组语义概念（对应于在我们的实验中最频繁的误差）的;和（3）无监督的嵌入。对媒体语料库语义槽分配任务，实验结果表明，所提出的H-载体提高模型的性能。</font>
</div>


<hr>
<div id="paper5"> <b>5. Integrating Discrete and Neural Features via Mixed-feature  Trans-dimensional Random Field Language Models</b>  <a href="https://arxiv.org/pdf/2002.05967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Silin Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ou%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhijian Ou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huifang Xu</a><br>
<font size="3">
Abstract: There has been a long recognition that discrete features (n-gram features) and neural network based features have complementary strengths for language models (LMs). Improved performance can be obtained by model interpolation, which is, however, a suboptimal two-step integration of discrete and neural features. The trans-dimensional random field (TRF) framework has the potential advantage of being able to flexibly integrate a richer set of features. However, either discrete or neural features are used alone in previous TRF LMs. This paper develops a mixed-feature TRF LM and demonstrates its advantage in integrating discrete and neural features. Various LMs are trained over PTB and Google one-billion-word datasets, and evaluated in N-best list rescoring experiments for speech recognition. Among all single LMs (i.e. without model interpolation), the mixed-feature TRF LMs perform the best, improving over both discrete TRF LMs and neural TRF LMs alone, and also being significantly better than LSTM LMs. Compared to interpolating two separately trained models with discrete and neural features respectively, the performance of mixed-feature TRF LMs matches the best interpolated model, and with simplified one-step training process and reduced training time. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：已经有很长的承认，离散特征（正语法特征）和基于神经网络的特点对语言模型（LMS）的互补优势。改进的性能可通过模型内插，然而其​​是，离散和神经功能次优的两个步骤的积分而获得。反式维随机场（TRF）框架具有能够灵活地集成更丰富的功能的潜在优势。然而，无论是分立或神经功能在以前的TRF LM的单独使用。本文开发的混合特征TRF LM并演示了在整合离散和神经功能的优势。各种LM的是培养了PTB和谷歌一十亿字的数据集，并评估了N最佳列表再评分实验语音识别。在所有单个的LM（即没有模型内插），混合特征TRF的LM执行最好的，改善优于分立的TRF LMS和神经TRF的LM单独，也被显著优于LSTM的LM。相比分别内插两个可单独训练的模型具有离散和神经功能，混合特征TRF的LM的性能最好的内插模型，并用简化的一步法训练过程和减少训练时间相匹配。</font>
</div>


<hr>
<div id="paper6"> <b>6. A Data Efficient End-To-End Spoken Language Understanding Architecture</b>  <a href="https://arxiv.org/pdf/2002.05955" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dinarelli%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marco Dinarelli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kapoor%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikita Kapoor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jabaian%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bassam Jabaian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laurent Besacier</a><br>
<font size="3">
Abstract: End-to-end architectures have been recently proposed for spoken language understanding (SLU) and semantic parsing. Based on a large amount of data, those models learn jointly acoustic and linguistic-sequential features. Such architectures give very good results in the context of domain, intent and slot detection, their application in a more complex semantic chunking and tagging task is less easy. For that, in many cases, models are combined with an external language model to enhance their performance. In this paper we introduce a data efficient system which is trained end-to-end, with no additional, pre-trained external module. One key feature of our approach is an incremental training procedure where acoustic, language and semantic models are trained sequentially one after the other. The proposed model has a reasonable size and achieves competitive results with respect to state-of-the-art while using a small training dataset. In particular, we reach 24.02% Concept Error Rate (CER) on MEDIA/test while training on MEDIA/train without any additional data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结束到终端的架构最近已提出了口语理解（SLU）和语义分析。基于大量的数据，这些模型学习共同声音和语言顺序功能。这种架构给域，意图和插槽检测，他们在更复杂的语义分块的应用程序的情况下非常好的效果和标记的任务是不容易。对于这一点，在许多情况下，模型与外部的语言模型相结合，以提高它们的性能。在本文中，我们介绍该训练端至端，没有额外的，预先训练外部模块数据有效的系统。我们的做法的一个重要功能就是声学，语言和语义模型依次经过培训的一前一后的增量训练过程。该模型有一个合理的规模，并实现了相对于同时使用一个小的训练数据集的国家的最先进的具有竞争力的结果。特别是，我们达到24.02％，概念错误率上的媒体（CER）/测试，而在媒体/火车训练没有任何额外的数据。</font>
</div>


<hr>
<div id="paper7"> <b>7. Zero-Resource Cross-Domain Named Entity Recognition</b>  <a href="https://arxiv.org/pdf/2002.05923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I" target="_blank" rel="noopener" style="color:#0000EE;">Genta Indra Winata</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pascale Fung</a><br>
<font size="3">
Abstract: Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains. However, collecting data for low-resource target domains is not only expensive but also time-consuming. Hence, we propose a cross-domain NER model that does not use any external resources. We first introduce Multi-Task Learning (MTL) by adding a new objective function to detect whether tokens are named entities or not. We then introduce a framework called Mixture of Entity Experts (MoEE) to improve the robustness for zero-resource domain adaptation. Finally, experimental results show that our model outperforms strong unsupervised cross-domain sequence labeling models, and the performance of our model is close to that of the state-of-the-art model which leverages extensive resources. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨域命名实体识别（NER）现有模型依赖于大量的未标记的语料库或目标域标记NER的训练数据。然而，对于低资源目标域收集数据不仅昂贵而且耗时。因此，我们建议不使用任何外部资源跨域NER模型。我们首先通过添加一个新的目标函数，以检测是否令牌被命名为实体或不引入多任务学习（MTL）。然后，我们引入了一个名为实体专家（MoEE）的混合物来改善零资源领域适应性的鲁棒性框架。最后，实验结果表明，我们的模型优于强监督的跨域序列标注模型，我们的模型的性能接近国家的最先进的模型，利用广泛的资源。</font>
</div>


<hr>
<div id="paper8"> <b>8. Understanding patient complaint characteristics using contextual  clinical BERT embeddings</b>  <a href="https://arxiv.org/pdf/2002.05902" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saha%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Budhaditya Saha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lisboa%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanal Lisboa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shameek Ghosh</a><br>
<font size="3">
Abstract: In clinical conversational applications, extracted entities tend to capture the main subject of a patient's complaint, namely symptoms or diseases. However, they mostly fail to recognize the characterizations of a complaint such as the time, the onset, and the severity. For example, if the input is "I have a headache and it is extreme", state-of-the-art models only recognize the main symptom entity - headache, but ignore the severity factor of "extreme", that characterizes headache. In this paper, we design a two-stage approach to detect the characterizations of entities like symptoms presented by general users in contexts where they would describe their symptoms to a clinician. We use Word2Vec and BERT to encode clinical text given by the patients. We transform the output and re-frame the task as multi-label classification problem. Finally, we combine the processed encodings with the Linear Discriminant Analysis (LDA) algorithm to classify the characterizations of the main entity. Experimental results demonstrate that our method achieves 40-50% improvement on the accuracy over the state-of-the-art models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在临床应用会话，提取的实体往往抓住患者的投诉，即症状或疾病的主要议题。然而，他们大多没有认识到投诉的刻画，如时间，发病和严重程度。例如，如果输入的是“我头疼，这是极端的”，国家的最先进的机型只承认主要症状实体 - 头痛，却忽略了“极端”的严重性因素，表征头痛。在本文中，我们设计了一个两阶段的方法来检测类似的环境中一般用户呈现症状实体的表征，他们会描述自己的症状给临床医生。我们使用Word2Vec和BERT通过给予患者临床文本进行编码。我们变换输出和重制帧任务的多标签分类问题。最后，我们用线性判别分析（LDA）算法相结合的处理编码的主要实体的表征进行分类。实验结果表明，我们的方法实现对精度超过国家的最先进的机型40-50％的改善。</font>
</div>


<hr>
<div id="paper9"> <b>9. Transformers as Soft Reasoners over Language</b>  <a href="https://arxiv.org/pdf/2002.05867" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Clark</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tafjord%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oyvind Tafjord</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Richardson%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kyle Richardson</a><br>
<font size="3">
Abstract: AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but using rules expressed in language, thus bypassing a formal representation. We provide the first demonstration that this is possible, and characterize the extent of this capability. To do this, we use a collection of synthetic datasets that test increasing levels of reasoning complexity (number of rules, presence of negation, and depth of chaining). We find transformers appear to learn rule-based reasoning with high (99%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as a limited "soft theorem prover" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. All datasets and a live demo are available at this http URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：AI一直在寻求具有系统原因，目标明确* *提供的知识，更适合建设表示已被证明具有挑战性。这里，我们探讨是否变压器同样可以学习的原因（或模拟推理），但使用在语言表达的规则，从而绕过了正式代表。我们提供首次证明这是可能的，而表征这种能力的程度。要做到这一点，我们使用合成的数据集的集合测试增加复杂的推理（规则数，否定的存在，和链接的深度）的水平。我们发现变压器出现学习规则推理高（99％）的精度对这些数据集，并在推广到测试数据需要大幅更深的链接比在训练数据（95％+）分的方式。我们还表明，该模型转移阱两个手创作的规则库，并转述成更自然的语言规则库。因为它表明变压器一个新角色，即作为一个有限的“软定理证明”工作语言在明确的理论这些发现显著。这又提出了explainability，可纠，并在问题回答反推理新的可能性。所有的数据集和现场演示都可以在这个HTTP URL</font>
</div>


<hr>
<div id="paper10"> <b>10. HULK: An Energy Efficiency Benchmark Platform for Responsible Natural  Language Processing</b>  <a href="https://arxiv.org/pdf/2002.05829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiyou Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyong Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y" target="_blank" rel="noopener" style="color:#0000EE;">William Yang Wang</a><br>
<font size="3">
Abstract: Computation-intensive pretrained models have been taking the lead of many natural language processing benchmarks such as GLUE. However, energy efficiency in the process of model training and inference becomes a critical bottleneck. We introduce HULK, a multi-task energy efficiency benchmarking platform for responsible natural language processing. With HULK, we compare pretrained models' energy efficiency from the perspectives of time and cost. Baseline benchmarking results are provided for further analysis. The fine-tuning efficiency of different pretrained models can differ a lot among different tasks and fewer parameter number does not necessarily imply better efficiency. We analyzed such phenomenon and demonstrate the method of comparing the multi-task efficiency of pretrained models. Our platform is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：计算密集型的预训练模式已经采取了许多自然语言处理基准测试中领先胶水等。然而，在模型训练和推理过程中的能源效率成为一个关键瓶颈。我们介绍HULK，多任务的能源效率基准平台负责自然语言处理。随着HULK，我们比较从时间和成本的角度预训练模型的能源效率。基线基准测试结果提供了进一步的分析。不同预训练模型的微调效率不同，不同的任务和更少的参数号中有很多并不一定意味着更好的效率。我们分析了这种现象，并证明比较预先训练模式的多任务效率的方法。我们的平台可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper11"> <b>11. Unsupervised Speaker Adaptation using Attention-based Speaker Memory for  End-to-End ASR</b>  <a href="https://arxiv.org/pdf/2002.06165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Sar%C4%B1%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leda Sarı</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Moritz%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Niko Moritz</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hori%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takaaki Hori</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Roux%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Le Roux</a><br>
<font size="3">
Abstract: We propose an unsupervised speaker adaptation method inspired by the neural Turing machine for end-to-end (E2E) automatic speech recognition (ASR). The proposed model contains a memory block that holds speaker i-vectors extracted from the training data and reads relevant i-vectors from the memory through an attention mechanism. The resulting memory vector (M-vector) is concatenated to the acoustic features or to the hidden layer activations of an E2E neural network model. The E2E ASR system is based on the joint connectionist temporal classification and attention-based encoder-decoder architecture. M-vector and i-vector results are compared for inserting them at different layers of the encoder neural network using the WSJ and TED-LIUM2 ASR benchmarks. We show that M-vectors, which do not require an auxiliary speaker embedding extraction system at test time, achieve similar word error rates (WERs) compared to i-vectors for single speaker utterances and significantly lower WERs for utterances in which there are speaker changes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议由神经图灵机的端至端（E2E）的启发无监督说话人自适应方法自动语音识别（ASR）。该模型包含一个保持从训练数据中提取扬声器的i-矢量，并通过关注机构从存储器读出有关的i-矢量的存储器块。将得到的存储器向量（M-矢量）级联到声学特征或到E2E神经网络模型的隐藏层的激活。该E2E ASR系统是基于基于共同的关注联结时间分类和编码器，解码器架构。 M-矢量和i-矢量结果在使用WSJ和TED-LIUM2 ASR基准编码器神经网络的不同层将它们插入比较。我们证明了M-载体，不需要辅助扬声器在测试时嵌入提取系统，相比于我的载体为单喇叭话语和显著降低WERS的言论，其中有扬声器的变化实现类似的字错误率（WERS） 。</font>
</div>


<hr>
<div id="paper12"> <b>12. Combining Visual and Textual Features for Semantic Segmentation of  Historical Newspapers</b>  <a href="https://arxiv.org/pdf/2002.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Barman%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raphaël Barman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ehrmann%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maud Ehrmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clematide%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Clematide</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oliveira%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Sofia Ares Oliveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaplan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Frédéric Kaplan</a><br>
<font size="3">
Abstract: The massive amounts of digitized historical documents acquired over the last decades naturally lend themselves to automatic processing and exploration. Research work seeking to automatically process facsimiles and extract information thereby are multiplying with, as a first essential step, document layout analysis. If the identification and categorization of segments of interest in document images have seen significant progress over the last years thanks to deep learning techniques, many challenges remain with, among others, the use of finer-grained segmentation typologies and the consideration of complex, heterogeneous documents such as historical newspapers. Besides, most approaches consider visual features only, ignoring textual signal. In this context, we introduce a multimodal approach for the semantic segmentation of historical newspapers that combines visual and textual features. Based on a series of experiments on diachronic Swiss and Luxembourgish newspapers, we investigate, among others, the predictive power of visual and textual features and their capacity to generalize across time and sources. Results show consistent improvement of multimodal models in comparison to a strong visual baseline, as well as better robustness to high material variance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在过去的十年中取得数字化的历史记录了大量的自然借给自己自动处理和探索。研究工作寻求自动处理传真和提取信息，从而与倍增，作为第一个重要步骤，文档布局分析。如果识别和文件图像的兴趣细分的分类已经看到过去几年中由于深学习技术了显著的进步，许多挑战仍然存在，等等，使用细粒度分割类型学和考虑复杂的异构文件如历史报纸。此外，大多数的方法只考虑视觉特征，忽略文本信号。在这方面，我们引入历史报纸的语义分割，结合视觉和文本特征的多模态的方法。基于一系列关于历时瑞士和卢森堡报纸实验，调查，除其他外，视觉和文字特征的预测能力和他们的能力，以跨越时间和来源一概而论。结果表明，比较多车型的持续改进，以强烈的视觉底线，以及更好的鲁棒性高的材料差异。</font>
</div>


<hr>
<div id="paper13"> <b>13. Exploring Chemical Space using Natural Language Processing Methodologies  for Drug Discovery</b>  <a href="https://arxiv.org/pdf/2002.06053" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/q-bio?searchtype=author&query=%C3%96zt%C3%BCrk%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hakime Öztürk</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=%C3%96zg%C3%BCr%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arzucan Özgür</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Schwaller%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philippe Schwaller</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Laino%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Teodoro Laino</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Ozkirimli%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elif Ozkirimli</a><br>
<font size="3">
Abstract: Text-based representations of chemicals and proteins can be thought of as unstructured languages codified by humans to describe domain-specific knowledge. Advances in natural language processing (NLP) methodologies in the processing of spoken languages accelerated the application of NLP to elucidate hidden knowledge in textual representations of these biochemical entities and then use it to construct models to predict molecular properties or to design novel molecules. This review outlines the impact made by these advances on drug discovery and aims to further the dialogue between medicinal chemists and computer scientists. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：化学物质和蛋白质的基于文本的表示可以被认为是人类编纂非结构化的语言来描述特定领域的知识。进展自然语言处理（NLP）的方法在语言的处理加速NLP的应用，阐明这些生化实体的文本表示隐含的知识，然后用它来构建模型来预测分子性质或设计新分子。本次审查概述了对药物发现和宗旨这些进步，以推动药物化学家和计算机科学家之间的对话的影响。</font>
</div>


<hr>
<div id="paper14"> <b>14. Deep Speaker Embeddings for Far-Field Speaker Recognition on Short  Utterances</b>  <a href="https://arxiv.org/pdf/2002.06033" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gusev%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aleksei Gusev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Volokhov%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vladimir Volokhov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Andzhukaev%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tseren Andzhukaev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Novoselov%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergey Novoselov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lavrentyeva%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Galina Lavrentyeva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Volkova%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marina Volkova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gazizullina%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alice Gazizullina</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shulipa%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrey Shulipa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gorlanov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Artem Gorlanov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Avdeeva%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anastasia Avdeeva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ivanov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Artem Ivanov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kozlov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Kozlov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pekhovsky%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timur Pekhovsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Matveev%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Matveev</a><br>
<font size="3">
Abstract: Speaker recognition systems based on deep speaker embeddings have achieved significant performance in controlled conditions according to the results obtained for early NIST SRE (Speaker Recognition Evaluation) datasets. From the practical point of view, taking into account the increased interest in virtual assistants (such as Amazon Alexa, Google Home, AppleSiri, etc.), speaker verification on short utterances in uncontrolled noisy environment conditions is one of the most challenging and highly demanded tasks. This paper presents approaches aimed to achieve two goals: a) improve the quality of far-field speaker verification systems in the presence of environmental noise, reverberation and b) reduce the system qualitydegradation for short utterances. For these purposes, we considered deep neural network architectures based on TDNN (TimeDelay Neural Network) and ResNet (Residual Neural Network) blocks. We experimented with state-of-the-art embedding extractors and their training procedures. Obtained results confirm that ResNet architectures outperform the standard x-vector approach in terms of speaker verification quality for both long-duration and short-duration utterances. We also investigate the impact of speech activity detector, different scoring models, adaptation and score normalization techniques. The experimental results are presented for publicly available data and verification protocols for the VoxCeleb1, VoxCeleb2, and VOiCES datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于深扬声器的嵌入说话人识别系统已经根据早期NIST SRE（说话人识别评价）的数据集所获得的结果在控制的条件下实现显著性能。从实用的角度出发，考虑到虚拟助理（如Alexa的亚马逊，谷歌主页，AppleSiri等），在不受控制嘈杂的环境条件下短话语说话人确认的兴趣增加是一个最具挑战性和高要求任务。本文礼物办法旨在实现两个目标：1）提高远场扬声器验证系统的环境噪声，混响和b的存在质量）减少短话语系统qualitydegradation。为了这些目的，我们认为是基于TDNN（纯滞后神经网络）和RESNET（残余神经网络）块深层神经网络结构。我们尝试与国家的最先进的嵌入提取和他们的训练程序。得到的结果证实，RESNET架构要优于标准的x向量方法在两个长持续时间和短持续时间的话语的说话者验证质量方面。我们还调查语音活动检测器，不同的评分模型，适应和分数标准化技术的影响。实验结果提出了可公开获得的数据和验证协议的VoxCeleb1，VoxCeleb2和声音的数据集。</font>
</div>


<hr>
<div id="paper15"> <b>15. Query2box: Reasoning over Knowledge Graphs in Vector Space using Box  Embeddings</b>  <a href="https://arxiv.org/pdf/2002.05969" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongyu Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weihua Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leskovec%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jure Leskovec</a><br>
<font size="3">
Abstract: Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\wedge$) and existential quantifiers ($\exists$). Handling queries with logical disjunctions ($\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\wedge$, $\vee$, and $\exists$ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with $\wedge$, $\vee$, $\exists$ in a scalable manner. We demonstrate the effectiveness of query2box on three large KGs and show that query2box achieves up to 25% relative improvement over the state of the art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型不全知识图（KGS）回答复杂的逻辑查询是一项基本而具有挑战性的任务。近日，有前途的方法这个问题已经嵌入KG实体以及查询到向量空间，使得回答查询实体嵌入接近查询。然而，以前的工作模式查询，在向量空间，这是问题，因为一个复杂的查询表示一个潜在的大集的答案实体，但目前还不清楚一套怎么这么可以表示为一个单点单点。此外，以前的工作只能处理的查询使用连词（$ \ $楔）和存在量词（$ \ $存在）。处理查询与逻辑或（$ \ $ V型）仍然是一个悬而未决的问题。在这里我们建议query2box，与$ \ $楔形，$ \ $ V型推理在任意查询基于嵌入的框架和$ \大规模和不完整的幼儿园存在$运营商。我们的主要观点是，查询可以嵌入为框（即超矩形），其中一组框对应内部点的一组查询的答案实体。我们表明，连词可以自然地表示为方框交叉，也证明了一个否定结果处理析取需要与尺寸比例嵌入到KG实体的数量。然而，我们表明，通过将查询到的析取范式，query2box能够处理任意逻辑查询与$ \ $楔形，$ \ $ V型，$ \以可扩展的方式存在$。我们证明query2box的三个大型幼儿园的有效性，并表明query2box在技术状态具有高达25％的相对改善。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>screen后台运行进程</title>
    <url>/2020/02/16/screen%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<p>我们常常需要将进程挂在后台运行，防止因关闭窗口或断开连接导致进程被杀掉。screen可以实现进程与当前窗口分离，即使断开连接了，进行仍可以继续运行；并且当我们重新连接后，仍可读取当前进程。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install screen</span><br></pre></td></tr></table></figure><h1 id="新建窗口"><a href="#新建窗口" class="headerlink" title="新建窗口"></a>新建窗口</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 方法一</span></span><br><span class="line">screen # 新建一个无名窗口，断开连接后仍可以后台运行，但是无法重新连接</span><br><span class="line"><span class="meta">#</span><span class="bash"> 方法二</span></span><br><span class="line">screen -S &lt;screen_name&gt; # 新建一个窗口并进入该窗口</span><br></pre></td></tr></table></figure><a id="more"></a>




<h1 id="运行后台程序"><a href="#运行后台程序" class="headerlink" title="运行后台程序"></a>运行后台程序</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">screen &lt;your_command&gt; # 在无名窗口执行命令 &lt;your_command&gt;</span><br></pre></td></tr></table></figure>
<p>或者在新建<code>&lt;screen_name&gt;</code>窗口后，直接运行相应程序就好</p>
<h1 id="会话分离"><a href="#会话分离" class="headerlink" title="会话分离"></a>会话分离</h1><p>退出该screen，让进程在后台运行，按住快捷键<strong><em>Ctrl + A + D</em></strong></p>
<h1 id="查看所有窗口"><a href="#查看所有窗口" class="headerlink" title="查看所有窗口"></a>查看所有窗口</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">screen -ls</span><br></pre></td></tr></table></figure>
<h1 id="恢复窗口"><a href="#恢复窗口" class="headerlink" title="恢复窗口"></a>恢复窗口</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 方法一</span></span><br><span class="line">screen -r &lt;PID_to_screen&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 方法二</span></span><br><span class="line">screen -r &lt;screen_name&gt;</span><br></pre></td></tr></table></figure>
<h1 id="杀死会话"><a href="#杀死会话" class="headerlink" title="杀死会话"></a>杀死会话</h1><h2 id="杀死当前会话窗口"><a href="#杀死当前会话窗口" class="headerlink" title="杀死当前会话窗口"></a>杀死当前会话窗口</h2><p>按住快捷键<strong><em>Ctrl + A + K</em></strong></p>
<h2 id="杀死指定会话窗口"><a href="#杀死指定会话窗口" class="headerlink" title="杀死指定会话窗口"></a>杀死指定会话窗口</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kill -9 &lt;PID_to_screen&gt;</span><br></pre></td></tr></table></figure>
<h1 id="清除僵尸窗口"><a href="#清除僵尸窗口" class="headerlink" title="清除僵尸窗口"></a>清除僵尸窗口</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">screen -wipe</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>screen</tag>
        <tag>后台</tag>
      </tags>
  </entry>
  <entry>
    <title>tgz文件压缩&解压</title>
    <url>/2020/02/15/tgz%E6%96%87%E4%BB%B6%E5%8E%8B%E7%BC%A9-%E8%A7%A3%E5%8E%8B/</url>
    <content><![CDATA[<h1 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h1><h2 id="压缩指定文件夹"><a href="#压缩指定文件夹" class="headerlink" title="压缩指定文件夹"></a>压缩指定文件夹</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar zcvf &lt;filename&gt;.tgz &lt;path_to_dir&gt;</span><br></pre></td></tr></table></figure><h1 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h1><h2 id="解压到当前文件夹"><a href="#解压到当前文件夹" class="headerlink" title="解压到当前文件夹"></a>解压到当前文件夹</h2><h3 id="保留原始压缩文件"><a href="#保留原始压缩文件" class="headerlink" title="保留原始压缩文件"></a>保留原始压缩文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar zxvf &lt;filename&gt;.tgz -C .</span><br></pre></td></tr></table></figure><h2 id="解压到指定文件夹"><a href="#解压到指定文件夹" class="headerlink" title="解压到指定文件夹"></a>解压到指定文件夹</h2><h3 id="保留原始压缩文件-1"><a href="#保留原始压缩文件-1" class="headerlink" title="保留原始压缩文件"></a>保留原始压缩文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar zxvf &lt;filename&gt;.tgz -C &lt;path_to_dir&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>tgz</tag>
        <tag>压缩</tag>
        <tag>解压</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-14</title>
    <url>/2020/02/15/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-14/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Looking Enhances Listening: Recovering Missing Speech Using Images <a href="https://arxiv.org/pdf/2002.05639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Pre-Training for Query Rewriting in A Spoken Language Understanding  System <a href="https://arxiv.org/pdf/2002.05607" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Sentiment Analysis Using Averaged Weighted Word Vector Features <a href="https://arxiv.org/pdf/2002.05606" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Sparse and Structured Visual Attention <a href="https://arxiv.org/pdf/2002.05556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Unsupervised Separation of Native and Loanwords for Malayalam and Telugu <a href="https://arxiv.org/pdf/2002.05527" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Comparison of Turkish Word Representations Trained on Different  Morphological Forms <a href="https://arxiv.org/pdf/2002.05417" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Keyphrase Extraction with Span-based Feature Representations <a href="https://arxiv.org/pdf/2002.05407" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Exploiting the Matching Information in the Support Set for Few Shot  Event Classification <a href="https://arxiv.org/pdf/2002.05295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> What Would You Ask the Machine Learning Model? Identification of User  Needs for Model Explanations Based on Human-Model Conversations <a href="https://arxiv.org/pdf/2002.05674" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Image-to-Image Translation with Text Guidance <a href="https://arxiv.org/pdf/2002.05235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Deep compositional robotic planners that follow natural language  commands <a href="https://arxiv.org/pdf/2002.05201" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> A Combined Stochastic and Physical Framework for Modeling Indoor 5G  Millimeter Wave Propagation <a href="https://arxiv.org/pdf/2002.05162" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Looking Enhances Listening: Recovering Missing Speech Using Images</b>  <a href="https://arxiv.org/pdf/2002.05639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tejas Srinivasan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sanabria%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ramon Sanabria</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Metze</a><br>
<font size="3">
Abstract: Speech is understood better by using visual context; for this reason, there have been many attempts to use images to adapt automatic speech recognition (ASR) systems. Current work, however, has shown that visually adapted ASR models only use images as a regularization signal, while completely ignoring their semantic content. In this paper, we present a set of experiments where we show the utility of the visual modality under noisy conditions. Our results show that multimodal ASR models can recover words which are masked in the input acoustic signal, by grounding its transcriptions using the visual representations. We observe that integrating visual context can result in up to 35% relative improvement in masked word recovery. These results demonstrate that end-to-end multimodal ASR systems can become more robust to noise by leveraging the visual context. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：言语理解通过视觉环境更好;因为这个原因，已经有许多尝试使用图片来适应自动语音识别（ASR）系统。目前的工作，但是，已经表明，在视觉上适应ASR机型只能使用图片作为正规化信号，而全然不顾自己的语义内容。在本文中，我们提出了一套，我们展示的视觉方式的噪声条件下的实用试验。我们的研究结果表明，多ASR模式可以恢复被掩盖在输入声音信号，通过使用可视化表示接地其改编的话。我们观察到，整合的视觉环境可以导致高达屏蔽字恢复35％的相对改善。这些结果表明，端至端多峰ASR系统可通过利用可视上下文成为噪声更为鲁棒。</font>
</div>


<hr>
<div id="paper2"> <b>2. Pre-Training for Query Rewriting in A Spoken Language Understanding  System</b>  <a href="https://arxiv.org/pdf/2002.05607" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xing Fan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Ling</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mathias%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lambert Mathias</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenlei Guo</a><br>
<font size="3">
Abstract: Query rewriting (QR) is an increasingly important technique to reduce customer friction caused by errors in a spoken language understanding pipeline, where the errors originate from various sources such as speech recognition errors, language understanding errors or entity resolution errors. In this work, we first propose a neural-retrieval based approach for query rewriting. Then, inspired by the wide success of pre-trained contextual language embeddings, and also as a way to compensate for insufficient QR training data, we propose a language-modeling (LM) based approach to pre-train query embeddings on historical user conversation data with a voice assistant. In addition, we propose to use the NLU hypotheses generated by the language understanding system to augment the pre-training. Our experiments show pre-training provides rich prior information and help the QR task achieve strong performance. We also show joint pre-training with NLU hypotheses has further benefit. Finally, after pre-training, we find a small set of rewrite pairs is enough to fine-tune the QR model to outperform a strong baseline by full training on all QR training data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：查询重写（QR）是减少在口语理解管道，其中的误差来自各种来源，如语音识别错误，语言理解错误或实体解析错误源于错误造成客户的摩擦日益重要的技术。在这项工作中，我们首先提出了查询重写神经检索基础的方法。然后，通过预先训练情境语言的嵌入的广泛成功的启发，并以此来弥补不足QR训练数据，我们提出了一个语言模型（LM）为基础的方法预火车上的历史用户会话数据查询的嵌入与语音助手。此外，我们建议使用通过了解系统，以加强前培训语言产生的NLU假设。我们的实验显示前培训提供了丰富的先验信息和帮助的QR任务实现强劲性能。我们还表明联合前培训NLU假设有另一个好处。最后，经过岗前培训，我们发现了一个小套重写对足以微调QR模型通过对所有QR训练数据全员培训跑赢强大的基线。</font>
</div>


<hr>
<div id="paper3"> <b>3. Sentiment Analysis Using Averaged Weighted Word Vector Features</b>  <a href="https://arxiv.org/pdf/2002.05606" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Erkan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Erkan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gungor%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tunga Gungor</a><br>
<font size="3">
Abstract: People use the world wide web heavily to share their experience with entities such as products, services, or travel destinations. Texts that provide online feedback in the form of reviews and comments are essential to make consumer decisions. These comments create a valuable source that may be used to measure satisfaction related to products or services. Sentiment analysis is the task of identifying opinions expressed in such text fragments. In this work, we develop two methods that combine different types of word vectors to learn and estimate polarity of reviews. We develop average review vectors from word vectors and add weights to this review vectors using word frequencies in positive and negative sensitivity-tagged reviews. We applied the methods to several datasets from different domains that are used as standard benchmarks for sentiment analysis. We ensemble the techniques with each other and existing methods, and we make a comparison with the approaches in the literature. The results show that the performances of our approaches outperform the state-of-the-art success rates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人们使用万维网巨资分享他们的实体，如产品，服务或旅游目的地体验。提供的评论和意见的形式在线反馈文本是必须要做出决定消费。这些意见创建可用于测量有关的产品或服务的满意度的重要来源。情感分析是识别这样的文本片段表达意见的任务。在这项工作中，我们开发了两个方法，结合不同类型的词矢量的学习和评估审查的极性。我们开发从词矢量的平均评价载体，并添加砝码使用在正面和负面的敏感性标记评论词频本次审查的载体。我们使用的方法从不同的域中的多个数据集，它们作为标准的基准情感分析。我们合奏彼此之间以及现有方法的技术，和我们做与对比文献的方法。结果表明，我们的方法的性能优于国家的最先进的成功率。</font>
</div>


<hr>
<div id="paper4"> <b>4. Sparse and Structured Visual Attention</b>  <a href="https://arxiv.org/pdf/2002.05556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P+H" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Henrique Martins</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niculae%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vlad Niculae</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marinho%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zita Marinho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">André Martins</a><br>
<font size="3">
Abstract: Visual attention mechanisms are widely used in multimodal tasks, such as image captioning and visual question answering (VQA). One drawback of softmax-based attention mechanisms is that they assign probability mass to all image regions, regardless of their adjacency structure and of their relevance to the text. In this paper, to better link the image structure with the text, we replace the traditional softmax attention mechanism with two alternative sparsity-promoting transformations: sparsemax, which is able to select the relevant regions only (assigning zero weight to the rest), and a newly proposed Total-Variation Sparse Attention (TVmax), which further encourages the joint selection of adjacent spatial locations. Experiments in image captioning and VQA, using both LSTM and Transformer architectures, show gains in terms of human-rated caption quality, attention relevance, and VQA accuracy, with improved interpretability. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉注意机制被广泛应用于多任务，如图像字幕和视觉问答（VQA）。基于SOFTMAX注意力机制的一个缺点是它们分配概率质量到所有的图像区域，无论其邻接结构及其相关的文字。在本文中，以更好地链接与文本的图像结构，我们更换两种可供选择的稀疏性，促进转变传统的SOFTMAX注意机制：sparsemax，这是能够选择相关区域中仅仅（分配权重为零的其余部分），和新提出的总的变化率稀疏注意（TVmax），其进一步鼓励相邻的空间位置的联合选择。在图像字幕和VQA，同时使用LSTM和变压器的架构实验，显示人类额定字幕质量，重视相关性，准确性VQA，具有完善的可解释性方面的收益。</font>
</div>


<hr>
<div id="paper5"> <b>5. Unsupervised Separation of Native and Loanwords for Malayalam and Telugu</b>  <a href="https://arxiv.org/pdf/2002.05527" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Prakhya%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sridhama Prakhya</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deepak P</a><br>
<font size="3">
Abstract: Quite often, words from one language are adopted within a different language without translation; these words appear in transliterated form in text written in the latter language. This phenomenon is particularly widespread within Indian languages where many words are loaned from English. In this paper, we address the task of identifying loanwords automatically and in an unsupervised manner, from large datasets of words from agglutinative Dravidian languages. We target two specific languages from the Dravidian family, viz., Malayalam and Telugu. Based on familiarity with the languages, we outline an observation that native words in both these languages tend to be characterized by a much more versatile stem - stem being a shorthand to denote the subword sequence formed by the first few characters of the word - than words that are loaned from other languages. We harness this observation to build an objective function and an iterative optimization formulation to optimize for it, yielding a scoring of each word's nativeness in the process. Through an extensive empirical analysis over real-world datasets from both Malayalam and Telugu, we illustrate the effectiveness of our method in quantifying nativeness effectively over available baselines for the task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：很多时候，从一种语言单词不用翻译不同语言中通过;这些词出现在写在后面的语言文本音译形式。这种现象是许多词是从英语借给印度语中特别普遍。在本文中，我们要解决自动在无人监督的方式识别外来词，从粘着达罗毗荼语系的单词大型数据集的任务。我们的目标从德拉威家庭两个特定的语言，即，马拉雅拉姆语和泰卢固语。根据与语言的熟悉程度，我们从整体上观察，在这两种语言的本地话往往被表征一个更通用的干 - 干是表示由单词的前几个字符构成的子字序列的速记 - 比的话从其他语言贷款。我们利用这些观测建立一个目标函数和迭代优化配方，以优化它，产生过程中的每个字的本土性的进球。通过以上来自马来亚和泰卢固语真实世界的数据集丰富的实证分析，说明我们在全球为任务提供基准有效量化本土化方法的有效性。</font>
</div>


<hr>
<div id="paper6"> <b>6. Comparison of Turkish Word Representations Trained on Different  Morphological Forms</b>  <a href="https://arxiv.org/pdf/2002.05417" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%BCler%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gökhan Güler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tantu%C4%9F%2C+A+C" target="_blank" rel="noopener" style="color:#0000EE;">A. Cüneyd Tantuğ</a><br>
<font size="3">
Abstract: Increased popularity of different text representations has also brought many improvements in Natural Language Processing (NLP) tasks. Without need of supervised data, embeddings trained on large corpora provide us meaningful relations to be used on different NLP tasks. Even though training these vectors is relatively easy with recent methods, information gained from the data heavily depends on the structure of the corpus language. Since the popularly researched languages have a similar morphological structure, problems occurring for morphologically rich languages are mainly disregarded in studies. For morphologically rich languages, context-free word vectors ignore morphological structure of languages. In this study, we prepared texts in morphologically different forms in a morphologically rich language, Turkish, and compared the results on different intrinsic and extrinsic tasks. To see the effect of morphological structure, we trained word2vec model on texts which lemma and suffixes are treated differently. We also trained subword model fastText and compared the embeddings on word analogy, text classification, sentimental analysis, and language model tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：增加不同的文字表述的人气也自然语言处理（NLP）任务带来了许多改进。而不需要监督的数据，培训了大量语料的嵌入提供我们要在不同的NLP任务时使用有意义的关系。虽然训练这些载体是相对容易与最近的方法，从数据中获得的信息很大程度上取决于语料库的语言结构。由于普遍研究的语言也有类似的形态结构，发生了形态丰富的语言主要是忽略在研究的问题。对于形态丰富的语言，上下文词矢量忽视的语言形态结构。在这项研究中，我们准备了形态不同形式的文本在形态丰富的语言，土耳其语和比较不同的内在和外在的任务的结果。看形态结构的影响，我们训练上引理和后缀区别对待文本word2vec模型。我们还培养了子字模型fastText和比较了字类比，文本分类，感性的分析和语言模型任务的嵌入。</font>
</div>


<hr>
<div id="paper7"> <b>7. Keyphrase Extraction with Span-based Feature Representations</b>  <a href="https://arxiv.org/pdf/2002.05407" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Funan Mu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenting Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">LiFeng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yequan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingyu Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yibo Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liqun Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Teng Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xing Zhou</a><br>
<font size="3">
Abstract: Keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. Since keyphrase extraction is able to facilitate the management, categorization, and retrieval of information, it has received much attention in recent years. There are three approaches to address keyphrase extraction: (i) traditional two-step ranking method, (ii) sequence labeling and (iii) generation using neural networks. Two-step ranking approach is based on feature engineering, which is labor intensive and domain dependent. Sequence labeling is not able to tackle overlapping phrases. Generation methods (i.e., Sequence-to-sequence neural network models) overcome those shortcomings, so they have been widely studied and gain state-of-the-art performance. However, generation methods can not utilize context information effectively. In this paper, we propose a novelty Span Keyphrase Extraction model that extracts span-based feature representation of keyphrase directly from all the content tokens. In this way, our model obtains representation for each keyphrase and further learns to capture the interaction between keyphrases in one document to get better ranking results. In addition, with the help of tokens, our model is able to extract overlapped keyphrases. Experimental results on the benchmark datasets show that our proposed model outperforms the existing methods by a large margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：关键字句能够提供语义元数据特征文件和产生文件的内容的概述的。由于关键词的提取是能够方便管理，分类和检索的信息，它受到很多关注在最近几年。有三种方法来解决的关键词提取：（ⅰ）传统的两步骤排序方法，（ⅱ）序列标签和（iii）使用神经网络的产生。两步排序方法是基于特征的工程，这是劳动密集和域依赖。序列标注是不能够解决重叠短语。产生方法（即，序列到序列神经网络模型）克服这些缺点，所以它们已经被广泛地研究和国家的最先进的增益性能。然而，代方法不能有效地利用上下文信息。在本文中，我们提出了一个新颖的跨度的关键词提取模型，提取跨度基于关键词短语特征表示直接从所有内容令牌。这样一来，我们的模型获得每个关键词的进一步获悉代表性捕捉关键短语之间的相互作用一个文档中获得更好的排名结果。此外，与令牌的帮助下，我们的模型是能够提取重叠的关键字句。基准的数据集实验结果表明，该模型优于大幅度现有的方法。</font>
</div>


<hr>
<div id="paper8"> <b>8. Exploiting the Matching Information in the Support Set for Few Shot  Event Classification</b>  <a href="https://arxiv.org/pdf/2002.05295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+V+D" target="_blank" rel="noopener" style="color:#0000EE;">Viet Dac Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dernoncourt%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Franck Dernoncourt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+H" target="_blank" rel="noopener" style="color:#0000EE;">Thien Huu Nguyen</a><br>
<font size="3">
Abstract: The existing event classification (EC) work primarily focuseson the traditional supervised learning setting in which models are unableto extract event mentions of new/unseen event types. Few-shot learninghas not been investigated in this area although it enables EC models toextend their operation to unobserved event types. To fill in this gap, inthis work, we investigate event classification under the few-shot learningsetting. We propose a novel training method for this problem that exten-sively exploit the support set during the training process of a few-shotlearning model. In particular, in addition to matching the query exam-ple with those in the support set for training, we seek to further matchthe examples within the support set themselves. This method providesmore training signals for the models and can be applied to every metric-learning-based few-shot learning methods. Our extensive experiments ontwo benchmark EC datasets show that the proposed method can improvethe best reported few-shot learning models by up to 10% on accuracyfor event classification </font>
<br>
<font size="2" style="line-height:30px;">
摘要：新的/看不见的事件类型的现有的事件分类（EC）的工作主要focuseson传统的监督式学习环境中，模型unableto提取物事件中提到。很少拍learninghas没有在这方面进行了研究，虽然它使EC车型toextend其操作未观察到的事件类型。为了填补这一空白，inthis工作中，我们很少拍learningsetting下调查事件分类。我们提出这个问题的新的训练方法EXTEN-sively开发过程中的几个-shotlearning模型的训练过程中的支集。特别是，除了查询考试-PLE与那些在训练支持组匹配，我们寻求进一步小组赛的例子中支持自己设定。为模特这个方法providesmore训练信号，并且可以应用于所有的基于度量学习几拍的学习方法。我们广泛的实验ontwo基准EC数据集表明，该方法可以improvethe最好的报道很少拍学习高达模型10％accuracyfor事件分类</font>
</div>


<hr>
<div id="paper9"> <b>9. What Would You Ask the Machine Learning Model? Identification of User  Needs for Model Explanations Based on Human-Model Conversations</b>  <a href="https://arxiv.org/pdf/2002.05674" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ku%C5%BAba%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michał Kuźba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Biecek%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Przemysław Biecek</a><br>
<font size="3">
Abstract: Recently we see a rising number of methods in the field of eXplainable Artificial Intelligence. To our surprise, their development is driven by model developers rather than a study of needs for human end users. To answer the question "What would a human operator like to ask the ML model?" we propose a conversational system explaining decisions of the predictive model. In this experiment, we implement a chatbot called dr_ant and train a model predicting survival odds on Titanic. People can talk to dr_ant about the model to understand the rationale behind its predictions. Having collected a corpus of 1000+ dialogues, we analyse the most common types of questions that users would like to ask. To our knowledge, it is the first study of needs for human operators in the context of conversations with an ML model. It is also a first study which uses a conversational system for interactive exploration of a predictive model trained on tabular data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近，我们看到了越来越多的在解释的人工智能领域的方法。令我们惊讶的是，他们的发展是由开发商模式，而不是对人类最终用户需求的研究驱动。要回答这个问题：“什么想人类操作员问ML模式？”我们提出了一个对讲系统解释预测模型的决定。在这个实验中，我们实施了一个名为dr_ant聊天机器人和培养模式上的泰坦尼克号预测生存概率。人们可以跟dr_ant有关的模型，以了解它的预测背后的基本原理。在收集的1000多个对话语料库，我们分析了最常见的问题是用户想请教。据我们所知，它是在与ML模型对话的背景下人工操作需要先学习。它也是使用对话系统，训练有素的表格数据的预测模型的互动探索第一个研究。</font>
</div>


<hr>
<div id="paper10"> <b>10. Image-to-Image Translation with Text Guidance</b>  <a href="https://arxiv.org/pdf/2002.05235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bowen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojuan Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torr%2C+P+H+S" target="_blank" rel="noopener" style="color:#0000EE;">Philip H. S. Torr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lukasiewicz%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Lukasiewicz</a><br>
<font size="3">
Abstract: The goal of this paper is to embed controllable factors, i.e., natural language descriptions, into image-to-image translation with generative adversarial networks, which allows text descriptions to determine the visual attributes of synthetic images. We propose four key components: (1) the implementation of part-of-speech tagging to filter out non-semantic words in the given description, (2) the adoption of an affine combination module to effectively fuse different modality text and image features, (3) a novel refined multi-stage architecture to strengthen the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to further improve discriminators to better distinguish real and synthetic images. Extensive experiments on the COCO dataset demonstrate that our method has a superior performance on both visual realism and semantic consistency with given descriptions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文的目的是嵌入可控因素，即，自然语言描述成图像到图像的平移与生成对抗性的网络，它允许文本描述，以确定合成图像的视觉属性。我们提出四个主要组成部分：（1）部分的词性标注的实施，以过滤出在给定的描述非语义字，（2）通过仿射组合模块的有效熔丝不同模态的文本和图像的特征， （3）一种新的改进的多级结构，以加强鉴别器的差动能力和发电机的整流能力，和（4）的新结构的损失，进一步提高鉴别器，以更好地分辨实际的和合成的图像。在COCO大量的实验数据集表明，我们的方法有两个逼真视觉效果，并与给定的描述语义一致性优越的性能。</font>
</div>


<hr>
<div id="paper11"> <b>11. Deep compositional robotic planners that follow natural language  commands</b>  <a href="https://arxiv.org/pdf/2002.05201" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yen-Ling Kuo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Katz%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Boris Katz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barbu%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrei Barbu</a><br>
<font size="3">
Abstract: We demonstrate how a sampling-based robotic planner can be augmented to learn to understand a sequence of natural language commands in a continuous configuration space to move and manipulate objects. Our approach combines a deep network structured according to the parse of a complex command that includes objects, verbs, spatial relations, and attributes, with a sampling-based planner, RRT. A recurrent hierarchical deep network controls how the planner explores the environment, determines when a planned path is likely to achieve a goal, and estimates the confidence of each move to trade off exploitation and exploration between the network and the planner. Planners are designed to have near-optimal behavior when information about the task is missing, while networks learn to exploit observations which are available from the environment, making the two naturally complementary. Combining the two enables generalization to new maps, new kinds of obstacles, and more complex sentences that do not occur in the training set. Little data is required to train the model despite it jointly acquiring a CNN that extracts features from the environment as it learns the meanings of words. The model provides a level of interpretability through the use of attention maps allowing users to see its reasoning steps despite being an end-to-end model. This end-to-end model allows robots to learn to follow natural language commands in challenging continuous environments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们展示了一个基于采样的机器人计划者可以如何增强学习理解自然语言指令在连续配置空间的顺序移动和操作物体。我们的方法结合了根据一个复杂的命令，其中包括对象，动词，空间关系和属性，具有基于采样的规划师，RRT的解析构成的深网络。规划者如何探索环境的反复出现的深层次的网络控制，确定何时有计划的路径是有可能实现一个目标，并估计每一个举动权衡网络和规划者之间的开采和勘探的信心。规划者被设计成具有当有关任务的信息丢失接近最优的行为，而网络学习利用观察其可从环境，使两个自然补充。两者结合能够推广到不训练集中出现新的地图，新的各种障碍，和更复杂的句子。小数据需要火车模型，尽管它共同取得CNN说，从提取的环境特征，因为它学习单词的含义。该模型提供通过使用注意地图让用户看到它的推理步骤，尽管是一个终端到高端机型的一个解释性的水平。这端至端模型允许机器人学会跟随在挑战不断的环境中的自然语言命令。</font>
</div>


<hr>
<div id="paper12"> <b>12. A Combined Stochastic and Physical Framework for Modeling Indoor 5G  Millimeter Wave Propagation</b>  <a href="https://arxiv.org/pdf/2002.05162" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nassif%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Georges Nassif</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gloaguen%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Catherine Gloaguen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philippe Martins</a><br>
<font size="3">
Abstract: Indoor coverage is a major challenge for 5G millimeter waves (mmWaves). In this paper, we address this problem through a novel theoretical framework that combines stochastic indoor environment modeling with advanced physical propagation simulation. This approach is particularly adapted to investigate indoor-to-indoor 5G mmWave propagation. Its system implementation, so-called iGeoStat, generates parameterized typical environments that account for the indoor spatial variations, then simulates radio propagation based on the physical interaction between electromagnetic waves and material properties. This framework is not dedicated to a particular environment, material, frequency or use case and aims to statistically understand the influence of indoor environment parameters on mmWave propagation properties, especially coverage and path loss. Its implementation raises numerous computational challenges that we solve by formulating an adapted link budget and designing new memory optimization algorithms. The first simulation results for two major 5G applications are validated with measurement data and show the efficiency of iGeoStat to simulate multiple diffusion in realistic environments, within a reasonable amount of time and memory resources. Generated output maps confirm that diffusion has a critical impact on indoor mmWave propagation and that proper physical modeling is of the utmost importance to generate relevant propagation models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：室内覆盖是5G毫米波（mmWaves）的一个重大挑战。在本文中，我们通过一个新的理论框架，解决这个问题，结合随机与先进的物理传播模拟室内环境建模。这种方法特别适合于调查室内至室内5G毫米波传播。其系统实施，所谓iGeoStat，生成参数即占室内空间变化典型的环境中，然后模拟基于电磁波和材料性能之间的物理相互作用的无线电传播。该框架不专用于特定的环境中，物质，频率或使用情况，其目的在于统计学理解对毫米波传输特性，特别是覆盖和路径损耗的室​​内环境参数的影响。它的实施，提高了我们通过制定适当链路预算和设计新的内存优化算法，解决了许多计算挑战。第一仿真结果两大5G应用程序验证用的测量数据，并显示iGeoStat的效率，以模拟真实的环境的多个扩散，时间和内存资源的合理量内。生成的输出贴图证实，扩散，对室内毫米波传播和适当的物理建模是极为重要的，以生成相关的传播模型具有关键性的影响。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-14</title>
    <url>/2020/02/15/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-14/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Automatically Discovering and Learning New Visual Categories with  Ranking Statistics <a href="https://arxiv.org/pdf/2002.05714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Classifying the classifier: dissecting the weight space of neural  networks <a href="https://arxiv.org/pdf/2002.05688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Summarizing the performances of a background subtraction algorithm  measured on several videos <a href="https://arxiv.org/pdf/2002.05654" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> GANILLA: Generative Adversarial Networks for Image to Illustration  Translation <a href="https://arxiv.org/pdf/2002.05638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Asynchronous Tracking-by-Detection on Adaptive Time Surfaces for  Event-based Object Tracking <a href="https://arxiv.org/pdf/2002.05583" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> SpotNet: Self-Attention Multi-Task Network for Object Detection <a href="https://arxiv.org/pdf/2002.05540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Replacing Mobile Camera ISP with a Single Deep Learning Model <a href="https://arxiv.org/pdf/2002.05509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Chaotic Phase Synchronization and Desynchronization in an Oscillator  Network for Object Selection <a href="https://arxiv.org/pdf/2002.05493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> EndoL2H: Deep Super-Resolution for Capsule Endoscopy <a href="https://arxiv.org/pdf/2002.05459" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Emotion Recognition for In-the-wild Videos <a href="https://arxiv.org/pdf/2002.05447" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Recurrent Attention Model with Log-Polar Mapping is Robust against  Adversarial Attacks <a href="https://arxiv.org/pdf/2002.05388" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Hypergraph Optimization for Multi-structural Geometric Model Fitting <a href="https://arxiv.org/pdf/2002.05350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Object Detection on Single Monocular Images through Canonical  Correlation Analysis <a href="https://arxiv.org/pdf/2002.05349" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Continual Universal Object Detection <a href="https://arxiv.org/pdf/2002.05347" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D  Vehicle Detection from Point Cloud <a href="https://arxiv.org/pdf/2002.05316" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Improving Efficiency in Neural Network Accelerator Using Operands  Hamming Distance optimization <a href="https://arxiv.org/pdf/2002.05293" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Solving Missing-Annotation Object Detection with Background  Recalibration Loss <a href="https://arxiv.org/pdf/2002.05274" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Leveraging Affect Transfer Learning for Behavior Prediction in an  Intelligent Tutoring System <a href="https://arxiv.org/pdf/2002.05242" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Image-to-Image Translation with Text Guidance <a href="https://arxiv.org/pdf/2002.05235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Cross-Iteration Batch Normalization <a href="https://arxiv.org/pdf/2002.05712" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> A Simple Framework for Contrastive Learning of Visual Representations <a href="https://arxiv.org/pdf/2002.05709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Generative-based Airway and Vessel Morphology Quantification on Chest CT  Images <a href="https://arxiv.org/pdf/2002.05702" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Neuromorphologicaly-preserving Volumetric data encoding using VQ-VAE <a href="https://arxiv.org/pdf/2002.05692" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains <a href="https://arxiv.org/pdf/2002.05665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Machines Learn Appearance Bias in Face Recognition <a href="https://arxiv.org/pdf/2002.05636" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Sparse and Structured Visual Attention <a href="https://arxiv.org/pdf/2002.05556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Superpixel Image Classification with Graph Attention Networks <a href="https://arxiv.org/pdf/2002.05544" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Deep Learning-based End-to-end Diagnosis System for Avascular Necrosis  of Femoral Head <a href="https://arxiv.org/pdf/2002.05536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Abnormal respiratory patterns classifier may contribute to large-scale  screening of people infected with COVID-19 in an accurate and unobtrusive  manner <a href="https://arxiv.org/pdf/2002.05534" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Real or Not Real, that is the Question <a href="https://arxiv.org/pdf/2002.05512" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> MLFcGAN: Multi-level Feature Fusion based Conditional GAN for Underwater  Image Color Correction <a href="https://arxiv.org/pdf/2002.05333" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral  Segmentation of Rock micro-CT Images <a href="https://arxiv.org/pdf/2002.05322" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> A Provably Robust Multiple Rotation Averaging Scheme for SO(2) <a href="https://arxiv.org/pdf/2002.05299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Geom-GCN: Geometric Graph Convolutional Networks <a href="https://arxiv.org/pdf/2002.05287" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine  Learning Models <a href="https://arxiv.org/pdf/2002.05271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Graph Similarity Using PageRank and Persistent Homology <a href="https://arxiv.org/pdf/2002.05158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Automatically Discovering and Learning New Visual Categories with  Ranking Statistics</b>  <a href="https://arxiv.org/pdf/2002.05714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rebuffi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sylvestre-Alvise Rebuffi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ehrhardt%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastien Ehrhardt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vedaldi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Vedaldi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zisserman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Zisserman</a><br>
<font size="3">
Abstract: We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. This setting is similar to semi-supervised learning, but significantly harder because there are no labelled examples for the new classes. The challenge, then, is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. In this work we address this problem by combining three ideas: (1) we suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) we use rank statistics to transfer the model's knowledge of the labelled classes to the problem of clustering the unlabelled images; and, (3) we train the data representation by optimizing a joint objective function on the labelled and unlabelled subsets of the data, improving both the supervised classification of the labelled data, and the clustering of the unlabelled data. We evaluate our approach on standard classification benchmarks and outperform current methods for novel category discovery by a significant margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们给出解决其他类的标识样本的图像集合中发现新类的问题。因为有新的类没有标识样本此设置类似于半监督学习，而是显著更难。我们面临的挑战，那么，是利用以学习通用的集群模式，并使用后者来识别未标记的数据的新类包含在标记图像的信息。在这项工作中，我们结合三个想法解决这个问题：（1）我们建议使用标记的数据自举图像表示的共同方法只引入了不必要的偏见，而这可以通过自我监督学习到火车避免从标记的和未标记的数据的联合划伤表示; （2）我们使用排名统计标记的类模型的知识传递给聚类未标记的图像的问题;和，（3），我们通过优化上的数据的标记和未标记的子集的联合目标函数，提高了标记的数据的两个监督分类，和未标记数据的聚类训练数据表示。我们评估的标准分类的基准方法，并超越由显著裕新类别发现目前的方法。</font>
</div>


<hr>
<div id="paper2"> <b>2. Classifying the classifier: dissecting the weight space of neural  networks</b>  <a href="https://arxiv.org/pdf/2002.05688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Eilertsen%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabriel Eilertsen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=J%C3%B6nsson%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Jönsson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ropinski%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timo Ropinski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Unger%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonas Unger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ynnerman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anders Ynnerman</a><br>
<font size="3">
Abstract: This paper presents an empirical study on the weights of neural networks, where we interpret each model as a point in a high-dimensional space -- the neural weight space. To explore the complex structure of this space, we sample from a diverse selection of training variations (dataset, optimization procedure, architecture, etc.) of neural network classifiers, and train a large number of models to represent the weight space. Then, we use a machine learning approach for analyzing and extracting information from this space. Most centrally, we train a number of novel deep meta-classifiers with the objective of classifying different properties of the training setup by identifying their footprints in the weight space. Thus, the meta-classifiers probe for patterns induced by hyper-parameters, so that we can quantify how much, where, and when these are encoded through the optimization process. This provides a novel and complementary view for explainable AI, and we show how meta-classifiers can reveal a great deal of information about the training setup and optimization, by only considering a small subset of randomly selected consecutive weights. To promote further research on the weight space, we release the neural weight space (NWS) dataset -- a collection of 320K weight snapshots from 16K individually trained deep neural networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了关于神经网络，在那里我们解释每个模型为高维空间中的点的权重进行了实证研究 - 神经权空间。为了探究这种空间的结构复杂，从我们的神经网络分类器的训练变化（数据集，优化过程，建筑等）的多样选择采样，并培养了大量的模型来表示重量的空间。然后，我们用从这个空间分析和提取信息的机器学习方法。最集中，我们通过鉴定权空间他们的足迹培养出一批新的深荟萃分类与客观的培训设置的不同性质进行分类的。因此，元分类探测由超参数引起的模式，让我们多少可以量化，在那里，当这些通过优化过程进行编码。这为解释的AI一种新颖的和互补的观点，我们展示荟萃分类如何揭示的有关训练的设置和优化的大量信息，只考虑随机选择的连续权重的一小部分。为了促进对重空间的进一步研究，我们释放神经权空间（NWS）数据集 - 的320K重量快照从16K集合单独训练深层神经网络。</font>
</div>


<hr>
<div id="paper3"> <b>3. Summarizing the performances of a background subtraction algorithm  measured on several videos</b>  <a href="https://arxiv.org/pdf/2002.05654" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pi%C3%A9rard%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sébastien Piérard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Droogenbroeck%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marc Van Droogenbroeck</a><br>
<font size="3">
Abstract: There exist many background subtraction algorithms to detect motion in videos. To help comparing them, datasets with ground-truth data such as CDNET or LASIESTA have been proposed. These datasets organize videos in categories that represent typical challenges for background subtraction. The evaluation procedure promoted by their authors consists in measuring performance indicators for each video separately and to average them hierarchically, within a category first, then between categories, a procedure which we name "summarization". While the summarization by averaging performance indicators is a valuable effort to standardize the evaluation procedure, it has no theoretical justification and it breaks the intrinsic relationships between summarized indicators. This leads to interpretation inconsistencies. In this paper, we present a theoretical approach to summarize the performances for multiple videos that preserves the relationships between performance indicators. In addition, we give formulas and an algorithm to calculate summarized performances. Finally, we showcase our observations on CDNET 2014. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：存在很多背景减除算法来检测视频中的运动。要比较它们的帮助下，与地面实况数据，如CDNET或LASIESTA数据集已经被提出。这些数据集在组织代表背景扣除的典型挑战类视频。它们的作者所倡导的评估过程包括分别测量性能指标为每个视频和他们的平均分层次，类别之内，然后再分类，这是我们的名字“汇总”的程序之间。虽然通过平均业绩指标汇总是一种宝贵的努力，以规范的评估程序，它没有理论依据和它打破了总结指标之间的内在关系。这导致解释不一致。在本文中，我们提出了一个理论方法总结为保留性能指标之间的关系多部影片的演出。另外，我们给出的公式和算法来计算总结演出。最后，我们上展示CDNET 2014我们的观察。</font>
</div>


<hr>
<div id="paper4"> <b>4. GANILLA: Generative Adversarial Networks for Image to Illustration  Translation</b>  <a href="https://arxiv.org/pdf/2002.05638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hicsonmez%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samet Hicsonmez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Samet%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nermin Samet</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Akbas%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emre Akbas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duygulu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pinar Duygulu</a><br>
<font size="3">
Abstract: In this paper, we explore illustrations in children's books as a new domain in unpaired image-to-image translation. We show that although the current state-of-the-art image-to-image translation models successfully transfer either the style or the content, they fail to transfer both at the same time. We propose a new generator network to address this issue and show that the resulting network strikes a better balance between style and content. There are no well-defined or agreed-upon evaluation metrics for unpaired image-to-image translation. So far, the success of image translation models has been based on subjective, qualitative visual comparison on a limited number of images. To address this problem, we propose a new framework for the quantitative evaluation of image-to-illustration models, where both content and style are taken into account using separate classifiers. In this new evaluation framework, our proposed model performs better than the current state-of-the-art models on the illustrations dataset. Our code and pretrained models can be found at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们将探讨在儿童读物插图不成对图像 - 图像转换一个新的领域。我们发现，虽然目前国家的最先进的图像到图像的翻译模式成功传输的样式或内容，他们不能在同一时间传送两者。我们提出了一个新的发电机网络，以解决这一问题，并表明，导致网络罢工的风格和内容之间实现更好的平衡。有没有明确的或商定的评估指标不成对图像 - 图像转换。到目前为止，图像平移模式的成功是基于图像的数量有限，主观的，定性的视觉比较。为了解决这个问题，我们提出了图像到图模型的定量评价，在内容和风格都使用单独的分类考虑到了新的框架。在这个新的评估框架，我们提出的模型比对说明当前国家的最先进的机型更好的数据集。我们的代码和预训练的模型可以在此HTTPS URL中找到。</font>
</div>


<hr>
<div id="paper5"> <b>5. Asynchronous Tracking-by-Detection on Adaptive Time Surfaces for  Event-based Object Tracking</b>  <a href="https://arxiv.org/pdf/2002.05583" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haosheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiangqiang Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanjie Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinbo Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanzi Wang</a><br>
<font size="3">
Abstract: Event cameras, which are asynchronous bio-inspired vision sensors, have shown great potential in a variety of situations, such as fast motion and low illumination scenes. However, most of the event-based object tracking methods are designed for scenarios with untextured objects and uncluttered backgrounds. There are few event-based object tracking methods that support bounding box-based object tracking. The main idea behind this work is to propose an asynchronous Event-based Tracking-by-Detection (ETD) method for generic bounding box-based object tracking. To achieve this goal, we present an Adaptive Time-Surface with Linear Time Decay (ATSLTD) event-to-frame conversion algorithm, which asynchronously and effectively warps the spatio-temporal information of asynchronous retinal events to a sequence of ATSLTD frames with clear object contours. We feed the sequence of ATSLTD frames to the proposed ETD method to perform accurate and efficient object tracking, which leverages the high temporal resolution property of event cameras. We compare the proposed ETD method with seven popular object tracking methods, that are based on conventional cameras or event cameras, and two variants of ETD. The experimental results show the superiority of the proposed ETD method in handling various challenging environments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事件相机，其是异步仿生视觉传感器，已在各种情况下，如快动作和低照明场景示出巨大的潜力。然而，大多数的基于事件的对象跟踪方法设计用于无网纹对象和整洁的背景场景。很少有基于事件的对象跟踪方法，借助现成支持边界对象跟踪。这背后工作的主要思想是提出了基于框包围仿制对象跟踪基于异步事件跟踪 - 通过检测（ETD）方法。为了实现这个目标，提出了一种自适应时表面与线性时间衰减（ATSLTD）事件到帧转换算法，它异步地和有效地经线异步视网膜事件的时空信息来ATSLTD帧的清晰目的的序列轮廓。我们从进料ATSLTD帧序列所提出的ETD方法来执行精确和高效的对象跟踪，它利用的事件摄像机的高时间分辨率特性。我们比较建议的ETD方法有七个流行的对象跟踪方法，是基于传统相机或事件相机和ETD的两个变种。实验结果表明，在处理各种复杂的环境下提出的ETD方法的优越性。</font>
</div>


<hr>
<div id="paper6"> <b>6. SpotNet: Self-Attention Multi-Task Network for Object Detection</b>  <a href="https://arxiv.org/pdf/2002.05540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Perreault%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hughes Perreault</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bilodeau%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillaume-Alexandre Bilodeau</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saunier%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Saunier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=H%C3%A9ritier%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maguelonne Héritier</a><br>
<font size="3">
Abstract: Humans are very good at directing their visual attention toward relevant areas when they search for different types of objects. For instance, when we search for cars, we will look at the streets, not at the top of buildings. The motivation of this paper is to train a network to do the same via a multi-task learning approach. To train visual attention, we produce foreground/background segmentation labels in a semi-supervised way, using background subtraction or optical flow. Using these labels, we train an object detection model to produce foreground/background segmentation maps as well as bounding boxes while sharing most model parameters. We use those segmentation maps inside the network as a self-attention mechanism to weight the feature map used to produce the bounding boxes, decreasing the signal of non-relevant areas. We show that by using this method, we obtain a significant mAP improvement on two traffic surveillance datasets, with state-of-the-art results on both UA-DETRAC and UAVDT. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类是在指导他们的视觉注意力转向当他们搜索不同类型的对象的相关领域的非常好。例如，当我们搜索汽车，我们将着眼于街道，而不是在建筑物的顶部。这篇文章的动机是培养网络通过多任务学习的方法来这样做。培养视觉注意，我们产生一个半监督方式前景/背景分割的标签，使用背景减除或光流。使用这些标签，我们培养的目标检测模型生成前景/背景分割映射以及边界框，而分享最模型参数。我们使用这些分割网络作为自注意机制加权用于生产边界框的功能地图内的地图，减小非相关领域的信号。我们表明，采用这种方法，我们获得显著改善地图上的两个交通监控的数据集，用两个UA-DETRAC和UAVDT国家的先进成果。</font>
</div>


<hr>
<div id="paper7"> <b>7. Replacing Mobile Camera ISP with a Single Deep Learning Model</b>  <a href="https://arxiv.org/pdf/2002.05509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ignatov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrey Ignatov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Gool%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luc Van Gool</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Timofte%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Radu Timofte</a><br>
<font size="3">
Abstract: As the popularity of mobile photography is growing constantly, lots of efforts are being invested now into building complex hand-crafted camera ISP solutions. In this work, we demonstrate that even the most sophisticated ISP pipelines can be replaced with a single end-to-end deep learning model trained without any prior knowledge about the sensor and optics used in a particular device. For this, we present PyNET, a novel pyramidal CNN architecture designed for fine-grained image restoration that implicitly learns to perform all ISP steps such as image demosaicing, denoising, white balancing, color and contrast correction, demoireing, etc. The model is trained to convert RAW Bayer data obtained directly from mobile camera sensor into photos captured with a professional high-end DSLR camera, making the solution independent of any particular mobile ISP implementation. To validate the proposed approach on the real data, we collected a large-scale dataset consisting of 10 thousand full-resolution RAW-RGB image pairs captured in the wild with the Huawei P20 cameraphone (12.3 MP Sony Exmor IMX380 sensor) and Canon 5D Mark IV DSLR. The experiments demonstrate that the proposed solution can easily get to the level of the embedded P20's ISP pipeline that, unlike our approach, is combining the data from two (RGB + B/W) camera sensors. The dataset, pre-trained models and codes used in this paper are available on the project website. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着移动摄影的普及在不断增加，大量的努力，现在正在投入到构建复杂的手工制作的相机ISP解决方案。在这项工作中，我们证明，即使是最先进的ISP管道可以用单端至端深学习模型没有关于特定设备中使用的传感器和光学任何先验知识培训的进行更换。对于这一点，我们现在PyNET，一种新颖的锥体CNN架构设计用于细粒度图像恢复隐式学习执行所有ISP作为图像去马赛克，去噪，白平衡，颜色和对比度校正，demoireing等模型被训练步骤，例如转换直接从移动照相机传感器获得的与一个专业高端数码单反相机拍摄的照片的RAW拜尔数据，使得溶液独立于任何特定的移动ISP实现的。为了验证对实际数据所提出的方法，我们收集了大规模的数据集，包括与华为P20拍照手机（12.3 MP的索尼Exmor IMX380传感器）和佳能5D Mark野外捕获10000全分辨率的RAW-RGB图像对IV数码单反相机。实验结果表明，所提出的解决方案可以轻松搞定嵌入式P20的ISP管线的水平，不像我们的做法，是结合两个（RGB + B / W）相机传感器的数据。该数据集，预先训练模式，本文使用的代码都可以在项目网站上。</font>
</div>


<hr>
<div id="paper8"> <b>8. Chaotic Phase Synchronization and Desynchronization in an Oscillator  Network for Object Selection</b>  <a href="https://arxiv.org/pdf/2002.05493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Breve%2C+F+A" target="_blank" rel="noopener" style="color:#0000EE;">Fabricio A Breve</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Quiles%2C+M+G" target="_blank" rel="noopener" style="color:#0000EE;">Marcos G Quiles</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Macau%2C+E+E+N" target="_blank" rel="noopener" style="color:#0000EE;">Elbert E. N. Macau</a><br>
<font size="3">
Abstract: Object selection refers to the mechanism of extracting objects of interest while ignoring other objects and background in a given visual scene. It is a fundamental issue for many computer vision and image analysis techniques and it is still a challenging task to artificial visual systems. Chaotic phase synchronization takes place in cases involving almost identical dynamical systems and it means that the phase difference between the systems is kept bounded over the time, while their amplitudes remain chaotic and may be uncorrelated. Instead of complete synchronization, phase synchronization is believed to be a mechanism for neural integration in brain. In this paper, an object selection model is proposed. Oscillators in the network representing the salient object in a given scene are phase synchronized, while no phase synchronization occurs for background objects. In this way, the salient object can be extracted. In this model, a shift mechanism is also introduced to change attention from one object to another. Computer simulations show that the model produces some results similar to those observed in natural vision systems. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对象选择是指提取关注对象，而忽略了在给定的视觉场景中其他对象和背景的机制。这是许多计算机视觉和图像分析技术的一个基本问题，它仍然是一个艰巨的任务，以人工视觉系统。乱相同步需要在涉及几乎相同的动力系统的情况下的地方，这意味着该系统之间的相位差被保持为界在时间，而它们的幅度保持混乱，并且可以是不相关的。相反，完全同步，相位同步被认为是对脑神经一体化的机制。在本文中，对象选择模型。在网络中的振荡器表示在给定的场景中的显着对象的相位同步，而没有相位同步发生为背景对象。通过这种方式，显着对象可以提取。在这种模式下，换档机构也被引入到变化的注意力从一个对象到另一个。计算机模拟表明，该模型产生相似于在自然视觉系统观察到了一定的成效。</font>
</div>


<hr>
<div id="paper9"> <b>9. EndoL2H: Deep Super-Resolution for Capsule Endoscopy</b>  <a href="https://arxiv.org/pdf/2002.05459" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Almalioglu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yasin Almalioglu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gokce%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abdulkadir Gokce</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Incetan%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kagan Incetan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Simsek%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Muhammed Ali Simsek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ararat%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kivanc Ararat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+R+J" target="_blank" rel="noopener" style="color:#0000EE;">Richard J. Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Durr%2C+N+J" target="_blank" rel="noopener" style="color:#0000EE;">Nichalos J. Durr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mahmood%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Faisal Mahmood</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Turan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehmet Turan</a><br>
<font size="3">
Abstract: Wireless capsule endoscopy is the preferred modality for diagnosis and assessment of small bowel disease. However, the poor resolution is a limitation for both subjective and automated diagnostics. Enhanced-resolution endoscopy has shown to improve adenoma detection rate for conventional endoscopy and is likely to do the same for capsule endoscopy. In this work, we propose and quantitatively validate a novel framework to learn a mapping from low-to-high resolution endoscopic images. We use conditional adversarial networks and spatial attention to improve the resolution by up to a factor of 8x. Our quantitative study demonstrates the superiority of our proposed approach over Super-Resolution Generative Adversarial Network (SRGAN) and bicubic interpolation. For qualitative analysis, visual Turing tests were performed by 16 gastroenterologists to confirm the clinical utility of the proposed approach. Our approach is generally applicable to any endoscopic capsule system and has the potential to improve diagnosis and better harness computational approaches for polyp detection and characterization. Our code and trained models are available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无线胶囊内窥镜是用于小肠疾病的诊断和评估的优选的形态。然而，可怜的分辨率是主观和自动诊断的限制。增强的分辨率内镜已经显示出改善常规胃镜腺瘤检出率，并有可能对胶囊内镜做同样的。在这项工作中，我们提出并定量验证新框架，从低到高清晰度内窥镜图像学的映射。我们使用条件对抗网络和空间注意改善了分辨率8X的一个因素。我们的定量研究表明我们提出的方法在超分辨率剖成对抗性网络（SRGAN）和双三次插值的优越性。对于定性分析，视觉图灵测试是由16名胃肠病来证实了该方法的临床应用。我们的做法是普遍适用于任何的胶囊内窥镜系统，并具有提高诊断和息肉检测和表征更好地利用计算方法的潜力。我们的代码和训练的模型可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper10"> <b>10. Emotion Recognition for In-the-wild Videos</b>  <a href="https://arxiv.org/pdf/2002.05447" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanyu Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiabei Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiguang Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xilin Chen</a><br>
<font size="3">
Abstract: This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文简要介绍我们提交情感行为分析的七种基本表情分类轨道在最狂野结合的自动面部和手势识别（FG）2020年，我们的方法结合了IEEE国际会议举行比赛深剩余网络（RESNET）和双向长短时记忆网络（BLSTM），实现了64.3％的准确率和43.4％的验证集的最终指标。</font>
</div>


<hr>
<div id="paper11"> <b>11. Recurrent Attention Model with Log-Polar Mapping is Robust against  Adversarial Attacks</b>  <a href="https://arxiv.org/pdf/2002.05388" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiritani%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taro Kiritani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ono%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Koji Ono</a><br>
<font size="3">
Abstract: Convolutional neural networks are vulnerable to small $\ell^p$ adversarial attacks, while the human visual system is not. Inspired by neural networks in the eye and the brain, we developed a novel artificial neural network model that recurrently collects data with a log-polar field of view that is controlled by attention. We demonstrate the effectiveness of this design as a defense against SPSA and PGD adversarial attacks. It also has beneficial properties observed in the animal visual system, such as reflex-like pathways for low-latency inference, fixed amount of computation independent of image size, and rotation and scale invariance. The code for experiments is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络很容易受到小$ \ ELL ^ P $敌对攻击，而人的视觉系统是没有的。通过在眼睛和大脑的神经网络的启发，我们开发了具有反复的观点，即由关注控制的数极场收集数据的新型人工神经网络模型。我们证明这种设计的对抗SPSA和PGD敌对攻击防御的有效性。它还具有在动物的视觉系统观察到的有益的性质，例如反射般途径低延迟推断，计算独立的图像尺寸的固定量，并且旋转和尺度不变性。用于实验的代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper12"> <b>12. Hypergraph Optimization for Multi-structural Geometric Model Fitting</b>  <a href="https://arxiv.org/pdf/2002.05350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuyuan Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guobao Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Suter%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Suter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanzi Wang</a><br>
<font size="3">
Abstract: Recently, some hypergraph-based methods have been proposed to deal with the problem of model fitting in computer vision, mainly due to the superior capability of hypergraph to represent the complex relationship between data points. However, a hypergraph becomes extremely complicated when the input data include a large number of data points (usually contaminated with noises and outliers), which will significantly increase the computational burden. In order to overcome the above problem, we propose a novel hypergraph optimization based model fitting (HOMF) method to construct a simple but effective hypergraph. Specifically, HOMF includes two main parts: an adaptive inlier estimation algorithm for vertex optimization and an iterative hyperedge optimization algorithm for hyperedge optimization. The proposed method is highly efficient, and it can obtain accurate model fitting results within a few iterations. Moreover, HOMF can then directly apply spectral clustering, to achieve good fitting performance. Extensive experimental results show that HOMF outperforms several state-of-the-art model fitting methods on both synthetic data and real images, especially in sampling efficiency and in handling data with severe outliers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近来，一些基于超图的方法已经被提出来处理在计算机视觉模型拟合的问题，主要是由于超图来表示数据点之间的复杂关系的卓越能力。然而，当输入数据包括大量的数据点（通常沾染噪声和异常值），这将增加显著的计算负担的超图变得极其复杂。为了克服上述问题，我们提出了一种新颖的超图优化的基于模型拟合（HOMF）方法来构造一个简单但有效的超图。具体而言，HOMF包括两个主要部分：用于顶点优化的自适应内点估计算法和用于超边优化迭代超边优化算法。所提出的方法是高效的，并且它可以在几次迭代中获得准确的模型拟合的结果。此外，HOMF就可以直接申请谱聚类，以达到良好的装配性能。广泛的实验结果表明，HOMF性能优于几个国家的最先进的模型拟合在两个合成数据和真实图像的方法，尤其是在采样效率，并与严重的异常值处理数据。</font>
</div>


<hr>
<div id="paper13"> <b>13. Object Detection on Single Monocular Images through Canonical  Correlation Analysis</b>  <a href="https://arxiv.org/pdf/2002.05349" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zifan Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=You%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suya You</a><br>
<font size="3">
Abstract: Without using extra 3-D data like points cloud or depth images for providing 3-D information, we retrieve the 3-D object information from single monocular images. The high-quality predicted depth images are recovered from single monocular images, and it is fed into the 2-D object proposal network with corresponding monocular images. Most existing deep learning frameworks with two-streams input data always fuse separate data by concatenating or adding, which views every part of a feature map can contribute equally to the whole task. However, when data are noisy, and too much information is redundant, these methods no longer produce predictions or classifications efficiently. In this report, we propose a two-dimensional CCA(canonical correlation analysis) framework to fuse monocular images and corresponding predicted depth images for basic computer vision tasks like image classification and object detection. Firstly, we implemented different structures with one-dimensional CCA and Alexnet to test the performance on the image classification task. And then, we applied one of these structures with 2D-CCA for object detection. During these experiments, we found that our proposed framework behaves better when taking predicted depth images as inputs with the model trained from ground truth depth. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：不使用额外的3-d数据，如点云或提供3 d信息的深度图像，我们从中检索单一单眼图像的3-d对象的信息。高品质的预测深度图像是从单个单目图像恢复，并且它被送入2-d对象提案网络与对应单眼图像。与大多数现有的深度学习框架，通过连接或添加，其浏览量特征图的每一个部分可以对整个任务同样有助于总是两流输入数据熔丝单独的数据。然而，当数据是嘈杂的，和太多的信息是多余的，这些方法不再生产预测或分类有效。在本报告中，我们提出了一种二维CCA（典型相关分析）框架，保险丝单眼图像和基本计算机视觉任务，如图像分类和物体检测对应的预测深度图像。首先，我们实施了不同的结构与一维CCA和Alexnet测试在图像分类任务性能。然后，我们应用这些结构的2D-CCA为对象检测中的一个。在这些实验中，我们发现，服用预测深度图像与地面真相的深度训练模型输入时，我们提出的框架的行为更好。</font>
</div>


<hr>
<div id="paper14"> <b>14. Continual Universal Object Detection</b>  <a href="https://arxiv.org/pdf/2002.05347" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xialei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ravichandran%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avinash Ravichandran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhotika%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rahul Bhotika</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soatto%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefano Soatto</a><br>
<font size="3">
Abstract: Object detection has improved significantly in recent years on multiple challenging benchmarks. However, most existing detectors are still domain-specific, where the models are trained and tested on a single domain. When adapting these detectors to new domains, they often suffer from catastrophic forgetting of previous knowledge. In this paper, we propose a continual object detector that can learn sequentially from different domains without forgetting. First, we explore learning the object detector continually in different scenarios across various domains and categories. Learning from the analysis, we propose attentive feature distillation leveraging both bottom-up and top-down attentions to mitigate forgetting. It takes advantage of attention to ignore the noisy background information and feature distillation to provide strong supervision. Finally, for the most challenging scenarios, we propose an adaptive exemplar sampling method to leverage exemplars from previous tasks for less forgetting effectively. The experimental results show the excellent performance of our proposed method in three different scenarios across seven different object detection datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目的检测已在多个具有挑战性的基准近年来显著改善。然而，大多数现有的探测器依然特定领域，其中模型训练和在单个域进行测试。当采用这些探测器新的领域，但常常会出现以前的知识的灾难性遗忘。在本文中，我们提出一种可从不同的域顺序地学习没有忘记一个连续的物体检测装置。首先，我们探究的学习对象检测器不断地在不同领域和类别不同的​​场景。从分析中学习，我们提出了周到的功能，利用蒸馏既自下而上和自上而下的注意力，以减轻遗忘。这需要关注的优势，忽略了嘈杂的背景信息和功能蒸馏提供强有力的监督。最后，最具挑战性的场景中，我们提出了有效的少遗忘的自适应典范抽样的方法，从以前的任务杠杆典范。实验结果表明，在三个不同的场景在七个不同的物体探测数据集我们提出的方法的优良性能。</font>
</div>


<hr>
<div id="paper15"> <b>15. SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D  Vehicle Detection from Point Cloud</b>  <a href="https://arxiv.org/pdf/2002.05316" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongwei Yi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shaoshuai Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingyu Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiankai Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kui Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoping Wang</a><br>
<font size="3">
Abstract: 3D vehicle detection based on point cloud is a challenging task in real-world applications such as autonomous driving. Despite significant progress has been made, we observe two aspects to be further improved. First, the semantic context information in LiDAR is seldom explored in previous works, which may help identify ambiguous vehicles. Second, the distribution of point cloud on vehicles varies continuously with increasing depths, which may not be well modeled by a single model. In this work, we propose a unified model SegVoxelNet to address the above two problems. A semantic context encoder is proposed to leverage the free-of-charge semantic segmentation masks in the bird's eye view. Suspicious regions could be highlighted while noisy regions are suppressed by this module. To better deal with vehicles at different depths, a novel depth-aware head is designed to explicitly model the distribution differences and each part of the depth-aware head is made to focus on its own target detection range. Extensive experiments on the KITTI dataset show that the proposed method outperforms the state-of-the-art alternatives in both accuracy and efficiency with point cloud as input only. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于点云3D车辆检测是在现实世界的应用，如自动驾驶一个具有挑战性的任务。尽管显著已经取得了进展，我们观察到两个方面有待进一步提高。首先，在激光雷达语义上下文信息在以前的作品中，这可能有助于识别模糊的车辆很少探讨。其次，点云上的车辆分布随深度，这可能不是一个单一的模式很好地模拟连续变化。在这项工作中，我们提出了一个统一的模型SegVoxelNet解决上述两个问题。语义上下文编码器，提出了利用自由充电语义分割口罩的鸟瞰视图。而嘈杂的区域由该模块抑制可疑的区域可以高亮显示。为了更好地应对在不同深度的车辆，新颖的深度感知的头被设计成分布差异和深度感知头部的每个部分是由专注于自己的目标的探测距离清晰的模型。对数据集KITTI表明，该方法优于国家的最先进的替代品在精度和效率与点云作为仅输入了广泛的实验。</font>
</div>


<hr>
<div id="paper16"> <b>16. Improving Efficiency in Neural Network Accelerator Using Operands  Hamming Distance optimization</b>  <a href="https://arxiv.org/pdf/2002.05293" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yilei Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierce Chuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangzhen Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chandra%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vikas Chandra</a><br>
<font size="3">
Abstract: Neural network accelerator is a key enabler for the on-device AI inference, for which energy efficiency is an important metric. The data-path energy, including the computation energy and the data movement energy among the arithmetic units, claims a significant part of the total accelerator energy. By revisiting the basic physics of the arithmetic logic circuits, we show that the data-path energy is highly correlated with the bit flips when streaming the input operands into the arithmetic units, defined as the hamming distance of the input operand matrices. Based on the insight, we propose a post-training optimization algorithm and a hamming-distance-aware training algorithm to co-design and co-optimize the accelerator and the network synergistically. The experimental results based on post-layout simulation with MobileNetV2 demonstrate on average 2.85X data-path energy reduction and up to 8.51X data-path energy reduction for certain layers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经网络加速器是用于在设备上的AI推理的一个关键因素，为此，能量效率是一个重要的度量。数据通路的能量，包括计算能量和运算单元之间的数据移动的能量，要求总能量加速器的显著一部分。通过重新审视的算术逻辑电路的基本物理中，我们表明，与流式输入操作数到运算单元，其定义为输入操作数矩阵的汉明距离，当位翻转数据通路能量高度相关。基于这样的认识，我们提出了一个培训后的优化算法和汉明距离感知训练算法协同设计和协同优化的加速器和网络协同。基于后布局仿真MobileNetV2实验结果表明，平均2.85X数据路径能量减少且至多为8.51X某些层数据路径能量削减。</font>
</div>


<hr>
<div id="paper17"> <b>17. Solving Missing-Annotation Object Detection with Background  Recalibration Loss</b>  <a href="https://arxiv.org/pdf/2002.05274" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fangyi Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiqiang Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiqi Hao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenchen Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Savvides%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marios Savvides</a><br>
<font size="3">
Abstract: This paper focuses on a novel and challenging detection scenario: A majority of true objects/instances is unlabeled in the datasets, so these missing-labeled areas will be regarded as the background during training. Previous art on this problem has proposed to use soft sampling to re-weight the gradients of RoIs based on the overlaps with positive instances, while their method is mainly based on the two-stage detector (i.e. Faster RCNN) which is more robust and friendly for the missing label scenario. In this paper, we introduce a superior solution called Background Recalibration Loss (BRL) that can automatically re-calibrate the loss signals according to the pre-defined IoU threshold and input image. Our design is built on the one-stage detector which is faster and lighter. Inspired by the Focal Loss formulation, we make several significant modifications to fit on the missing-annotation circumstance. We conduct extensive experiments on the curated PASCAL VOC and MS COCO datasets. The results demonstrate that our proposed method outperforms the baseline and other state-of-the-arts by a large margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文着重于新颖和具有挑战性的检测情况：多数真正的对象/实例中的数据集未标记的，所以这些丢失的标记区将被视为训练期间的背景。对这个问题以前的技术已提出了使用软采样重新重量基于与正实例的重叠ROI的梯度，而他们的方法主要是基于两阶段检测器上（即，更快的RCNN），这是更健壮的和友好的失踪标签的情况。在本文中，我们引入称为背景重新校准损失（BRL）优越的解决方案根据所述预定义的阈值IOU和输入图像，可以自动重新校准损失信号。我们的设计是建立在一个阶段的检测速度更快，更轻的。由焦点损失配方的启发，我们做几个显著的修改，以适应失踪的注释情况。我们进行的策划PASCAL VOC和MS COCO数据集大量的实验。结果表明，我们提出的方法优于大幅度基线和其他国家的的美术馆。</font>
</div>


<hr>
<div id="paper18"> <b>18. Leveraging Affect Transfer Learning for Behavior Prediction in an  Intelligent Tutoring System</b>  <a href="https://arxiv.org/pdf/2002.05242" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruiz%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nataniel Ruiz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jalal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mona Jalal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ablavsky%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vitaly Ablavsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Allessio%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danielle Allessio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Magee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Magee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Whitehill%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jacob Whitehill</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arroyo%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivon Arroyo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Woolf%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Beverly Woolf</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sclaroff%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stan Sclaroff</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Betke%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Margrit Betke</a><br>
<font size="3">
Abstract: In the context of building an intelligent tutoring system (ITS), which improves student learning outcomes by intervention, we set out to improve prediction of student problem outcome. In essence, we want to predict the outcome of a student answering a problem in an ITS from a video feed by analyzing their face and gestures. For this, we present a novel transfer learning facial affect representation and a user-personalized training scheme that unlocks the potential of this representation. We model the temporal structure of video sequences of students solving math problems using a recurrent neural network architecture. Additionally, we extend the largest dataset of student interactions with an intelligent online math tutor by a factor of two. Our final model, coined ATL-BP (Affect Transfer Learning for Behavior Prediction) achieves an increase in mean F-score over state-of-the-art of 45% on this new dataset in the general case and 50% in a more challenging leave-users-out experimental setting when we use a user-personalized training scheme. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在建设智能教学系统（ITS），它通过干预提高了学生的学习成果的背景下，我们着手提高学生的问题结果的预测。从本质上说，我们希望通过分析他们的脸和手势来预测一个学生从视频源的ITS回答问题的结果。为此，我们提出了一个新的转移学习的面部影响表现和解锁此表示的潜在用户个性化的培训方案。我们的学生解决使用递归神经网络结构的数学题的视频序列的时间结构建模。此外，我们通过两个因素具有智能在线数学家教延长学生互动的最大的数据集。我们的最终模型，创造了ATL-BP（影响对行为预测迁移学习）达到平均F-得分超过国家的最先进的45％，在这个新的数据集在一般情况下增加，在50％以上挑战假用户出实验设置，当我们使用用户个性化的培训方案。</font>
</div>


<hr>
<div id="paper19"> <b>19. Image-to-Image Translation with Text Guidance</b>  <a href="https://arxiv.org/pdf/2002.05235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bowen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojuan Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torr%2C+P+H+S" target="_blank" rel="noopener" style="color:#0000EE;">Philip H. S. Torr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lukasiewicz%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Lukasiewicz</a><br>
<font size="3">
Abstract: The goal of this paper is to embed controllable factors, i.e., natural language descriptions, into image-to-image translation with generative adversarial networks, which allows text descriptions to determine the visual attributes of synthetic images. We propose four key components: (1) the implementation of part-of-speech tagging to filter out non-semantic words in the given description, (2) the adoption of an affine combination module to effectively fuse different modality text and image features, (3) a novel refined multi-stage architecture to strengthen the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to further improve discriminators to better distinguish real and synthetic images. Extensive experiments on the COCO dataset demonstrate that our method has a superior performance on both visual realism and semantic consistency with given descriptions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文的目的是嵌入可控因素，即，自然语言描述成图像到图像的平移与生成对抗性的网络，它允许文本描述，以确定合成图像的视觉属性。我们提出四个主要组成部分：（1）部分的词性标注的实施，以过滤出在给定的描述非语义字，（2）通过仿射组合模块的有效熔丝不同模态的文本和图像的特征， （3）一种新的改进的多级结构，以加强鉴别器的差动能力和发电机的整流能力，和（4）的新结构的损失，进一步提高鉴别器，以更好地分辨实际的和合成的图像。在COCO大量的实验数据集表明，我们的方法有两个逼真视觉效果，并与给定的描述语义一致性优越的性能。</font>
</div>


<hr>
<div id="paper20"> <b>20. Cross-Iteration Batch Normalization</b>  <a href="https://arxiv.org/pdf/2002.05712" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuliang Yao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuxin Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gao Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Lin</a><br>
<font size="3">
Abstract: A well-known issue of Batch Normalization is its significantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statistics upon which the normalization is defined cannot be reliably estimated from it during a training iteration. To address this problem, we present Cross-Iteration Batch Normalization (CBN), in which examples from multiple recent iterations are jointly utilized to enhance estimation quality. A challenge of computing statistics over multiple iterations is that the network activations from different iterations are not comparable to each other due to changes in network weights. We thus compensate for the network weight changes via a proposed technique based on Taylor polynomials, so that the statistics can be accurately estimated and batch normalization can be effectively applied. On object detection and image classification with small mini-batch sizes, CBN is found to outperform the original batch normalization and a direct calculation of statistics over previous iterations without the proposed compensation technique. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：知名批标准化的问题是迷你小批量的情况下，其显著降低效果。当小批量包含几个例子，在其上归一化定义的统计数据不能可靠地从它训练迭代期间估计。为了解决这个问题，我们提出了交叉迭代批标准化（CBN），其最近多次迭代的例子共同利用，以提高估计质量。在多次迭代计算统计数据的一个挑战是，从不同的迭代中的网络激活没有可比性彼此由于在网络权的变化。因此，我们补偿通过基于泰勒多项式一个提出的技术的网络的重量变化，从而使统计数据可以精确地估计和批量标准化可以有效的应用。关于物体检测及图像分类与迷你小批量大小，CBN发现优于原始批标准化并在先前迭代统计而不拟议补偿技术直接计算。</font>
</div>


<hr>
<div id="paper21"> <b>21. A Simple Framework for Contrastive Learning of Visual Representations</b>  <a href="https://arxiv.org/pdf/2002.05709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Ting Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kornblith%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Kornblith</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Norouzi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Norouzi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hinton%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geoffrey Hinton</a><br>
<font size="3">
Abstract: This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍SimCLR：对于视觉表现的对比学习一个简单的框架。我们简化了最近提出的对比自我监督学习算法，而不需要专门的架构或存储库。为了了解什么能使对比预测任务学习用的表现，我们系统地研究我们的框架的主要组成部分。我们显示数据增扩的：（1）组合物在定义有效的预测任务（2）将所述表示和所述对比损耗基本上之间的可以学习的非线性变换关键作用，改善了学习表示的质量，和（3）对比学习从大批量和更多的培训措施的好处相比，监督学习。通过结合这些研究结果，我们能够显着跑赢上ImageNet自我监督和半监督学习以前的方法。训练由SimCLR了解到自监督表示的线性分类器达到76.5％顶-1精度，这比以前的国家的最先进的7％的相对改善，匹配的性能监督RESNET-50。当只有1％的标签的微调，我们达到85.8％，排名前五的准确性，跑赢AlexNet与100X较少的标签。</font>
</div>


<hr>
<div id="paper22"> <b>22. Generative-based Airway and Vessel Morphology Quantification on Chest CT  Images</b>  <a href="https://arxiv.org/pdf/2002.05702" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Nardelli%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pietro Nardelli</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ross%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">James C. Ross</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Est%C3%A9par%2C+R+S+J" target="_blank" rel="noopener" style="color:#0000EE;">Raúl San José Estépar</a><br>
<font size="3">
Abstract: Accurately and precisely characterizing the morphology of small pulmonary structures from Computed Tomography (CT) images, such as airways and vessels, is becoming of great importance for diagnosis of pulmonary diseases. The smaller conducting airways are the major site of increased airflow resistance in chronic obstructive pulmonary disease (COPD), while accurately sizing vessels can help identify arterial and venous changes in lung regions that may determine future disorders. However, traditional methods are often limited due to image resolution and artifacts. We propose a Convolutional Neural Regressor (CNR) that provides cross-sectional measurement of airway lumen, airway wall thickness, and vessel radius. CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth. For validation, we first use synthetically generated airways and vessels produced by the proposed generative model to compute the relative error and directly evaluate the accuracy of CNR in comparison with traditional methods. Then, in-vivo validation is performed by analyzing the association between the percentage of the predicted forced expiratory volume in one second (FEV1\%) and the value of the Pi10 parameter, two well-known measures of lung function and airway disease, for airways. For vessels, we assess the correlation between our estimate of the small-vessel blood volume and the lungs' diffusing capacity for carbon monoxide (DLCO). The results demonstrate that Convolutional Neural Networks (CNNs) provide a promising direction for accurately measuring vessels and airways on chest CT images with physiological correlates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：准确且精确地从计算机断层摄影术表征小肺结构的形态（CT）图像，如气道和血管中，对肺部疾病的诊断成为非常重要的。较小的传导气道中是慢性阻塞性肺疾病（COPD）增加的气流阻力的主要部位，而准确地定径容器可以帮助识别在肺部区域的动脉和静脉的变化可确定未来失调。然而，传统的方法往往是有限的，由于图像分辨率和文物。我们提出了一个卷积神经回归（CNR），其提供气道内腔，气道壁的厚度，和容器半径的横截面测量。 CNR进行训练由在与模拟和无监督剖成对抗式网络（SimGAN）来创建仿真和精制气道和与已知的地面实况容器组合使用的合成结构的生成模型创建的数据。进行验证，我们首先使用由所提出的生成模型产生合成产生的气道和血管以计算的相对误差，并直接评价与传统的方法相比CNR的精度。然后，在体内验证是通过分析所预测的用力呼气体积的百分比之间的关联在一秒钟（FEV1 \％）和PI10参数的值中，两个肺功能的公知的措施及气道疾病的药物，进行气道。对于容器，我们评估我们的小血管血液体积的估计和所述肺的一氧化碳弥散（弥散）容量之间的相关性。结果表明，卷积神经网络（细胞神经网络）提供用于准确测量与生理相关因素胸部CT图像的血管和气道有希望的方向。</font>
</div>


<hr>
<div id="paper23"> <b>23. Neuromorphologicaly-preserving Volumetric data encoding using VQ-VAE</b>  <a href="https://arxiv.org/pdf/2002.05692" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Tudosiu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Petru-Daniel Tudosiu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Varsavsky%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Varsavsky</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Shaw%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Shaw</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Graham%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mark Graham</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Nachev%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Parashkev Nachev</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ourselin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastien Ourselin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sudre%2C+C+H" target="_blank" rel="noopener" style="color:#0000EE;">Carole H. Sudre</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cardoso%2C+M+J" target="_blank" rel="noopener" style="color:#0000EE;">M. Jorge Cardoso</a><br>
<font size="3">
Abstract: The increasing efficiency and compactness of deep learning architectures, together with hardware improvements, have enabled the complex and high-dimensional modelling of medical volumetric data at higher resolutions. Recently, Vector-Quantised Variational Autoencoders (VQ-VAE) have been proposed as an efficient generative unsupervised learning approach that can encode images to a small percentage of their initial size, while preserving their decoded fidelity. Here, we show a VQ-VAE inspired network can efficiently encode a full-resolution 3D brain volume, compressing the data to $0.825\%$ of the original size while maintaining image fidelity, and significantly outperforming the previous state-of-the-art. We then demonstrate that VQ-VAE decoded images preserve the morphological characteristics of the original data through voxel-based morphology and segmentation experiments. Lastly, we show that such models can be pre-trained and then fine-tuned on different datasets without the introduction of bias. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提高效率和深度学习体系结构紧凑，与硬件的改进在一起，已经使医疗容积数据的复杂性和高维模型在更高的分辨率。近日，矢量量化变自动编码（VQ-VAE）已被提议作为一种高效生成无监督的学习方法，可以对图像进行编码，以他们的初始大小的一小部分，同时保留其解码的保真度。在这里，我们展示了一个VQ-VAE启发网络可以有效地编码全分辨率3D脑容量，数据压缩至$在0.825 \％的原始大小的$同时保持图像保真度，并显著超越以前的状态的最先进的。然后，我们证明了VQ-VAE解码图像通过基于体素的形态学和分割实验保留原始数据的形态特征。最后，我们表明，这种模式可以预先培训，并在不同的数据集，然后微调不引入偏见。</font>
</div>


<hr>
<div id="paper24"> <b>24. FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains</b>  <a href="https://arxiv.org/pdf/2002.05665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Harb%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeanine Harb</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=R%C3%A9b%C3%A9na%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Rébéna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chosidow%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raphaël Chosidow</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roblin%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Grégoire Roblin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Potarusov%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roman Potarusov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hajri%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hatem Hajri</a><br>
<font size="3">
Abstract: In the realm of autonomous transportation, there have been many initiatives for open-sourcing self-driving cars datasets, but much less for alternative methods of transportation such as trains. In this paper, we aim to bridge the gap by introducing FRSign, a large-scale and accurate dataset for vision-based railway traffic light detection and recognition. Our recordings were made on selected running trains in France and benefited from carefully hand-labeled annotations. An illustrative dataset which corresponds to ten percent of the acquired data to date is published in open source with the paper. It contains more than 100,000 images illustrating six types of French railway traffic lights and their possible color combinations, together with the relevant information regarding their acquisition such as date, time, sensor parameters, and bounding boxes. This dataset is published in open-source at the address \url{this https URL}. We compare, analyze various properties of the dataset and provide metrics to express its variability. We also discuss specific challenges and particularities related to autonomous trains in comparison to autonomous cars. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在自治区交通运输领域，已经出现了开放式采购自动驾驶汽车的数据集诸多举措，但对于运输的替代方法，如火车要少得多。在本文中，我们的目标是通过引入FRSign，一个大型和准确的数据集用于基于视觉的铁路交通灯检测和识别，以缩小差距。我们的记录作了在法国选择运行列车和精心手工标记注释中受益。其对应于所获取的数据的最新的百分之十的示例性数据集发表在开源与纸。它包含了超过10万个图像，说明六类法国铁路交通信号灯和他们可能的颜色组合，连同有关的信息对他们的收购，如日期，时间，传感器参数，和边框。该数据集是在地址\ {URL这HTTPS URL}刊登在开源。我们比较，分析数据集的各种属性和提供指标来表达它的可变性。我们还讨论具体的挑战，相较于自主车与自主列车特殊性。</font>
</div>


<hr>
<div id="paper25"> <b>25. Machines Learn Appearance Bias in Face Recognition</b>  <a href="https://arxiv.org/pdf/2002.05636" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Steed%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryan Steed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caliskan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aylin Caliskan</a><br>
<font size="3">
Abstract: We seek to determine whether state-of-the-art, black box face recognition techniques can learn first-impression appearance bias from human annotations. With FaceNet, a popular face recognition architecture, we train a transfer learning model on human subjects' first impressions of personality traits in other faces. We measure the extent to which this appearance bias is embedded and benchmark learning performance for six different perceived traits. In particular, we find that our model is better at judging a person's dominance based on their face than other traits like trustworthiness or likeability, even for emotionally neutral faces. We also find that our model tends to predict emotions for deliberately manipulated faces with higher accuracy than for randomly generated faces, just like a human subject. Our results lend insight into the manner in which appearance biases may be propagated by standard face recognition models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们试图确定是否国家的最先进的，黑盒脸部识别技术可以借鉴人类注释的第一印象，外观偏向。随着FaceNet，一个流行的脸部识别架构，我们培养对人的在其他面性格特征的第一印象是一个转移的学习模式。我们测量到这次出现偏差被嵌入的程度和基准学习表现为六个不同的感知特性。特别是，我们发现，我们的模型是基于判断自己的脸比其他性状一样可信性或喜爱程度一个人的主导地位，甚至情绪中性面孔更好。我们还发现，我们的模型往往会预测故意操纵面孔的情绪比为随机生成的面部更高的精确度，就像一个人的问题。我们的结果借洞察其外观偏见可以通过标准的面部识别模型来传播的方式。</font>
</div>


<hr>
<div id="paper26"> <b>26. Sparse and Structured Visual Attention</b>  <a href="https://arxiv.org/pdf/2002.05556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P+H" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Henrique Martins</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niculae%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vlad Niculae</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marinho%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zita Marinho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">André Martins</a><br>
<font size="3">
Abstract: Visual attention mechanisms are widely used in multimodal tasks, such as image captioning and visual question answering (VQA). One drawback of softmax-based attention mechanisms is that they assign probability mass to all image regions, regardless of their adjacency structure and of their relevance to the text. In this paper, to better link the image structure with the text, we replace the traditional softmax attention mechanism with two alternative sparsity-promoting transformations: sparsemax, which is able to select the relevant regions only (assigning zero weight to the rest), and a newly proposed Total-Variation Sparse Attention (TVmax), which further encourages the joint selection of adjacent spatial locations. Experiments in image captioning and VQA, using both LSTM and Transformer architectures, show gains in terms of human-rated caption quality, attention relevance, and VQA accuracy, with improved interpretability. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉注意机制被广泛应用于多任务，如图像字幕和视觉问答（VQA）。基于SOFTMAX注意力机制的一个缺点是它们分配概率质量到所有的图像区域，无论其邻接结构及其相关的文字。在本文中，以更好地链接与文本的图像结构，我们更换两种可供选择的稀疏性，促进转变传统的SOFTMAX注意机制：sparsemax，这是能够选择相关区域中仅仅（分配权重为零的其余部分），和新提出的总的变化率稀疏注意（TVmax），其进一步鼓励相邻的空间位置的联合选择。在图像字幕和VQA，同时使用LSTM和变压器的架构实验，显示人类额定字幕质量，重视相关性，准确性VQA，具有完善的可解释性方面的收益。</font>
</div>


<hr>
<div id="paper27"> <b>27. Superpixel Image Classification with Graph Attention Networks</b>  <a href="https://arxiv.org/pdf/2002.05544" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Avelar%2C+P+H+C" target="_blank" rel="noopener" style="color:#0000EE;">Pedro H. C. Avelar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tavares%2C+A+R" target="_blank" rel="noopener" style="color:#0000EE;">Anderson R. Tavares</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=da+Silveira%2C+T+L+T" target="_blank" rel="noopener" style="color:#0000EE;">Thiago L. T. da Silveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+C+R" target="_blank" rel="noopener" style="color:#0000EE;">Cláudio R. Jung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lamb%2C+L+C" target="_blank" rel="noopener" style="color:#0000EE;">Luís C. Lamb</a><br>
<font size="3">
Abstract: This document reports the use of Graph Attention Networks for classifying oversegmented images, as well as a general procedure for generating oversegmented versions of image-based datasets. The code and learnt models for/from the experiments are available on github. The experiments were ran from June 2019 until December 2019. We obtained better results than the baseline models that uses geometric distance-based attention by using instead self attention, in a more sparsely connected graph network. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文报道了使用图形注意网络用于生成基于图像的数据集oversegmented版本oversegmented图像，以及作为一般程序进行分类。用于/代码，学习模型从实验都可以在GitHub上。该实验是跑到离2019年6月至2019年12月，我们获得比使用，而不是自我的关注，更稀疏连通图网络采用基于几何距离，注意基线模型更好的效果。</font>
</div>


<hr>
<div id="paper28"> <b>28. Deep Learning-based End-to-end Diagnosis System for Avascular Necrosis  of Femoral Head</b>  <a href="https://arxiv.org/pdf/2002.05536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tian%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hua Tian</a><br>
<font size="3">
Abstract: As the first diagnostic imaging modality of avascular necrosis of the femoral head (AVNFH), accurately staging AVNFH from a plain radiograph is critical and challenging for orthopedists. Thus, we propose a deep learning-based AVNFH diagnosis system (AVN-net). The proposed AVN-net reads plain radiographs of the pelvis, conducts diagnosis, and visualizes results automatically. Deep convolutional neural networks are trained to provide an end-to-end diagnosis solution, covering femoral head detection, exam-view/sides identification, AVNFH diagnosis, and key clinical note generation subtasks. AVN-net is able to obtain state-of-the-art testing AUC of 0.95 (95% CI: 0.92-0.98) in AVNFH detection and significantly greater F1 scores (p<0.01) 1 4 than less-to-moderately experienced orthopedists in all diagnostic tests. furthermore, two real-world pilot studies were conducted for diagnosis support and education assistance, respectively, to assess the utility of avn-net. experimental results are promising. with avn-net as a reference, accuracy consistency considerably improved while requiring only time. students self-studying avnfh using can learn better faster control group. best our knowledge, this study is first research on prospective use deep learning-based system by conducting representing application scenarios. we have demonstrated that proposed achieves expert-level performance, provides efficient clinical decision-making, effectively passes experience students. < font>
<br>
<font size="2" style="line-height:30px;">
摘要：股骨头缺血性坏死（AVNFH）的所述第一诊断成像模态，准确地从平片分级AVNFH是关键的，并且对骨科挑战。因此，我们提出了一个深刻的学习型AVNFH诊断系统（AVN-网）。所提出的AVN网自动读取骨盆，行为诊断的平片，以及可视化的结果。深卷积神经网络被训练，以提供端至端诊断溶液，覆盖股骨头检测，考试视点/边识别，AVNFH诊断，和关键临床音符生成的子任务。 AVN网能够获得国家的最先进的测试0.95的AUC（95％CI：0.92-0.98）中AVNFH检测和显著更大F1分数（P <0.01）小于到适度在所有经验骨科诊断测试。此外，两个真实世界的试点研究，诊断支持和教育协助下进行，分别评估avn网的效用。实验结果是有希望的。与avn网诊断，因为所有的骨科的基准，诊断的准确性和一致性，同时仅需要的时间的1 4大幅度地改善。学生自主学习使用avn网能够更好地学习和速度比对照组avnfh诊断。据我们所知，这研究是开展代表现实世界的应用场景的两个试点研究，在未来的使用了avnfh深基础的学习诊断系统的第一个研究。我们已经证明，所提出的avn网达到专家级avnfh诊断性能，提供了在临床决策的有效支持，并有效地传递临床经验的学生。< font>
</0.01）小于到适度在所有经验骨科诊断测试。此外，两个真实世界的试点研究，诊断支持和教育协助下进行，分别评估avn网的效用。实验结果是有希望的。与avn网诊断，因为所有的骨科的基准，诊断的准确性和一致性，同时仅需要的时间的1></font></0.01)></font></div>


<hr>
<div id="paper29"> <b>29. Abnormal respiratory patterns classifier may contribute to large-scale  screening of people infected with COVID-19 in an accurate and unobtrusive  manner</b>  <a href="https://arxiv.org/pdf/2002.05534" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunlu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Menghan Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingli Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao-Ping Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guangtao Zhai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Yao</a><br>
<font size="3">
Abstract: Research significance: During the epidemic prevention and control period, our study can be helpful in prognosis, diagnosis and screening for the patients infected with COVID-19 (the novel coronavirus) based on breathing characteristics. According to the latest clinical research, the respiratory pattern of COVID-19 is different from the respiratory patterns of flu and the common cold. One significant symptom that occurs in the COVID-19 is Tachypnea. People infected with COVID-19 have more rapid respiration. Our study can be utilized to distinguish various respiratory patterns and our device can be preliminarily put to practical use. Demo videos of this method working in situations of one subject and two subjects can be downloaded online. Research details: Accurate detection of the unexpected abnormal respiratory pattern of people in a remote and unobtrusive manner has great significance. In this work, we innovatively capitalize on depth camera and deep learning to achieve this goal. The challenges in this task are twofold: the amount of real-world data is not enough for training to get the deep model; and the intra-class variation of different types of respiratory patterns is large and the outer-class variation is small. In this paper, considering the characteristics of actual respiratory signals, a novel and efficient Respiratory Simulation Model (RSM) is first proposed to fill the gap between the large amount of training data and scarce real-world data. Subsequently, we first apply a GRU neural network with bidirectional and attentional mechanisms (BI-AT-GRU) to classify 6 clinically significant respiratory patterns (Eupnea, Tachypnea, Bradypnea, Biots, Cheyne-Stokes and Central-Apnea). The proposed deep model and the modeling ideas have the great potential to be extended to large scale applications such as public places, sleep scenario, and office environment. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：研究意义：在疫情防控期间，我们的研究可以在预后，有助于诊断和筛查感染基于呼吸特性COVID-19（该新型冠状病毒）的患者。根据最新的临床研究，COVID-19的呼吸模式是由流感的呼吸模式和普通感冒不同。发生在COVID-19的一个显著的症状是呼吸急促。感染COVID-19的人有更多的呼吸急促。我们的研究可以用来区分不同的呼吸模式和我们的设备可以预先投入实际使用。这种方法在一个主体和两个科目的情况下工作的演示视频可以在网上下载。研究细节：人在一个偏僻的和不显眼的方式意想不到的异常呼吸模式的准确的检测具有重要的意义。在这项工作中，我们创新性地利用深度相机和深度学习到实现这一目标。此任务中的挑战是双重的：真实世界的数据量是不够的训练得到深层模型;和不同类型的呼吸模式的类内变化较大和外级变化小。在本文中，考虑到实际的呼吸信号，一种新颖且有效的呼吸仿真模型（RSM）的特性被首次提出以填充大量的训练数据和稀缺真实世界的数据之间的间隙。随后，我们首先应用具有双向和注意力机制（BI-AT-GRU）一GRU神经网络分类6种临床显著的呼吸模式（正常呼吸，呼吸急促，Bradypnea，Biots，陈 - 施氏及中环呼吸暂停）。所提出的深层模型和建模的思想有很大的潜力可扩展到大规模应用，如公共场所，睡眠情况，以及办公环境。</font>
</div>


<hr>
<div id="paper30"> <b>30. Real or Not Real, that is the Question</b>  <a href="https://arxiv.org/pdf/2002.05512" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiangli%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanbo Xiangli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yubin Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Dai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Loy%2C+C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Change Loy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dahua Lin</a><br>
<font size="3">
Abstract: While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper we generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. Compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN architecture to generate realistic images at 1024*1024 resolution when trained from scratch. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然生成对抗网络（GAN）已经在各种主题被广泛采用，在本文中，我们通过治疗真实性为可以从多个角度来估计一个随机变量概括的标准GAN到一个新的视角。在此广义框架中，被称为RealnessGAN，鉴别器输出一个分发真实性的量度。虽然RealnessGAN分享相似的理论保证与标准甘，它提供了对抗的学习更多的见解。相比于多基线，RealnessGAN提供了更强的指导发电机，实现对合成和真实世界的数据集的改进。此外，它使基本DCGAN架构在1024 * 1024分辨率从头开始训练的时候，产生逼真的图像。</font>
</div>


<hr>
<div id="paper31"> <b>31. MLFcGAN: Multi-level Feature Fusion based Conditional GAN for Underwater  Image Color Correction</b>  <a href="https://arxiv.org/pdf/2002.05333" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaodong Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Gao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhi Gao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chen%2C+B+M" target="_blank" rel="noopener" style="color:#0000EE;">Ben M. Chen</a><br>
<font size="3">
Abstract: Color correction for underwater images has received increasing interests, due to its critical role in facilitating available mature vision algorithms for underwater scenarios. Inspired by the stunning success of deep convolutional neural networks (DCNNs) techniques in many vision tasks, especially the strength in extracting features in multiple scales, we propose a deep multi-scale feature fusion net based on the conditional generative adversarial network (GAN) for underwater image color correction. In our network, multi-scale features are extracted first, followed by augmenting local features on each scale with global features. This design was verified to facilitate more effective and faster network learning, resulting in better performance in both color correction and detail preservation. We conducted extensive experiments and compared with the state-of-the-art approaches quantitatively and qualitatively, showing that our method achieves significant improvements. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：颜色校正水下图像已经受到越来越多的利益，由于在促进现有成熟的视觉算法用于水下场景中的关键作用。深卷积神经网络（DCNNs）在许多视觉任务的技术，尤其是在多个尺度提取特征的实力令人惊叹的成功的启发，我们提出了一个深刻的多尺度特征融合基础条件生成对抗网络（GAN）的净水下图像颜色校正。在我们的网络，多尺度特征首先提取，然后在全球各功能扩充规模的局部特征。这种设计进行了验证，以促进更有效和更快的网络学习，导致这两个色彩校正和细节保持更好的性能。我们进行了广泛的实验，相比定量和定性的方法，这表明我们的方法实现显著的改善最先进的国家的的。</font>
</div>


<hr>
<div id="paper32"> <b>32. Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral  Segmentation of Rock micro-CT Images</b>  <a href="https://arxiv.org/pdf/2002.05322" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Da+Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ying Da Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Shabaninejad%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehdi Shabaninejad</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Armstrong%2C+R+T" target="_blank" rel="noopener" style="color:#0000EE;">Ryan T. Armstrong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mostaghimi%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peyman Mostaghimi</a><br>
<font size="3">
Abstract: Segmentation of 3D micro-Computed Tomographic uCT) images of rock samples is essential for further Digital Rock Physics (DRP) analysis, however, conventional methods such as thresholding, watershed segmentation, and converging active contours are susceptible to user-bias. Deep Convolutional Neural Networks (CNNs) have produced accurate pixelwise semantic segmentation results with natural images and $\mu$CT rock images, however, physical accuracy is not well documented. The performance of 4 CNN architectures is tested for 2D and 3D cases in 10 configurations. Manually segmented uCT images of Mt. Simon Sandstone are treated as ground truth and used as training and validation data, with a high voxelwise accuracy (over 99%) achieved. Downstream analysis is then used to validate physical accuracy. The topology of each segmented phase is calculated, and the absolute permeability and multiphase flow is modelled with direct simulation in single and mixed wetting cases. These physical measures of connectivity, and flow characteristics show high variance and uncertainty, with models that achieve 95\%+ in voxelwise accuracy possessing permeabilities and connectivities orders of magnitude off. A new network architecture is also introduced as a hybrid fusion of U-net and ResNet, combining short and long skip connections in a Network-in-Network configuration. The 3D implementation outperforms all other tested models in voxelwise and physical accuracy measures. The network architecture and the volume fraction in the dataset (and associated weighting), are factors that not only influence the accuracy trade-off in the voxelwise case, but is especially important in training a physically accurate model for segmentation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：3D的分割微计算机断层UCT）岩石样品的图像是用于进一步数字岩石物理（DRP）分析必不可少的，然而，常规方法如阈值，分水岭分割，并会聚主动轮廓很容易受到用户偏置。深卷积神经网络（细胞神经网络）已经产生与自然图像和$ \亩$ CT岩石的图像，然而，物理精度不会有据可查的准确按像素语义分割结果。的4 CNN架构性能为2D和3D的情况下在10个配置测试。人工分割山UCT图片西蒙砂岩被视为基础事实和用作训练和验证数据，以实现高的精度voxelwise（超过99％）。然后向下游分析用于验证物理精度。每个分段的相位的拓扑计算，并且绝对渗透率和多相流建模与单一和混合润湿的情况下直接模拟。连接的这些物理措施和流动特性表现出较大差异性和不确定性，与在voxelwise准确性拥有幅度的渗透性和连通性关闭订单达到95 \％+车型。一个新的网络架构也被引入作为U型网和RESNET，结合短期和长期跳过在以网络为在网络配置连接的杂合融合。三维实现优于所有其他测试车型voxelwise和物理精度的措施。的网络体系结构和在数据集（和相关联的权重）的体积分数，是不仅影响精度的折衷在voxelwise情况下因素，但是在训练物理上精确模型分割尤其重要。</font>
</div>


<hr>
<div id="paper33"> <b>33. A Provably Robust Multiple Rotation Averaging Scheme for SO(2)</b>  <a href="https://arxiv.org/pdf/2002.05299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/math?searchtype=author&query=Maunu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tyler Maunu</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Lerman%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gilad Lerman</a><br>
<font size="3">
Abstract: We give adversarial robustness results for synchronization on the rotation group over $\mathbb{R}^2$, $\mathrm{SO}(2)$. In particular, we consider an adversarial corruption setting, where an adversary can choose which measurements to corrupt as well as what to corrupt them to. In this setting, we first show that some common nonconvex formulations, which are categorized as "multiple rotation averaging", may fail. We then discuss a new fast algorithm, called Trimmed Averaging Synchronization, which has exact recovery and linear convergence up to an outlier percentage of $1/4$. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：给予超过$ \ mathbb {R} ^ 2，$ \ mathrm {SO}（2）$上旋转组同步对抗性鲁棒性的结果。特别是，我们考虑一个对抗性腐败的设置，其中一个对手可以选择测量腐败是什么，以及腐败他们。在这种背景下，我们首先表明，一些常见的非凸制剂，其被归类为“多回转平均”，可能会失败。然后，我们讨论了一个新的快速算法，称为修剪平均化同步，其中有确切的恢复和线性收敛高达$ 1/4 $离群值百分比。</font>
</div>


<hr>
<div id="paper34"> <b>34. Geom-GCN: Geometric Graph Convolutional Networks</b>  <a href="https://arxiv.org/pdf/2002.05287" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pei%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongbin Pei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bingzhe Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K+C" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Chen-Chuan Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Lei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Yang</a><br>
<font size="3">
Abstract: Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed GeomGCN, to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：消息传递神经网络（MPNNs）已经在各种实际应用中已成功地应用于表示学习上的图表。然而，MPNNs'聚合的两个根本性的弱点限制了他们的代表图结构数据的能力：失去街区节点的结构信息，缺乏捕捉到远距离的依赖于异配图的能力。很少有研究发现从不同的角度弱点。从经典的神经网络和网络上的几何形状的观察结果，我们提出了图形神经网络克服了两个弱点新颖的几何集成方案。后面的基本思想是在曲线图上的聚合可以从一个连续的空间图形底层受益。建议的聚合方案是排列不变和由三个模块组成，节点嵌入，结构附近，以及两级聚集。我们还提出在图形卷积网络计划的实施，被称为GeomGCN，对图形进行式学习。实验结果表明，所提出的Geom-GCN上大量的图形的开放数据集的实现状态的最先进的性能。代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper35"> <b>35. HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine  Learning Models</b>  <a href="https://arxiv.org/pdf/2002.05271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianwen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alexander%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Alexander</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pegg%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jack Pegg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huamin Qu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Min Chen</a><br>
<font size="3">
Abstract: In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a "concept" or "feature" may benefit or hinder a ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing data is transformed to a visual representation for rapid observation of the conclusions and the logical flow between the testing data and hypotheses.We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了实现的机器学习（ML）模型基于假设的评估可视化分析工具。我们描述了一种新ML-测试框架，结合有关的多个假设的结论，逻辑推理传统的统计假设检验（在实证研究常用）。该框架定义了用于测试多个假设是否以及如何对一个“概念”或“功能”一些额外的信息可能会受益或阻碍ML模型控制的配置。因为推理多个假设并不总是简单的，我们提供HypoML作为视觉分析工具，与其中，多线程测试数据被变换为结论的快速观察和测试数据和假设之间的逻辑流程的可视化表示。我们应用HypoML到多个虚拟的概念，展示了可视化分析的直观解释的性质。</font>
</div>


<hr>
<div id="paper36"> <b>36. Graph Similarity Using PageRank and Persistent Homology</b>  <a href="https://arxiv.org/pdf/2002.05158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hajij%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mustafa Hajij</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Munch%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elizabeth Munch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Rosen</a><br>
<font size="3">
Abstract: The PageRank of a graph is a scalar function defined on the node set of the graph which encodes nodes centrality information of the graph. In this work, we utilize the PageRank function on the lower-star filtration of the graph as input to persistent homology to study the problem of graph similarity. By representing each graph as a persistence diagram, we can then compare outputs using the bottleneck distance. We show the effectiveness of our method by utilizing it on two shape mesh datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一个图的PageRank是在节点集的图表，其编码节点的曲线图的中心性信息的定义的标量函数。在这项工作中，我们利用图表上的输入，持续的同源性研究图形的相似问题的低星级过滤的PageRank功能。由表示每个图形作为持久图，我们可以然后比较输出使用所述瓶颈距离。我们利用这两个状的网数据集显示了该方法的有效性。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-13</title>
    <url>/2020/02/14/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-13/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Rembrandts and Robots: Using Neural Networks to Explore Authorship in  Painting <a href="https://arxiv.org/pdf/2002.05107" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Component Analysis for Visual Question Answering Architectures <a href="https://arxiv.org/pdf/2002.05104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> AlignNet: A Unifying Approach to Audio-Visual Alignment <a href="https://arxiv.org/pdf/2002.05070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Detect and Correct Bias in Multi-Site Neuroimaging Datasets <a href="https://arxiv.org/pdf/2002.05049" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Intra-Camera Supervised Person Re-Identification <a href="https://arxiv.org/pdf/2002.05046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Learning light field synthesis with Multi-Plane Images: scene encoding  as a recurrent segmentation task <a href="https://arxiv.org/pdf/2002.05028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Hi-Net: Hybrid-fusion Network for Multi-modal MR Image Synthesis <a href="https://arxiv.org/pdf/2002.05000" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Real-Time Semantic Background Subtraction <a href="https://arxiv.org/pdf/2002.04993" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Hierarchical Auto-Regressive Model for Image Compression Incorporating  Object Saliency and a Deep Perceptual Loss <a href="https://arxiv.org/pdf/2002.04988" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Towards Precise Intra-camera Supervised Person Re-identification <a href="https://arxiv.org/pdf/2002.04932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> A Zero-Shot based Fingerprint Presentation Attack Detection System <a href="https://arxiv.org/pdf/2002.04908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Bi-Directional Generation for Unsupervised Domain Adaptation <a href="https://arxiv.org/pdf/2002.04869" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Analysis Of Multi Field Of View Cnn And Attention Cnn On H&amp;E Stained  Whole-slide Images On Hepatocellular Carcinoma <a href="https://arxiv.org/pdf/2002.04836" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> End-to-End Face Parsing via Interlinked Convolutional Neural Networks <a href="https://arxiv.org/pdf/2002.04831" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Uniform Interpolation Constrained Geodesic Learning on Data Manifold <a href="https://arxiv.org/pdf/2002.04829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Deep-HR: Fast Heart Rate Estimation from Face Video Under Realistic  Conditions <a href="https://arxiv.org/pdf/2002.04821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> A Visual-inertial Navigation Method for High-Speed Unmanned Aerial  Vehicles <a href="https://arxiv.org/pdf/2002.04791" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> MFFW: A new dataset for multi-focus image fusion <a href="https://arxiv.org/pdf/2002.04780" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Efficient Training of Deep Convolutional Neural Networks by Augmentation  in Embedding Space <a href="https://arxiv.org/pdf/2002.04776" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Progressive Object Transfer Detection <a href="https://arxiv.org/pdf/2002.04741" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Improving Place Recognition Using Dynamic Object Detection <a href="https://arxiv.org/pdf/2002.04698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Learning spatio-temporal representations with temporal squeeze pooling <a href="https://arxiv.org/pdf/2002.04685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Object Detection as a Positive-Unlabeled Problem <a href="https://arxiv.org/pdf/2002.04672" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Validating uncertainty in medical image translation <a href="https://arxiv.org/pdf/2002.04639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Finding novelty with uncertainty <a href="https://arxiv.org/pdf/2002.04626" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Patternless Adversarial Attacks on Video Recognition Networks <a href="https://arxiv.org/pdf/2002.05123" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> From IC Layout to Die Photo: A CNN-Based Data-Driven Approach <a href="https://arxiv.org/pdf/2002.04967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Synaptic Integration of Spatiotemporal Features with a Dynamic  Neuromorphic Processor <a href="https://arxiv.org/pdf/2002.04924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Machine-Learning-Based Multiple Abnormality Prediction with Large-Scale  Chest Computed Tomography Volumes <a href="https://arxiv.org/pdf/2002.04752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for  Healthcare <a href="https://arxiv.org/pdf/2002.04700" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> fastai: A Layered API for Deep Learning <a href="https://arxiv.org/pdf/2002.04688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> A Non-Intrusive Correction Algorithm for Classification Problems with  Corrupted Data <a href="https://arxiv.org/pdf/2002.04658" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Neuroevolution of Neural Network Architectures Using CoDeepNEAT and  Keras <a href="https://arxiv.org/pdf/2002.04634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Rembrandts and Robots: Using Neural Networks to Explore Authorship in  Painting</b>  <a href="https://arxiv.org/pdf/2002.05107" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+S+J" target="_blank" rel="noopener" style="color:#0000EE;">Steven J. Frank</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+A+M" target="_blank" rel="noopener" style="color:#0000EE;">Andrea M. Frank</a><br>
<font size="3">
Abstract: We use convolutional neural networks to analyze authorship questions surrounding works of representational art. Trained on the works of an artist under study and visually comparable works of other artists, our system can identify forgeries and provide attributions. Our system can also assign classification probabilities within a painting, revealing mixed authorship and identifying regions painted by different hands. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们使用卷积神经网络来分析周围的代表性艺术作品的著作权问题。培训了一个艺术家的所研究的作品和其他艺术家的视觉作品相媲美，我们的系统可以识别伪造并提供归属。我们的系统可以画内还可以指派分类概率，揭示了混合署名权，并确定由不同的双手涂上地区。</font>
</div>


<hr>
<div id="paper2"> <b>2. Component Analysis for Visual Question Answering Architectures</b>  <a href="https://arxiv.org/pdf/2002.05104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kolling%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Camila Kolling</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wehrmann%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jônatas Wehrmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barros%2C+R+C" target="_blank" rel="noopener" style="color:#0000EE;">Rodrigo C. Barros</a><br>
<font size="3">
Abstract: Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在计算机视觉和自然语言处理的最新研究进展介绍，铺平解决AI完全问题的方式新颖任务。其中的一个任务被称为视觉答疑（VQA）。一个VQA系统必须采取的图像和关于图像的自由形式的，开放式的自然语言问题，而产生的自然语言回答作为输出。这样的任务已经从科学界，其产生的旨在提高VQA预测精度接近过多的高度关注。他们中的大多数包括三个主要组成部分：（一）独立表示学习图像和问题; （ⅱ）特征融合因此模型可以使用来自两个源的信息来回答问题视觉;及（iii）在自然语言的正确答案的产生。有了这么多的方法被新近推出的，它变得不明朗的各成分的模型的最终性能的真正的贡献。本文的主要目的是提供关于每个组件的VQA模型的影响进行全面分析。我们广泛组实验涵盖视觉和文本元素，以及这些表象的融合，注重机制的形式组合。我们的主要贡献是确定培训VQA车型的核心部件，以最大限度地提高其预测性能。</font>
</div>


<hr>
<div id="paper3"> <b>3. AlignNet: A Unifying Approach to Audio-Visual Alignment</b>  <a href="https://arxiv.org/pdf/2002.05070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianren Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhaoyuan Fang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Zhao</a><br>
<font size="3">
Abstract: We present AlignNet, a model that synchronizes videos with reference audios under non-uniform and irregular misalignments. AlignNet learns the end-to-end dense correspondence between each frame of a video and an audio. Our method is designed according to simple and well-established principles: attention, pyramidal processing, warping, and affinity function. Together with the model, we release a dancing dataset Dance50 for training and evaluation. Qualitative, quantitative and subjective evaluation results on dance-music alignment and speech-lip alignment demonstrate that our method far outperforms the state-of-the-art methods. Project video and code are available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前AlignNet，即同步与下不均匀和不规则的错位参考音频视频的模式。 AlignNet得知一个视频的每个帧和音频之间的端至端致密的对应关系。注意，金字塔形处理，翘曲和亲和功能：我们的方法是根据简单的和行之有效的原则设计的。连同模型，我们发布一个跳舞数据集Dance50的培训和考核。定性，舞蹈，音乐对准和语音唇对准定量和主观评价结果表明，我们的方法远优于国家的最先进的方法。项目视频和代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper4"> <b>4. Detect and Correct Bias in Multi-Site Neuroimaging Datasets</b>  <a href="https://arxiv.org/pdf/2002.05049" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wachinger%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Wachinger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rieckmann%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Rieckmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%C3%B6lsterl%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastian Pölsterl</a><br>
<font size="3">
Abstract: The desire to train complex machine learning algorithms and to increase the statistical power in association studies drives neuroimaging research to use ever-larger datasets. The most obvious way to increase sample size is by pooling scans from independent studies. However, simple pooling is often ill-advised as selection, measurement, and confounding biases may creep in and yield spurious correlations. In this work, we combine 35,320 magnetic resonance images of the brain from 17 studies to examine bias in neuroimaging. In the first experiment, Name That Dataset, we provide empirical evidence for the presence of bias by showing that scans can be correctly assigned to their respective dataset with 71.5% accuracy. Given such evidence, we take a closer look at confounding bias, which is often viewed as the main shortcoming in observational studies. In practice, we neither know all potential confounders nor do we have data on them. Hence, we model confounders as unknown, latent variables. Kolmogorov complexity is then used to decide whether the confounded or the causal model provides the simplest factorization of the graphical model. Finally, we present methods for dataset harmonization and study their ability to remove bias in imaging features. In particular, we propose an extension of the recently introduced ComBat algorithm to control for global variation across image features, inspired by adjusting for population stratification in genetics. Our results demonstrate that harmonization can reduce dataset-specific information in image features. Further, confounding bias can be reduced and even turned into a causal relationship. However, harmonziation also requires caution as it can easily remove relevant subject-specific information. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：培养复杂的机器学习算法，提高了统计功率关联研究神经影像学驱动器研究使用越来越大的数据集的愿望。增加样本规模最明显的方法是由独立的研究集中扫描。然而，简单的池通常不明智作为选择，测量和混杂偏差可能在蠕变和屈服虚假相关。在这项工作中，我们结合大脑的35320个磁共振图像从17项研究审查神经影像学的偏见。在第一个实验中，名称该数据集，我们提供了由表示扫描可以正确地分配给它们各自的数据集与71.5％的准确度存在偏差的经验证据。鉴于这些证据，我们就在混杂的偏见，这通常被视为观察性研究的主要缺点一探究竟。在实践中，我们不知道，所有的潜在混杂因素我们也没有对他们的数据。因此，我们的模型混杂因素未知的，潜在变量。然后Kolmogorov复杂被用来决定是否混淆或因果模型提供图形模型的最简单的因式分解。最后，我们对数据集协调本发明的方法，并研究其去除影像学特征偏差的能力。特别是，我们提出了最近推出的打击算法的扩展来控制整个图像的功能，通过调整遗传学群体分层的启发全球变化。我们的研究结果表明，统一可以降低图像特征数据集的特定信息。另外，混杂偏压可以减少，甚至变成了因果关系。然而，harmonziation也需要谨慎，因为它可以很容易地删除相关对象特定信息。</font>
</div>


<hr>
<div id="paper5"> <b>5. Intra-Camera Supervised Person Re-Identification</b>  <a href="https://arxiv.org/pdf/2002.05046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangping Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiatian Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minxian Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morerio%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pietro Morerio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Murino%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vittorio Murino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shaogang Gong</a><br>
<font size="3">
Abstract: Existing person re-identification (re-id) methods mostly exploit a large set of cross-camera identity labelled training data. This requires a tedious data collection and annotation process, leading to poor scalability in practical re-id applications. On the other hand unsupervised re-id methods do not need identity label information, but they usually suffer from much inferior and insufficient model performance. To overcome these fundamental limitations, we propose a novel person re-identification paradigm based on an idea of independent per-camera identity annotation. This eliminates the most time-consuming and tedious inter-camera identity labelling process, significantly reducing the amount of human annotation efforts. Consequently, it gives rise to a more scalable and more feasible setting, which we call Intra-Camera Supervised (ICS) person re-id, for which we formulate a Multi-tAsk mulTi-labEl (MATE) deep learning method. Specifically, MATE is designed for self-discovering the cross-camera identity correspondence in a per-camera multi-task inference framework. Extensive experiments demonstrate the cost-effectiveness superiority of our method over the alternative approaches on three large person re-id datasets. For example, MATE yields 88.7% rank-1 score on Market-1501 in the proposed ICS person re-id setting, significantly outperforming unsupervised learning models and closely approaching conventional fully supervised learning competitors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有人员重新鉴定（重新编号）方法主要是利用大集交叉的摄像机标识标记的训练数据。这需要繁琐的数据收集和注释过程，从而导致实际的再ID应用程序可扩展性差。在另一方面监督的重新编号方法不需要身份标签信息，但它们通常是从远不如和不足模型的性能受到影响。为了克服这些基本的限制，提出了一种基于独立的每个摄像机的身份注解的想法新颖的人重新鉴定的范例。这消除了最耗时和繁琐的摄像装置间的身份标记过程，显著减少人为批注努力的量。因此，产生了更多的可扩展性和更可行的设置，我们称之为摄像机内监督（ICS）的人重新编号，为此我们制定一个多任务多标签（MATE）深的学习方法。具体而言，MATE被设计用于在每个摄像机多任务推理框架自发现横相机身份对应。大量的实验证明我们的方法超过三个大的人重新编号数据集替代方法的成本效益优势。例如，MATE产生在建议ICS人再ID设置，以市场为1501 88.7％秩1的比分，显著跑赢无监督学习模式，并密切接近传统的完全监督学习的竞争对手。</font>
</div>


<hr>
<div id="paper6"> <b>6. Learning light field synthesis with Multi-Plane Images: scene encoding  as a recurrent segmentation task</b>  <a href="https://arxiv.org/pdf/2002.05028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=V%C3%B6lker%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomás Völker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Boisson%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillaume Boisson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chupeau%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bertrand Chupeau</a><br>
<font size="3">
Abstract: In this paper we address the problem of view synthesis from large baseline light fields, by turning a sparse set of input views into a Multi-plane Image (MPI). Because available datasets are scarce, we propose a lightweight network that does not require extensive training. Unlike latest approaches, our model does not learn to estimate RGB layers but only encodes the scene geometry within MPI alpha layers, which comes down to a segmentation task. A Learned Gradient Descent (LGD) framework is used to cascade the same convolutional network in a recurrent fashion in order to refine the volumetric representation obtained. Thanks to its low number of parameters, our model trains successfully on a small light field video dataset and provides visually appealing results. It also exhibits convenient generalization properties regarding both the number of input views, the number of depth planes in the MPI, and the number of refinement iterations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们解决视图合成的问题从大基线光场，通过转动稀疏集合的输入视图到多平面图像（MPI）。由于可用的数据集是稀缺的，我们建议，不需要大量的培训一个轻量级的网络。与最新的方法，我们的模型不学习估算RGB层，但仅编码MPI阿尔法层内的场景几何，这可以归结为一个细分任务。习得梯度下降（LGD）框架用于级联中一个反复出现的方式相同的卷积网络，以便改进所获得的体积表示。由于其数量少的参数，我们的模型成功列车小亮场图像数据集，并提供视觉吸引力的结果。这也显示出关于输入两种意见的数量，深度平面中的MPI的数量，和改进的迭代次数方便的泛化性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. Hi-Net: Hybrid-fusion Network for Multi-modal MR Image Synthesis</b>  <a href="https://arxiv.org/pdf/2002.05000" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huazhu Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianbing Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Ling Shao</a><br>
<font size="3">
Abstract: Magnetic resonance imaging (MRI) is a widely used neuroimaging technique that can provide images of different contrasts (i.e., modalities). Fusing this multi-modal data has proven particularly effective for boosting model performance in many tasks. However, due to poor data quality and frequent patient dropout, collecting all modalities for every patient remains a challenge. Medical image synthesis has been proposed as an effective solution to this, where any missing modalities are synthesized from the existing ones. In this paper, we propose a novel Hybrid-fusion Network (Hi-Net) for multi-modal MR image synthesis, which learns a mapping from multi-modal source images (i.e., existing modalities) to target images (i.e., missing modalities). In our Hi-Net, a modality-specific network is utilized to learn representations for each individual modality, and a fusion network is employed to learn the common latent representation of multi-modal data. Then, a multi-modal synthesis network is designed to densely combine the latent representation with hierarchical features from each modality, acting as a generator to synthesize the target images. Moreover, a layer-wise multi-modal fusion strategy is presented to effectively exploit the correlations among multiple modalities, in which a Mixed Fusion Block (MFB) is proposed to adaptively weight different fusion strategies (i.e., element-wise summation, product, and maximization). Extensive experiments demonstrate that the proposed model outperforms other state-of-the-art medical image synthesis methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：磁共振成像（MRI）是一种广泛使用的神经成像技术，其可以提供不同的对比度（即，模式）的图像。这个融合多模态数据已经证明，在许多任务提高模型的性能特别有效。然而，由于不良的数据质量和频繁的患者差，收集所有方式为每位患者仍然是一个挑战。医用图像合成已经被提出作为一种有效的解决方案，这一点，其中任何缺失的方式从现有的合成。在本文中，我们提出一种用于多模态MR图像合成的新型混合融合网络（高净），该学习到的多模态的源图像的映射（即，现有的模式）到目标图像（即，丢失的方式） 。在我们的Hi-网，一个特定的模态网络被用于学习的表示为每个单独的模式，并且采用的融合网络学习多模态数据的共同潜表示。然后，多模式合成网被设计成密集地结合具有分级特性的潜表示从每个模态，作为发电机来合成目标图像。此外，逐层多模态融合策略被呈现给有效地利用其中混合融合块（MFB）提出了自适应重量不同融合策略（即，逐元素求和，产品和多个模态之间的相关性，最大化）。广泛的实验表明，该模型优于其他国家的最先进的医用图像的合成方法。</font>
</div>


<hr>
<div id="paper8"> <b>8. Real-Time Semantic Background Subtraction</b>  <a href="https://arxiv.org/pdf/2002.04993" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cioppa%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anthony Cioppa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Droogenbroeck%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marc Van Droogenbroeck</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Braham%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marc Braham</a><br>
<font size="3">
Abstract: Semantic background subtraction SBS has been shown to improve the performance of most background subtraction algorithms by combining them with semantic information, derived from a semantic segmentation network. However, SBS requires high-quality semantic segmentation masks for all frames, which are slow to compute. In addition, most state-of-the-art background subtraction algorithms are not real-time, which makes them unsuitable for real-world applications. In this paper, we present a novel background subtraction algorithm called Real-Time Semantic Background Subtraction (denoted RT-SBS) which extends SBS for real-time constrained applications while keeping similar performances. RT-SBS effectively combines a real-time background subtraction algorithm with high-quality semantic information which can be provided at a slower pace, independently for each pixel. We show that RT-SBS coupled with ViBe sets a new state of the art for real-time background subtraction algorithms and even competes with the non real-time state-of-the-art ones. Note that python CPU and GPU implementations of RT-SBS will be released soon. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语义背景减除SBS已经显示出通过将它们与语义信息，从一个语义分割网络衍生组合以改善的最背景减除算法的性能。然而，SBS需要对所有帧，这是缓慢的计算高质量的语义分割口罩。此外，国家的最先进最背景减除算法不是实时的，这使得它们不适合于现实世界的应用。在本文中，我们提出了所谓的实时语义背景减法（表示RT-SBS）一种新型的背景减除算法延伸SBS实时受限的应用，同时保持性能相似。 RT-SBS有效地结合有能够以较慢的速度被提供，独立地对每个像素的高品质的语义信息的实时背景减除算法。我们表明，RT-SBS加上盛传将艺术进行实时背景减除算法的一个新的状态，甚至与非实时状态的最先进的人竞争。需要注意的是RT-SBS的蟒蛇CPU和GPU的实现将很快被释放。</font>
</div>


<hr>
<div id="paper9"> <b>9. Hierarchical Auto-Regressive Model for Image Compression Incorporating  Object Saliency and a Deep Perceptual Loss</b>  <a href="https://arxiv.org/pdf/2002.04988" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Patel%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yash Patel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Appalaraju%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Srikar Appalaraju</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manmatha%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">R. Manmatha</a><br>
<font size="3">
Abstract: We propose a new end-to-end trainable model for lossy image compression which includes a number of novel components. This approach incorporates 1) a hierarchical auto-regressive model; 2)it also incorporates saliency in the images and focuses on reconstructing the salient regions better; 3) in addition, we empirically demonstrate that the popularly used evaluations metrics such as MS-SSIM and PSNR are inadequate for judging the performance of deep learned image compression techniques as they do not align well with human perceptual similarity. We, therefore propose an alternative metric, which is learned on perceptual similarity data specific to image compression. Our experiments show that this new metric aligns significantly better with human judgments when compared to other hand-crafted or learned metrics. The proposed compression model not only generates images that are visually better but also gives superior performance for subsequent computer vision tasks such as object detection and segmentation when compared to other engineered or learned codecs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了有损图像压缩，其包括许多新颖的部件的新的端至端的可训练模型。这种方法结合1）分层自回归模型; 2）它还采用在图像中的显着性，侧重于重构显着区域更好; 3）此外，我们凭经验证明普遍使用的评价指标，例如MS-SSIM和PSNR是不足判断的深了解到图像压缩技术的性能，因为它们不与人类感知相似井对齐。我们因此提出替代度量，这是在感知相似数据中的特定图像压缩获知。我们的实验表明，这种新的度量与对齐人为判断显著更好时，相对于其他手工制作或学习指标。所提出的压缩模式，不仅产生视觉上更好的图像，但相对于其他工程或学习编解码器时，也给出了后续的计算机视觉任务，如对象检测和分割卓越的性能。</font>
</div>


<hr>
<div id="paper10"> <b>10. Towards Precise Intra-camera Supervised Person Re-identification</b>  <a href="https://arxiv.org/pdf/2002.04932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Menglin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baisheng Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haokun Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianqiang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojin Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xian-Sheng Hua</a><br>
<font size="3">
Abstract: Intra-camera supervision (ICS) for person re-identification (Re-ID) assumes that identity labels are independently annotated within each camera view and no inter-camera identity association is labeled. It is a new setting proposed recently to reduce the burden of annotation while expect to maintain desirable Re-ID performance. However, the lack of inter-camera labels makes the ICS Re-ID problem much more challenging than the fully supervised counterpart. By investigating the characteristics of ICS, this paper proposes camera-specific non-parametric classifiers, together with a hybrid mining quintuplet loss, to perform intra-camera learning. Then, an inter-camera learning module consisting of a graph-based ID association step and a Re-ID model updating step is conducted. Extensive experiments on three large-scale Re-ID datasets show that our approach outperforms all existing ICS works by a great margin. Our approach performs even comparable to state-of-the-art fully supervised methods in two of the datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：帧内照相机监管（ICS），用于人重新鉴定（再ID）假设身份标签每个摄像机视图内独立地注释和没有摄影机间身份关联被标记。这是最近提出的减少注释的负担，同时希望保持理想的再ID性能的新设置。然而，由于缺乏摄像装置间的标签使得ICS重新编号的问题远远超过了充分监督对口挑战。通过调查ICS的特点，提出了具体的摄像头，非参数的分类，与混合动力采矿五元组一起损失，执行摄像机内学习。然后，将由基于图的ID关联步骤和再ID模型更新步骤的相机间学习模块中进行。三个大型再ID的数据集大量的实验表明，我们的方法有很大裕度优于所有现有的ICS作品。我们的方法执行甚至可以媲美国家的最先进的完全监督两个数据集的方法。</font>
</div>


<hr>
<div id="paper11"> <b>11. A Zero-Shot based Fingerprint Presentation Attack Detection System</b>  <a href="https://arxiv.org/pdf/2002.04908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haozhe Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wentian Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guojie Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Liu</a><br>
<font size="3">
Abstract: With the development of presentation attacks, Automated Fingerprint Recognition Systems(AFRSs) are vulnerable to presentation attack. Thus, numerous methods of presentation attack detection(PAD) have been proposed to ensure the normal utilization of AFRS. However, the demand of large-scale presentation attack images and the low-level generalization ability always astrict existing PAD methods' actual performances. Therefore, we propose a novel Zero-Shot Presentation Attack Detection Model to guarantee the generalization of the PAD model. The proposed ZSPAD-Model based on generative model does not utilize any negative samples in the process of establishment, which ensures the robustness for various types or materials based presentation attack. Different from other auto-encoder based model, the Fine-grained Map architecture is proposed to refine the reconstruction error of the auto-encoder networks and a task-specific gaussian model is utilized to improve the quality of clustering. Meanwhile, in order to improve the performance of the proposed model, 9 confidence scores are discussed in this article. Experimental results showed that the ZSPAD-Model is the state of the art for ZSPAD, and the MS-Score is the best confidence score. Compared with existing methods, the proposed ZSPAD-Model performs better than the feature-based method and under the multi-shot setting, the proposed method overperforms the learning based method with little training data. When large training data is available, their results are similar. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着演示攻击的发展，指纹自动识别系统（AFRSs）很容易受到攻击的演示。因此，已提出演示攻击检测（PAD）的多种方法，以确保AFRS的正常使用。然而，大规模的进攻呈现图像的需求和低级别的泛化能力总是astrict现有PAD方法的实际表现。因此，我们提出了一个新颖的零射击演示攻击检测模型，以保证PAD模型的泛化。基于生成模型的提出ZSPAD的模型没有利用任何负面样本中建立的过程中，这样可以确保基于演示攻击各种类型或材料的坚固性。从其他自动编码器来基于模型不同的是，细粒度的地图架构提出了改进自动编码器来网络和特定任务的高斯模型被用来提高聚类质量的重建误差。同时，为了提高该模型的性能，9个信心分数本文中讨论。实验结果表明，ZSPAD-模型是本领域中用于ZSPAD的状态，并且MS-分数是最好的置信度得分。与现有的方法相比，该ZSPAD-模型比基于特征的方法，并在多合一设定好，所提出的方法overperforms很少训练数据的学习为基础的方法。当大量的训练数据是可用的，其结果是相似的。</font>
</div>


<hr>
<div id="paper12"> <b>12. Bi-Directional Generation for Unsupervised Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2002.04869" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guanglei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haifeng Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingli Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengming Ding</a><br>
<font size="3">
Abstract: Unsupervised domain adaptation facilitates the unlabeled target domain relying on well-established source domain information. The conventional methods forcefully reducing the domain discrepancy in the latent space will result in the destruction of intrinsic data structure. To balance the mitigation of domain gap and the preservation of the inherent structure, we propose a Bi-Directional Generation domain adaptation model with consistent classifiers interpolating two intermediate domains to bridge source and target domains. Specifically, two cross-domain generators are employed to synthesize one domain conditioned on the other. The performance of our proposed method can be further enhanced by the consistent classifiers and the cross-domain alignment constraints. We also design two classifiers which are jointly optimized to maximize the consistency on target sample prediction. Extensive experiments verify that our proposed model outperforms the state-of-the-art on standard cross domain visual benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督领域适应性方便了未标记的目标域依托完善的源域信息。常规方法强行降低潜在空间域差异将导致固有的数据结构的破坏。为了平衡域间隙和内在结构的保存缓解，我们提出一致分类插两个中间域弥合源和目标域的双向代域适应模式。具体而言，两个交叉域发电机被用于合成一种域调节为另一方。我们提出的方法的性能可以由一致的分类器和跨域对齐约束来进一步增强。我们还设计了两个分类被联合优化，最大化的目标样本预测的一致性。大量的实验验证，我们提出的模型优于标准的跨域视觉基准的国家的最先进的。</font>
</div>


<hr>
<div id="paper13"> <b>13. Analysis Of Multi Field Of View Cnn And Attention Cnn On H&amp;E Stained  Whole-slide Images On Hepatocellular Carcinoma</b>  <a href="https://arxiv.org/pdf/2002.04836" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Say%C4%B1c%C4%B1%2C+M+B" target="_blank" rel="noopener" style="color:#0000EE;">Mehmet Burak Sayıcı</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yamashita%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rikiya Yamashita</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeanne Shen</a><br>
<font size="3">
Abstract: Hepatocellular carcinoma (HCC) is a leading cause of cancer-related death worldwide. Whole-slide imaging which is a method of scanning glass slides have been employed for diagnosis of HCC. Using high resolution Whole-slide images is infeasible for Convolutional Neural Network applications. Hence tiling the Whole-slide images is a common methodology for assigning Convolutional Neural Networks for classification and segmentation. Determination of the tile size affects the performance of the algorithms since small field of view can not capture the information on a larger scale and large field of view can not capture the information on a cellular scale. In this work, the effect of tile size on performance for classification problem is analysed. In addition, Multi Field of View CNN is assigned for taking advantage of the information provided by different tile sizes and Attention CNN is assigned for giving the capability of voting most contributing tile size. It is found that employing more than one tile size significantly increases the performance of the classification by 3.97% and both algorithms are found successful over the algorithm which uses only one tile size. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：肝细胞癌（HCC）是癌症相关死亡的全球的主要原因。全滑动成像这是已被用于HCC的诊断扫描的载玻片的方法。使用高分辨率全幻灯片图像是不可行的卷积神经网络的应用。因此平铺全幻灯片图像是用于分类和分割分配卷积神经网络共同的方法。平铺尺寸的测定影响到自视野小的算法的性能不能捕获在更大的规模和大视场的信息不能捕获在细胞水平的信息。在这项工作中，瓷砖的大小对分类问题性能的影响进行了分析。另外，查看CNN的多场被分配用于拍摄的CNN被分配给了投票贡献最大平铺尺寸的能力不同瓷砖的大小和注意事项中提供的信息优势。研究发现，使用一个以上的瓷砖尺寸由3.97％显著提高分类的性能和算法都被发现在成功只使用一个分块大小的算法。</font>
</div>


<hr>
<div id="paper14"> <b>14. End-to-End Face Parsing via Interlinked Convolutional Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.04831" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zi Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yiu%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Valentin Yiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Tang</a><br>
<font size="3">
Abstract: Face parsing is an important computer vision task that requires accurate pixel segmentation of facial parts (such as eyes, nose, mouth, etc.), providing a basis for further face analysis, modification, and other applications. In this paper, we introduce a simple, end-to-end face parsing framework: STN-aided iCNN (STN-iCNN), which extends interlinked Convolutional Neural Network (iCNN) by adding a Spatial Transformer Network (STN) between the two isolated stages. The STN-iCNN uses the STN to provide a trainable connection to the original two-stage iCNN pipe-line, making end-to-end joint training possible. Moreover, as a by-product, STN also provides more precise cropped parts than the original cropper. Due to the two advantages, our approach significantly improves the accuracy of the original model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面对解析是一项重要的计算机视觉任务，需要面部成分精确的像素分割（如眼睛，鼻子，嘴等），为进一步面上的分析，修改和其他应用程序的基础。在本文中，我们介绍一个简单的，端 - 端面上解析框架：STN辅助ICNN（STN-ICNN），其延伸通过两个分离之间添加空间变换器网（STN）相通卷积神经网络（ICNN）阶段。的STN-ICNN使用STN提供到原来的两阶段ICNN管线可训练连接，使得端至端联合培养成为可能。此外，作为副产物，STN还提供比原来的裁剪机更精确的裁切部分。由于两个优势，我们的做法显著提高了原有模型的准确性。</font>
</div>


<hr>
<div id="paper15"> <b>15. Uniform Interpolation Constrained Geodesic Learning on Data Manifold</b>  <a href="https://arxiv.org/pdf/2002.04829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cong Geng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jia Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenbo Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chu Chu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyong Gao</a><br>
<font size="3">
Abstract: In this paper, we propose a method to learn a minimizing geodesic within a data manifold. Along the learned geodesic, our method can generate high-quality interpolations between two given data samples. Specifically, we use an autoencoder network to map data samples into latent space and perform interpolation via an interpolation net-work. We add prior geometric information to regularize our autoencoder for the convexity of representations so that for any given interpolation approach, the generated interpolations remain within the distribution of the data manifold. Before the learning of a geodesic, a proper Riemannianmetric should be defined. Therefore, we induce a Riemannian metric by the canonical metric in the Euclidean space which the data manifold is isometrically immersed in. Based on this defined Riemannian metric, we introduce a constant speed loss and a minimizing geodesic loss to regularize the interpolation network to generate uniform interpolation along the learned geodesic on the manifold. We provide a theoretical analysis of our model and use image translation as an example to demonstrate the effectiveness of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了学习数据歧管内的测地最小化的方法。除了学习大地，我们的方法可以产生两个给定的数据样本之间的高品质插值。具体地，我们使用自动编码器网络来的数据样本映射到潜在空间，并通过一个插网络执行内插。我们之前添加的几何信息来规范我们的交涉的凸自动编码，这样对于任何给定的插值方法，生成插值保持数据流形的分布范围内。测地的学习之前，适当Riemannianmetric应该被定义。因此，我们通过在欧几里德空间中的典型度量其中数据歧管等距浸入诱导黎曼度量。在此基础上定义的黎曼度量，我们引入一个恒定的速度损失和最小化测地损失到正规化的内插网络，以产生均匀的沿着歧管上的教训测地内插。我们提供我们的模型的理论分析和使用图像的平移作为一个例子来证明我们的方法的有效性。</font>
</div>


<hr>
<div id="paper16"> <b>16. Deep-HR: Fast Heart Rate Estimation from Face Video Under Realistic  Conditions</b>  <a href="https://arxiv.org/pdf/2002.04821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabokrou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Sabokrou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pourreza%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masoud Pourreza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaobai Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fathy%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahmood Fathy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoying Zhao</a><br>
<font size="3">
Abstract: This paper presents a novel method for remote heart rate (HR) estimation. Recent studies have proved that blood pumping by the heart is highly correlated to the intense color of face pixels, and surprisingly can be utilized for remote HR estimation. Researchers successfully proposed several methods for this task, but making it work in realistic situations is still a challenging problem in computer vision community. Furthermore, learning to solve such a complex task on a dataset with very limited annotated samples is not reasonable. Consequently, researchers do not prefer to use the deep learning approaches for this problem. In this paper, we propose a simple yet efficient approach to benefit the advantages of the Deep Neural Network (DNN) by simplifying HR estimation from a complex task to learning from very correlated representation to HR. Inspired by previous work, we learn a component called Front-End (FE) to provide a discriminative representation of face videos, afterward a light deep regression auto-encoder as Back-End (BE) is learned to map the FE representation to HR. Regression task on the informative representation is simple and could be learned efficiently on limited training samples. Beside of this, to be more accurate and work well on low-quality videos, two deep encoder-decoder networks are trained to refine the output of FE. We also introduce a challenging dataset (HR-D) to show that our method can efficiently work in realistic conditions. Experimental results on HR-D and MAHNOB datasets confirm that our method could run as a real-time method and estimate the average HR better than state-of-the-art ones. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了远程心脏速率（HR）估计的新方法。最近的研究已经证明，泵血由心脏高度相关面的像素的强烈的色彩，并出人意料地可用于远程HR估计。研究人员成功地提出了这个任务的几种方法，但使其工作在实际情况仍然是计算机视觉社区一个具有挑战性的问题。此外，学习来解决非常有限的注释样本数据集这样一个复杂的任务，是不合理的。因此，研究人员并不喜欢使用深层学习方法针对此问题。在本文中，我们提出了一个简单而有效的方法，由一个复杂的任务简化HR估计从非常相关的代表性学习人力资源，以造福于深层神经网络（DNN）的优点。通过前期工作的启发，我们了解到一个叫做前端（FE）组件来提供的面部视频的具有区分表示，后来光深回归自动编码器来作为后端（BE）被学习映射FE表示对HR。在信息表示回归的任务很简单，并且可以在有限的训练样本有效地学习。除了这一点，更准确，并且运作良好的低质量的视频，两道深深的编码器，解码器网络进行培训，以完善FE的输出。我们还引入了一个具有挑战性的数据集（HR-d）表明我们的方法可以有效地在现实条件下工作。在HR-d和MAHNOB数据集实验结果证实了我们的方法可以作为一个实时运行的方法，更好地估计平均HR比国家的最先进的。</font>
</div>


<hr>
<div id="paper17"> <b>17. A Visual-inertial Navigation Method for High-Speed Unmanned Aerial  Vehicles</b>  <a href="https://arxiv.org/pdf/2002.04791" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin-long Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jia-hui Lv</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geng Sun</a><br>
<font size="3">
Abstract: This paper investigates the localization problem of high-speed high-altitude unmanned aerial vehicle (UAV) with a monocular camera and inertial navigation system. It proposes a navigation method utilizing the complementarity of vision and inertial devices to overcome the singularity which arises from the horizontal flight of UAV. Furthermore, it modifies the mathematical model of localization problem via separating linear parts from nonlinear parts and replaces a nonlinear least-squares problem with a linearly equality-constrained optimization problem. In order to avoid the ill-condition property near the optimal point of sequential unconstrained minimization techniques(penalty methods), it constructs a semi-implicit continuous method with a trust-region technique based on a differential-algebraic dynamical system to solve the linearly equality-constrained optimization problem. It also analyzes the global convergence property of the semi-implicit continuous method in an infinity integrated interval other than the traditional convergence analysis of numerical methods for ordinary differential equations in a finite integrated interval. Finally, the promising numerical results are also presented. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文使用单眼照相机和惯性导航系统调查高速高空无人驾驶飞行器（UAV）的定位问题。它提出了利用视觉和惯性器件的互补性，以克服其来自UAV的水平飞行的奇点的导航方法。此外，通过分离非线性份线性部分修改定位问题的数学模型，并替换一个非线性最小二乘问题线性等式约束的优化问题。为了避免顺序无约束极小化技术（惩罚的方法）的最佳点附近的病态属性，它构造与基于一个微分代数动力系统上的信赖域技术的半隐式连续方法，解决了线性平等T-受约束的优化问题。还分析在无限远的半隐式连续法的全局收敛性集成间隔以外的用于在有限常微分方程的数值方法的传统的收敛性分析集成间隔。最后，有前途的数值结果也。</font>
</div>


<hr>
<div id="paper18"> <b>18. MFFW: A new dataset for multi-focus image fusion</b>  <a href="https://arxiv.org/pdf/2002.04780" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoli Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunxia Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junmin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiangshe Zhang</a><br>
<font size="3">
Abstract: Multi-focus image fusion (MFF) is a fundamental task in the field of computational photography. Current methods have achieved significant performance improvement. It is found that current methods are evaluated on simulated image sets or Lytro dataset. Recently, a growing number of researchers pay attention to defocus spread effect, a phenomenon of real-world multi-focus images. Nonetheless, defocus spread effect is not obvious in simulated or Lytro datasets, where popular methods perform very similar. To compare their performance on images with defocus spread effect, this paper constructs a new dataset called MFF in the wild (MFFW). It contains 19 pairs of multi-focus images collected on the Internet. We register all pairs of source images, and provide focus maps and reference images for part of pairs. Compared with Lytro dataset, images in MFFW significantly suffer from defocus spread effect. In addition, the scenes of MFFW are more complex. The experiments demonstrate that most state-of-the-art methods on MFFW dataset cannot robustly generate satisfactory fusion images. MFFW can be a new baseline dataset to test whether an MMF algorithm is able to deal with defocus spread effect. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多聚焦图像融合（MFF）是计算摄影领域的根本任务。目前的方法都取得了显著的性能提升。研究发现，目前的方法是在模拟图像集或数据集Lytro公司评估。近来，越来越多的研究者的注意散焦波及效应，真实世界的多聚焦图像的现象。尽管如此，散焦散布效果并不模拟或Lytro公司的数据集，其中常用的方法执行非常相似的明显。比较其与散焦散布效果的图像性能，本文构建了一个在野外（MFFW）称为MFF新的数据集。它包含19对收集互联网上的多聚焦图像。我们注册所有对源图像，以及对部分重点提供地图和参考图像。与Lytro公司的数据集相比，MFFW图像显著遭受散焦散布效果。此外，MFFW的场景都比较复杂。实验证明上MFFW数据集，大多数国家的最先进的方法不能生成鲁棒令人满意融合图像。 MFFW可以是一个新的基准数据集测试的MMF算法是否能够处理散焦散布效果。</font>
</div>


<hr>
<div id="paper19"> <b>19. Efficient Training of Deep Convolutional Neural Networks by Augmentation  in Embedding Space</b>  <a href="https://arxiv.org/pdf/2002.04776" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Abrishami%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Saeed Abrishami</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Eshratifar%2C+A+E" target="_blank" rel="noopener" style="color:#0000EE;">Amir Erfan Eshratifar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Eigen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Eigen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanzhi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nazarian%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shahin Nazarian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pedram%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Massoud Pedram</a><br>
<font size="3">
Abstract: Recent advances in the field of artificial intelligence have been made possible by deep neural networks. In applications where data are scarce, transfer learning and data augmentation techniques are commonly used to improve the generalization of deep learning models. However, fine-tuning a transfer model with data augmentation in the raw input space has a high computational cost to run the full network for every augmented input. This is particularly critical when large models are implemented on embedded devices with limited computational and energy resources. In this work, we propose a method that replaces the augmentation in the raw input space with an approximate one that acts purely in the embedding space. Our experimental results show that the proposed method drastically reduces the computation, while the accuracy of models is negligibly compromised. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在人工智能领域的最新进展已经通过深层神经网络成为可能。在数据稀少申请，转让学习和数据增强技术常用来改善深学习模式的推广。然而，微调的原始输入空间数据增强传输模型运行完整网络为每一个扩充输入计算成本高。当大型模型与有限的计算资源和能源的嵌入式设备中实现这一点特别重要。在这项工作中，我们提出了取代在大约一个在嵌入空间完全作用于原始输入空间增强的方法。我们的实验结果表明，该方法大大减少了计算量，而模型的准确性受到影响可以忽略不计。</font>
</div>


<hr>
<div id="paper20"> <b>20. Progressive Object Transfer Detection</b>  <a href="https://arxiv.org/pdf/2002.04741" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yali Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoyou Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Qiao</a><br>
<font size="3">
Abstract: Recent development of object detection mainly depends on deep learning with large-scale benchmarks. However, collecting such fully-annotated data is often difficult or expensive for real-world applications, which restricts the power of deep neural networks in practice. Alternatively, humans can detect new objects with little annotation burden, since humans often use the prior knowledge to identify new objects with few elaborately-annotated examples, and subsequently generalize this capacity by exploiting objects from wild images. Inspired by this procedure of learning to detect, we propose a novel Progressive Object Transfer Detection (POTD) framework. Specifically, we make three main contributions in this paper. First, POTD can leverage various object supervision of different domains effectively into a progressive detection procedure. Via such human-like learning, one can boost a target detection task with few annotations. Second, POTD consists of two delicate transfer stages, i.e., Low-Shot Transfer Detection (LSTD), and Weakly-Supervised Transfer Detection (WSTD). In LSTD, we distill the implicit object knowledge of source detector to enhance target detector with few annotations. It can effectively warm up WSTD later on. In WSTD, we design a recurrent object labelling mechanism for learning to annotate weakly-labeled images. More importantly, we exploit the reliable object supervision from LSTD, which can further enhance the robustness of target detector in the WSTD stage. Finally, we perform extensive experiments on a number of challenging detection benchmarks with different settings. The results demonstrate that, our POTD outperforms the recent state-of-the-art approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目标检测的最新发展主要依赖于与大型基准深度学习。然而，收集这些完全标注的数据往往是困难的或昂贵的现实世界的应用，制约深层神经网络的力量，在实践中。另外，人类可以检测几乎没有注释负担新的对象，因为人们经常使用的先验知识来识别与几个精心标注的例子新的对象，然后通过利用野生图像中物体推广这方面的能力。通过学习来检测这个过程的启发，我们提出了一个新的进步对象传输检测（POTD）框架。具体来说，我们在这三个主要贡献。首先，POTD可以利用不同的域的各种对象监督有效成逐行检测过程。通过这种类似人类的学习，可以提高很少注释的目标探测任务。其次，POTD由两个精致的传输段的，亦即，低射击转移侦测（LSTD），和弱监督转移侦测（WSTD）。在LSTD，我们提炼源检测的隐式对象的知识，加强与一些注释靶标检测。它可以有效地热身WSTD以后。在WSTD，我们设计了一个经常性的对象标识机制，学习注释弱标记的图像。更重要的是，我们利用从LSTD可靠对象的监督，这可以进一步提高目标检测的鲁棒性的WSTD阶段。最后，我们在许多不同的设置具有挑战性的检测基准进行大量的实验。结果表明，我们的POTD优于近期国家的最先进的方法。</font>
</div>


<hr>
<div id="paper21"> <b>21. Improving Place Recognition Using Dynamic Object Detection</b>  <a href="https://arxiv.org/pdf/2002.04698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Munoz%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Juan Pablo Munoz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dexter%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Scott Dexter</a><br>
<font size="3">
Abstract: Traditional appearance-based place recognition algorithms based on handcrafted features have proven inadequate in environments with a significant presence of dynamic objects -- objects that may or may not be present in an agent's subsequent visits. Place representations from features extracted using Deep Learning approaches have gained popularity for their robustness and because the algorithms that used them yield better accuracy. Nevertheless, handcrafted features are still popular in devices that have limited resources. This article presents a novel approach that improves place recognition in environments populated by dynamic objects by incorporating the very knowledge of these objects to improve the overall quality of the representations of places used for matching. The proposed approach fuses object detection and place description, Deep Learning and handcrafted features, with the significance of reducing memory and storage requirements. This article demonstrates that the proposed approach yields improved place recognition accuracy, and was evaluated using both synthetic and real-world datasets. The adoption of the proposed approach will significantly improve place recognition results in environments populated by dynamic objects, and explored by devices with limited resources, with particular utility in both indoor and outdoor environments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于手工提供传统的外观，基于位置识别算法已经在环境中证明是不充分的动态对象的显著存在 - 对象可能会或可能不会出现在代理的后续访问。从使用功能的地方交涉提取深层学习方法已经得到普及为他们的鲁棒性和因为用他们的算法产生更好的精度。然而，手工制作的功能仍然在具有有限资源的设备上普及。本文给出了一个改善通过将这些对象的非常知识，提高用于匹配的地方交涉的整体质量动态对象居住环境的地方认同的新方法。所提出的方法保险丝目标检测与地方的描述，深入学习和手工制作的特点，以减少内存和存储需求的意义。本文演示了该方法的产量提高了地方的识别精度，并使用合成和真实世界的数据集进行了评价。该方法的通过将显著改善动态对象居住环境处的识别结果，并通过设备资源有限探索，在室内和室外环境中特别有用。</font>
</div>


<hr>
<div id="paper22"> <b>22. Learning spatio-temporal representations with temporal squeeze pooling</b>  <a href="https://arxiv.org/pdf/2002.04685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoxi Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bors%2C+A+G" target="_blank" rel="noopener" style="color:#0000EE;">Adrian G. Bors</a><br>
<font size="3">
Abstract: In this paper, we propose a new video representation learning method, named Temporal Squeeze (TS) pooling, which can extract the essential movement information from a long sequence of video frames and map it into a set of few images, named Squeezed Images. By embedding the Temporal Squeeze pooling as a layer into off-the-shelf Convolution Neural Networks (CNN), we design a new video classification model, named Temporal Squeeze Network (TeSNet). The resulting Squeezed Images contain the essential movement information from the video frames, corresponding to the optimization of the video classification task. We evaluate our architecture on two video classification benchmarks, and the results achieved are compared to the state-of-the-art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个新的视频表示学习方法，命名为颞挤压（TS）池，它可以从视频帧的长序列中提取必要的运动信息，并将其映射到一组几张图片，命名为压缩图像的。通过嵌入的时空挤压池作为一个层进入关闭的，现成的卷积神经网络（CNN），我们设计了一个新的视频分类模型，命名为颞挤压网络（TeSNet）。得到的压缩映像包含视频帧的基本运动信息，对应的视频分类任务的最优化。我们评估我们对两个视频分类的基准架构，以及所取得的结果相比，国家的最先进的。</font>
</div>


<hr>
<div id="paper23"> <b>23. Object Detection as a Positive-Unlabeled Problem</b>  <a href="https://arxiv.org/pdf/2002.04672" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuewei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+K+J" target="_blank" rel="noopener" style="color:#0000EE;">Kevin J Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lawrence Carin</a><br>
<font size="3">
Abstract: As with other deep learning methods, label quality is important for learning modern convolutional object detectors. However, the potentially large number and wide diversity of object instances that can be found in complex image scenes makes constituting complete annotations a challenging task; objects missing annotations can be observed in a variety of popular object detection datasets. These missing annotations can be problematic, as the standard cross-entropy loss employed to train object detection models treats classification as a positive-negative (PN) problem: unlabeled regions are implicitly assumed to be background. As such, any object missing a bounding box results in a confusing learning signal, the effects of which we observe empirically. To remedy this, we propose treating object detection as a positive-unlabeled (PU) problem, which removes the assumption that unlabeled regions must be negative. We demonstrate that our proposed PU classification loss outperforms the standard PN loss on PASCAL VOC and MS COCO across a range of label missingness, as well as on Visual Genome and DeepLesion with full labels. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：与其他深的学习方法，标签质量是学习现代卷积对象探测器重要。然而，潜在的大量和对象实例的广泛多样性，可以在复杂的图像场景中找到使构成完整注释的具有挑战性的任务;对象缺少注释可以在各种流行的物体检测的数据集的被观察到。这些缺失的注释可以是有问题的，作为标准的交叉熵损失用于列车对象检测模型对待分类为正 - 负（PN）问题：未标记的区域被隐含地假定为背景。因此，任何物体缺少一个令人困惑的学习信号边框效果，其影响的我们经验观察。为了解决这个问题，我们提出治疗目标检测为阳性，未标记（PU）的问题，这消除假设未标记的区域必须是负的。我们证明了我们提出的PU分类损失优于上PASCAL VOC和MS COCO标准PN损失在一系列标签missingness的，以及对视觉基因组与DeepLesion全标签。</font>
</div>


<hr>
<div id="paper24"> <b>24. Validating uncertainty in medical image translation</b>  <a href="https://arxiv.org/pdf/2002.04639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Reinhold%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">Jacob C. Reinhold</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yufan He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shizhong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunqiang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dashan Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junghoon Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prince%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jerry L. Prince</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carass%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aaron Carass</a><br>
<font size="3">
Abstract: Medical images are increasingly used as input to deep neural networks to produce quantitative values that aid researchers and clinicians. However, standard deep neural networks do not provide a reliable measure of uncertainty in those quantitative values. Recent work has shown that using dropout during training and testing can provide estimates of uncertainty. In this work, we investigate using dropout to estimate epistemic and aleatoric uncertainty in a CT-to-MR image translation task. We show that both types of uncertainty are captured, as defined, providing confidence in the output uncertainty estimates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：医学图像越来越多地用作输入深层神经网络，产生定量值援助研究人员和临床医生。但是，标准的深层神经网络的不确定性提供了可靠的测量这些定量值。最近的研究表明，训练期间使用辍学和测试可以提供不确定性的估计。在这项工作中，我们探讨用差来估计在CT对MR图像平移任务认知和肆意的不确定性。我们发现，这两种类型的不确定性被捕获，定义，提供的输出不确定性估计的信心。</font>
</div>


<hr>
<div id="paper25"> <b>25. Finding novelty with uncertainty</b>  <a href="https://arxiv.org/pdf/2002.04626" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Reinhold%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">Jacob C. Reinhold</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yufan He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shizhong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunqiang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dashan Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junghoon Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prince%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jerry L. Prince</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carass%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aaron Carass</a><br>
<font size="3">
Abstract: Medical images are often used to detect and characterize pathology and disease; however, automatically identifying and segmenting pathology in medical images is challenging because the appearance of pathology across diseases varies widely. To address this challenge, we propose a Bayesian deep learning method that learns to translate healthy computed tomography images to magnetic resonance images and simultaneously calculates voxel-wise uncertainty. Since high uncertainty occurs in pathological regions of the image, this uncertainty can be used for unsupervised anomaly segmentation. We show encouraging experimental results on an unsupervised anomaly segmentation task by combining two types of uncertainty into a novel quantity we call scibilic uncertainty. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：医学图像常常被用来检测和表征病理和疾病;然而，自动地识别和在医学图像中分割病理学是具有挑战性，因为病理的跨疾病的外观变化很大。为了应对这一挑战，我们提出了一个贝叶斯深度学习方法学会翻译健康的计算机断层成像图像磁共振图像，同时计算出体素明智的不确定性。由于高的不确定性在图像的病理区域发生时，这种不确定性可用于无监督异常分割。我们展示两种类型的不确定性组合为我们称之为scibilic不确定性的一种新型的数量，鼓励在无人监督的异常分割任务的实验结果。</font>
</div>


<hr>
<div id="paper26"> <b>26. Patternless Adversarial Attacks on Video Recognition Networks</b>  <a href="https://arxiv.org/pdf/2002.05123" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Naeh%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Itay Naeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pony%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roi Pony</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mannor%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shie Mannor</a><br>
<font size="3">
Abstract: Deep neural networks for classification of videos, just like image classification networks, may be subjected to adversarial manipulation. The main difference between image classifiers and video classifiers is that the latter usually use temporal information contained within the video in the form of optical flow or implicitly by various differences between adjacent frames. In this work we present a manipulation scheme for fooling video classifiers by introducing a spatial patternless temporal perturbation that is practically unnoticed by human observers and undetectable by leading image adversarial pattern detection algorithms. After demonstrating the manipulation of action classification of single videos, we generalize the procedure to make adversarial patterns with temporal invariance that generalizes across different classes for both targeted and untargeted attacks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频分类深层神经网络，就像图像分类网络，可能会受到敌对操作。图像分类器和分类器的视频之间的主要区别是，后者通常是通过在相邻帧之间的各种差异使用在光流的形式包含在所述视频内的时间信息或隐式。在这项工作中，我们通过引入用于呈现视频嘴硬分类器的操作方案的空间无图案颞扰动是通过实际上人类观察者忽视和领先的图像对抗性图案检测算法检测到。展示的单一视频行为分类的操作后，我们推广的过程，使对抗模式与时间不变性跨越不同类别归纳为有针对性和无针对性的攻击。</font>
</div>


<hr>
<div id="paper27"> <b>27. From IC Layout to Die Photo: A CNN-Based Data-Driven Approach</b>  <a href="https://arxiv.org/pdf/2002.04967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Shao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao-Chiang Shao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Peng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chao-Yi Peng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun-Rei Wu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chia-Wen Lin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shao-Yun Fang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tsai%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pin-Yen Tsai</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan-Hsiu Liu</a><br>
<font size="3">
Abstract: Since IC fabrication is costly and time-consuming, it is highly desirable to develop virtual metrology tools that can predict the properties of a wafer based on fabrication configurations without performing physical measurements on a fabricated IC. We propose a deep learning-based data-driven framework consisting of two convolutional neural networks: i) LithoNet that predicts the shape deformations on a circuit due to IC fabrication, and ii) OPCNet that suggests IC layout corrections to compensate for such shape deformations. By learning the shape correspondence between pairs of layout design patterns and their SEM images of the product wafer thereof, given an IC layout pattern, LithoNet can mimic the fabrication procedure to predict its fabricated circuit shape for virtual metrology. Furthermore, LithoNet can take the wafer fabrication parameters as a latent vector to model the parametric product variations that can be inspected on SEM images. In addition, traditional lithography simulation methods used to suggest a correction on a lithographic photomask is computationally expensive. Our proposed OPCNet mimics the optical proximity correction (OPC) procedure and efficiently generates a corrected photomask by collaborating with LithoNet to examine if the shape of a fabricated IC circuitry best matches its original layout design. As a result, the proposed LithoNet-OPCNet framework cannot only predict the shape of a fabricated IC from its layout pattern, but also suggests a layout correction according to the consistency between the predicted shape and the given layout. Experimental results with several benchmark layout patterns demonstrate the effectiveness of the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由于IC制造是昂贵和费时的，这是非常需要开发虚拟计量工具，可以预测在晶片的基础上制造的配置属性，而无需在制造IC执行的物理测量。我们提出了一个深基于学习的数据驱动框架由两个卷积神经网络的：ⅰ）LithoNet，预测由于IC制造中的电路上的形状的变形，以及ii）OPCNet即表明IC布局校正以补偿这样的形状变形。通过学习的布局设计模式对以及它们的晶片，给定的IC布局图案的产品的它们的SEM图像之间的形状的对应关系，LithoNet可以模仿的制造程序，以预测其制造的电路形状为虚拟测量。此外，LithoNet可以采取在晶片制造参数作为潜矢量的是可在SEM图像被检参数变型产品进行建模。此外，用来建议光刻掩膜修正传统的光刻仿真方法在计算上是昂贵的。我们提出的OPCNet模仿光学邻近校正（OPC）过程，有效地生成由与LithoNet合作，以检查是否a的形状制造的IC电路最佳地匹配它的原始布局设计校正光掩模。其结果是，所提出的LithoNet-OPCNet框架不仅可以预测的形状从其布局图案制造IC，但也表明根据所预测的形状和给定​​的布局之间的一致性的布局校正。与几个基准布局模式的实验结果证明了该方法的有效性。</font>
</div>


<hr>
<div id="paper28"> <b>28. Synaptic Integration of Spatiotemporal Features with a Dynamic  Neuromorphic Processor</b>  <a href="https://arxiv.org/pdf/2002.04924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nilsson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mattias Nilsson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Foteini Liwicki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sandin%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fredrik Sandin</a><br>
<font size="3">
Abstract: Spiking neurons can perform spatiotemporal feature detection by nonlinear synaptic and dendritic integration of presynaptic spike patterns. Multicompartment models of nonlinear dendrites and related neuromorphic circuit designs enable faithful imitation of such dynamic integration processes, but these approaches are also associated with a relatively high computing cost or circuit size. Here, we investigate synaptic integration of spatiotemporal spike patterns with multiple dynamic synapses on point-neurons in the DYNAP-SE neuromorphic processor, which can offer a complementary resource-efficient, albeit less flexible, approach to feature detection. We investigate how previously proposed excitatory--inhibitory pairs of dynamic synapses can be combined to integrate multiple inputs, and we generalize that concept to a case in which one inhibitory synapse is combined with multiple excitatory synapses. We characterize the resulting delayed excitatory postsynaptic potentials (EPSPs) by measuring and analyzing the membrane potentials of the neuromorphic neuronal circuits. We find that biologically relevant EPSP delays, with variability of order 10 milliseconds per neuron, can be realized in the proposed manner by selecting different synapse combinations, thanks to device mismatch. Based on these results, we demonstrate that a single point-neuron with dynamic synapses in the DYNAP-SE can respond selectively to presynaptic spikes with a particular spatiotemporal structure, which enables, for instance, visual feature tuning of single neurons. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：扣球神经元可以通过非线性突触和突触前尖峰图案树突集成执行时​​空特征检测。非线性树突和相关神经形态电路设计的多室模型使这样的动态集成过程的忠实模仿，但这些方法也具有相对高的计算成本或电路尺寸相关联。在这里，我们调查的时空秒杀模式的突触整合与点神经元多个动态突触在DYNAP-SE神经形态处理器，可提供互补资源利用率高，尽管不那么灵活，方法特征检测。我们研究如何先前提出的兴奋 - 动态突触抑制对可以合并整合多个输入，并推广了这一概念，其中一个抑制性突触与多个兴奋性突触结合的情况下。我们通过测量和分析的神经形态电路的神经元的膜电位表征所得延迟兴奋性突触后电位（EPSPS）。我们发现，生物学相关的EPSP延误，有秩序神经元每10毫秒，能够在建议的方式通过对设备不匹配选择不同的突触的组合，由于可以实现的可变性。基于这些结果，我们表明，单个点的神经元与所述DYNAP-SE动态突触可以选择性到突触前尖峰与特定时空结构，这使得能够，例如，单神经元的视觉特征的调谐响应。</font>
</div>


<hr>
<div id="paper29"> <b>29. Machine-Learning-Based Multiple Abnormality Prediction with Large-Scale  Chest Computed Tomography Volumes</b>  <a href="https://arxiv.org/pdf/2002.04752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Draelos%2C+R+L" target="_blank" rel="noopener" style="color:#0000EE;">Rachel Lea Draelos</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dov%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Dov</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mazurowski%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Maciej A. Mazurowski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lo%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Joseph Y. Lo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Henao%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ricardo Henao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Rubin%2C+G+D" target="_blank" rel="noopener" style="color:#0000EE;">Geoffrey D. Rubin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Carin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lawrence Carin</a><br>
<font size="3">
Abstract: Developing machine learning models for radiology requires large-scale imaging data sets with labels for abnormalities, but the process is challenging due to the size and complexity of the data as well as the cost of labeling. We curated and analyzed a chest computed tomography (CT) data set of 36,316 volumes from 20,201 unique patients. This is the largest multiply-annotated chest CT data set reported. To annotate this data set, we developed a rule-based method for automatically extracting abnormality labels from radiologist free-text reports with an average F-score of 0.976 (min 0.941, max 1.0). We also developed a model for multilabel abnormality classification of chest CT volumes that uses a deep convolutional neural network (CNN). This model reached a classification performance of AUROC greater than 0.90 for 18 abnormalities, with an average AUROC of 0.773 for all 83 abnormalities, demonstrating the feasibility of learning from unfiltered whole volume CT data. We show that training on more labels improves performance significantly: for a subset of 9 labels - nodule, opacity, atelectasis, pleural effusion, consolidation, mass, pericardial effusion, cardiomegaly, and pneumothorax - the model's average AUROC increased by 10 percent when the number of training labels was increased from 9 to all 83. All code for volume preprocessing, automated label extraction, and the volume abnormality prediction model will be made publicly available. The 36,316 CT volumes and labels will also be made publicly available pending institutional approval. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：放射开发机器学习模型需要大规模成像数据集的标签异常，但这一进程因以及标签的成本数据的规模和复杂性挑战。我们策划并分析了从20,201独特患者36,316卷的胸部CT扫描（CT）数据集。这是最大的多重注解胸部CT数据集的报道。为了诠释这组数据中，我们开发了从放射科医生自由文本报告，其中的0.976的平均F-得分（分0.941，最大1.0）自动提取异常标签基于规则的方法。我们还开发了一个使用深卷积神经网络（CNN）胸部CT卷的多标签分类异常的模型。这种模式达到更高的AUROC比0.90分类表现为18点的异常，与0.773为所有83种异常的平均AUROC，展示了从未经过滤的全容积CT数据中学习的可行性。我们展示更多的标签，培训提高性能显著：对于9个标签的一个子集 - 结节，不透明度，肺不张，胸腔积液，整合，质量，心包积液，心脏扩大，气胸 - 模型的平均AUROC增加时，10％的数量训练标签的从9增加到所有83.体积预处理的所有代码，自动标签提取，并且体积异常预测模型将被公开。在36,316 CT容积和标签也将公之于众未决机构的认可。</font>
</div>


<hr>
<div id="paper30"> <b>30. A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for  Healthcare</b>  <a href="https://arxiv.org/pdf/2002.04700" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziyang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deligianni%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fani Deligianni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Voiculescu%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Irina Voiculescu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guang-Zhong Yang</a><br>
<font size="3">
Abstract: With the increasing awareness of high-quality life, there is a growing need for health monitoring devices running robust algorithms in home environment. Health monitoring technologies enable real-time analysis of users' health status, offering long-term healthcare support and reducing hospitalization time. The purpose of this work is twofold, the software focuses on the analysis of gait, which is widely adopted for joint correction and assessing any lower limb or spinal problem. On the hardware side, we design a novel marker-less gait analysis device using a low-cost RGB camera mounted on a mobile tele-robot. As gait analysis with a single camera is much more challenging compared to previous works utilizing multi-cameras, a RGB-D camera or wearable sensors, we propose using vision-based human pose estimation approaches. More specifically, based on the output of two state-of-the-art human pose estimation models (Openpose and VNect), we devise measurements for four bespoke gait parameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot progression angles. We thereby classify walking patterns into normal, supination, pronation and limp. We also illustrate how to run the purposed machine learning models in low-resource environments such as a single entry-level CPU. Experiments show that our single RGB camera method achieves competitive performance compared to state-of-the-art methods based on depth cameras or multi-camera motion capture system, at smaller hardware costs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着越来越多的高品质生活的意识，人们越来越需要健康监测运行在家庭环境中稳定的算法设备。健康监测技术使用户的健康状况进行实时分析，提供长期的医疗支持，减少住院时间。这项工作的目的是双重的，该软件侧重于步态分析，广泛联合校正和评估任何下肢或脊柱问题采纳。在硬件方面，我们使用搭载于移动远程机器人低成本RGB照相机设计的新型无标记步态分析装置。如同一台摄像机步态分析更具有挑战性相比，利用多摄像机以往的作品，一个RGB-d相机或穿戴式传感器，我们建议采用基于视觉的人体姿势估计方法。更具体地，基于对状态的最先进的两种人类姿势估计模型（Openpose和VNect）的输出中，我们设计出四个定制步态参数测量：反转/外翻，背屈/跖，踝关节和脚的进展角度。我们由此分类行走模式为正常，外旋，内旋和跛行。我们还说明了如何运行旨意机器学习在低资源环境等车型单一的入门级CPU。实验结果表明，相比于基于深度相机或多相机运动捕捉系统，在较小的硬件成本的国家的最先进的方法提供了单个RGB相机方法实现有竞争力的性能。</font>
</div>


<hr>
<div id="paper31"> <b>31. fastai: A Layered API for Deep Learning</b>  <a href="https://arxiv.org/pdf/2002.04688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Howard%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeremy Howard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gugger%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sylvain Gugger</a><br>
<font size="3">
Abstract: fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：fastai是深学习库，其提供从业人员与高级别组件，可以快速且容易地提供先进的最先进的结果在标准深度学习域，并且为研究人员提供低级别组件，可以是混合和匹配建立新的方法。它的目的是做没有实质性妥协的东西都在易用性，灵活性和性能。这可能要归功于精心分层的体系结构，它表达了解耦抽象的条款处理技术的许多深学习的共同的基本模式和数据。这些抽象能够借助底层Python语言的活力和PyTorch库的灵活性言简意赅地表达。 fastai包括：用于Python与语义类型层次结构张量沿着一个新型调度系统;一个GPU优化计算机视觉库可以在纯Python进行扩展;优化器，其refactors出现代优化的通用功能分成两个基本块，从而允许优化算法，以在4-5线的代码来实现;一种新颖的2路回调系统，可以访问该数据，模型，或优化器的任何部分，并在训练过程中的任何点进行更改;一个新的数据块的API;以及更多。我们使用这个库成功地创建一个完整的深度学习过程中，我们能够比使用以前的方法更快速地编写，并且代码更加清晰。该库已经在研究，工业和教学用途广。</font>
</div>


<hr>
<div id="paper32"> <b>32. A Non-Intrusive Correction Algorithm for Classification Problems with  Corrupted Data</b>  <a href="https://arxiv.org/pdf/2002.04658" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kailiang Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongbin Xiu</a><br>
<font size="3">
Abstract: A novel correction algorithm is proposed for multi-class classification problems with corrupted training data. The algorithm is non-intrusive, in the sense that it post-processes a trained classification model by adding a correction procedure to the model prediction. The correction procedure can be coupled with any approximators, such as logistic regression, neural networks of various architectures, etc. When training dataset is sufficiently large, we prove that the corrected models deliver correct classification results as if there is no corruption in the training data. For datasets of finite size, the corrected models produce significantly better recovery results, compared to the models without the correction algorithm. All of the theoretical findings in the paper are verified by our numerical examples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一种新的校正算法，提出了多类分类问题已损坏的训练数据。该算法是非侵入性的，通过将校正过程的模型预测在这个意义上它后处理一个训练的分类模型。修正过程可以配上任何逼近，如逻辑回归，各种结构的神经网络，等等。当训练数据集是足够大的，我们证明了修正模型提供正确的分类结果作为是否有在训练数据中没有损坏。对于有限大小的数据集，校正模型产生显著更好的恢复效果，相比于没有纠错算法模型。所有在纸上的理论成果都是由我们的算例验证。</font>
</div>


<hr>
<div id="paper33"> <b>33. Neuroevolution of Neural Network Architectures Using CoDeepNEAT and  Keras</b>  <a href="https://arxiv.org/pdf/2002.04634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=da+Silveira+Bohrer%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonas da Silveira Bohrer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grisci%2C+B+I" target="_blank" rel="noopener" style="color:#0000EE;">Bruno Iochins Grisci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dorn%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marcio Dorn</a><br>
<font size="3">
Abstract: Machine learning is a huge field of study in computer science and statistics dedicated to the execution of computational tasks through algorithms that do not require explicit instructions but instead rely on learning patterns from data samples to automate inferences. A large portion of the work involved in a machine learning project is to define the best type of algorithm to solve a given problem. Neural networks - especially deep neural networks - are the predominant type of solution in the field. However, the networks themselves can produce very different results according to the architectural choices made for them. Finding the optimal network topology and configurations for a given problem is a challenge that requires domain knowledge and testing efforts due to a large number of parameters that need to be considered. The purpose of this work is to propose an adapted implementation of a well-established evolutionary technique from the neuroevolution field that manages to automate the tasks of topology and hyperparameter selection. It uses a popular and accessible machine learning framework - Keras - as the back-end, presenting results and proposed changes concerning the original algorithm. The implementation is available at GitHub (this https URL) with documentation and examples to reproduce the experiments performed for this work. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器学习是通过不需要明确的指示，而是依赖于从数据样本的学习模式自动推理算法专用于计算任务的执行在计算机科学和统计学研究的一个巨大的领域。参与机器学习项目工作中的很大一部分是定义算法来解决特定问题的最佳类型。神经网络 - 尤其是深层神经网络 - 是主要的类型在该领域的解决方案。然而，网络本身可以根据他们做出的架构选择产生非常不同的结果。寻找最佳的网络拓扑和配置，对于给定的问题是需要专业知识和测试工作，由于大量的需要考虑的参数是一个挑战。这项工作的目的是提出一种适合实现从neuroevolution现场管理自动拓扑和超参数选择的任务一套行之有效的进化技术。它采用了流行的和可访问的机器学习框架 -  Keras  - 作为后端，呈现的结果和有关原始算法修改建议。实施可在GitHub上（此HTTPS URL）与文档和示例重现对这项工作进行的实验。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-13</title>
    <url>/2020/02/14/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-13/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Learning to Compare for Better Training and Evaluation of Open Domain  Natural Language Generation Models <a href="https://arxiv.org/pdf/2002.05058" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Joint Embedding in Named Entity Linking on Sentence Level <a href="https://arxiv.org/pdf/2002.04936" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis  and Natural Language Inference <a href="https://arxiv.org/pdf/2002.04815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and  Diagnosing Dialogue Systems <a href="https://arxiv.org/pdf/2002.04793" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Two Huge Title and Keyword Generation Corpora of Research Articles <a href="https://arxiv.org/pdf/2002.04689" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Adjusting Image Attributes of Localized Regions with Low-level Dialogue <a href="https://arxiv.org/pdf/2002.04678" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Constructing a Highlight Classifier with an Attention Based LSTM Neural  Network <a href="https://arxiv.org/pdf/2002.04608" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Attentional Speech Recognition Models Misbehave on Out-of-domain  Utterances <a href="https://arxiv.org/pdf/2002.05150" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> DeepMutation: A Neural Mutation Tool <a href="https://arxiv.org/pdf/2002.04760" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> On Layer Normalization in the Transformer Architecture <a href="https://arxiv.org/pdf/2002.04745" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Superbloom: Bloom filter meets Transformer <a href="https://arxiv.org/pdf/2002.04723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> A Non-Intrusive Correction Algorithm for Classification Problems with  Corrupted Data <a href="https://arxiv.org/pdf/2002.04658" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Learning to Compare for Better Training and Evaluation of Open Domain  Natural Language Generation Models</b>  <a href="https://arxiv.org/pdf/2002.05058" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangchunshu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Xu</a><br>
<font size="3">
Abstract: Automated evaluation of open domain natural language generation (NLG) models remains a challenge and widely used metrics such as BLEU and Perplexity can be misleading in some cases. In our paper, we propose to evaluate natural language generation models by learning to compare a pair of generated sentences by fine-tuning BERT, which has been shown to have good natural language understanding ability. We also propose to evaluate the model-level quality of NLG models with sample-level comparison results with skill rating system. While able to be trained in a fully self-supervised fashion, our model can be further fine-tuned with a little amount of human preference annotation to better imitate human judgment. In addition to evaluating trained models, we propose to apply our model as a performance indicator during training for better hyperparameter tuning and early-stopping. We evaluate our approach on both story generation and chit-chat dialogue response generation. Experimental results show that our model correlates better with human preference compared with previous automated evaluation approaches. Training with the proposed metric yields better performance in human evaluation, which further demonstrates the effectiveness of the proposed model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：开域自然语言生成（NLG）模型自动评估仍然是一个挑战和广泛使用的指标，如BLEU和困惑可以在某些情况下会产生误导。在本文中，我们提出通过学习来比较微调BERT，这已被证明具有良好的自然语言理解能力对生成的句子来评估自然语言代车型。此外，我们建议评估NLG模型与样本级别的比较结果与技能等级系统模型的质量水平。虽然能在完全自我监督的方式进行训练，我们的模型可以进一步微调以人偏好补充说明的一点量，以便更好地模仿人的判断。除了评估训练的模型，我们建议采用我们的模型为更好地调整超参数和早期停止训练时的性能指标。我们评估我们的两个故事的产生和闲聊对话响应生成方法。实验结果表明，我们的模型相关因素与人类的喜好更好地与之前的自动评价方法进行了比较。在人的评价，这进一步表明了该模型的有效性提出的指标性能更好的训练。</font>
</div>


<hr>
<div id="paper2"> <b>2. Joint Embedding in Named Entity Linking on Sentence Level</b>  <a href="https://arxiv.org/pdf/2002.04936" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyuan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiwei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hong Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J+X" target="_blank" rel="noopener" style="color:#0000EE;">Jeffrey Xu Yu</a><br>
<font size="3">
Abstract: Named entity linking is to map an ambiguous mention in documents to an entity in a knowledge base. The named entity linking is challenging, given the fact that there are multiple candidate entities for a mention in a document. It is difficult to link a mention when it appears multiple times in a document, since there are conflicts by the contexts around the appearances of the mention. In addition, it is difficult since the given training dataset is small due to the reason that it is done manually to link a mention to its mapping entity. In the literature, there are many reported studies among which the recent embedding methods learn vectors of entities from the training dataset at document level. To address these issues, we focus on how to link entity for mentions at a sentence level, which reduces the noises introduced by different appearances of the same mention in a document at the expense of insufficient information to be used. We propose a new unified embedding method by maximizing the relationships learned from knowledge graphs. We confirm the effectiveness of our method in our experimental studies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：命名实体链接是在文档的暧昧中提到的知识库映射到实体。命名实体链接是具有挑战性的，鉴于有对文档中提及多个候选实体。这是很难当多次出现在文档中链接一提，因为有通过围绕提的出场语境冲突。此外，它是困难的，因为给定的训练数据集是小，由于它是手工完成的提链接到它的映射实体的原因。在文献中，有许多研究报道其中最近嵌入方法在文件级学会从训练数据集实体的载体。为了解决这些问题，我们把重点放在如何链接实体提到在句子水平，从而降低了使用通过在信息不足的费用的文件在同一提的不同外观引入了噪音。我们通过最大限度地从知识图学的关系，提出了一种新的统一的嵌入方法。我们确认我们的方法在我们的实验研究的有效性。</font>
</div>


<hr>
<div id="paper3"> <b>3. Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis  and Natural Language Inference</b>  <a href="https://arxiv.org/pdf/2002.04815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youwei Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiahai Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiwei Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyue Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Jiang</a><br>
<font size="3">
Abstract: Aspect based sentiment analysis aims to identify the sentimental tendency towards a given aspect in text. Fine-tuning of pretrained BERT performs excellent on this task and achieves state-of-the-art performances. Existing BERT-based works only utilize the last output layer of BERT and ignore the semantic knowledge in the intermediate layers. This paper explores the potential of utilizing BERT intermediate layers to enhance the performance of fine-tuning of BERT. To the best of our knowledge, no existing work has been done on this research. To show the generality, we also apply this approach to a natural language inference task. Experimental results demonstrate the effectiveness and generality of the proposed approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于看点情感分析的目的是确定对在文本中给定方面的感伤倾向。预训练的BERT执行优秀这项任务和微调实现了国家的最先进的性能。现有的基于BERT-作品只能利用BERT的最后一个输出层，而忽略中间层的语义知识。本文探讨利用BERT中间层，以提高BERT的微调的性能的潜力。据我们所知，没有现有的工作已经在这项研究完成的。要显示的普遍性，我们也将这种方法用于自然语言推理任务。实验结果表明，该方法的有效性和普遍性。</font>
</div>


<hr>
<div id="paper4"> <b>4. ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and  Diagnosing Dialogue Systems</b>  <a href="https://arxiv.org/pdf/2002.04793" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Fang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Takanobu%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryuichi Takanobu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinchao Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baolin Peng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyan Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minlie Huang</a><br>
<font size="3">
Abstract: We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab (Lee et al., 2019b), ConvLab-2 inherits ConvLab's framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前ConvLab-2，一个开源工具包，使研究人员能够与国家的最先进的机型构建面向任务的对话系统，执行结束到终端的评价，并诊断系统的弱点。作为ConvLab的继任者（Lee等，2019b），ConvLab-2继承ConvLab的框架，但集成功能更强大的对话模式，并支持多个数据集。此外，我们已经开发了一个分析工具和互动的工具，以帮助研究人员在诊断对话系统。该分析工具提供丰富的统计和总结了模拟对话，这有利于误差分析和系统改进常见的错误。交互式工具提供了一个用户接口，它允许开发者通过与系统进行交互和修改每个系统部件的输出来诊断组装对话系统。</font>
</div>


<hr>
<div id="paper5"> <b>5. Two Huge Title and Keyword Generation Corpora of Research Articles</b>  <a href="https://arxiv.org/pdf/2002.04689" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=%C3%87ano%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erion Çano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Ondřej Bojar</a><br>
<font size="3">
Abstract: Recent developments in sequence-to-sequence learning with neural networks have considerably improved the quality of automatically generated text summaries and document keywords, stipulating the need for even bigger training corpora. Metadata of research articles are usually easy to find online and can be used to perform research on various tasks. In this paper, we introduce two huge datasets for text summarization (OAGSX) and keyword generation (OAGKX) research, containing 34 million and 23 million records, respectively. The data were retrieved from the Open Academic Graph which is a network of research profiles and publications. We carefully processed each record and also tried several extractive and abstractive methods of both tasks to create performance baselines for other researchers. We further illustrate the performance of those methods previewing their outputs. In the near future, we would like to apply topic modeling on the two sets to derive subsets of research articles from more specific disciplines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：按顺序对序列的最新发展与神经网络学习具有显着提高自动生成的文本摘要及文档的关键字的质量，规定了更大的训练库的需要。研究文章的元数据通常是很容易在网上找到，可用于对各种任务进行研究。在本文中，我们介绍了文摘（OAGSX）和关键字生成（OAGKX）研究两个巨大的数据集，包含3400万个23万条记录，分别。该数据来自于不限学历图是研究概况和出版物网络检索。我们认真处理每一条记录，也尝试过的两个任务数，并提取方法，抽象创建其他研究者的性能基准。我们进一步说明这些方法预览它们的输出性能。在不久的将来，我们想从更具体的学科上两套应用主题建模研究文章的派生子集。</font>
</div>


<hr>
<div id="paper6"> <b>6. Adjusting Image Attributes of Localized Regions with Low-level Dialogue</b>  <a href="https://arxiv.org/pdf/2002.04678" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tzu-Hsiang Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rudnicky%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Rudnicky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trung Bui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D+S" target="_blank" rel="noopener" style="color:#0000EE;">Doo Soon Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean Oh</a><br>
<font size="3">
Abstract: Natural Language Image Editing (NLIE) aims to use natural language instructions to edit images. Since novices are inexperienced with image editing techniques, their instructions are often ambiguous and contain high-level abstractions that tend to correspond to complex editing steps to accomplish. Motivated by this inexperience aspect, we aim to smooth the learning curve by teaching the novices to edit images using low-level commanding terminologies. Towards this end, we develop a task-oriented dialogue system to investigate low-level instructions for NLIE. Our system grounds language on the level of edit operations, and suggests options for a user to choose from. Though compelled to express in low-level terms, a user evaluation shows that 25% of users found our system easy-to-use, resonating with our motivation. An analysis shows that users generally adapt to utilizing the proposed low-level language interface. In this study, we identify that object segmentation as the key factor to the user satisfaction. Our work demonstrates the advantages of the low-level, direct language-action mapping approach that can be applied to other problem domains beyond image editing such as audio editing or industrial design. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自然语言图像编辑（NLIE）旨在利用自然语言指令来进行编辑图像。由于新手与图像编辑技术经验不足，他们的指示往往模糊不清，而含有高层次的抽象，往往对应于复杂的编辑步骤来完成。这一方面缺乏经验的启发，我们的目标是通过采用低层次的指挥用语教新手编辑图像平滑的学习曲线。为此，我们开发了一个面向任务的对话系统，以调查NLIE低级指令。我们对编辑操作的层次体系理由的语言，并提出了一个用户从选择的选项。虽然被迫在低层次的形式来表示，用户评价结果显示，25％的用户发现，我们的系统易于使用，我们的动机共鸣。分析表明，用户一般适应于利用提出的低级语言界面。在这项研究中，我们确定对象分割为用户满意度的关键因素。我们的工作表明，低层次的，直接的语言，动作映射，可以应用到其他问题领域超越图像编辑，如音频编辑或工业设计方法的优点。</font>
</div>


<hr>
<div id="paper7"> <b>7. Constructing a Highlight Classifier with an Attention Based LSTM Neural  Network</b>  <a href="https://arxiv.org/pdf/2002.04608" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuehne%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Kuehne</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Radu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marius Radu</a><br>
<font size="3">
Abstract: Data is being produced in larger quantities than ever before in human history. It's only natural to expect a rise in demand for technology that aids humans in sifting through and analyzing this inexhaustible supply of information. This need exists in the market research industry, where large amounts of consumer research data is collected through video recordings. At present, the standard method for analyzing video data is human labor. Market researchers manually review the vast majority of consumer research video in order to identify relevant portions - highlights. The industry state of the art turnaround ratio is 2.2 - for every hour of video content 2.2 hours of manpower are required. In this study we present a novel approach for NLP-based highlight identification and extraction based on a supervised learning model that aides market researchers in sifting through their data. Our approach hinges on a manually curated user-generated highlight clips constructed from long and short-form video data. The problem is best suited for an NLP approach due to the availability of video transcription. We evaluate multiple classes of models, from gradient boosting to recurrent neural networks, comparing their performance in extraction and identification of highlights. The best performing models are then evaluated using four sampling methods designed to analyze documents much larger than the maximum input length of the classifiers. We report very high performances for the standalone classifiers, ROC AUC scores in the range 0.93-0.94, but observe a significant drop in effectiveness when evaluated on large documents. Based on our results we suggest combinations of models/sampling algorithms for various use cases. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据数量较多，比以往任何时候被生产在人类历史之前。这是很自然的期望在技术，帮助人们通过筛选和分析的信息，这取之不尽的需求量明显上升。这需要存在于市场研究行业，大量的消费者调研数据通过录像收集。目前，用于分析视频数据的标准方法是人的劳动。市场研究人员人工审核广大消费者的研究视频，以确定有关的部分 - 亮点。本领域的周转率的行业状态是2.2  - 用于视频内容每一小时都需要2.2小时人力。在这项研究中，我们提出了基于监督学习模型基于NLP高亮识别和提取的新方法是幕僚市场研究人员通过他们的数据进行筛选。我们的方法的铰链上从长和短形式的视频数据构成的手动管理用户生成的精彩片段。这个问题是最适合的NLP方法由于视频转录的可用性。我们评估多个类别的车型，从梯度提高到回归神经网络，比较它们的提取和亮点的识别性能。然后表现最好的模型使用设计用来分析文档比分类器的最大输入长度大得多的四个采样方法进行评价。我们提出非常高的演出独立分类，范围为0.93-0.94 ROC AUC分数，但对大文件进行评估时，观察有效性显著下跌。根据我们的结果，我们建议的模型组合/取样各种用例的算法。</font>
</div>


<hr>
<div id="paper8"> <b>8. Attentional Speech Recognition Models Misbehave on Out-of-domain  Utterances</b>  <a href="https://arxiv.org/pdf/2002.05150" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Keung%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phillip Keung</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Niu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Niu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yichao Lu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Salazar%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julian Salazar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bhardwaj%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vikas Bhardwaj</a><br>
<font size="3">
Abstract: We discuss the problem of echographic transcription in autoregressive sequence-to-sequence attentional architectures for automatic speech recognition, where a model produces very long sequences of repetitive outputs when presented with out-of-domain utterances. We decode audio from the British National Corpus with an attentional encoder-decoder model trained solely on the LibriSpeech corpus. We observe that there are many 5-second recordings that produce more than 500 characters of decoding output (i.e. more than 100 characters per second). A frame-synchronous hybrid (DNN-HMM) model trained on the same data does not produce these unusually long transcripts. These decoding issues are reproducible in a speech transformer model from ESPnet, and to a lesser extent in a self-attention CTC model, suggesting that these issues are intrinsic to the use of the attention mechanism. We create a separate length prediction model to predict the correct number of wordpieces in the output, which allows us to identify and truncate problematic decoding results without increasing word error rates on the LibriSpeech task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们讨论的自动语音识别，其中当与领域外的言论提出了一个模型产生重复输出很长的序列自回归序列对序列注意力架构回声转录的问题。我们解码来自英国国家语料库音频，单就LibriSpeech语料训练的一个所注意的编码器，解码器模型。我们观察到，有产生解码输出的超过500个字符（即，超过每秒100个字符）许多5秒记录。受过训练的对相同的数据的帧同步混合（DNN-HMM）模型不产生这些不寻常的长转录物。这些解码的问题是从ESPnet讲话变压器模型重复性好，在自我关注CTC模式在较小程度上，这表明这些问题是固有的使用注意机制。我们创建一个单独的长度预测模型来预测在输出中，这允许我们无需在LibriSpeech任务增加字错误率确定并截断问题的解码结果wordpieces的正确数目。</font>
</div>


<hr>
<div id="paper9"> <b>9. DeepMutation: A Neural Mutation Tool</b>  <a href="https://arxiv.org/pdf/2002.04760" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tufano%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michele Tufano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kimko%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Kimko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiya Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Watson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cody Watson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bavota%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabriele Bavota</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Di+Penta%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Massimiliano Di Penta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poshyvanyk%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Denys Poshyvanyk</a><br>
<font size="3">
Abstract: Mutation testing can be used to assess the fault-detection capabilities of a given test suite. To this aim, two characteristics of mutation testing frameworks are of paramount importance: (i) they should generate mutants that are representative of real faults; and (ii) they should provide a complete tool chain able to automatically generate, inject, and test the mutants. To address the first point, we recently proposed an approach using a Recurrent Neural Network Encoder-Decoder architecture to learn mutants from ~787k faults mined from real programs. The empirical evaluation of this approach confirmed its ability to generate mutants representative of real faults. In this paper, we address the second point, presenting DeepMutation, a tool wrapping our deep learning model into a fully automated tool chain able to generate, inject, and test mutants learned from real faults. Video: this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：突变的测试可用于评估给定的测试套件的故障检测功能。为了达到这个目的，突变测试框架两个特点是极为重要的：（i）它们应产生代表实际故障的突变体;及（ii）它们应该提供一个完整的工具链能够自动生成，注入，并测试突变体。为了解决第一个问题，我们最近提出使用递归神经网络编码器，解码器架构，了解从现实的方案开采〜787k故障突变体的方法。这种方法的实证评价确认了其产生的突变体代表实际故障的能力。在本文中，我们要解决的第二个点，呈现DeepMutation，一个工具包，我们深切的学习模式到一个完全自动化的工具链能够产生，注入，并从实际故障了解到测试的突变体。视频：此HTTPS URL</font>
</div>


<hr>
<div id="paper10"> <b>10. On Layer Normalization in the Transformer Architecture</b>  <a href="https://arxiv.org/pdf/2002.04745" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruibin Xiong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunchang Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Di He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuxin Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huishuai Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanyan Lan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liwei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tie-Yan Liu</a><br>
<font size="3">
Abstract: The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：变压器被广泛应用于自然语言处理任务。要培养一个变压器然而，人们通常需要一个精心设计的学习速度的热身阶段，这被证明是最后的表现至关重要，但会优化，带来更多超，参数调整放缓。在本文中，我们首先研究在理论上为什么学习率热身阶段是必要的，表明层正常化问题的位置。具体来说，我们用平均场理论证明，在初始化时，对原设计的后LN变压器，这使残余块之间的层正常化，输出层附近的参数的预期梯度很大。因此，使用这些渐变大的学习速率使得培训不稳定。在热身阶段，是为了避免这个问题实际上是有帮助的。在另一方面，我们的理论也表明，如果层标准化就是把剩余的块（最近提出的预LN变压器）内，梯度在初始化乖巧。这促使我们删除热身阶段为预LN变形金刚的培训。我们发现在我们的实验中，如果没有热身阶段预LN变压器可以达到基准比较的结果，同时要求显著减少培训时间和Hyper-参数整定了广泛的应用。</font>
</div>


<hr>
<div id="paper11"> <b>11. Superbloom: Bloom filter meets Transformer</b>  <a href="https://arxiv.org/pdf/2002.04723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Anderson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingqing Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krichene%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Walid Krichene</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rendle%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steffen Rendle</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Zhang</a><br>
<font size="3">
Abstract: We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们延长的字块在自然语言模型对不透明IDS机器学习任务的想法。这通过将散列函数到每个ID多个散列令牌映射在一个更小的空间中，类似于一个布隆过滤器来实现的。我们发现，通过将多层变压器这些布隆过滤器摘要，我们能够高精度地获取模型。他们胜过一个同样大小的模型没有散列和，在很大程度上，一个更大尺寸的模型中使用抽样SOFTMAX与相同的计算预算培训。我们的主要发现是，它使用了多层互感器布隆过滤器消化在散列输入歧义除去是很重要的。我们认为，这提供了另一种解决大词汇量的问题。</font>
</div>


<hr>
<div id="paper12"> <b>12. A Non-Intrusive Correction Algorithm for Classification Problems with  Corrupted Data</b>  <a href="https://arxiv.org/pdf/2002.04658" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kailiang Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongbin Xiu</a><br>
<font size="3">
Abstract: A novel correction algorithm is proposed for multi-class classification problems with corrupted training data. The algorithm is non-intrusive, in the sense that it post-processes a trained classification model by adding a correction procedure to the model prediction. The correction procedure can be coupled with any approximators, such as logistic regression, neural networks of various architectures, etc. When training dataset is sufficiently large, we prove that the corrected models deliver correct classification results as if there is no corruption in the training data. For datasets of finite size, the corrected models produce significantly better recovery results, compared to the models without the correction algorithm. All of the theoretical findings in the paper are verified by our numerical examples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一种新的校正算法，提出了多类分类问题已损坏的训练数据。该算法是非侵入性的，通过将校正过程的模型预测在这个意义上它后处理一个训练的分类模型。修正过程可以配上任何逼近，如逻辑回归，各种结构的神经网络，等等。当训练数据集是足够大的，我们证明了修正模型提供正确的分类结果作为是否有在训练数据中没有损坏。对于有限大小的数据集，校正模型产生显著更好的恢复效果，相比于没有纠错算法模型。所有在纸上的理论成果都是由我们的算例验证。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【论文笔记】Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation</title>
    <url>/2020/02/13/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Unsupervised-Domain-Adaptation-for-Neural-Machine-Translation-with-Iterative-Back-Translation/</url>
    <content><![CDATA[<p><strong>Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation</strong>. Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits. AAAI 2020. <a href="https://arxiv.org/pdf/2001.08140.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>构造领域平行数据成本很高，如何在没有领域平行数据的情况下训练领域翻译模型显得尤为重要。本文想要解决的就是非监督领域适应NMT问题，提出了一种新的构造领域平行数据的方法：迭代回翻。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="/images/UDA_IBT_1.png" alt></p>
<p>本文提出一种基于transformer的模型框架，修改了transformer的encoder和decoder的输入，加入了language embedding，该模型具有以下特点：<br><br>1.源语言和目标语言共享bpe词表 <br><br>2.源语言和目标语言共享隐空间 <br></p>
<p>本文使用该模型用来训练语言模型、S2T翻译模型、T2S翻译模型，并且它们共享参数。<br>训练过程分三个步骤：<br>1.使用领域单语数据训练语言模型<br><img src="/images/UDA_IBT_3.png" alt></p>
<p>2.使用S2T翻译模型构造伪平行数据训练T2S模型，使用T2S翻译模型构造伪平行数据训练S2T模型<br><img src="/images/UDA_IBT_4.png" alt><br><font style="color: red">*公式中应该是作者笔误，顺序写错了。</font></p>
<p>3.使用平行数据训练模型<br><img src="/images/UDA_IBT_5.png" alt></p>
<p>不断迭代三个步骤直到参数收敛。</p>
<p>算法表示如下</p>
<p><img src="/images/UDA_IBT_2.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><img src="/images/UDA_IBT_6.png" alt></p>
<ul>
<li><strong>COPY</strong>：混合(t_in, t_in)和(s_out, t_out)，一起训练nmt</li>
<li><strong>BACK</strong>：使用Model_out构造伪平行in-domain数据，混合out-domain数据</li>
<li><strong>DALI</strong>：使用in-domain词表翻译t_in sent，构造伪平行数据，finetune  Model_out</li>
<li><strong>DAFE</strong>：多任务，NMT_out和LM_in (insert domain and task embedding)</li>
<li><strong>IBT</strong>: 迭代回翻，但不使用out-domain数据（也就是没有步骤三，完全无监督翻译）</li>
<li><strong>IBT+OUTD</strong>: 使用out-domain数据训练步骤三</li>
<li><strong>IBT+BACK</strong>: 使用伪平行数据和out-domain数据一起训练步骤三</li>
</ul>
<p>消融实验<br><img src="/images/UDA_IBT_7.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>AAAI</tag>
        <tag>迭代回翻</tag>
        <tag>非监督</tag>
        <tag>领域适应</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-12</title>
    <url>/2020/02/12/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-12/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> The Rumour Mill: Making Misinformation Spread Visible and Tangible <a href="https://arxiv.org/pdf/2002.04494" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning <a href="https://arxiv.org/pdf/2002.04326" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Learning Coupled Policies for Simultaneous Machine Translation <a href="https://arxiv.org/pdf/2002.04306" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Non-Autoregressive Neural Dialogue Generation <a href="https://arxiv.org/pdf/2002.04250" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity  Recognition and Sentiment Analysis of Political Tweets <a href="https://arxiv.org/pdf/2002.04181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Training with Streaming Annotation <a href="https://arxiv.org/pdf/2002.04165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Automatic Discourse Segmentation: an evaluation in French <a href="https://arxiv.org/pdf/2002.04095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> An experiment exploring the theoretical and methodological challenges in  developing a semi-automated approach to analysis of small-N qualitative data <a href="https://arxiv.org/pdf/2002.04513" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> HGAT: Hierarchical Graph Attention Network for Fake News Detection <a href="https://arxiv.org/pdf/2002.04397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Convolutional Neural Networks and a Transfer Learning Strategy to  Classify Parkinson's Disease from Speech in Three Different Languages <a href="https://arxiv.org/pdf/2002.04374" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Adversarial Filters of Dataset Biases <a href="https://arxiv.org/pdf/2002.04108" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. The Rumour Mill: Making Misinformation Spread Visible and Tangible</b>  <a href="https://arxiv.org/pdf/2002.04494" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Inie%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nanna Inie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Olesen%2C+J+F" target="_blank" rel="noopener" style="color:#0000EE;">Jeanette Falk Olesen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Derczynski%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leon Derczynski</a><br>
<font size="3">
Abstract: The spread of misinformation presents a technological and social threat to society. With the advance of AI-based language models, automatically generated texts have become difficult to identify and easy to create at scale. We present the "Rumour Mill", a playful art piece, designed as a commentary on the spread of rumours and automatically-generated misinformation. The mill is a tabletop interactive machine, which invites a user to experience the process of creating believable text by interacting with different tangible controls on the mill. The user manipulates visible parameters to adjust the genre and type of an automatically generated text rumour. The Rumour Mill is a physical demonstration of the state of NLP technology and its ability to generate and manipulate natural language text, and of the act of starting and spreading rumours. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：误传的传播呈现给社会技术和社会的威胁。随着基于人工智能语言模型的推进，自动生成的文本已经变得难以识别，容易大规模制造。我们提出了“传闻”，一个好玩艺术片，设计为传言和自动生成的误传传播的评注。该工厂是一个桌面交互的机器，它邀请用户通过与轧机上不同的实际控制交互体验创造可信的文本的过程。用户操纵可见参数来调整体裁和键入一个自动生成的文本的传言。谣言是NLP技术状况及其产生和处理自然语言文字能力的物理演示，以及启动和传播谣言的行为。</font>
</div>


<hr>
<div id="paper2"> <b>2. ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning</b>  <a href="https://arxiv.org/pdf/2002.04326" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weihao Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanfei Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiashi Feng</a><br>
<font size="3">
Abstract: Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text. In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text. In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set. Empirical results show that state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set. However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近期强大的预先训练的语言模型已经达到上最流行的数据集的表现可圈可点阅读理解。现在是时候推出更多具有挑战性的数据集，以推动这一领域的发展对文本的更全面的推理。在本文中，我们介绍一种新的阅读理解数据集需要从标准化的研究生入学考试中提取的逻辑推理（ReClor）。正如之前的研究表明，人类的注解数据集通常包含的偏见，这往往是由模型利用来达到较高的精度没有真正理解课文。为了全面评估对ReClor车型的逻辑推理能力，我们建议确定偏置数据点，将它们分开成易于设置，而其余硬集。实证结果表明，国家的最先进的机型有出色的能力，包含在与EASY集高精度数据集中采集偏见。然而，他们在HARD组与斗争是附近随机猜测的表现不佳，表示需要基本上提高现有模式的逻辑推理能力进行更多的研究。</font>
</div>


<hr>
<div id="paper3"> <b>3. Learning Coupled Policies for Simultaneous Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.04306" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Arthur%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philip Arthur</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trevor Cohn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gholamreza Haffari</a><br>
<font size="3">
Abstract: In simultaneous machine translation, the system needs to incrementally generate the output translation before the input sentence ends. This is a coupled decision process consisting of a programmer and interpreter. The programmer's policy decides about when to WRITE the next output or READ the next input, and the interpreter's policy decides what word to write. We present an imitation learning (IL) approach to efficiently learn effective coupled programmer-interpreter policies. To enable IL, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. We attribute the effectiveness of the learned coupled policies to (i) scheduled sampling addressing the coupled exposure bias, and (ii) quality of oracle actions capturing enough information from the partial input before writing the output. Experiments show our method outperforms strong baselines in terms of translation quality and delay, when translating from German/Arabic/Czech/Bulgarian/Romanian to English. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在同时机器翻译系统需要逐步产生输入句子结束前的输出转换。这是由一个程序员和解释器的耦合决定处理。程序员的政策决定什么时候写下一个输出或读取下一个输入，而口译的政策决定写什么字。我们提出了一个模仿学习（IL）的方法来有效地学习有效耦合程序员解释政策。为了使IL，我们提出了一个算法甲骨文的Oracle产品的读/写操作训练用的词对齐的概念双语句子对。我们认为所学习的耦合政策的效力与（i）预定的采样寻址耦合曝光偏置，和（ii）的Oracle动作写入输出之前捕获来自所述部分输入足够的信息的质量。实验证明我们的方法优于在翻译质量和延迟方面强大的基线，从德国/阿拉伯文/捷克/保加利亚/罗马尼亚翻译成英文。</font>
</div>


<hr>
<div id="paper4"> <b>4. Non-Autoregressive Neural Dialogue Generation</b>  <a href="https://arxiv.org/pdf/2002.04250" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxian Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwei Li</a><br>
<font size="3">
Abstract: Maximum Mutual information (MMI), which models the bidirectional dependency between responses ($y$) and contexts ($x$), i.e., the forward probability $\log p(y|x)$ and the backward probability $\log p(x|y)$, has been widely used as the objective in the \sts model to address the dull-response issue in open-domain dialog generation. Unfortunately, under the framework of the \sts model, direct decoding from $\log p(y|x) + \log p(x|y)$ is infeasible since the second part (i.e., $p(x|y)$) requires the completion of target generation before it can be computed, and the search space for $y$ is enormous. Empirically, an N-best list is first generated given $p(y|x)$, and $p(x|y)$ is then used to rerank the N-best list, which inevitably results in non-globally-optimal solutions. In this paper, we propose to use non-autoregressive (non-AR) generation model to address this non-global optimality issue. Since target tokens are generated independently in non-AR generation, $p(x|y)$ for each target word can be computed as soon as it's generated, and does not have to wait for the completion of the whole sequence. This naturally resolves the non-global optimal issue in decoding. Experimental results demonstrate that the proposed non-AR strategy produces more diverse, coherent, and appropriate responses, yielding substantive gains in BLEU scores and in human evaluations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最大互信息（MMI），该款机型的响应（$ Y $）和环境（$ X $）之间，即双向依赖，前向概率$ \日志P（Y | X）$和反向概率$ \日志p（X | Y）$，已被广泛用作目标在\ STS模式，以解决开域对话生成的平淡反应的问题。不幸的是，\ STS模型的框架下，直接解码从$ \日志P（Y | X）+ \日志P（X | Y）$是因为第二部分（即，$ P（X不可行| Y）$ ）要求目标生成的完成可以计算它之前，和$ Y $的搜索空间是巨大的。根据经验，N-最佳列表首先生成给出$ P（Y | X）$和$ P（X | Y），则$用于重新排名的N最佳列表，这不可避免地导致了非全局最优解。在本文中，我们建议使用非自回归（非AR）代车型，以解决这种非全局最优的问题。由于目标令牌在非AR生成独立产生，$ P（X | Y）$每个目标词可以很快，因为它是生成的计算，而不必等待整个序列的完成。这自然解决了解码非全局最优的问题。实验结果表明，所提出的非AR战略产生更多样化的，连贯的，适当的反应，产生的BLEU分数和人类评估实质性收益。</font>
</div>


<hr>
<div id="paper5"> <b>5. Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity  Recognition and Sentiment Analysis of Political Tweets</b>  <a href="https://arxiv.org/pdf/2002.04181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jalal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mona Jalal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mays%2C+K+K" target="_blank" rel="noopener" style="color:#0000EE;">Kate K. Mays</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Betke%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Margrit Betke</a><br>
<font size="3">
Abstract: We report results of a comparison of the accuracy of crowdworkers and seven NaturalLanguage Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition (NER) and entity-level sentiment(ELS) analysis. We here focus on a challenging dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates,i.e., four named entities. The groundtruth, established by experts in political communication, has entity-level sentiment information for each candidate mentioned in the tweet. We tested several commercial and open-source tools. Our experiments show that, for our dataset of political tweets, the most accurate NER system, Google Cloud NL, performed almost on par with crowdworkers, but the most accurate ELS analysis system, TensiStrength, did not match the accuracy of crowdworkers by a large margin of more than 30 percent points. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们报告crowdworkers在解决两个重要的NLP任务，命名实体识别（NER）和公司层面的情绪（ELS）分析的准确性和七个自然语言处理（NLP）工具包的比较结果。在这里，我们专注于一个具有挑战性的数据集，1000在2016年二月，美国总统初选每个鸣叫是指四个总统候选人至少一个，即，四个命名实体期间收集政治鸣叫。真实状况，由专家在政治传播成立以来，一直在鸣叫提到的每个候选实体层面的情绪信息。我们测试了几个商业和开源工具。我们的实验表明，对于我们的政治鸣叫，最准确的命名实体识别系统，谷歌云NL，几乎堪与crowdworkers执行的数据集，但最准确的ELS分析系统，TensiStrength，并没有大幅度匹配crowdworkers的准确性超过30个百分点。</font>
</div>


<hr>
<div id="paper6"> <b>6. Training with Streaming Annotation</b>  <a href="https://arxiv.org/pdf/2002.04165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tongtao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heng Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shih-Fu Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Freedman%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marjorie Freedman</a><br>
<font size="3">
Abstract: In this paper, we address a practical scenario where training data is released in a sequence of small-scale batches and annotation in earlier phases has lower quality than the later counterparts. To tackle the situation, we utilize a pre-trained transformer network to preserve and integrate the most salient document information from the earlier batches while focusing on the annotation (presumably with higher quality) from the current batch. Using event extraction as a case study, we demonstrate in the experiments that our proposed framework can perform better than conventional approaches (the improvement ranges from 3.6 to 14.9% absolute F-score gain), especially when there is more noise in the early annotation; and our approach spares 19.1% time with regard to the best conventional method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们要解决这里的训练数据被释放的小规模批次序列和注释在早期阶段具有比同行晚低质量的实际情况。为了应对这种情况，我们利用预先训练变压器网络维护和最显着的文档信息从早期批次集成，并重点标注（大概有更高质量的）从目前的批次。使用事件提取作为个案研究，我们证明在实验中，我们提出的架构可以比传统方法更好地履行（改善的范围从3.6到14.9％的绝对F-分数增益），尤其是当有更多的噪音在早期的注释;而我们的方法免去19.1％的时间就以最好的常规方法。</font>
</div>


<hr>
<div id="paper7"> <b>7. Automatic Discourse Segmentation: an evaluation in French</b>  <a href="https://arxiv.org/pdf/2002.04095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saksik%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rémy Saksik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Molina-Villegas%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alejandro Molina-Villegas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Linhares%2C+A+C" target="_blank" rel="noopener" style="color:#0000EE;">Andréa Carneiro Linhares</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torres-Moreno%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juan-Manuel Torres-Moreno</a><br>
<font size="3">
Abstract: In this article, we describe some discursive segmentation methods as well as a preliminary evaluation of the segmentation quality. Although our experiment were carried for documents in French, we have developed three discursive segmentation models solely based on resources simultaneously available in several languages: marker lists and a statistic POS labeling. We have also carried out automatic evaluations of these systems against the Annodis corpus, which is a manually annotated reference. The results obtained are very encouraging. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这篇文章中，我们介绍一些话语分割方法以及分割质量的初步评估。标记列表和统计POS标签：虽然我们的实验中，进行了在法国的文件中，我们基于几种语言的同时可利用的资源仅开发了三种话语分割模型。我们还进行了针对Annodis语料库，其是手动注释参考这些系统的自动评估。得到的结果是非常令人鼓舞的。</font>
</div>


<hr>
<div id="paper8"> <b>8. An experiment exploring the theoretical and methodological challenges in  developing a semi-automated approach to analysis of small-N qualitative data</b>  <a href="https://arxiv.org/pdf/2002.04513" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tsang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sandro Tsang</a><br>
<font size="3">
Abstract: This paper experiments with designing a semi-automated qualitative data analysis (QDA) algorithm to analyse 20 transcripts by using freeware. Text-mining (TM) and QDA were guided by frequency and association measures, because these statistics remain robust when the sample size is small. The refined TM algorithm split the text into various sizes based on a manually revised dictionary. This lemmatisation approach may reflect the context of the text better than uniformly tokenising the text into one single size. TM results were used for initial coding. Code repacking was guided by association measures and external data to implement a general inductive QDA approach. The information retrieved by TM and QDA was depicted in subgraphs for comparisons. The analyses were completed in 6-7 days. Both algorithms retrieved contextually consistent and relevant information. However, the QDA algorithm retrieved more specific information than TM alone. The QDA algorithm does not strictly comply with the convention of TM or of QDA, but becomes a more efficient, systematic and transparent text analysis approach than a conventional QDA approach. Scaling up QDA to reliably discover knowledge from text was exactly the research purpose. This paper also sheds light on understanding the relations between information technologies, theory and methodologies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：设计一个半自动化定性数据分析（QDA）算法实验通过使用免费软件来分析20组的转录本。文本挖掘（TM）和QDA通过频率和关联的措施引导，因为这些统计信息保持稳健当样本大小是小的。精制TM算法分割文成基于手动订正字典各种尺寸。这lemmatisation做法可能反映了文本的语境比文字均匀tokenising成一个单一的大小更好。 TM结果用于初始编码。代码重新包装是由协会的措施和外部数据导入到实施一般的感应QDA方法。由TM和QDA检索的信息在子图用于比较被描绘。分析是在6-7天内完成。这两种算法检索上下文一致的相关信息。然而，QDA算法获取更具体的信息比TM孤独。该QDA算法不严格遵守TM或QDA的惯例，但比传统的QDA方法更高效，系统，透明的文本分析方法。扩大QDA可靠地发现从文本知识正是研究目的。本文还揭示了理解信息技术，理论和方法之间的关系光。</font>
</div>


<hr>
<div id="paper9"> <b>9. HGAT: Hierarchical Graph Attention Network for Fake News Detection</b>  <a href="https://arxiv.org/pdf/2002.04397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxiang Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiawei Zhang</a><br>
<font size="3">
Abstract: The explosive growth of fake news has eroded the credibility of medias and governments. Fake news detection has become an urgent task. News articles along with other related components like news creators and news subjects can be modeled as a heterogeneous information network (HIN for short). In this paper, we focus on studying the HIN- based fake news detection problem. We propose a novel fake news detection framework, namely Hierarchical Graph Attention Network (HGAT) which employs a novel hierarchical attention mechanism to detect fake news by classifying news article nodes in the HIN. This method can effectively learn information from different types of related nodes through node-level and schema-level attention. Experiments with real-world fake news data show that our model can outperform text-based models and other network-based models. Besides, the experiments also demonstrate the expandability and potential of HGAT for heterogeneous graphs representation learning in the future. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：假新闻的爆炸性增长已经侵蚀媒体和政府的公信力。假新闻的检测已成为一项紧迫的任务。与像新闻创作者和新闻主体等相关组件一起的新闻文章可以模拟成一个异构信息网络（HIN的简称）。在本文中，我们重点研究基于HIN-假新闻的检测问题。我们提出了一个新的假新闻的检测框架，即层次图关注网络（HGAT），它采用了新的分级注意机制由HIN新闻文章分类节点检测到假新闻。这种方法可以有效地学习，通过节点级和模式的高度重视，从不同类型的相关节点的信息。与现实世界的假新闻数据实验表明，我们的模型可以超越基于文本的模型和其他基于网络的模型。此外，实验还证明HGAT的在未来的异构图形表示学习可扩展性和潜力。</font>
</div>


<hr>
<div id="paper10"> <b>10. Convolutional Neural Networks and a Transfer Learning Strategy to  Classify Parkinson's Disease from Speech in Three Different Languages</b>  <a href="https://arxiv.org/pdf/2002.04374" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=V%C3%A1squez-Correa%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">J. C. Vásquez-Correa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arias-Vergara%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">T. Arias-Vergara</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rios-Urrego%2C+C+D" target="_blank" rel="noopener" style="color:#0000EE;">C. D. Rios-Urrego</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">M. Schuster</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rusz%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">J. Rusz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Orozco-Arroyave%2C+J+R" target="_blank" rel="noopener" style="color:#0000EE;">J. R. Orozco-Arroyave</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=N%C3%B6th%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">E. Nöth</a><br>
<font size="3">
Abstract: Parkinson's disease patients develop different speech impairments that affect their communication capabilities. The automatic assessment of the speech of the patients allows the development of computer aided tools to support the diagnosis and the evaluation of the disease severity. This paper introduces a methodology to classify Parkinson's disease from speech in three different languages: Spanish, German, and Czech. The proposed approach considers convolutional neural networks trained with time frequency representations and a transfer learning strategy among the three languages. The transfer learning scheme aims to improve the accuracy of the models when the weights of the neural network are initialized with utterances from a different language than the used for the test set. The results suggest that the proposed strategy improves the accuracy of the models in up to 8\% when the base model used to initialize the weights of the classifier is robust enough. In addition, the results obtained after the transfer learning are in most cases more balanced in terms of specificity-sensitivity than those trained without the transfer learning strategy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：帕金森氏症患者制定影响其通信能力不同的语言障碍。患者的语音的自动评估允许的计算机辅助工具的开发，以支持诊断和疾病严重程度的评估。本文介绍一种方法，帕金森氏病从语音三种语言进行分类，西班牙语，德语和捷克。所提出的方法考虑了频率随时间的陈述和三种语言之间的迁移学习策略训练的卷积神经网络。转移学习方案，目的是提高模型的准确性时，神经网络的权与话语从不同的语言不是用于测试集初始化。结果表明，该策略提高了多达8 \％的模型的准确性时使用的基础模型来初始化权重的分类是足够强大的。此外，转移学习之后得到的结果都在比那些没有转移学习策略训练的特异性，灵敏度方面更平衡大多数情况下。</font>
</div>


<hr>
<div id="paper11"> <b>11. Adversarial Filters of Dataset Biases</b>  <a href="https://arxiv.org/pdf/2002.04108" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L" target="_blank" rel="noopener" style="color:#0000EE;">Ronan Le Bras</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Swayamdipta%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Swabha Swayamdipta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhagavatula%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chandra Bhagavatula</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zellers%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rowan Zellers</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E" target="_blank" rel="noopener" style="color:#0000EE;">Matthew E. Peters</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabharwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashish Sabharwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yejin Choi</a><br>
<font size="3">
Abstract: Large neural models have demonstrated human-level performance on language and vision benchmarks such as ImageNet and Stanford Natural Language Inference (SNLI). Yet, their performance degrades considerably when tested on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting on spurious dataset biases. We investigate one recently proposed approach, AFLite, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLite, by situating it in the generalized framework for optimum bias reduction. Our experiments show that as a result of the substantial reduction of these biases, models trained on the filtered datasets yield better generalization to out-of-distribution tasks, especially when the benchmarks used for training are over-populated with biased samples. We show that AFLite is broadly applicable to a variety of both real and synthetic datasets for reduction of measurable dataset biases and provide extensive supporting analyses. Finally, filtering results in a large drop in model performance (e.g., from 92% to 63% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型神经模型已经证明在语言和视觉基准，如ImageNet和斯坦福大学自然语言推理（SNLI）人类水平的性能。然而，它们的性能会下降显着，当上对抗或分发外的样品进行测试。这就提出了这些模型是否已经学会对过度拟合数据集虚假偏见，解决了数据集而不是底层任务的问题。我们调查一个最近提出的方法，AFLite，这adversarially过滤器，例如数据集的偏见，作为一种手段来减轻整机性能普遍高估。我们为AFLite提供理论的理解，通过在最佳偏置降低广义框架的情境吧。我们的实验显示，这些偏见的大幅度减少的结果，训练有素的过滤数据集模型产生更好的推广到外的配送任务，特别是当用于训练的基准测试过填充偏置样品。我们表明，AFLite广泛适用于各种实际和综合数据集的减少衡量数据集的偏见，并提供广泛的支持分析。最后，过滤结果在模型的性能（例如SNLI，从92％至63％）大的下降，而人的性能仍然保持为高。因此，我们的工作表明，这种过滤的数据集可以作为升级基准构成了强大的推广新研究挑战。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-11</title>
    <url>/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-11/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Upper, Middle and Lower Region Learning for Facial Action Unit Detection <a href="https://arxiv.org/pdf/2002.04023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Deep Convolutional Neural Networks with Spatial Regularization, Volume  and Star-shape Priori for Image Segmentation <a href="https://arxiv.org/pdf/2002.03989" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Unconstrained Periocular Recognition: Using Generative Deep Learning  Frameworks for Attribute Normalization <a href="https://arxiv.org/pdf/2002.03985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> StickyPillars: Robust feature matching on point clouds using Graph  Neural Networks <a href="https://arxiv.org/pdf/2002.03983" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Joint Encoding of Appearance and Motion Features with Self-supervision  for First Person Action Recognition <a href="https://arxiv.org/pdf/2002.03982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> RePose: Learning Deep Kinematic Priors for Fast Human Pose Estimation <a href="https://arxiv.org/pdf/2002.03933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> 6DoF Object Pose Estimation via Differentiable Proxy Voting Loss <a href="https://arxiv.org/pdf/2002.03923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Hierarchical Multi-Process Fusion for Visual Place Recognition <a href="https://arxiv.org/pdf/2002.03895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> CIFAR-10 Image Classification Using Feature Ensembles <a href="https://arxiv.org/pdf/2002.03846" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Exploiting Temporal Coherence for Multi-modal Video Categorization <a href="https://arxiv.org/pdf/2002.03844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Attentive Group Equivariant Convolutional Networks <a href="https://arxiv.org/pdf/2002.03830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Level Three Synthetic Fingerprint Generation <a href="https://arxiv.org/pdf/2002.03809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Automatic image-based identification and biomass estimation of  invertebrates <a href="https://arxiv.org/pdf/2002.03807" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> CONVINCE: Collaborative Cross-Camera Video Analytics at the Edge <a href="https://arxiv.org/pdf/2002.03797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Deep Learning for Classifying Food Waste <a href="https://arxiv.org/pdf/2002.03786" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Multi-stream Faster RCNN for Mitosis Counting in Breast Cancer Images <a href="https://arxiv.org/pdf/2002.03781" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Towards Deep Machine Reasoning: a Prototype-based Deep Neural Network  with Decision Tree Inference <a href="https://arxiv.org/pdf/2002.03776" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Deriving Emotions and Sentiments from Visual Content: A Disaster  Analysis Use Case <a href="https://arxiv.org/pdf/2002.03773" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Improving the Evaluation of Generative Models with Fuzzy Logic <a href="https://arxiv.org/pdf/2002.03772" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Learning Numerical Observers using Unsupervised Domain Adaptation <a href="https://arxiv.org/pdf/2002.03763" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Music2Dance: Music-driven Dance Generation using WaveNet <a href="https://arxiv.org/pdf/2002.03761" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> An Empirical Study of Person Re-Identification with Attributes <a href="https://arxiv.org/pdf/2002.03752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Weighted Average Precision: Adversarial Example Detection in the Visual  Perception of Autonomous Vehicles <a href="https://arxiv.org/pdf/2002.03751" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> An Overview of Two Age Synthesis and Estimation Techniques <a href="https://arxiv.org/pdf/2002.03750" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> DFKI Cabin Simulator: A Test Platform for Visual In-Cabin Monitoring  Functions <a href="https://arxiv.org/pdf/2002.03749" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Black Box Explanation by Learning Image Exemplars in the Latent Feature  Space <a href="https://arxiv.org/pdf/2002.03746" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Dynamic Error-bounded Lossy Compression (EBLC) to Reduce the Bandwidth  Requirement for Real-time Vision-based Pedestrian Safety Applications <a href="https://arxiv.org/pdf/2002.03742" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Efficient Scene Text Detection with Textual Attention Tower <a href="https://arxiv.org/pdf/2002.03741" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Convolutional Hierarchical Attention Network for Query-Focused Video  Summarization <a href="https://arxiv.org/pdf/2002.03740" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Localizing Multi-scale Semantic Patches for Image Classification <a href="https://arxiv.org/pdf/2002.03737" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Universal Semantic Segmentation for Fisheye Urban Driving Images <a href="https://arxiv.org/pdf/2002.03736" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Real-Time Object Detection and Recognition on Low-Compute Humanoid  Robots using Deep Learning <a href="https://arxiv.org/pdf/2002.03735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Iterative energy-based projection on a normal data manifold for anomaly  localization <a href="https://arxiv.org/pdf/2002.03734" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Robust Multimodal Image Registration Using Deep Recurrent Reinforcement  Learning <a href="https://arxiv.org/pdf/2002.03733" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Impact of Data Quality on Deep Neural Network Training <a href="https://arxiv.org/pdf/2002.03732" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> RSnet: An improvement for Darknet <a href="https://arxiv.org/pdf/2002.03729" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Driver Drowsiness Detection Model Using Convolutional Neural Networks  Techniques for Android Application <a href="https://arxiv.org/pdf/2002.03728" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Durocmien: A deep framework for duroc skeleton extraction in constraint  environment <a href="https://arxiv.org/pdf/2002.03727" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Deep Frequent Spatial Temporal Learning for Face Anti-Spoofing <a href="https://arxiv.org/pdf/2002.03723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Unsupervised deep clustering for predictive texture pattern discovery in  medical images <a href="https://arxiv.org/pdf/2002.03721" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Fabricated Pictures Detection with Graph Matching <a href="https://arxiv.org/pdf/2002.03720" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Knowledge Distillation for Brain Tumor Segmentation <a href="https://arxiv.org/pdf/2002.03688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Deep Multi-task Multi-label CNN for Effective Facial Attribute  Classification <a href="https://arxiv.org/pdf/2002.03683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Uncertainty Estimation for End-To-End Learned Dense Stereo Matching via  Probabilistic Deep Learning <a href="https://arxiv.org/pdf/2002.03663" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Distribution Distillation Loss: Generic Approach for Improving Face  Recognition from Hard Samples <a href="https://arxiv.org/pdf/2002.03662" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> CRVOS: Clue Refining Network for Video Object Segmentation <a href="https://arxiv.org/pdf/2002.03651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> Collaborative Training of Balanced Random Forests for Open Set Domain  Adaptation <a href="https://arxiv.org/pdf/2002.03642" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> End-to-End Facial Deep Learning Feature Compression with Teacher-Student  Enhancement <a href="https://arxiv.org/pdf/2002.03627" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Post-Comparison Mitigation of Demographic Bias in Face Recognition Using  Fair Score Normalization <a href="https://arxiv.org/pdf/2002.03592" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> Prototype Refinement Network for Few-Shot Segmentation <a href="https://arxiv.org/pdf/2002.03579" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> Automatic detection and counting of retina cell nuclei using deep  learning <a href="https://arxiv.org/pdf/2002.03563" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> FAU, Facial Expressions, Valence and Arousal: A Multi-task Solution <a href="https://arxiv.org/pdf/2002.03557" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> Vehicle Driving Assistant <a href="https://arxiv.org/pdf/2002.03556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> From Anchor Generation to Distribution Alignment: Learning a  Discriminative Embedding Space for Zero-Shot Recognition <a href="https://arxiv.org/pdf/2002.03554" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> UGRWO-Sampling: A modified random walk under-sampling approach based on  graphs to imbalanced data classification <a href="https://arxiv.org/pdf/2002.03521" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> A New Perspective for Flexible Feature Gathering in Scene Text  Recognition Via Character Anchor Pooling <a href="https://arxiv.org/pdf/2002.03509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Segmenting unseen industrial components in a heavy clutter using rgb-d  fusion and synthetic data <a href="https://arxiv.org/pdf/2002.03501" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
<div id="title58">
<b>58.</b> ABBA: Saliency-Regularized Motion-Based Adversarial Blur Attack <a href="https://arxiv.org/pdf/2002.03500" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper58" style="color:#0000EE;">摘要</a><br></div>
<div id="title59">
<b>59.</b> Medical Image Registration Using Deep Neural Networks: A Comprehensive  Review <a href="https://arxiv.org/pdf/2002.03401" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper59" style="color:#0000EE;">摘要</a><br></div>
<div id="title60">
<b>60.</b> Two-Stream Aural-Visual Affect Analysis in the Wild <a href="https://arxiv.org/pdf/2002.03399" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper60" style="color:#0000EE;">摘要</a><br></div>
<div id="title61">
<b>61.</b> MS-Net: Multi-Site Network for Improving Prostate Segmentation with  Heterogeneous MRI Data <a href="https://arxiv.org/pdf/2002.03366" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper61" style="color:#0000EE;">摘要</a><br></div>
<div id="title62">
<b>62.</b> Weakly Supervised Attention Pyramid Convolutional Neural Network for  Fine-Grained Visual Classification <a href="https://arxiv.org/pdf/2002.03353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper62" style="color:#0000EE;">摘要</a><br></div>
<div id="title63">
<b>63.</b> Dynamic Inference: A New Approach Toward Efficient Video Action  Recognition <a href="https://arxiv.org/pdf/2002.03342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper63" style="color:#0000EE;">摘要</a><br></div>
<div id="title64">
<b>64.</b> VIFB: A Visible and Infrared Image Fusion Benchmark <a href="https://arxiv.org/pdf/2002.03322" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper64" style="color:#0000EE;">摘要</a><br></div>
<div id="title65">
<b>65.</b> Unlabeled Data Deployment for Classification of Diabetic Retinopathy  Images Using Knowledge Transfer <a href="https://arxiv.org/pdf/2002.03321" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper65" style="color:#0000EE;">摘要</a><br></div>
<div id="title66">
<b>66.</b> FSD-10: A Dataset for Competitive Sports Content Analysis <a href="https://arxiv.org/pdf/2002.03312" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper66" style="color:#0000EE;">摘要</a><br></div>
<div id="title67">
<b>67.</b> Face Hallucination with Finishing Touches <a href="https://arxiv.org/pdf/2002.03308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper67" style="color:#0000EE;">摘要</a><br></div>
<div id="title68">
<b>68.</b> Splitting Convolutional Neural Network Structures for Efficient  Inference <a href="https://arxiv.org/pdf/2002.03302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper68" style="color:#0000EE;">摘要</a><br></div>
<div id="title69">
<b>69.</b> Convolutional Neural Network Pruning Using Filter Attenuation <a href="https://arxiv.org/pdf/2002.03299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper69" style="color:#0000EE;">摘要</a><br></div>
<div id="title70">
<b>70.</b> PointHop++: A Lightweight Learning Model on Point Sets for 3D  Classification <a href="https://arxiv.org/pdf/2002.03281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper70" style="color:#0000EE;">摘要</a><br></div>
<div id="title71">
<b>71.</b> Asymmetric Rejection Loss for Fairer Face Recognition <a href="https://arxiv.org/pdf/2002.03276" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper71" style="color:#0000EE;">摘要</a><br></div>
<div id="title72">
<b>72.</b> Learning efficient structured dictionary for image classification <a href="https://arxiv.org/pdf/2002.03271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper72" style="color:#0000EE;">摘要</a><br></div>
<div id="title73">
<b>73.</b> Weakly-Supervised Multi-Person Action Recognition in 360$^{\circ}$  Videos <a href="https://arxiv.org/pdf/2002.03266" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper73" style="color:#0000EE;">摘要</a><br></div>
<div id="title74">
<b>74.</b> GradMix: Multi-source Transfer across Domains and Tasks <a href="https://arxiv.org/pdf/2002.03264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper74" style="color:#0000EE;">摘要</a><br></div>
<div id="title75">
<b>75.</b> Ensemble of Deep Convolutional Neural Networks for Automatic Pavement  Crack Detection and Measurement <a href="https://arxiv.org/pdf/2002.03241" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper75" style="color:#0000EE;">摘要</a><br></div>
<div id="title76">
<b>76.</b> Multi-Label Class Balancing Algorithm for Action Unit Detection <a href="https://arxiv.org/pdf/2002.03238" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper76" style="color:#0000EE;">摘要</a><br></div>
<div id="title77">
<b>77.</b> Intrinsic Dimension Estimation via Nearest Constrained Subspace  Classifier <a href="https://arxiv.org/pdf/2002.03228" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper77" style="color:#0000EE;">摘要</a><br></div>
<div id="title78">
<b>78.</b> Exocentric to Egocentric Image Generation via Parallel Generative  Adversarial Network <a href="https://arxiv.org/pdf/2002.03219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper78" style="color:#0000EE;">摘要</a><br></div>
<div id="title79">
<b>79.</b> Spatial-Temporal Multi-Cue Network for Continuous Sign Language  Recognition <a href="https://arxiv.org/pdf/2002.03187" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper79" style="color:#0000EE;">摘要</a><br></div>
<div id="title80">
<b>80.</b> Sparsity-Aware Deep Learning for Automatic 4D Facial Expression  Recognition <a href="https://arxiv.org/pdf/2002.03157" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper80" style="color:#0000EE;">摘要</a><br></div>
<div id="title81">
<b>81.</b> CTM: Collaborative Temporal Modeling for Action Recognition <a href="https://arxiv.org/pdf/2002.03152" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper81" style="color:#0000EE;">摘要</a><br></div>
<div id="title82">
<b>82.</b> Multi-Modality Cascaded Fusion Technology for Autonomous Driving <a href="https://arxiv.org/pdf/2002.03138" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper82" style="color:#0000EE;">摘要</a><br></div>
<div id="title83">
<b>83.</b> Symbiotic Attention with Privileged Information for Egocentric Action  Recognition <a href="https://arxiv.org/pdf/2002.03137" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper83" style="color:#0000EE;">摘要</a><br></div>
<div id="title84">
<b>84.</b> Variable-Viewpoint Representations for 3D Object Recognition <a href="https://arxiv.org/pdf/2002.03131" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper84" style="color:#0000EE;">摘要</a><br></div>
<div id="title85">
<b>85.</b> Attacking Optical Character Recognition (OCR) Systems with Adversarial  Watermarks <a href="https://arxiv.org/pdf/2002.03095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper85" style="color:#0000EE;">摘要</a><br></div>
<div id="title86">
<b>86.</b> Bone Suppression on Chest Radiographs With Adversarial Learning <a href="https://arxiv.org/pdf/2002.03073" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper86" style="color:#0000EE;">摘要</a><br></div>
<div id="title87">
<b>87.</b> Local Facial Attribute Transfer through Inpainting <a href="https://arxiv.org/pdf/2002.03040" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper87" style="color:#0000EE;">摘要</a><br></div>
<div id="title88">
<b>88.</b> Unsupervised Discovery of Interpretable Directions in the GAN Latent  Space <a href="https://arxiv.org/pdf/2002.03754" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper88" style="color:#0000EE;">摘要</a><br></div>
<div id="title89">
<b>89.</b> Learning End-to-End Lossy Image Compression: A Benchmark <a href="https://arxiv.org/pdf/2002.03711" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper89" style="color:#0000EE;">摘要</a><br></div>
<div id="title90">
<b>90.</b> Distributed Bayesian Matrix Decomposition for Big Data Mining and  Clustering <a href="https://arxiv.org/pdf/2002.03703" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper90" style="color:#0000EE;">摘要</a><br></div>
<div id="title91">
<b>91.</b> Adversarial TCAV -- Robust and Effective Interpretation of Intermediate  Layers in Neural Networks <a href="https://arxiv.org/pdf/2002.03549" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper91" style="color:#0000EE;">摘要</a><br></div>
<div id="title92">
<b>92.</b> Multi-object Monocular SLAM for Dynamic Environments <a href="https://arxiv.org/pdf/2002.03528" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper92" style="color:#0000EE;">摘要</a><br></div>
<div id="title93">
<b>93.</b> Ultra High Fidelity Image Compression with $\ell_\infty$-constrained  Encoding and Deep Decoding <a href="https://arxiv.org/pdf/2002.03482" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper93" style="color:#0000EE;">摘要</a><br></div>
<div id="title94">
<b>94.</b> Semi-Supervised Class Discovery <a href="https://arxiv.org/pdf/2002.03480" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper94" style="color:#0000EE;">摘要</a><br></div>
<div id="title95">
<b>95.</b> A Deep Learning Approach to Automate High-Resolution Blood Vessel  Reconstruction on Computerized Tomography Images With or Without the Use of  Contrast Agent <a href="https://arxiv.org/pdf/2002.03463" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper95" style="color:#0000EE;">摘要</a><br></div>
<div id="title96">
<b>96.</b> A Unified End-to-End Framework for Efficient Deep Image Compression <a href="https://arxiv.org/pdf/2002.03370" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper96" style="color:#0000EE;">摘要</a><br></div>
<div id="title97">
<b>97.</b> Multi-Task Learning by a Top-Down Control Network <a href="https://arxiv.org/pdf/2002.03335" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper97" style="color:#0000EE;">摘要</a><br></div>
<div id="title98">
<b>98.</b> Out-of-Distribution Detection with Distance Guarantee in Deep Generative  Models <a href="https://arxiv.org/pdf/2002.03328" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper98" style="color:#0000EE;">摘要</a><br></div>
<div id="title99">
<b>99.</b> Holographic Image Sensing <a href="https://arxiv.org/pdf/2002.03314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper99" style="color:#0000EE;">摘要</a><br></div>
<div id="title100">
<b>100.</b> Soft Threshold Weight Reparameterization for Learnable Sparsity <a href="https://arxiv.org/pdf/2002.03231" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper100" style="color:#0000EE;">摘要</a><br></div>
<div id="title101">
<b>101.</b> Correction of Chromatic Aberration from a Single Image Using Keypoints <a href="https://arxiv.org/pdf/2002.03196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper101" style="color:#0000EE;">摘要</a><br></div>
<div id="title102">
<b>102.</b> Deep No-reference Tone Mapped Image Quality Assessment <a href="https://arxiv.org/pdf/2002.03165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper102" style="color:#0000EE;">摘要</a><br></div>
<div id="title103">
<b>103.</b> Ramifications and Diminution of Image Noise in Iris Recognition System <a href="https://arxiv.org/pdf/2002.03125" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper103" style="color:#0000EE;">摘要</a><br></div>
<div id="title104">
<b>104.</b> An Empirical Evaluation of Perturbation-based Defenses <a href="https://arxiv.org/pdf/2002.03080" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper104" style="color:#0000EE;">摘要</a><br></div>
<div id="title105">
<b>105.</b> Predictive online optimisation with applications to optical flow <a href="https://arxiv.org/pdf/2002.03053" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper105" style="color:#0000EE;">摘要</a><br></div>
<div id="title106">
<b>106.</b> Cognitive Anthropomorphism of AI: How Humans and Computers Classify  Images <a href="https://arxiv.org/pdf/2002.03024" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper106" style="color:#0000EE;">摘要</a><br></div>
<div id="title107">
<b>107.</b> Improving the Adversarial Robustness of Transfer Learning via Noisy  Feature Distillation <a href="https://arxiv.org/pdf/2002.02998" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper107" style="color:#0000EE;">摘要</a><br></div>
<div id="title108">
<b>108.</b> DropCluster: A structured dropout for convolutional networks <a href="https://arxiv.org/pdf/2002.02997" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper108" style="color:#0000EE;">摘要</a><br></div>
<div id="title109">
<b>109.</b> SS-Auto: A Single-Shot, Automatic Structured Weight Pruning Framework of  DNNs with Ultra-High Efficiency <a href="https://arxiv.org/pdf/2001.08839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper109" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Upper, Middle and Lower Region Learning for Facial Action Unit Detection</b>  <a href="https://arxiv.org/pdf/2002.04023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yao Xia</a><br>
<font size="3">
Abstract: Facial action units (AUs) detection is fundamental to facial expression analysis. As AU occur only in a small area of face, region based learning has been widely recognized useful for AU detection. Most region based studies focus on a small region where the AU occurs. Focusing on a specific region is helpful in eliminating the influence of identity, but to be risk for losing information. It is difficult to find balance. In this study, I propose a simple strategy. I divide the face into three large regions, upper, middle and lower region, and group AUs based on where it occurs. I propose a new end-to-end deep learning framework named three regions based attention network (TRA-Net). After extracting the global feature, TRA-Net uses a hard attention module to extract three feature maps, each of which contains only a specific region. Each region-specific feature map is fed to an independent branch. For each branch, three continuous soft attention modules are used to extract higher-level features for final AU detection. In the DISFA dataset, this model achieves the highest F1 scores for the detection of AU1, AU2 and AU4, and produces the highest accuracy in comparison with the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面部动作单元（AU）检测是面部表情分析的基础。由于AU只发生在脸上的小区域，基于区域的学习已得到广泛认可的AU检测有用。大多数基于区域的研究重点放在非盟发生小区域。专注于一个特定的区域是在消除身份的影响力有帮助，但对信息丢失的风险。这是很难找到平衡点。在这项研究中，我提出了一个简单的策略。我划分面为三个大区域，上部，中部和下部区域，并且组的AU基于其中它发生。我建议命名为三个区域以关注网络（TRA-网）一个新的终端到终端的深度学习的框架。提取全局特征后，TRA-Net使用硬关注模块中提取三个特征的地图，每一个都包含只针对特定区域。每个区域特异性特征地图被馈送到一个独立的分支。对于每个分支，三个连续软注意模块用于提取最终AU检测较高级别的功能。在DISFA数据集，该模型获得了最高的分数F1用于检测AU1，AU2和AU4的，并产生最高的精度在与国家的最先进的方法相比。</font>
</div>


<hr>
<div id="paper2"> <b>2. Deep Convolutional Neural Networks with Spatial Regularization, Volume  and Star-shape Priori for Image Segmentation</b>  <a href="https://arxiv.org/pdf/2002.03989" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyue Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tai%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xue-cheng Tai</a><br>
<font size="3">
Abstract: We use Deep Convolutional Neural Networks (DCNNs) for image segmentation problems. DCNNs can well extract the features from natural images. However, the classification functions in the existing network architecture of CNNs are simple and lack capabilities to handle important spatial information in a way that have been done for many well-known traditional variational models. Prior such as spatial regularity, volume prior and object shapes cannot be well handled by existing DCNNs. We propose a novel Soft Threshold Dynamics (STD) framework which can easily integrate many spatial priors of the classical variational models into the DCNNs for image segmentation. The novelty of our method is to interpret the softmax activation function as a dual variable in a variational problem, and thus many spatial priors can be imposed in the dual space. From this viewpoint, we can build a STD based framework which can enable the outputs of DCNNs to have many special priors such as spatial regularity, volume constraints and star-shape priori. The proposed method is a general mathematical framework and it can be applied to any semantic segmentation DCNNs. To show the efficiency and accuracy of our method, we applied it to the popular DeepLabV3+ image segmentation network, and the experiments results show that our method can work efficiently on data-driven image segmentation DCNNs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们使用深卷积神经网络（DCNNs）图像分割问题。 DCNNs能很好地提取自然图像的功能。然而，在细胞神经网络的现有网络架构的分类功能简单，缺乏能力来处理已为许多著名的传统模式变做一种方式重要的空间信息。现有如空间规律性，体积之前和对象的形状不能被很好地现有DCNNs处理。我们提出了一个新颖的软阈值的动力学（STD）的框架，可以很容易的经典车型变了许多空间先验融入DCNNs的图像分割。我们的方法的新颖性在于解释SOFTMAX激活函数如在变分问题双重可变，因此许多空间先验可以在对偶空间的罚款。从该观点出发，我们可以建立一个基于STD框架，可以使DCNNs的输出以有许多特殊的先验诸如空间规律性，体积限制和星形先验。所提出的方法是一般的数学框架，它可以被应用到任何语义分割DCNNs。为了显示我们的方法的效率和准确性，我们将其运用到流行DeepLabV3 +图像分割网络，实验结果表明，该方法可以在数据驱动的图像分割DCNNs提高工作效率。</font>
</div>


<hr>
<div id="paper3"> <b>3. Unconstrained Periocular Recognition: Using Generative Deep Learning  Frameworks for Attribute Normalization</b>  <a href="https://arxiv.org/pdf/2002.03985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zanlorensi%2C+L+A" target="_blank" rel="noopener" style="color:#0000EE;">Luiz A. Zanlorensi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Proen%C3%A7a%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hugo Proença</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Menotti%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Menotti</a><br>
<font size="3">
Abstract: Ocular biometric systems working in unconstrained environments usually face the problem of small within-class compactness caused by the multiple factors that jointly degrade the quality of the obtained data. In this work, we propose an attribute normalization strategy based on deep learning generative frameworks, that reduces the variability of the samples used in pairwise comparisons, without reducing their discriminability. The proposed method can be seen as a preprocessing step that contributes for data regularization and improves the recognition accuracy, being fully agnostic to the recognition strategy used. As proof of concept, we consider the "eyeglasses" and "gaze" factors, comparing the levels of performance of five different recognition methods with/without using the proposed normalization strategy. Also, we introduce a new dataset for unconstrained periocular recognition, composed of images acquired by mobile devices, particularly suited to perceive the impact of "wearing eyeglasses" in recognition effectiveness. Our experiments were performed in two different datasets, and support the usefulness of our attribute normalization scheme to improve the recognition performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：眼在不受约束的环境中工作的生物识别系统通常面临所造成的多种因素共同降解所获得的数据的质量小的类内紧凑的问题。在这项工作中，我们提出了一种基于深度学习生成框架属性正常化的策略，即减少了两两比较用的样品的可变性，而不会降低他们的辨别力。所提出的方法可以被看作是一个预处理步骤，对于数据的正则化有助于，提高了识别精度，被完全不可知的使用的识别策略。作为概念验证，我们认为“眼镜”和“凝视”的因素，在不使用所提出的标准化战略比较与/五种不同的识别方法的性能水平。此外，我们介绍的无约束眼周识别一个新的数据集，由移动设备，特别适合于感知“戴眼镜”的在识别有效性的影响获取的图像所组成。我们的实验是在两个不同的数据集进行，并支持我们的属性正常化方案，以提高识别性能的实用性。</font>
</div>


<hr>
<div id="paper4"> <b>4. StickyPillars: Robust feature matching on point clouds using Graph  Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.03983" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Simon%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Simon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fischer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Fischer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milz%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Milz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Witt%2C+C+T" target="_blank" rel="noopener" style="color:#0000EE;">Christian Tobias Witt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gross%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Horst-Michael Gross</a><br>
<font size="3">
Abstract: StickyPillars introduces a sparse feature matching method on point clouds. It is the first approach applying Graph Neural Networks on point clouds to stick points of interest. The feature estimation and assignment relies on the optimal transport problem, where the cost is based on the neural network itself. We utilize a Graph Neural Network for context aggregation with the aid of multihead self and cross attention. In contrast to image based feature matching methods, the architecture learns feature extraction in an end-to-end manner. Hence, the approach does not rely on handcrafted features. Our method outperforms state-of-the art matching algorithms, while providing real-time capability. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：StickyPillars介绍了点云稀疏特征匹配方法。它是将点云图的神经网络坚持的兴趣点的第一种方法。该功能估计和分配依赖于最佳的交通问题，其中成本是基于神经网络本身。我们利用图的神经网络模型多头自我和交叉关注的援助范围内聚集。与基于图像特征匹配方法，该架构获悉设有在端至端的方式提取。因此，该方法不依赖于手工制作的特点。我们的方法优于国家的本领域匹配算法，同时提供实时能力。</font>
</div>


<hr>
<div id="paper5"> <b>5. Joint Encoding of Appearance and Motion Features with Self-supervision  for First Person Action Recognition</b>  <a href="https://arxiv.org/pdf/2002.03982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Planamente%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mirco Planamente</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bottino%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Bottino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caputo%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Barbara Caputo</a><br>
<font size="3">
Abstract: Wearable cameras are becoming more and more popular in several applications, increasing the interest of the research community in developing approaches for recognizing actions from a first-person point of view. An open challenge is how to cope with the limited amount of motion information available about the action itself, as opposed to the more investigated third-person action recognition scenario. When focusing on manipulation tasks, videos tend to record only parts of the movement, making crucial the understanding of the objects being manipulated and of their context. Previous works addressed this issue with two-stream architectures, one dedicated to modeling the appearance of objects involved in the action, another dedicated to extracting motion features from optical flow. In this paper, we argue that features from these two information channels should be learned jointly to capture the spatio-temporal correlations between the two in a better way. To this end, we propose a single stream architecture able to do so, thanks to the addition of a self-supervised block that uses a pretext motion segmentation task to intertwine motion and appearance knowledge. Experiments on several publicly available databases show the power of our approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可穿戴式摄像机正变得越来越流行在几个应用程序，增加了研究界在发展从一个第一人称的角度认识行动方案的兴趣。一个开放的挑战是如何应对提供了有关行动本身数量有限的运动信息，而不是更多的研究第三人称动作识别场景。当着眼于操作任务，视频往往只记录运动的部件，使得关键的被操纵的对象的理解和他们的背景。以前的作品中解决了这个问题有两个流架构下，一个专门用于模拟参与行动对象的外观，另一个专门用于提取运动从光流的特征。在本文中，我们认为，这两个信息渠道功能应共同学会了捕捉两者之间的时空相关性以更好的方式。为此，我们提出了一个单一的数据流架构能够这样做，由于增加使用的借口运动分割任务纠结运动和外观知识自我监督的块。几个公共数据库实验证明我们的方法的力量。</font>
</div>


<hr>
<div id="paper6"> <b>6. RePose: Learning Deep Kinematic Priors for Fast Human Pose Estimation</b>  <a href="https://arxiv.org/pdf/2002.03933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Isack%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossam Isack</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haene%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Haene</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keskin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cem Keskin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bouaziz%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sofien Bouaziz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Boykov%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Boykov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Izadi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shahram Izadi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khamis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sameh Khamis</a><br>
<font size="3">
Abstract: We propose a novel efficient and lightweight model for human pose estimation from a single image. Our model is designed to achieve competitive results at a fraction of the number of parameters and computational cost of various state-of-the-art methods. To this end, we explicitly incorporate part-based structural and geometric priors in a hierarchical prediction framework. At the coarsest resolution, and in a manner similar to classical part-based approaches, we leverage the kinematic structure of the human body to propagate convolutional feature updates between the keypoints or body parts. Unlike classical approaches, we adopt end-to-end training to learn this geometric prior through feature updates from data. We then propagate the feature representation at the coarsest resolution up the hierarchy to refine the predicted pose in a coarse-to-fine fashion. The final network effectively models the geometric prior and intuition within a lightweight deep neural network, yielding state-of-the-art results for a model of this size on two standard datasets, Leeds Sports Pose and MPII Human Pose. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们从一个单一的形象提出了人体姿势估计一种新型高效和轻质的模型。我们的模型设计在参数和各种先进设备，最先进的方法计算成本的一小部分，以实现竞争的结果。为此，我们明确地纳入一个分层的预测基于框架部分结构和几何先验。在粗糙的分辨率，并以类似经典的基于部分的方法的方式，我们利用人体的运动结构传播的关键点或身体部位之间的卷积功能更新。不同于传统的方法，我们采用终端到终端的培训，学习这种几何之前通过功能从数据更新。然后，我们传播的特征表示，在最粗分辨率高达层次细化预测姿态在粗到精的方式。最终的网络有效地模型轻质深层神经网络内的几何之前和直觉，产生国家的最先进的结果对于该尺寸的两种标准数据集的模型，利兹体育姿和MPII人体姿势。</font>
</div>


<hr>
<div id="paper7"> <b>7. 6DoF Object Pose Estimation via Differentiable Proxy Voting Loss</b>  <a href="https://arxiv.org/pdf/2002.03923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheyu Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koniusz%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Koniusz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongdong Li</a><br>
<font size="3">
Abstract: Estimating a 6DOF object pose from a single image is very challenging due to occlusions or textureless appearances. Vector-field based keypoint voting has demonstrated its effectiveness and superiority on tackling those issues. However, direct regression of vector-fields neglects that the distances between pixels and keypoints also affect the deviations of hypotheses dramatically. In other words, small errors in direction vectors may generate severely deviated hypotheses when pixels are far away from a keypoint. In this paper, we aim to reduce such errors by incorporating the distances between pixels and keypoints into our objective. To this end, we develop a simple yet effective differentiable proxy voting loss (DPVL) which mimics the hypothesis selection in the voting procedure. By exploiting our voting loss, we are able to train our network in an end-to-end manner. Experiments on widely used datasets, i.e. LINEMOD and Occlusion LINEMOD, manifest that our DPVL improves pose estimation performance significantly and speeds up the training convergence. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从估计单个图像6自由度对象姿势非常由于遮挡或无纹理出场挑战。矢量场根据关键点投票已经证明对解决这些问题，它的有效性和优越性。然而，矢量场忽略的直接回归的像素和关键点之间的距离也影响假设的偏差显着。换言之，在方向矢量小误差可能产生严重偏离假设当像素远离关键点。在本文中，我们的目标是通过将像素和关键点之间的距离为我们的目标，以减少此类错误。为此，我们开发了一个简单而有效的微代理投票损失（DPVL），它模仿了投票过程中的假设选择。通过利用我们的投票损失，我们能够训练我们的网络中的终端到终端的方式。广泛使用的数据集的实验，即LINEMOD和闭塞LINEMOD，体现我们的DPVL显著改善姿势估计性能并加快训练收敛。</font>
</div>


<hr>
<div id="paper8"> <b>8. Hierarchical Multi-Process Fusion for Visual Place Recognition</b>  <a href="https://arxiv.org/pdf/2002.03895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hausler%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Hausler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milford%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Milford</a><br>
<font size="3">
Abstract: Combining multiple complementary techniques together has long been regarded as a way to improve performance. In visual localization, multi-sensor fusion, multi-process fusion of a single sensing modality, and even combinations of different localization techniques have been shown to result in improved performance. However, merely fusing together different localization techniques does not account for the varying performance characteristics of different localization techniques. In this paper we present a novel, hierarchical localization system that explicitly benefits from three varying characteristics of localization techniques: the distribution of their localization hypotheses, their appearance- and viewpoint-invariant properties, and the resulting differences in where in an environment each system works well and fails. We show how two techniques deployed hierarchically work better than in parallel fusion, how combining two different techniques works better than two levels of a single technique, even when the single technique has superior individual performance, and develop two and three-tier hierarchical structures that progressively improve localization performance. Finally, we develop a stacked hierarchical framework where localization hypotheses from techniques with complementary characteristics are concatenated at each layer, significantly improving retention of the correct hypothesis through to the final localization stage. Using two challenging datasets, we show the proposed system outperforming state-of-the-art techniques. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结合使用多种互补技术的配合一直被认为是提高性能的一种方式。在视觉定位，多传感器融合，单个感测模态的多进程融合，和不同的定位技术，即使组合已显示导致改善的性能。然而，仅仅融合在一起不同的定位技术不考虑不同的定位技术不同的性能特点。在本文中，我们提出了一种新的分层定位系统，从定位技术3个变特征明确的好处：其本地化的假说，他们appearance-和观点不变性质的分配，并在一个环境中，其中产生的差异各系统的工作原理以及与失败。我们发现分级部署两种技术如何更好地工作比并行融合，如何结合两种不同的技术更好地工作比单一技术两个层面，即使在单一技术具有优异的个人表现和发展二，三梯队层次结构是渐进提高定位性能。最后，我们开发了一个层叠的分级框架，其中从具有互补特性的技术定位的假设，在每个层级联，通过对最终定位阶段显著提高正确假设的保持。使用两个挑战数据集，我们证明了该系统超越国家的最先进的技术。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-11</title>
    <url>/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-11/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> End-to-End Multi-speaker Speech Recognition with Transformer <a href="https://arxiv.org/pdf/2002.03921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> A Probabilistic Formulation of Unsupervised Text Style Transfer <a href="https://arxiv.org/pdf/2002.03912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><div id="title3">
<b>3.</b> A Study of Human Summaries of Scientific Articles <a href="https://arxiv.org/pdf/2002.03604" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>


<div id="title4">
<b>4.</b> What Changed Your Mind: The Roles of Dynamic Topics and Discourse in  Argumentation Process <a href="https://arxiv.org/pdf/2002.03536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Multilingual Alignment of Contextual Word Representations <a href="https://arxiv.org/pdf/2002.03518" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Limits of Detecting Text Generated by Large-Scale Language Models <a href="https://arxiv.org/pdf/2002.03438" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Abstractive Summarization for Low Resource Data using Domain Transfer  and Data Synthesis <a href="https://arxiv.org/pdf/2002.03407" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Attend to the beginning: A study on using bidirectional attention for  extractive summarization <a href="https://arxiv.org/pdf/2002.03405" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Short Text Classification via Knowledge powered Attention with  Similarity Matrix based CNN <a href="https://arxiv.org/pdf/2002.03350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Rough Set based Aggregate Rank Measure &amp; its Application to Supervised  Multi Document Summarization <a href="https://arxiv.org/pdf/2002.03259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Mining Commonsense Facts from the Physical World <a href="https://arxiv.org/pdf/2002.03149" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> HHH: An Online Medical Chatbot System based on Knowledge Graph and  Hierarchical Bi-Directional Attention <a href="https://arxiv.org/pdf/2002.03140" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> LAVA NAT: A Non-Autoregressive Translation Model with Look-Around  Decoding and Vocabulary Attention <a href="https://arxiv.org/pdf/2002.03084" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Blank Language Models <a href="https://arxiv.org/pdf/2002.03079" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Description Based Text Classification with Reinforcement Learning <a href="https://arxiv.org/pdf/2002.03067" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> autoNLP: NLP Feature Recommendations for Text Analytics Applications <a href="https://arxiv.org/pdf/2002.03056" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Snippext: Semi-supervised Opinion Mining with Augmented Data <a href="https://arxiv.org/pdf/2002.03049" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Pre-training Tasks for Embedding-based Large-scale Retrieval <a href="https://arxiv.org/pdf/2002.03932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> A Novel Kuhnian Ontology for Epistemic Classification of STM Scholarly  Articles <a href="https://arxiv.org/pdf/2002.03531" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> SPA: Verbal Interactions between Agents and Avatars in Shared Virtual  Environments using Propositional Planning <a href="https://arxiv.org/pdf/2002.03246" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Time-aware Large Kernel Convolutions <a href="https://arxiv.org/pdf/2002.03184" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. End-to-End Multi-speaker Speech Recognition with Transformer</b>  <a href="https://arxiv.org/pdf/2002.03921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuankai Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangyou Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanmin Qian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roux%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Le Roux</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shinji Watanabe</a><br>
<font size="3">
Abstract: Recently, fully recurrent neural network (RNN) based end-to-end models have been proven to be effective for multi-speaker speech recognition in both the single-channel and multi-channel scenarios. In this work, we explore the use of Transformer models for these tasks by focusing on two aspects. First, we replace the RNN-based encoder-decoder in the speech recognition model with a Transformer architecture. Second, in order to use the Transformer in the masking network of the neural beamformer in the multi-channel case, we modify the self-attention component to be restricted to a segment rather than the whole sequence in order to reduce computation. Besides the model architecture improvements, we also incorporate an external dereverberation preprocessing, the weighted prediction error (WPE), enabling our model to handle reverberated signals. Experiments on the spatialized wsj1-2mix corpus show that the Transformer-based models achieve 40.9% and 25.6% relative WER reduction, down to 12.1% and 6.4% WER, under the anechoic condition in single-channel and multi-channel tasks, respectively, while in the reverberant case, our methods achieve 41.5% and 13.8% relative WER reduction, down to 16.5% and 15.2% WER. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，完全回归神经网络（RNN）的端至高端机型已被证明是有效的在单通道和多通道两种情况下多说话者声音识别。在这项工作中，我们侧重于两个方面探讨使用Transformer模型为这些任务的。首先，我们更换了变压器架构的语音识别模型基于RNN编码器，解码器。其次，为了使用Transformer在多通道情况下，神经波束形成器的屏蔽网络中，我们修改了自注意成分被限制在一个段，而不是整个序列，以减少计算量。除了模型体系结构的改进，我们还包含一个外部去混响预处理，加权预测误差（WPE），使我们的模型来处理混响信号。在空间化wsj1-2mix语料库表明，基于变压器的模型达到40.9％和25.6％的相对减少WER，下降到12.1％和6.4％WER，在单通道和多通道任务的消声条件下，分别的实验，而在混响情况下，我们的方法达到41.5％和13.8％的相对减少WER，下降到16.5％和15.2％WER。</font>
</div>


<hr>
<div id="paper2"> <b>2. A Probabilistic Formulation of Unsupervised Text Style Transfer</b>  <a href="https://arxiv.org/pdf/2002.03912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junxian He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Graham Neubig</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taylor Berg-Kirkpatrick</a><br>
<font size="3">
Abstract: We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了统一了先前提出的非生殖技术监督的文本样式转移了深刻的生成模型。从两个结构域为部分观察到平行语料库我们的概率方法模型非并行数据。通过假设到生成每个观察到的序列的并行潜序列，我们的模型学习从一个域变换序列到另一个在完全无监督方式。与传统的生成序列的模型（例如，HMM）相比之下，我们的模型使得它生成数据一些假设：它采用的是复发性语言模型作为先验和编码器 - 解码器作为转导分布。虽然边际数据可能性的计算是在这个模型类棘手，我们表明，摊销变推理承认一个现实的替代。此外，通过我们的目标变和其他最近的无监督式的转移和机器翻译技术之间绘制连接，我们将展示我们的概率观点如何能够统一一些已知的非生成目标，如回译和对抗性的损失。最后，我们证明我们的方法对大范围的无监督式的传输任务，包括情绪转移，转让手续，文字解读，作者模仿，以及相关的语言翻译的有效性。在所有风格的传输任务，我们的做法得到了国家的最先进的非生成基线大有斩获，其中包括国家的最先进的无监督的机器翻译技术，我们的方法推广。此外，我们在标准无监督的机器翻译任务进行实验，发现我们统一的方法当前国家的最先进的匹配。</font>
</div>


<hr>
<div id="paper3"> <b>3. A Study of Human Summaries of Scientific Articles</b>  <a href="https://arxiv.org/pdf/2002.03604" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Boni%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Odellia Boni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feigenblat%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guy Feigenblat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Doron Cohen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roitman%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haggai Roitman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Konopnicki%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Konopnicki</a><br>
<font size="3">
Abstract: Researchers and students face an explosion of newly published papers which may be relevant to their work. This led to a trend of sharing human summaries of scientific papers. We analyze the summaries shared in one of these platforms this http URL. The goal is to characterize human summaries of scientific papers, and use some of the insights obtained to improve and adapt existing automatic summarization systems to the domain of scientific papers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：研究人员和学生面临的新发表的论文可能是与其工作相关的爆炸。这导致了共享的科学论文人类总结的趋势。我们分析在这些平台上的这个HTTP URL一个共享的摘要。我们的目标是表征的科学论文人类汇总，并使用一些得到改善和现有的自动摘要系统适应的科学论文域的见解。</font>
</div>


<hr>
<div id="paper4"> <b>4. What Changed Your Mind: The Roles of Dynamic Topics and Discourse in  Argumentation Process</b>  <a href="https://arxiv.org/pdf/2002.03536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jichuan Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulan He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cuiyun Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Michael R. Lyu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=King%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Irwin King</a><br>
<font size="3">
Abstract: In our world with full of uncertainty, debates and argumentation contribute to the progress of science and society. Despite of the increasing attention to characterize human arguments, most progress made so far focus on the debate outcome, largely ignoring the dynamic patterns in argumentation processes. This paper presents a study that automatically analyzes the key factors in argument persuasiveness, beyond simply predicting who will persuade whom. Specifically, we propose a novel neural model that is able to dynamically track the changes of latent topics and discourse in argumentative conversations, allowing the investigation of their roles in influencing the outcomes of persuasion. Extensive experiments have been conducted on argumentative conversations on both social media and supreme court. The results show that our model outperforms state-of-the-art models in identifying persuasive arguments via explicitly exploring dynamic factors of topic and discourse. We further analyze the effects of topics and discourse on persuasiveness, and find that they are both useful - topics provide concrete evidence while superior discourse styles may bias participants, especially in social media arguments. In addition, we draw some findings from our empirical results, which will help people better engage in future persuasive conversations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在我们与充满不确定性，辩论和论证的世界做出贡献的科学和社会的进步。尽管日益关注人类的特征参数，大部分取得的进展至今专注于辩论结果如何，在很大程度上忽视了在论证过程中的动态模式。本文提出了一种研究一种能够自动分析的关键因素，论证的说服力，超越了简单的预测谁将会说服谁。具体来说，我们提出了一种新的神经模型，该模型能够动态跟踪的潜在主题和议论交谈变化的话语，让自己的角色的影响说服效果的调查。大量的实验已经在这两个社交媒体和最高法院议论对话进行。结果表明，我们的模型优于国家的最先进的车型在通过主题和话语的明确探索动态因素识别有说服力的论据。我们进一步分析主题和话语的影响说服力，并且发现它们都是有用的 - 主题提供了确凿的证据，而优越的话语风格可偏向的参与者，尤其是在社交媒体上的参数。此外，我们从实证结果，这将帮助人们更好地参与未来有说服力的交谈得出一些结论。</font>
</div>


<hr>
<div id="paper5"> <b>5. Multilingual Alignment of Contextual Word Representations</b>  <a href="https://arxiv.org/pdf/2002.03518" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steven Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kitaev%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikita Kitaev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dan Klein</a><br>
<font size="3">
Abstract: We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs written in different scripts, that are corrected by the alignment procedure. These results support contextual alignment as a useful concept for understanding large multilingual pre-trained models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了评估和加强情境嵌入定位，并表明他们是在分析和改善多语种BERT有用的程序。特别是，我们提出的调整过程之后，BERT展品显著上XNLI相比基本模型提高零射门的表现，非常匹配伪充分监督翻译火车模型，保加利亚和希腊。此外，测量校准的程度，我们介绍检索词的上下文版本，并表明它与下游的零次转让很好的相关性。使用这个检索词的任务，我们也分析BERT，发现它具有系统性缺陷，例如对于用不同的脚本开放类零件的词性和词的对，由校准程序纠正糟糕对齐。这些结果支持上下文定位为了解大型多语种预训练模型一个有用的概念。</font>
</div>


<hr>
<div id="paper6"> <b>6. Limits of Detecting Text Generated by Large-Scale Language Models</b>  <a href="https://arxiv.org/pdf/2002.03438" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Varshney%2C+L+R" target="_blank" rel="noopener" style="color:#0000EE;">Lav R. Varshney</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keskar%2C+N+S" target="_blank" rel="noopener" style="color:#0000EE;">Nitish Shirish Keskar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Socher</a><br>
<font size="3">
Abstract: Some consider large-scale language models that can generate long and coherent pieces of text as dangerous, since they may be used in misinformation campaigns. Here we formulate large-scale language model output detection as a hypothesis testing problem to classify text as genuine or generated. We show that error exponents for particular language models are bounded in terms of their perplexity, a standard measure of language generation performance. Under the assumption that human language is stationary and ergodic, the formulation is extended from considering specific language models to considering maximum likelihood language models, among the class of k-order Markov approximations; error probabilities are characterized. Some discussion of incorporating semantic side information is also given. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：有些人认为大型语言模型，可以产生长期而连贯的作品文本的危险，因为它们可能在误导广告系列。在这里，我们制定的大型语言模型输出检测为假设检验问题进行分类文本作为真正的或产生的。我们表明，特定的语言模型误差的指数在他们困惑的语言生成的性能衡量标准方面是有界的。在假设人类语言是固定的，并且遍历，该制剂是从考虑特定的语言模型来考虑最大似然语言模型中，类k阶马尔可夫近似值之间延伸;错误概率表征。结合语义方面信息的一些讨论也给出。</font>
</div>


<hr>
<div id="paper7"> <b>7. Abstractive Summarization for Low Resource Data using Domain Transfer  and Data Synthesis</b>  <a href="https://arxiv.org/pdf/2002.03407" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Magooda%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmed Magooda</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Litman%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diane Litman</a><br>
<font size="3">
Abstract: Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：培训抽象概括模型通常需要大量的数据，这可能是许多领域的限制。在本文中，我们探讨使用域传输和数据合成在应用于学生思考的小语料库提高近期抽象总结方法的性能。首先，我们探讨的培训在报纸上的数据的艺术模型的第二调谐状态是否能提高学生反映数据的表现。评估表明，通过调谐模型产生摘要相比，模型中训练的只是学生的反射数据，或只报数据来实现更高的分数ROUGE。调谐模型相比也萃取汇总基线实现较高的分数，并且还判定为产生更为一致的和人类可读的评价汇总。其次，我们探讨是否合成学生数据的汇总可以额外提高性能。我们提出了一个基于模板的模型来合成新的数据，这些数据在纳入培训进一步提高ROUGE得分。最后，我们显示，与域转移组合数据合成相比仅使用两种方法之一来实现更高ROUGE分数。</font>
</div>


<hr>
<div id="paper8"> <b>8. Attend to the beginning: A study on using bidirectional attention for  extractive summarization</b>  <a href="https://arxiv.org/pdf/2002.03405" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Magooda%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmed Magooda</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marcjan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cezary Marcjan</a><br>
<font size="3">
Abstract: Forum discussion data differ in both structure and properties from generic form of textual data such as news. Henceforth, summarization techniques should, in turn, make use of such differences, and craft models that can benefit from the structural nature of discussion data. In this work, we propose attending to the beginning of a document, to improve the performance of extractive summarization models when applied to forum discussion data. Evaluations demonstrated that with the help of bidirectional attention mechanism, attending to the beginning of a document (initial comment/post) in a discussion thread, can introduce a consistent boost in ROUGE scores, as well as introducing a new State Of The Art (SOTA) ROUGE scores on the forum discussions dataset. Additionally, we explored whether this hypothesis is extendable to other generic forms of textual data. We make use of the tendency of introducing important information early in the text, by attending to the first few sentences in generic textual data. Evaluations demonstrated that attending to introductory sentences using bidirectional attention, improves the performance of extractive summarization models when even applied to more generic form of textual data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：论坛讨论数据在结构和性能的文本数据的一般形式不同，如新闻。今后，概括技术应该反过来，利用这种差异，工艺模型，可以从讨论数据的结构性质中受益。在这项工作中，我们建议参加到文档的开头，当应用到论坛讨论数据，以提高采掘总结模型的性能。评估表明，随着双向注意机制的帮助下，参加到讨论线索文件（初始评论/后）的开始，也会引入ROUGE分数一致的提振，以及引入一个新的国家的艺术（SOTA在论坛上讨论的数据集）ROUGE得分。此外，我们探讨这个假设是否扩展到文本数据的其他一般形式。我们利用文本早期引进的重要信息，通过参加在通用文本数据的前几句的倾向。评估表明，使用双向注意参加到介绍性的句子，提高采掘总结机型的表现时，甚至应用于文本数据的更通用的形式。</font>
</div>


<hr>
<div id="paper9"> <b>9. Short Text Classification via Knowledge powered Attention with  Similarity Matrix based CNN</b>  <a href="https://arxiv.org/pdf/2002.03350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingchen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clinton%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabtone.Clinton</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yijia Miao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Gao</a><br>
<font size="3">
Abstract: Short text is becoming more and more popular on the web, such as Chat Message, SMS and Product Reviews. Accurately classifying short text is an important and challenging task. A number of studies have difficulties in addressing this problem because of the word ambiguity and data sparsity. To address this issue, we propose a knowledge powered attention with similarity matrix based convolutional neural network (KASM) model, which can compute comprehensive information by utilizing the knowledge and deep neural network. We use knowledge graph (KG) to enrich the semantic representation of short text, specially, the information of parent-entity is introduced in our model. Meanwhile, we consider the word interaction in the literal-level between short text and the representation of label, and utilize similarity matrix based convolutional neural network (CNN) to extract it. For the purpose of measuring the importance of knowledge, we introduce the attention mechanisms to choose the important information. Experimental results on five standard datasets show that our model significantly outperforms state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：短文本正在变得越来越流行网络，比如聊天信息，短信和产品评论的。准确分类短文本是一项重要而艰巨的任务。许多研究都在解决，因为这个词的模糊性和数据稀疏的这个问题的困难。为了解决这个问题，我们提出了基于相似矩阵卷积神经网络（KASM）模型，它可以利用的知识和深层神经网络计算的综合信息知识供电关注。我们用知识图（KG）充实简短的文字，特别是，母公司的实体的信息在我们的模型引入的语义表示。同时，我们认为短文本和标签的表示之间的文字级别的字互动，并利用相似矩阵基于卷积神经网络（CNN）将其解压。用来衡量知识的重要性的目的，我们引入注意机制选择的重要信息。五个标准数据集实验结果表明，我们的模型显著优于国家的最先进的方法。</font>
</div>


<hr>
<div id="paper10"> <b>10. Rough Set based Aggregate Rank Measure &amp; its Application to Supervised  Multi Document Summarization</b>  <a href="https://arxiv.org/pdf/2002.03259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yadav%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nidhika Yadav</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chatterjee%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Niladri Chatterjee</a><br>
<font size="3">
Abstract: Most problems in Machine Learning cater to classification and the objects of universe are classified to a relevant class. Ranking of classified objects of universe per decision class is a challenging problem. We in this paper propose a novel Rough Set based membership called Rank Measure to solve to this problem. It shall be utilized for ranking the elements to a particular class. It differs from Pawlak Rough Set based membership function which gives an equivalent characterization of the Rough Set based approximations. It becomes paramount to look beyond the traditional approach of computing memberships while handling inconsistent, erroneous and missing data that is typically present in real world problems. This led us to propose the aggregate Rank Measure. The contribution of the paper is three fold. Firstly, it proposes a Rough Set based measure to be utilized for numerical characterization of within class ranking of objects. Secondly, it proposes and establish the properties of Rank Measure and aggregate Rank Measure based membership. Thirdly, we apply the concept of membership and aggregate ranking to the problem of supervised Multi Document Summarization wherein first the important class of sentences are determined using various supervised learning techniques and are post processed using the proposed ranking measure. The results proved to have significant improvement in accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在机器学习的大多数问题迎合分类和宇宙的对象分类的相关类别。每个决策类宇宙的分类对象的排名是一个具有挑战性的问题。我们在本文中提出了所谓的排名衡量一个新的基于粗糙集的成员来解决这个问题。它应被用于排序的元素到一个特定的类。它不同于帕夫拉克基于粗糙集的隶属度函数这给基于粗糙集近似的等价刻画。它成为极为重要的超越计算成员在处理不一致的，错误的，缺少通常存在于现实世界的问题数据的传统方式。这使我们提出的总排名措施。本文的贡献是三倍。首先，提出了将要用于的内类对象的排名数值表征粗集基于度量。其次，提出并建立等级测量和总浏览量措施的会员的属性。第三，我们申请会员资格的概念和总排名，其中第一使用各种监督学习技术确定句子的重要的一类，并利用所提出的衡量排名的后处理监督多文档文摘的问题。结果证明，在精度显著的改善。</font>
</div>


<hr>
<div id="paper11"> <b>11. Mining Commonsense Facts from the Physical World</b>  <a href="https://arxiv.org/pdf/2002.03149" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanyan Zou</a><br>
<font size="3">
Abstract: Textual descriptions of the physical world implicitly mention commonsense facts, while the commonsense knowledge bases explicitly represent such facts as triples. Compared to dramatically increased text data, the coverage of existing knowledge bases is far away from completion. Most of the prior studies on populating knowledge bases mainly focus on Freebase. To automatically complete commonsense knowledge bases to improve their coverage is under-explored. In this paper, we propose a new task of mining commonsense facts from the raw text that describes the physical world. We build an effective new model that fuses information from both sequence text and existing knowledge base resource. Then we create two large annotated datasets each with approximate 200k instances for commonsense knowledge base completion. Empirical results demonstrate that our model significantly outperforms baselines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对物理世界的文本描述隐含提到常识的事实，而常识性的知识基础明确表示这样的事实三倍。相比大幅增加文本数据，现有的知识基础的覆盖面是远离完成。对大多数填充知识库事先研究主要集中在游离碱自动完成常识性的知识基础，提高其覆盖面是充分开发。在本文中，我们提出了从描述物理世界的原始文本挖掘常识性事实的新任务。我们构建一个融合了来自两个序列的文本和已有的知识基础资源信息的有效新模式。然后，我们创建每两个大型注释的数据集与常识的知识基础完成近似200K实例。实证结果表明，我们的模型显著优于基准。</font>
</div>


<hr>
<div id="paper12"> <b>12. HHH: An Online Medical Chatbot System based on Knowledge Graph and  Hierarchical Bi-Directional Attention</b>  <a href="https://arxiv.org/pdf/2002.03140" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiming Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Ni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiamou Liu</a><br>
<font size="3">
Abstract: This paper proposes a chatbot framework that adopts a hybrid model which consists of a knowledge graph and a text similarity model. Based on this chatbot framework, we build HHH, an online question-and-answer (QA) Healthcare Helper system for answering complex medical questions. HHH maintains a knowledge graph constructed from medical data collected from the Internet. HHH also implements a novel text representation and similarity deep learning model, Hierarchical BiLSTM Attention Model (HBAM), to find the most similar question from a large QA dataset. We compare HBAM with other state-of-the-art language models such as bidirectional encoder representation from transformers (BERT) and Manhattan LSTM Model (MaLSTM). We train and test the models with a subset of the Quora duplicate questions dataset in the medical area. The experimental results show that our model is able to achieve a superior performance than these existing methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种聊天机器人框架，采用它由一个知识图形和文本相似模型的混合模式。在此基础上聊天机器人框架，我们建立HHH，在线提问和回答（QA）医疗辅助系统是回答复杂的医学问题。 HHH保持从网上收集的医疗数据构建一个知识图谱。 HHH还实现了一个新的文本表示和相似性深度学习模型，分层BiLSTM注意力模型（HBAM），发现从大的QA数据集的最类似的问题。我们比较HBAM与国家的最先进的其他语言模型如变压器双向编码表示（BERT）和曼哈顿LSTM模型（MaLSTM）。我们培养和使用的Quora的重复问题的一个子集测试模型在医疗领域的数据集。实验结果表明，我们的模型能够实现比现有的这些方法优越的性能。</font>
</div>


<hr>
<div id="paper13"> <b>13. LAVA NAT: A Non-Autoregressive Translation Model with Look-Around  Decoding and Vocabulary Attention</b>  <a href="https://arxiv.org/pdf/2002.03084" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoya Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxian Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arianna Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwei Li</a><br>
<font size="3">
Abstract: Non-autoregressive translation (NAT) models generate multiple tokens in one forward pass and is highly efficient at inference stage compared with autoregressive translation (AT) methods. However, NAT models often suffer from the multimodality problem, i.e., generating duplicated tokens or missing tokens. In this paper, we propose two novel methods to address this issue, the Look-Around (LA) strategy and the Vocabulary Attention (VA) mechanism. The Look-Around strategy predicts the neighbor tokens in order to predict the current token, and the Vocabulary Attention models long-term token dependencies inside the decoder by attending the whole vocabulary for each position to acquire knowledge of which token is about to generate. %We also propose a dynamic bidirectional decoding approach to accelerate the inference process of the LAVA model while preserving the high-quality of the generated output. Our proposed model uses significantly less time during inference compared with autoregressive models and most other NAT models. Our experiments on four benchmarks (WMT14 En$\rightarrow$De, WMT14 De$\rightarrow$En, WMT16 Ro$\rightarrow$En and IWSLT14 De$\rightarrow$En) show that the proposed model achieves competitive performance compared with the state-of-the-art non-autoregressive and autoregressive models while significantly reducing the time cost in inference phase. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：非自回归转换（NAT）模型生成一个直传多个令牌，并在推论阶段高效自回归转换（AT）方法相比。然而，NAT模式经常遭受来自多模式问题，即，产生重复的令牌或丢失令牌。在本文中，我们提出了两种新的方法来解决这个问题，环视（LA）策略和词汇注意（VA）的机制。环视战略，每个位置上的所有词汇主治地获取知识，其中令牌即将产生预测，以预测当前令牌邻居令牌，解码器内部的词汇注意模型的长期令牌的依赖。 ％我们也提出了一个动态的双向解码方式，加快LAVA模型的推理过程，同时保留生成的输出的高品质。我们提出的模型采用与自回归模型和其他大多数NAT车型相比推理过程中显著的时间更少。我们的四个基准试验（WMT14恩$ \ RIGHTARROW $德，WMT14德$ \ RIGHTARROW $恩，WMT16滚装$ \ RIGHTARROW $恩和IWSLT14德$ \ RIGHTARROW $恩）显示，随着国家相比，该模型实现了有竞争力的性能-of最先进的非自回归和自回归模型，同时显著降低推断阶段的时间成本。</font>
</div>


<hr>
<div id="paper14"> <b>14. Blank Language Models</b>  <a href="https://arxiv.org/pdf/2002.03079" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianxiao Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Quach%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Victor Quach</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barzilay%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Regina Barzilay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jaakkola%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tommi Jaakkola</a><br>
<font size="3">
Abstract: We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. Unlike previous masked language models or the Insertion Transformer, BLM uses blanks to control which part of the sequence to expand. This fine-grained control of generation is ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood, and achieves perplexity comparable to traditional left-to-right language models on the Penn Treebank and WikiText datasets. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出空白语言模型（BLM），通过动态地创建和填补空白生成序列模型。不同于以往的蒙面语言模型或插入变压器，BLM使用空格来控制流程的一部分，扩大它。这一代的细粒度控制是适用于各种文本编辑和重写任务。该模型可以从一个单一的空白开始或部分完成具有在指定位置空白文本。它反复确定哪些词来代替一个空白，是否插入新的空白，而当没有空格都留给填充停止发电。 BLM可以使用下界边际数据的可能性被有效的培训，并达到相当的困惑对宾州树库和数据集wikitext的传统左到右的语言模型。在填充缺失的文本片段的任务，BLM显著优于在准确性和流畅性方面的所有其他基线。款式转移和破坏古文字复原实验证明该框架为广泛的应用潜力。</font>
</div>


<hr>
<div id="paper15"> <b>15. Description Based Text Classification with Reinforcement Learning</b>  <a href="https://arxiv.org/pdf/2002.03067" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duo Chai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwei Li</a><br>
<font size="3">
Abstract: The task of text classification is usually divided into two stages: {\it text feature extraction} and {\it classification}. In this standard formalization categories are merely represented as indexes in the label vocabulary, and the model lacks for explicit instructions on what to classify. Inspired by the current trend of formalizing NLP problems as question answering tasks, we propose a new framework for text classification, in which each category label is associated with a category description. Descriptions are generated by hand-crafted templates or using abstractive/extractive models from reinforcement learning. The concatenation of the description and the text is fed to the classifier to decide whether or not the current label should be assigned to the text. The proposed strategy forces the model to attend to the most salient texts with respect to the label, which can be regarded as a hard version of attention, leading to better performances. We observe significant performance boosts over strong baselines on a wide range of text classification tasks including single-label classification, multi-label classification and multi-aspect sentiment analysis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文本分类的任务通常分为两个阶段：{\它的文本特征提取}和{\它分类}。在这个标准形式化类别只是表示为标签的词汇索引，该模型缺少什么就分类明确的指示。通过正式NLP问题答疑任务的当前趋势的启发，我们提出了文本分类的新框架，其中每个类别标签与类别描述相关联。说明由手工制作的模板或使用抽象/采掘车型从强化学习产生。描述和文字的级联被送到分类，以决定当前标签是否应该被分配到的文本。拟议的战略力量模型出席中最突出的文字相对于标签，这可以看作是人们关注的硬的版本，从而获得更好的性能。我们观察到了一个大范围的文本分类的任务，包括单标签分类，多标签分类和多方位的情感分析强基线显著的性能提升。</font>
</div>


<hr>
<div id="paper16"> <b>16. autoNLP: NLP Feature Recommendations for Text Analytics Applications</b>  <a href="https://arxiv.org/pdf/2002.03056" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Misra%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Janardan Misra</a><br>
<font size="3">
Abstract: While designing machine learning based text analytics applications, often, NLP data scientists manually determine which NLP features to use based upon their knowledge and experience with related problems. This results in increased efforts during feature engineering process and renders automated reuse of features across semantically related applications inherently difficult. In this paper, we argue for standardization in feature specification by outlining structure of a language for specifying NLP features and present an approach for their reuse across applications to increase likelihood of identifying optimal features. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在设计基于机器学习的文本分析应用中，常，NLP数据科学家手动确定NLP功能，才能使用根据其与相关问题的知识和经验。在功能设计过程，这导致加大工作力度和渲染自动化的跨越语义相关的应用程序本身就难以功能重用。在本文中，我们通过概述用于指定NLP特征的语言的结构主张在特征规格标准化和呈现的方法用于其再利用跨应用程序以增加识别最佳特征的可能性。</font>
</div>


<hr>
<div id="paper17"> <b>17. Snippext: Semi-supervised Opinion Mining with Augmented Data</b>  <a href="https://arxiv.org/pdf/2002.03049" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengjie Miao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuliang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wang-Chiew Tan</a><br>
<font size="3">
Abstract: Online services are interested in solutions to opinion mining, which is the problem of extracting aspects, opinions, and sentiments from text. One method to mine opinions is to leverage the recent success of pre-trained language models which can be fine-tuned to obtain high-quality extractions from reviews. However, fine-tuning language models still requires a non-trivial amount of training data. In this paper, we study the problem of how to significantly reduce the amount of labeled training data required in fine-tuning language models for opinion mining. We describe Snippext, an opinion mining system developed over a language model that is fine-tuned through semi-supervised learning with augmented data. A novelty of Snippext is its clever use of a two-prong approach to achieve state-of-the-art (SOTA) performance with little labeled training data through: (1) data augmentation to automatically generate more labeled training data from existing ones, and (2) a semi-supervised learning technique to leverage the massive amount of unlabeled data in addition to the (limited amount of) labeled data. We show with extensive experiments that Snippext performs comparably and can even exceed previous SOTA results on several opinion mining tasks with only half the training data required. Furthermore, it achieves new SOTA results when all training data are leveraged. By comparison to a baseline pipeline, we found that Snippext extracts significantly more fine-grained opinions which enable new opportunities of downstream applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在线服务，有兴趣的解决方案，意见挖掘，这是从文本中提取方面，意见和情绪的问题。一种方法矿井的意见是利用近期的预先训练语言模型，可以进行微调，以从审查获得高品质的提取成功。但是，微调语言模型仍然需要训练数据的不平凡的量。在本文中，我们研究如何显著减少微调语言模型所需的意见挖掘标记的训练数据量的问题。我们描述Snippext，发展了语言模型的意见挖掘系统进行微调，通过半监督学习与增强的数据。 Snippext的一个新的特点是其巧妙利用一个双叉方式通过与小标记的训练数据实现状态的最先进的（SOTA）性能：（1）数据的增强自动生成从现有的多个标记的训练数据，和（2）半监督学习技术来利用除了标记的数据（的限制量）的未标记数据的巨量。我们发现有大量的实验证明，Snippext执行同等而且甚至超过几个意见挖掘任务以前SOTA结果只需要训练数据的一半。此外，实现了新的SOTA结果时，所有的训练数据利用。通过比较基线管道，我们发现，Snippext提取显著更细粒度的意见这使下游应用新的机遇。</font>
</div>


<hr>
<div id="paper18"> <b>18. Pre-training Tasks for Embedding-based Large-scale Retrieval</b>  <a href="https://arxiv.org/pdf/2002.03932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Cheng Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+F+X" target="_blank" rel="noopener" style="color:#0000EE;">Felix X. Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yin-Wen Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiming Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanjiv Kumar</a><br>
<font size="3">
Abstract: We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们认为大规模的查询，文献检索问题：给定一个查询（例如，一个问题），返回一组相关文件（例如，含有答案的段落）从一个大的文档语料库。这个问题往往解决了两个步骤。检索阶段一来降低解空间，返回候选文档的子集。该评价阶段再重新排名文档。重要的是，检索算法不仅欲火高涨的召回，也要求必须高效，及时次线性回归考生文件的数量。不同于评价阶段最近由于跨关注车型BERT式前培训任务目睹显著的进步，检索阶段仍不很好的研究。大多数以前的作品依靠传统的信息检索（IR）的方法，如BM-25（令牌匹配+ TF-IDF权重）。这些模型只接受稀疏手工制作的特点和利益不同的下游任务不能被优化。在本文中，我们进行的基于嵌入的检索模型的综合研究。我们证明了浓厚的学习基于嵌入变压器模型的关键成分是一组前培训任务。随着设计恰当段落级前的训练任务时，Transformer模型可以显着提高与广泛使用的BM-25以及嵌入模型没有变压器。我们研究的段落级前的训练任务是反完形填空任务（ICT），身体的第一选择（BFS），维基连结预测（WLP），三个人的组合。</font>
</div>


<hr>
<div id="paper19"> <b>19. A Novel Kuhnian Ontology for Epistemic Classification of STM Scholarly  Articles</b>  <a href="https://arxiv.org/pdf/2002.03531" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saqr%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">Khalid M. Saqr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Elsharawy%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abdelrahman Elsharawy</a><br>
<font size="3">
Abstract: Thomas Kuhn proposed his paradigmatic view of scientific discovery five decades ago. The concept of paradigm has not only explained the progress of science, but has also become the central epistemic concept among STM scientists. Here, we adopt the principles of Kuhnian philosophy to construct a novel ontology aims at classifying and evaluating the impact of STM scholarly articles. First, we explain how the Kuhnian cycle of science describes research at different epistemic stages. Second, we show how the Kuhnian cycle could be reconstructed into modular ontologies which classify scholarly articles according to their contribution to paradigm-centred knowledge. The proposed ontology and its scenarios are discussed. To the best of the authors knowledge, this is the first attempt for creating an ontology for describing scholarly articles based on the Kuhnian paradigmatic view of science. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：托马斯·库恩建议他的科学发现的范式观五十年前。范式的概念不仅解释科学的进步，而且已经成为STM科学家中央认知概念。在这里，我们采用库恩的哲学原理在分类和评估STM学术文章的影响，构建了一个新的本体的目的。首先，我们解释科学的库恩的周期是如何描述在不同认知阶段的研究。其次，我们展示了库恩的周期怎么可能被重建成分类根据自己的范式为中心的知识贡献学术文章模块化本体。所提出的本体及其情景进行了讨论。为了最好的作者的知识，这是一个用于创建用于描述基于科学的范式库恩视学术文章本体的第一次尝试。</font>
</div>


<hr>
<div id="paper20"> <b>20. SPA: Verbal Interactions between Agents and Avatars in Shared Virtual  Environments using Propositional Planning</b>  <a href="https://arxiv.org/pdf/2002.03246" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Best%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Best</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Narang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sahil Narang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manocha%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dinesh Manocha</a><br>
<font size="3">
Abstract: We present a novel approach for generating plausible verbal interactions between virtual human-like agents and user avatars in shared virtual environments. Sense-Plan-Ask, or SPA, extends prior work in propositional planning and natural language processing to enable agents to plan with uncertain information, and leverage question and answer dialogue with other agents and avatars to obtain the needed information and complete their goals. The agents are additionally able to respond to questions from the avatars and other agents using natural-language enabling real-time multi-agent multi-avatar communication environments. Our algorithm can simulate tens of virtual agents at interactive rates interacting, moving, communicating, planning, and replanning. We find that our algorithm creates a small runtime cost and enables agents to complete their goals more effectively than agents without the ability to leverage natural-language communication. We demonstrate quantitative results on a set of simulated benchmarks and detail the results of a preliminary user-study conducted to evaluate the plausibility of the virtual interactions generated by SPA. Overall, we find that participants prefer SPA to prior techniques in 84\% of responses including significant benefits in terms of the plausibility of natural-language interactions and the positive impact of those interactions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出一个新的方法，用于产生之间合理的口头交互虚拟人样在共享虚拟环境代理和用户化身。感-计划-ASK，或SPA，扩展了命题规划和自然语言处理以前的工作，使代理商计划，不确定信息，并利用问题，并与其他代理和化身答案对话，以获得所需的信息，并完成自己的目标。这些代理还能够使用自然语言实现实时多Agent多具象通信环境从化身的问题和其他代理人回应。我们的算法可以在互动率模拟几十虚拟代理交互，移动，通信，规划，重新规划和。我们发现，我们的算法创建一个小的运行成本，使代理人没有充分利用自然语言交流的能力比药物更有效地完成自己的目标。我们展示了一套模拟基准和细节进行评估通过SPA生成的虚拟互动的合理性进行初步的用户研究结果的定量结果。总体而言，我们发现，参与者更喜欢SPA现有技术中的反应，包括在自然语言交互的真实性和这些交互的积极影响方面显著收益84 \％。</font>
</div>


<hr>
<div id="paper21"> <b>21. Time-aware Large Kernel Convolutions</b>  <a href="https://arxiv.org/pdf/2002.03184" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lioutas%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasileios Lioutas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhong Guo</a><br>
<font size="3">
Abstract: To date, most state-of-the-art sequence modelling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using the fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation and language modelling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：到目前为止，国家的最先进最序列建模架构使用时注意建立生成模型基于语言的任务。其中一些模型使用所有可用的序列令牌来产生注意力分布，结果在时间的$ O（N ^ 2）$的复杂性。可替换地，它们利用在深度方向上与卷积归SOFTMAX大小$ $ķ的内核充当有限窗口自关注，导致时间的O- $（K {\ CDOT} N）$复杂性。在本文中，我们引入时间感知较大的内核（TALK）卷积，一个新的自适应卷积运算该学习如何预测求和内核，而不是使用固定尺寸的内核矩阵的大小。该方法得到的O- $（n）的一个$时间复杂度，有效地使编码处理线到的令牌的数量的序列。我们评估对大型标准机器翻译和语言模型的数据集，表明该方法会说话的卷积构成超越其它基于关注/卷积方法的有效改善。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-10</title>
    <url>/2020/02/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-10/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Revisiting Spatial Invariance with Low-Rank Local Connectivity <a href="https://arxiv.org/pdf/2002.02959" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> $M^3$T: Multi-Modal Continuous Valence-Arousal Estimation in the Wild <a href="https://arxiv.org/pdf/2002.02957" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> On the Robustness of Face Recognition Algorithms Against Attacks and  Bias <a href="https://arxiv.org/pdf/2002.02942" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> How Does Gender Balance In Training Data Affect Face Recognition  Accuracy? <a href="https://arxiv.org/pdf/2002.02934" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> SPN-CNN: Boosting Sensor-Based Source Camera Attribution With Deep  Learning <a href="https://arxiv.org/pdf/2002.02927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Subspace Capsule Network <a href="https://arxiv.org/pdf/2002.02924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Temporal Segmentation of Surgical Sub-tasks through Deep Learning with  Multiple Data Sources <a href="https://arxiv.org/pdf/2002.02921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> iqiyi Submission to ActivityNet Challenge 2019 Kinetics-700 challenge:  Hierarchical Group-wise Attention <a href="https://arxiv.org/pdf/2002.02918" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Data augmentation with Möbius transformations <a href="https://arxiv.org/pdf/2002.02917" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Domain Embedded Multi-model Generative Adversarial Networks for  Image-based Face Inpainting <a href="https://arxiv.org/pdf/2002.02909" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy  Images <a href="https://arxiv.org/pdf/2002.02857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Input Dropout for Spatially Aligned Modalities <a href="https://arxiv.org/pdf/2002.02852" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Switchable Precision Neural Networks <a href="https://arxiv.org/pdf/2002.02815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding  Network <a href="https://arxiv.org/pdf/2002.02814" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> FourierNet: Compact mask representation for instance segmentation using  differentiable shape decoders <a href="https://arxiv.org/pdf/2002.02709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Deep Robust Multilevel Semantic Cross-Modal Hashing <a href="https://arxiv.org/pdf/2002.02698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Learning Class Regularized Features for Action Recognition <a href="https://arxiv.org/pdf/2002.02651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Statistical Outlier Identification in Multi-robot Visual SLAM using  Expectation Maximization <a href="https://arxiv.org/pdf/2002.02638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> SideInfNet: A Deep Neural Network for Semi-Automatic Semantic  Segmentation with Side Information <a href="https://arxiv.org/pdf/2002.02634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Visual search over billions of aerial and satellite images <a href="https://arxiv.org/pdf/2002.02624" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Image Fine-grained Inpainting <a href="https://arxiv.org/pdf/2002.02609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Adaptive Deep Metric Embeddings for Person Re-Identification under  Occlusions <a href="https://arxiv.org/pdf/2002.02603" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Object-Adaptive LSTM Network for Real-time Visual Tracking with  Adversarial Data Augmentation <a href="https://arxiv.org/pdf/2002.02598" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Poisson Kernel Avoiding Self-Smoothing in Graph Convolutional Networks <a href="https://arxiv.org/pdf/2002.02589" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Learning Hyperspectral Feature Extraction and Classification with  ResNeXt Network <a href="https://arxiv.org/pdf/2002.02585" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Impact of ImageNet Model Selection on Domain Adaptation <a href="https://arxiv.org/pdf/2002.02559" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Opposite Structure Learning for Semi-supervised Domain Adaptation <a href="https://arxiv.org/pdf/2002.02545" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Continuous Geodesic Convolutions for Learning on 3D Shapes <a href="https://arxiv.org/pdf/2002.02506" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Activation Density driven Energy-Efficient Pruning in Training <a href="https://arxiv.org/pdf/2002.02949" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> AnimePose: Multi-person 3D pose estimation and animation <a href="https://arxiv.org/pdf/2002.02792" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Trust Your Model: Iterative Label Improvement and Robust Training by  Confidence Based Filtering and Dataset Partitioning <a href="https://arxiv.org/pdf/2002.02705" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Optimization of Structural Similarity in Mathematical Imaging <a href="https://arxiv.org/pdf/2002.02657" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Quantifying the Value of Lateral Views in Deep Learning for Chest X-rays <a href="https://arxiv.org/pdf/2002.02582" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Closing the Dequantization Gap: PixelCNN as a Single-Layer Flow <a href="https://arxiv.org/pdf/2002.02547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>



<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Revisiting Spatial Invariance with Low-Rank Local Connectivity</b>  <a href="https://arxiv.org/pdf/2002.02959" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Elsayed%2C+G+F" target="_blank" rel="noopener" style="color:#0000EE;">Gamaleldin F. Elsayed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramachandran%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prajit Ramachandran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shlens%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathon Shlens</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kornblith%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Kornblith</a><br>
<font size="3">
Abstract: Convolutional neural networks are among the most successful architectures in deep learning. This success is at least partially attributable to the efficacy of spatial invariance as an inductive bias. Locally connected layers, which differ from convolutional layers in their lack of spatial invariance, usually perform poorly in practice. However, these observations still leave open the possibility that some degree of relaxation of spatial invariance may yield a better inductive bias than either convolution or local connectivity. To test this hypothesis, we design a method to relax the spatial invariance of a network layer in a controlled manner. In particular, we create a \textit{low-rank} locally connected layer, where the filter bank applied at each position is constructed as a linear combination of basis set of filter banks. By varying the number of filter banks in the basis set, we can control the degree of departure from spatial invariance. In our experiments, we find that relaxing spatial invariance improves classification accuracy over both convolution and locally connected layers across MNIST, CIFAR-10, and CelebA datasets. These results suggest that spatial invariance in convolution layers may be overly restrictive. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络在深学习最成功的架构之中。这一成功是至少部分地归因于空间不变性的功效为感应偏压。本地连接的层，其从卷积层的区别在于它们缺少空间不变性的，通常在实践中表现不佳。然而，这些意见仍然保持打开的可能性，一定程度的空间不变性的放松可能会产生比任何回旋或本地连接更好的归纳偏置。为了检验这一假设，我们设计放松的网络层的空间不变性以受控的方式的方法。特别是，我们创建了一个\ textit {低秩}本地连接的层，其中，所述滤波器组施加在每个位置被构造为基组滤波器组的线性组合。通过改变基组滤波器组的数量，我们可以控制背离空间不变性的程度。在我们的实验中，我们发现，放松的空间不变性在两个卷积提高了分类的准确性和本地连接跨MNIST，CIFAR-10和CelebA数据集层。这些结果表明，在卷积层的空间不变性可能过于严格。</font>
</div>


<hr>
<div id="paper2"> <b>2. $M^3$T: Multi-Modal Continuous Valence-Arousal Estimation in the Wild</b>  <a href="https://arxiv.org/pdf/2002.02957" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan-Hang Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rulin Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiabei Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiguang Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xilin Chen</a><br>
<font size="3">
Abstract: This report describes a multi-modal multi-task ($M^3$T) approach underlying our submission to the valence-arousal estimation track of the Affective Behavior Analysis in-the-wild (ABAW) Challenge, held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. In the proposed $M^3$T framework, we fuse both visual features from videos and acoustic features from the audio tracks to estimate the valence and arousal. The spatio-temporal visual features are extracted with a 3D convolutional network and a bidirectional recurrent neural network. Considering the correlations between valence / arousal, emotions, and facial actions, we also explores mechanisms to benefit from other tasks. We evaluated the $M^3$T framework on the validation set provided by ABAW and it significantly outperforms the baseline method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：该报告描述了我们提交基本的情感行为分析的价觉醒估计轨道多模式多任务（$ M ^ 3 $ T）的方式在最狂野结合举行（ABAW）的挑战，在自动面部和手势识别（FG）2020年提出的$ M ^ 3 $ T框架的IEEE国际会议，我们融合从音轨视频和声音特征的视觉特征来估计效价和唤醒。时空视觉特征与3D卷积网络和双向回归神经网络萃取。考虑价/觉醒，情绪和面部动作之间的相关性，我们还探讨了其他的任务机制的好处。我们评估了由ABAW提供的验证集的$ M ^ 3 $ T框架，它显著优于基线法。</font>
</div>


<hr>
<div id="paper3"> <b>3. On the Robustness of Face Recognition Algorithms Against Attacks and  Bias</b>  <a href="https://arxiv.org/pdf/2002.02942" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richa Singh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Akshay Agarwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maneet Singh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nagpal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shruti Nagpal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vatsa%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mayank Vatsa</a><br>
<font size="3">
Abstract: Face recognition algorithms have demonstrated very high recognition performance, suggesting suitability for real world applications. Despite the enhanced accuracies, robustness of these algorithms against attacks and bias has been challenged. This paper summarizes different ways in which the robustness of a face recognition algorithm is challenged, which can severely affect its intended working. Different types of attacks such as physical presentation attacks, disguise/makeup, digital adversarial attacks, and morphing/tampering using GANs have been discussed. We also present a discussion on the effect of bias on face recognition models and showcase that factors such as age and gender variations affect the performance of modern algorithms. The paper also presents the potential reasons for these challenges and some of the future research directions for increasing the robustness of face recognition models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人脸识别算法已经证明非常高的识别性能，这对于现实世界的应用程序的适用性。尽管提高精度，这些算法受到攻击与偏见稳健性受到了挑战。本文总结了不同的方式，其中人脸识别算法的鲁棒性受到质疑，这会严重影响其预期工作。不同类型的攻击，例如物理呈现攻击，伪装/化妆，数字对抗攻击，变形/篡改使用甘斯进行了讨论。我们还提出关于面部识别模型偏差的影响的讨论，也展示了因素，如年龄和性别变化而影响的现代算法的性能。本文还介绍了这些挑战和一些未来的研究方向为增加脸部识别模型的鲁棒性的潜在原因。</font>
</div>


<hr>
<div id="paper4"> <b>4. How Does Gender Balance In Training Data Affect Face Recognition  Accuracy?</b>  <a href="https://arxiv.org/pdf/2002.02934" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Albiero%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vítor Albiero</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bowyer%2C+K+W" target="_blank" rel="noopener" style="color:#0000EE;">Kevin W. Bowyer</a><br>
<font size="3">
Abstract: Even though deep learning methods have greatly increased the overall accuracy of face recognition, an old problem still persists: accuracy is higher for men than for women. Previous researchers have speculated that the difference could be due to cosmetics, head pose, or hair covering the face. It is also often speculated that the lower accuracy for women is caused by women being under-represented in the training data. This work aims to investigate if gender imbalance in the training data is actually the cause of lower accuracy for females. Using a state-of-the-art deep CNN, three different loss functions, and two training datasets, we train each on seven subsets with different male/female ratios, totaling forty two train-ings. The trained face matchers are then tested on three different testing datasets. Results show that gender-balancing the dataset has an overall positive effect, with higher accuracy for most of the combinations of loss functions and datasets when a balanced subset is used. However, for the best combination of loss function and dataset, the original training dataset shows better accuracy on 3 out of 4 times. We observe that test accuracy for males is higher when the training data is all male. However, test accuracy for females is not maximized when the training data is all female. Fora number of combinations of loss function and test dataset, accuracy for females is higher when only 75% of the train-ing data is female than when 100% of the training data is female. This suggests that lower accuracy for females is nota simple result of the fraction of female training data. By clustering face features, we show that in general, male faces are closer to other male faces than female faces, and female faces are closer to other female faces than male faces </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管深度学习方法已经人脸识别的整体精度大大提高，一个老问题仍然存在：精度是男性高于女性。先前的研究人员推测，差异可能是由于化妆品，头部姿势，或头发遮住了脸。它也经常被推测为女性较低的精度是由妇女是在训练数据代表性不足引起的。这项工作旨在调查，如果在训练数据性别失衡实际上是低精度为女性的原因。用一个国家的最先进的深CNN，三种不同的损失函数，和两个训练数据集，我们每次训练七子集与不同的男性/女性的比例，共计42列车英格斯。训练有素的脸的匹配，然后在三个不同的测试数据集进行测试。结果表明，两性平衡数据集具有总体积极作用，以较高的精度对大多数的损失函数和数据集的组合中的，当使用平衡子集。然而，对于损失函数和数据集，3开出4次原训练数据集显示了更好的精确度的最佳组合。我们观察到，测试精度男性要高，当训练数据是所有男性。然而，当训练数据是所有女性为女性测试精度没有最大化。损失函数和测试数据集的组合的数目论坛，精度为女性更高时只有75％的训练数据的情况相比，在训练数据的100％是女女。这表明，对于女性低精度诺塔女训练数据的分数的简单的结果。通过聚类面部特征，我们表明，在一般情况下，男性的面孔更接近其他男性的脸比女性的面孔，和女性的面孔更接近其他女性的面孔比男性面孔</font>
</div>


<hr>
<div id="paper5"> <b>5. SPN-CNN: Boosting Sensor-Based Source Camera Attribution With Deep  Learning</b>  <a href="https://arxiv.org/pdf/2002.02927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kirchner%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Kirchner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cameron Johnson</a><br>
<font size="3">
Abstract: We explore means to advance source camera identification based on sensor noise in a data-driven framework. Our focus is on improving the sensor pattern noise (SPN) extraction from a single image at test time. Where existing works suppress nuisance content with denoising filters that are largely agnostic to the specific SPN signal of interest, we demonstrate that a~deep learning approach can yield a more suitable extractor that leads to improved source attribution. A series of extensive experiments on various public datasets confirms the feasibility of our approach and its applicability to image manipulation localization and video source attribution. A critical discussion of potential pitfalls completes the text. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于在数据驱动框架传感器噪声探索手段预先源摄像机识别。我们的重点是在测试时间改善从单个图像传感器图案噪声（SPN）萃取。如果现有的工作与抑制去噪是很大程度上不可知的感兴趣的特定SPN​​信号过滤器扰民的内容，我们证明了〜深深的学习方法可以产生更适合提取这会改善来源归属。一系列的各种公共数据集大量的实验证明我们的方法和适用于图像处理的定位和视频源归属的可行性。潜在缺陷的一个重要讨论完成的文字。</font>
</div>


<hr>
<div id="paper6"> <b>6. Subspace Capsule Network</b>  <a href="https://arxiv.org/pdf/2002.02924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Edraki%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marzieh Edraki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahnavard%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nazanin Rahnavard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mubarak Shah</a><br>
<font size="3">
Abstract: Convolutional neural networks (CNNs) have become a key asset to most of fields in AI. Despite their successful performance, CNNs suffer from a major drawback. They fail to capture the hierarchy of spatial relation among different parts of an entity. As a remedy to this problem, the idea of capsules was proposed by Hinton. In this paper, we propose the SubSpace Capsule Network (SCN) that exploits the idea of capsule networks to model possible variations in the appearance or implicitly defined properties of an entity through a group of capsule subspaces instead of simply grouping neurons to create capsules. A capsule is created by projecting an input feature vector from a lower layer onto the capsule subspace using a learnable transformation. This transformation finds the degree of alignment of the input with the properties modeled by the capsule subspace. We show that SCN is a general capsule network that can successfully be applied to both discriminative and generative models without incurring computational overhead compared to CNN during test time. Effectiveness of SCN is evaluated through a comprehensive set of experiments on supervised image classification, semi-supervised image classification and high-resolution image generation tasks using the generative adversarial network (GAN) framework. SCN significantly improves the performance of the baseline models in all 3 tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）已经成为一个重要的资产，以最人工智能领域。尽管他们的成功表现，细胞神经网络从一大缺点。他们并没有捕捉到一个实体的不同部分之间的空间关系的层次结构。作为补救这一问题，胶囊的想法被提出韩丁。在本文中，我们提出了子空间胶囊网络（SCN），它利用胶囊网络的想法通过一组子空间胶囊而不是简单地分组的神经元来创建胶囊以一个实体的外观或隐式地定义的属性可能的变化进行建模。胶囊是通过使用可学习变换从较低层的输入特征向量投影到子空间胶囊创建。这种转变中找到与由胶囊子空间模型化的特性输入的对准程度。我们发现，SCN是可以成功地应用到辨别和生成模型，而不会在测试时间招致相比，CNN的计算开销一般胶囊网络。 SCN的有效性是通过一套综合的有关使用生成对抗网络（GAN）框架监督图像分类，半监督图像分类和高分辨率图像生成任务实验进行评价。 SCN显著提高了基准模型中的所有3个任务的性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. Temporal Segmentation of Surgical Sub-tasks through Deep Learning with  Multiple Data Sources</b>  <a href="https://arxiv.org/pdf/2002.02921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yidan Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pedram%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Sahba Aghajani Pedram</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feyzabadi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seyedshams Feyzabadi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Allan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Max Allan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McLeod%2C+A+J" target="_blank" rel="noopener" style="color:#0000EE;">A. Jonathan McLeod</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Burdick%2C+J+W" target="_blank" rel="noopener" style="color:#0000EE;">Joel W. Burdick</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Azizian%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahdi Azizian</a><br>
<font size="3">
Abstract: Many tasks in robot-assisted surgeries (RAS) can be represented by finite-state machines (FSMs), where each state represents either an action (such as picking up a needle) or an observation (such as bleeding). A crucial step towards the automation of such surgical tasks is the temporal perception of the current surgical scene, which requires a real-time estimation of the states in the FSMs. The objective of this work is to estimate the current state of the surgical task based on the actions performed or events occurred as the task progresses. We propose Fusion-KVE, a unified surgical state estimation model that incorporates multiple data sources including the Kinematics, Vision, and system Events. Additionally, we examine the strengths and weaknesses of different state estimation models in segmenting states with different representative features or levels of granularity. We evaluate our model on the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), as well as a more complex dataset involving robotic intra-operative ultrasound (RIOUS) imaging, created using the da Vinci Xi surgical system. Our model achieves a superior frame-wise state estimation accuracy up to 89.4%, which improves the state-of-the-art surgical state estimation models in both JIGSAWS suturing dataset and our RIOUS dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在机器人辅助的外科手术（RAS）的许多任务可以通过有限状态机（FSM），其中每个状态代表任一种动作（诸如拿起针）或观察（如出血）来表示。对这样的手术任务的自动化的一个关键步骤是目前的手术场景，这需要在有限状态机的状态的实时估计的时间感知。这项工作的目的是评估基础上进行的操作或事件发生的任务进展手术任务的当前状态。我们提出了Fusion-KVE，了采用多种数据源，包括运动学，视觉，和系统事件统一的手术状态估计模型。此外，我们研究不同的状态估计模型的优势和劣势在具有不同代表​​性的特征或粒度级别分割的状态。我们评估我们在约翰霍普金斯大学，ISI手势和技能评估工作组（拼图）模型，以及一个涉及机器人术中超声（RIOUS）成像更复杂的数据集，使用达芬奇手术兮系统创建。我们的模型实现了卓越的逐帧状态估计精度高达89.4％，提高了两个拼图的国家的最先进的手术状态估计模型缝合数据集，我们RIOUS数据集。</font>
</div>


<hr>
<div id="paper8"> <b>8. iqiyi Submission to ActivityNet Challenge 2019 Kinetics-700 challenge:  Hierarchical Group-wise Attention</b>  <a href="https://arxiv.org/pdf/2002.02918" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongyang Cai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Wang</a><br>
<font size="3">
Abstract: In this report, the method for the iqiyi submission to the task of ActivityNet 2019 Kinetics-700 challenge is described. Three models are involved in the model ensemble stage: TSN, HG-NL and StNet. We propose the hierarchical group-wise non-local (HG-NL) module for frame-level features aggregation for video classification. The standard non-local (NL) module is effective in aggregating frame-level features on the task of video classification but presents low parameters efficiency and high computational cost. The HG-NL method involves a hierarchical group-wise structure and generates multiple attention maps to enhance performance. Basing on this hierarchical group-wise structure, the proposed method has competitive accuracy, fewer parameters and smaller computational cost than the standard NL. For the task of ActivityNet 2019 Kinetics-700 challenge, after model ensemble, we finally obtain an averaged top-1 and top-5 error percentage 28.444% on the test set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这个报告中，被描述为爱奇艺提交ActivityNet 2019动力学-700挑战任务的方法。三种型号都参与模式集合阶段：TSN，HG-NL和StNet。我们提出了帧级的分级组明智的非本地（HG-NL）模块支持视频分类聚集。标准的非本地（NL）模块是有效的聚合帧级特征的视频分类，但呈现低参数效率和高的计算成本的任务。在HG-NL方法涉及分级组明智的结构和生成多个关注的地图，以提高性能。在此基础上分级组明智结构，所提出的方法具有竞争力的精确度，更少的参数和比标准NL较小的计算成本。对于ActivityNet 2019动力学-700挑战的任务，模式集合后，我们终于在测试组获得的平均最高-1和前五名误差百分比28.444％。</font>
</div>


<hr>
<div id="paper9"> <b>9. Data augmentation with Möbius transformations</b>  <a href="https://arxiv.org/pdf/2002.02917" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sharon Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiequan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lundh%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Torbjörn Lundh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+A+Y" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Y. Ng</a><br>
<font size="3">
Abstract: Data augmentation has led to substantial improvements in the performance and generalization of deep models, and remain a highly adaptable method to evolving model architectures and varying amounts of data---in particular, extremely scarce amounts of available training data. In this paper, we present a novel method of applying Möbius transformations to augment input images during training. Möbius transformations are bijective conformal maps that generalize image translation to operate over complex inversion in pixel space. As a result, Möbius transformations can operate on the sample level and preserve data labels. We show that the inclusion of Möbius transformations during training enables improved generalization over prior sample-level data augmentation techniques such as cutout and standard crop-and-flip transformations, most notably in low data regimes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据增强导致了深模型的性能和泛化实质性的改善，并保持高度适应性的方法来进化模型架构和不同的数据量---尤其是极为稀缺的可用金额的训练数据。在本文中，我们提出了训练期间施加莫比乌斯变换到扩充输入图像的新方法。莫比乌斯变换是双射共形映射是广义含图像平移了在像素空间复杂反转来操作。其结果是，莫比乌斯转换可以在样品上水平和操作保持数据的标签。我们发现，莫比乌斯变换的训练中列入允许超过前一个样级别的数据增强技术，如切口和标准的作物和翻动的转换，特别是在低数据制度改进的概括。</font>
</div>


<hr>
<div id="paper10"> <b>10. Domain Embedded Multi-model Generative Adversarial Networks for  Image-based Face Inpainting</b>  <a href="https://arxiv.org/pdf/2002.02909" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xian Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Kong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youbing Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siwei Lyu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiancheng Lv</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Canghong Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojie Li</a><br>
<font size="3">
Abstract: Prior knowledge of face shape and location plays an important role in face inpainting. However, traditional facing inpainting methods mainly focus on the generated image resolution of the missing portion but without consideration of the special particularities of the human face explicitly and generally produce discordant facial parts. To solve this problem, we present a stable variational latent generative model for large inpainting of face images. We firstly represent only face regions with the latent variable space but simultaneously constraint the random vectors to offer control over the distribution of latent variables, and combine with the non-face parts textures to generate a face image with plausible contents. Two adversarial discriminators are finally used to judge whether the generated distribution is close to the real distribution or not. It can not only synthesize novel image structures but also explicitly utilize the latent space with Eigenfaces to make better predictions. Furthermore, our work better evaluates the side face impainting problem. Experiments on both CelebA and CelebA-HQ face datasets demonstrate that our proposed approach generates higher quality inpainting results than existing ones. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：脸的形状和位置的先验知识起着面修补了重要作用。然而，传统的面向图像修复方法主要集中在缺失部分的所产生的图像分辨率，但不考虑人脸的特殊特殊性明确和一般产生不和谐的面部部分。为了解决这个问题，我们提出了一个稳定的潜在变生成模型对于大修补面部图像。我们首先仅代表面孔区域与潜变量空间，但同时在潜变量的分布约束随机向量提供控制，并与非人脸部分的纹理相结合，生成具有合理内容的人脸图像。两个敌对的鉴别最终用于判断产生的分布是否接近真实分布与否。它不仅可以合成新的图像结构，而且还明确利用与特征脸的潜在空间，以做出更好的预测。此外，我们的工作更好地评估侧面impainting问题。两个CelebA和CelebA-HQ面数据集实验证明，我们提出的方法生成更高质量的图像修补效果比现有的。</font>
</div>


<hr>
<div id="paper11"> <b>11. An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy  Images</b>  <a href="https://arxiv.org/pdf/2002.02857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hirsch%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Hirsch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kainmueller%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dagmar Kainmueller</a><br>
<font size="3">
Abstract: Segmentation of cell nuclei in microscopy images is a prevalent necessity in cell biology. Especially for three-dimensional datasets, manual segmentation is prohibitively time-consuming, motivating the need for automated methods. Learning-based methods trained on pixel-wise ground-truth segmentations have been shown to yield state-of-the-art results on 2d benchmark image data of nuclei, yet a respective benchmark is missing for 3d image data. In this work, we perform a comparative evaluation of nuclei segmentation algorithms on a database of manually segmented 3d light microscopy volumes. We propose a novel learning strategy that boosts segmentation accuracy by means of a simple auxiliary task, thereby robustly outperforming each of our baselines. Furthermore, we show that one of our baselines, the popular three-label model, when trained with our proposed auxiliary task, outperforms the recent StarDist-3D. As an additional, practical contribution, we benchmark nuclei segmentation against nuclei detection, i.e. the task of merely pinpointing individual nuclei without generating respective pixel-accurate segmentations. For learning nuclei detection, large 3d training datasets of manually annotated nuclei center points are available. However, the impact on detection accuracy caused by training on such sparse ground truth as opposed to dense pixel-wise ground truth has not yet been quantified. To this end, we compare nuclei detection accuracy yielded by training on dense vs. sparse ground truth. Our results suggest that training on sparse ground truth yields competitive nuclei detection rates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在显微镜图像细胞核的分割是细胞生物学中普遍存在的必要性。特别是对于三维数据集，手动分割是过于费时的，激励为自动化方法的需要。基于学习训练的上逐像素地面实况分割方法已经显示出产生关于细胞核的2D基准图像数据状态的最先进的结果，但各自的基准缺少3D图像数据。在这项工作中，我们手动分割的3D光学显微镜卷的数据库上执行的细胞核分割算法的比较评价。我们提出了一种新的学习策略，提升分割精度通过简单的辅助任务的手段，从而有力跑赢我们每一个基线。此外，我们表明，我们的基准之一，流行的三标签模型，当我们提出的辅助任务的训练，优于近期StarDist-3D。作为一个附加的，实用的贡献，对细胞核检测我们基准细胞核分割，即，仅仅确定个体细胞核，而不会产生相应的像素精确的分割的任务。对于学习核检测，人工标注的核中心点大型3D训练数据是可用的。然而，由这种稀疏的地面实况训练，而不是密集的逐像素的地面实况对检测精度的影响尚未量化。为此，我们通过比较致密与稀疏地面实况训练产生的核检测精度。我们的研究结果表明，在稀疏的地面实况训练产生有竞争力的核检测率。</font>
</div>


<hr>
<div id="paper12"> <b>12. Input Dropout for Spatially Aligned Modalities</b>  <a href="https://arxiv.org/pdf/2002.02852" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Blois%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sébastien de Blois</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garon%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mathieu Garon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gagn%C3%A9%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Gagné</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lalonde%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-François Lalonde</a><br>
<font size="3">
Abstract: Computer vision datasets containing multiple modalities such as color, depth, and thermal properties are now commonly accessible and useful for solving a wide array of challenging tasks. However, deploying multi-sensor heads is not possible in many scenarios. As such many practical solutions tend to be based on simpler sensors, mostly for cost, simplicity and robustness considerations. In this work, we propose a training methodology to take advantage of these additional modalities available in datasets, even if they are not available at test time. By assuming that the modalities have a strong spatial correlation, we propose Input Dropout, a simple technique that consists in stochastic hiding of one or many input modalities at training time, while using only the canonical (e.g. RGB) modalities at test time. We demonstrate that Input Dropout trivially combines with existing deep convolutional architectures, and improves their performance on a wide range of computer vision tasks such as dehazing, 6-DOF object tracking, pedestrian detection and object classification. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：含有多种方式，如颜色，深度和热性能的计算机视觉的数据集，现在是解决了各种各样的挑战性的任务阵列通常访问并从中受益。但是，在部署多传感器头是不可能在许多情况下。因此许多切实可行的解决方案往往是基于简单的传感器，主要是出于成本，简单性和稳健性的考虑。在这项工作中，我们提出了一种培训方法采取数据集提供这些附加模式的优势，即使他们不提供测试时间。通过假设方式具有很强的空间相关性，我们建议输入差，一个简单的技术，其在于在训练时间的一个或多个输入模态随机遮盖力，而只使用的规范（例如，RGB）模式在测试时间。我们表明，输入差平凡与现有的深卷积架构相结合，并提高他们对范围广泛的计算机视觉任务，如除雾，6-DOF目标跟踪，行人检测和对象分类性能。</font>
</div>


<hr>
<div id="paper13"> <b>13. Switchable Precision Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.02815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guerra%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luis Guerra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bohan Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ian Reid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Drummond%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Drummond</a><br>
<font size="3">
Abstract: Instantaneous and on demand accuracy-efficiency trade-off has been recently explored in the context of neural networks slimming. In this paper, we propose a flexible quantization strategy, termed Switchable Precision neural Networks (SP-Nets), to train a shared network capable of operating at multiple quantization levels. At runtime, the network can adjust its precision on the fly according to instant memory, latency, power consumption and accuracy demands. For example, by constraining the network weights to 1-bit with switchable precision activations, our shared network spans from BinaryConnect to Binarized Neural Network, allowing to perform dot-products using only summations or bit operations. In addition, a self-distillation scheme is proposed to increase the performance of the quantized switches. We tested our approach with three different quantizers and demonstrate the performance of SP-Nets against independently trained quantized models in classification accuracy for Tiny ImageNet and ImageNet datasets using ResNet-18 and MobileNet architectures. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：瞬时和按需精度效率的权衡已经在最近减肥神经网络的环境中探索。在本文中，我们提出了一种灵活的量化策略，称为切换精密神经网络（SP-网），培养能够在多个量化等级操作的共享的网络。在运行时，网络可以根据即时记忆，延迟，功耗和精度要求在飞行中调整其精度。例如，通过限制网络的权重为1位具有可切换精度激活，我们的共享网络从跨度到BinaryConnect二值化神经网络，允许进行点副产物仅使用加法运算或位操作。此外，自蒸馏方案提出增加量化开关的性能。我们使用RESNET-18和MobileNet架构测试我们有三个不同的量化方法，并展示SP-篮网对独立训练的量化模型，分类准确率的表现为微小ImageNet和ImageNet数据集。</font>
</div>


<hr>
<div id="paper14"> <b>14. Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding  Network</b>  <a href="https://arxiv.org/pdf/2002.02814" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Long%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongzi Long</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shouling Ji</a><br>
<font size="3">
Abstract: This paper strives to learn fine-grained fashion similarity. In this similarity paradigm, one should pay more attention to the similarity in terms of a specific design/attribute among fashion items, which has potential values in many fashion related applications such as fashion copyright protection. To this end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn multiple attribute-specific embeddings in an end-to-end manner, thus measure the fine-grained similarity in the corresponding space. With two attention modules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel Attention, ASEN is able to locate the related regions and capture the essential patterns under the guidance of the specified attribute, thus make the learned attribute-specific embeddings better reflect the fine-grained similarity. Extensive experiments on four fashion-related datasets show the effectiveness of ASEN for fine-grained fashion similarity learning and its potential for fashion reranking. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文力求学习细粒度的方式相似。在这种相似的模式，应该在当中的时尚单品的特定设计/属性，它在很多时尚相关的应用，如时尚版权保护的潜在价值方面更注重的相似性。为此，提出了一种属性特定嵌入网络（ASEN）共同学习多个属性特定的嵌入在端至端的方式，从而测量在相应的空间中的细粒的相似性。有两个注意模块，即属性感知空间注意和属性感知通道注意，日月能够找到相关的区域和指定属性的指导下拍摄的基本模式，从而使学习特定属性的嵌入更好地反映细粒度的相似性。在四大时装相关的数据集大量的实验表明日月的细粒度方式相似的学习和有效性及其对时尚的重新排名的潜力。</font>
</div>


<hr>
<div id="paper15"> <b>15. FourierNet: Compact mask representation for instance segmentation using  differentiable shape decoders</b>  <a href="https://arxiv.org/pdf/2002.02709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Benbarka%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nuri Benbarka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Riaz%2C+H+u+M" target="_blank" rel="noopener" style="color:#0000EE;">Hamd ul Moqeet Riaz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zell%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andreas Zell</a><br>
<font size="3">
Abstract: We present FourierNet a single shot, anchor-free, fully convolutional instance segmentation method, which predicts a shape vector that is converted into contour points using a numerical transformation. Compared to previous methods, we introduce a new training technique, where we utilize a differentiable shape decoder, which achieves automatic weight balancing of the shape vector's coefficients. Fourier series was utilized as a shape encoder because of its coefficient interpretability and fast implementation. By using its lower frequencies we were able to retrieve smooth and compact masks. FourierNet shows promising results compared to polygon representation methods, achieving 30.6 mAP on the MS COCO 2017 benchmark. At lower image resolutions, it runs at 26.6 FPS with 24.3 mAP. It achieves 23.3 mAP using just 8 parameters to represent the mask, which is double the amount of parameters to predict a bounding box. Code will be available at: this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出FourierNet单杆，锚自由，充分卷积实例分割方法，该方法预测，被转换成使用数字变换的轮廓点的形状向量。相比以前的方法中，我们引入一个新的训练技术，在这里我们利用微分的形状解码器，它实现了自动重平衡形状矢量的系数。傅立叶系列被用作形状编码器，因为它的系数解释性和快速实现的。通过使用它的频率较低，我们能够取得光滑紧致口罩。 FourierNet显示有希望的结果相比，多边形表示方法，实现对MS COCO 2017年基准30.6地图。在较低的图像分辨率，它运行在26.6 FPS 24.3地图。它实现只用8个参数来表示掩模，这是参数双倍量来预测的边界框23.3地图。代码将可在：这个HTTP URL。</font>
</div>


<hr>
<div id="paper16"> <b>16. Deep Robust Multilevel Semantic Cross-Modal Hashing</b>  <a href="https://arxiv.org/pdf/2002.02698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Ge Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyang Tan</a><br>
<font size="3">
Abstract: Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel Robust Multilevel Semantic Hashing (RMSH) for more accurate cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics, while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this, we give an effective bound of this value based on the information coding-theoretic analysis, and the above goals are embodied into a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics, alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds, and our method achieves state-of-the-art performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于散列的跨模态获取最近取得显著的进展。但来自不同模态直接的数据嵌入到一个关节海明空间不可避免地会产生虚假的代码由于固有形态差异和噪声。我们提出了一个新颖的多级鲁棒语义散列（RMSH），用于更精确的跨通道检索。它旨在保护具有丰富的语义数据中细粒度的相似性，同时明确要求不同的点之间的距离比为较强的鲁棒性的特定值。对于这一点，我们给出一个有效的结合的该值的基础上，信息编码-理论分析，和上述目标被实现成一个余量自适应三重态损耗。此外，我们通过融合多个散列码来探索很少见过语义，减轻相似信息的稀疏问题引入伪代码。在三个基准实验表明派生边界的有效性，以及我们的方法实现国家的最先进的性能。</font>
</div>


<hr>
<div id="paper17"> <b>17. Learning Class Regularized Features for Action Recognition</b>  <a href="https://arxiv.org/pdf/2002.02651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Stergiou%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandros Stergiou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poppe%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ronald Poppe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Veltkamp%2C+R+C" target="_blank" rel="noopener" style="color:#0000EE;">Remco C. Veltkamp</a><br>
<font size="3">
Abstract: Training Deep Convolutional Neural Networks (CNNs) is based on the notion of using multiple kernels and non-linearities in their subsequent activations to extract useful features. The kernels are used as general feature extractors without specific correspondence to the target class. As a result, the extracted features do not correspond to specific classes. Subtle differences between similar classes are modeled in the same way as large differences between dissimilar classes. To overcome the class-agnostic use of kernels in CNNs, we introduce a novel method named Class Regularization that performs class-based regularization of layer activations. We demonstrate that this not only improves feature search during training, but also allows an explicit assignment of features per class during each stage of the feature extraction process. We show that using Class Regularization blocks in state-of-the-art CNN architectures for action recognition leads to systematic improvement gains of 1.8%, 1.2% and 1.4% on the Kinetics, UCF-101 and HMDB-51 datasets, respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：培训深卷积神经网络（细胞神经网络）是基于使用多个内核和非线性在其随后的激活，提取有用的功能的概念。将所述核用作为一般特征提取器没有具体的对应于目标类。其结果是，所提取的特征不对应于特定的类。相似的类之间的细微差别以同样的方式被建模为不同阶层之间的巨大差异。为了克服类无关的使用在细胞神经网络内核，我们引入已命名的类的正则化，其执行基于类的层的激活的正则化的新方法。我们证明，这不仅提高了训练中的搜索功能，还允许在特征提取过程的每一个阶段，每级功能的明确任务。我们分别显示在国家的最先进的美国有线电视新闻网的架构，使用正则班块动作识别导致的1.8％，1.2％和1.4％，在动力学，UCF-101和HMDB-51数据集系统化改善收益。</font>
</div>


<hr>
<div id="paper18"> <b>18. Statistical Outlier Identification in Multi-robot Visual SLAM using  Expectation Maximization</b>  <a href="https://arxiv.org/pdf/2002.02638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Karimian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arman Karimian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqi Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tron%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roberto Tron</a><br>
<font size="3">
Abstract: This paper introduces a novel and distributed method for detecting inter-map loop closure outliers in simultaneous localization and mapping (SLAM). The proposed algorithm does not rely on a good initialization and can handle more than two maps at a time. In multi-robot SLAM applications, maps made by different agents have nonidentical spatial frames of reference which makes initialization very difficult in the presence of outliers. This paper presents a probabilistic approach for detecting incorrect orientation measurements prior to pose graph optimization by checking the geometric consistency of rotation measurements. Expectation-Maximization is used to fine-tune the model parameters. As ancillary contributions, a new approximate discrete inference procedure is presented which uses evidence on loops in a graph and is based on optimization (Alternate Direction Method of Multipliers). This method yields superior results compared to Belief Propagation and has convergence guarantees. Simulation and experimental results are presented that evaluate the performance of the outlier detection method and the inference algorithm on synthetic and real-world data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了一种新颖的和用于同时定位和地图创建（SLAM）检测地图间环路闭合离群值分布的方法。该算法不依赖于良好的初始化，可以同时处理两个以上的地图。在多机器人SLAM应用，地图由不同试剂制成具有不相同的参考帧的空间，这使得在异常值的存在初始化非常困难。本文提出了通过检查转动测量值的几何一致性检测姿态图形优化之前不正确的方向测量值的概率方法。期望最大化用于微调模型参数。作为辅助的贡献，提出了一种新的近似离散推理过程，它使用的环路证据的曲线图，并且基于优化（乘法器的交替方向法）。这种方法可以得到比置信传播效果出众，具有收敛的保证。仿真和实验结果都认为评估异常检测方法的性能和合成和真实数据的推理算法。</font>
</div>


<hr>
<div id="paper19"> <b>19. SideInfNet: A Deep Neural Network for Semi-Automatic Semantic  Segmentation with Side Information</b>  <a href="https://arxiv.org/pdf/2002.02634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Koh%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Jing Yu Koh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+D+T" target="_blank" rel="noopener" style="color:#0000EE;">Duc Thanh Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Truong%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Quang-Trung Truong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yeung%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sai-Kit Yeung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Binder%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Binder</a><br>
<font size="3">
Abstract: Fully-automatic execution is the ultimate goal for many Computer Vision applications. However, this objective is not always realistic in tasks associated with high failure costs, such as medical applications. For these tasks, a compromise between fully-automatic execution and user interactions is often preferred due to desirable accuracy and performance. Semi-automatic methods require minimal effort from experts by allowing them to provide cues that guide computer algorithms. Inspired by the practicality and applicability of the semi-automatic approach, this paper proposes a novel deep neural network architecture, namely SideInfNet that effectively integrates features learnt from images with side information extracted from user annotations to produce high quality semantic segmentation results. To evaluate our method, we applied the proposed network to three semantic segmentation tasks and conducted extensive experiments on benchmark datasets. Experimental results and comparison with prior work have verified the superiority of our model, suggesting the generality and effectiveness of the model in semi-automatic semantic segmentation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：全自动执行是许多计算机视觉应用的终极目标。然而，这个目标并非总是与高失败成本，如医疗应用相关的任务逼真。对于这些任务的，完全自动执行与用户的交互之间的折中通常优选的，因为所希望的精度和性能。半自动方法，让他们提供线索引导计算机算法需要专家最小的努力。本文通过实用性和半自动方法的适用性的启发，提出了一种新颖深层神经网络体系结构，即SideInfNet有效地集成了各种功能从与来自用户的注释提取以生产高品质的语义分割结果侧信息图像获知。为了评估我们的方法，我们应用所提出的网络三个语义分割任务，并进行了基准数据集广泛的实验。实验结果与以前的工作相比，已经验证了我们的模型的优势，这在半自动语义分割模型的通用性和有效性。</font>
</div>


<hr>
<div id="paper20"> <b>20. Visual search over billions of aerial and satellite images</b>  <a href="https://arxiv.org/pdf/2002.02624" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Keisler%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryan Keisler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Skillman%2C+S+W" target="_blank" rel="noopener" style="color:#0000EE;">Samuel W. Skillman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gonnabathula%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sunny Gonnabathula</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poehnelt%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Poehnelt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rudelis%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xander Rudelis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Warren%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Michael S. Warren</a><br>
<font size="3">
Abstract: We present a system for performing visual search over billions of aerial and satellite images. The purpose of visual search is to find images that are visually similar to a query image. We define visual similarity using 512 abstract visual features generated by a convolutional neural network that has been trained on aerial and satellite imagery. The features are converted to binary values to reduce data and compute requirements. We employ a hash-based search using Bigtable, a scalable database service from Google Cloud. Searching the continental United States at 1-meter pixel resolution, corresponding to approximately 2 billion images, takes approximately 0.1 seconds. This system enables real-time visual search over the surface of the earth, and an interactive demo is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种系统，用于超千亿航空和卫星图像，进行视觉搜索。视觉搜索的目的是找到在视觉上类似于查询图像的图像。我们定义使用经过训练的空中和卫星图像卷积神经网络产生512个抽象的视觉特征视觉相似。特征被转换为二进制值，以减少数据和计算要求。我们采用使用Bigtable的，从谷歌云可扩展的数据库服务基于散列的搜索。搜索美国大陆在1米像素的分辨率，对应于大约2十亿图像，需要大约为0.1秒。这个系统使地球表面上的实时可视化搜索和互动演示可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper21"> <b>21. Image Fine-grained Inpainting</b>  <a href="https://arxiv.org/pdf/2002.02609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Hui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiumei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinbo Gao</a><br>
<font size="3">
Abstract: Image inpainting techniques have shown promising improvement with the assistance of generative adversarial networks (GANs) recently. However, most of them often suffered from completed results with unreasonable structure or blurriness. To mitigate this problem, in this paper, we present a one-stage model that utilizes dense combinations of dilated convolutions to obtain larger and more effective receptive fields. Benefited from the property of this network, we can more easily recover large regions in an incomplete image. To better train this efficient generator, except for frequently-used VGG feature matching loss, we design a novel self-guided regression loss for concentrating on uncertain areas and enhancing the semantic details. Besides, we devise a geometrical alignment constraint item to compensate for the pixel-based distance between prediction features and ground-truth ones. We also employ a discriminator with local and global branches to ensure local-global contents consistency. To further improve the quality of generated images, discriminator feature matching on the local branch is introduced, which dynamically minimizes the similarity of intermediate features between synthetic and ground-truth patches. Extensive experiments on several public datasets demonstrate that our approach outperforms current state-of-the-art methods. Code is available at~\url{this https URL}. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图像修复技术已显示出大有希望与生成对抗网络（甘斯）最近的协助改善。然而，大多数人往往是因与结构不合理或模糊完成结果遭遇。为了缓解这个问题，在该论文中，我们提出了利用扩张卷积的致密组合，以获得更大的和更有效的感受域的一阶段的模型。从这个网络的性能中受益，我们可以更容易在不完整的图像恢复大区。为了更好地培养这种高效的发电机，除了常用VGG特征匹配的损失，我们设计了一个新的自导回归亏损集中在不确定的领域，提高语义细节。此外，我们设计的几何对齐约束的项目，以弥补预测的功能和地面实况的人之间基于像素的距离。我们还采用了与本地和全球的分支机构鉴别，以确保地方 - 全球内容的一致性。为了进一步提高生成的图像的质量，对本地分支鉴别特征匹配被引入，其动态地最小化的合成的和地面实况贴片之间的中间特征的相似性。在几个公开的数据集大量的实验证明我们的方法优于国家的最先进的通用方法。代码可以在〜\ {URL这HTTPS URL}。</font>
</div>


<hr>
<div id="paper22"> <b>22. Adaptive Deep Metric Embeddings for Person Re-Identification under  Occlusions</b>  <a href="https://arxiv.org/pdf/2002.02603" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wanxiang Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Si Chen</a><br>
<font size="3">
Abstract: Person re-identification (ReID) under occlusions is a challenging problem in video surveillance. Most of existing person ReID methods take advantage of local features to deal with occlusions. However, these methods usually independently extract features from the local regions of an image without considering the relationship among different local regions. In this paper, we propose a novel person ReID method, which learns the spatial dependencies between the local regions and extracts the discriminative feature representation of the pedestrian image based on Long Short-Term Memory (LSTM), dealing with the problem of occlusions. In particular, we propose a novel loss (termed the adaptive nearest neighbor loss) based on the classification uncertainty to effectively reduce intra-class variations while enlarging inter-class differences within the adaptive neighborhood of the sample. The proposed loss enables the deep neural network to adaptively learn discriminative metric embeddings, which significantly improve the generalization capability of recognizing unseen person identities. Extensive comparative evaluations on challenging person ReID datasets demonstrate the significantly improved performance of the proposed method compared with several state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人重新鉴定（里德）下闭塞是视频监控一个具有挑战性的问题。大多数现有的人里德方法利用的地方特色，以应对闭塞。然而，这些方法通常是独立地从提取的图像的局部区域的特征而没有考虑不同的局部区域之间的关系。在本文中，我们提出了一种新的人雷德法，其学习的局部区域之间的空间的依赖，并提取基于长短期记忆（LSTM）行人图像的判别特征表示，处理阻塞的问题。特别是，我们提出基于分类的不确定性，以有效地减少类内变化，同时增大样本的自适应邻域内类间差异的新型的损失（称为自适应最近邻损失）。所提出的损失使深层神经网络自适应学习判别指标的嵌入，这显著提高认识看不见人身份的泛化能力。上具有挑战性的人里德数据集广泛比较评价证明了该方法的显著改进的性能与国家的最先进的几种方法进行比较。</font>
</div>


<hr>
<div id="paper23"> <b>23. Object-Adaptive LSTM Network for Real-time Visual Tracking with  Adversarial Data Augmentation</b>  <a href="https://arxiv.org/pdf/2002.02598" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yihan Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Si Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Hua</a><br>
<font size="3">
Abstract: In recent years, deep learning based visual tracking methods have obtained great success owing to the powerful feature representation ability of Convolutional Neural Networks (CNNs). Among these methods, classification-based tracking methods exhibit excellent performance while their speeds are heavily limited by the expensive computation for massive proposal feature extraction. In contrast, matching-based tracking methods (such as Siamese networks) possess remarkable speed superiority. However, the absence of online updating renders these methods unadaptable to significant object appearance variations. In this paper, we propose a novel real-time visual tracking method, which adopts an object-adaptive LSTM network to effectively capture the video sequential dependencies and adaptively learn the object appearance variations. For high computational efficiency, we also present a fast proposal selection strategy, which utilizes the matching-based tracking method to pre-estimate dense proposals and selects high-quality ones to feed to the LSTM network for classification. This strategy efficiently filters out some irrelevant proposals and avoids the redundant computation for feature extraction, which enables our method to operate faster than conventional classification-based tracking methods. In addition, to handle the problems of sample inadequacy and class imbalance during online tracking, we adopt a data augmentation technique based on the Generative Adversarial Network (GAN) to facilitate the training of the LSTM network. Extensive experiments on four visual tracking benchmarks demonstrate the state-of-the-art performance of our method in terms of both tracking accuracy and speed, which exhibits great potentials of recurrent structures for visual tracking. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，深度学习基于视觉跟踪方法已获得由于卷积神经网络（细胞神经网络）的强大功能表现能力，取得巨大成功。在这些方法中，基于分类的跟踪方法表现出优异的性能，而他们的速度很大程度上受到了大量的建议特征提取昂贵的计算限制。相反，基于匹配追踪方法（如连体网络）具有显着的速度优势。然而，不存在在线更新的呈现这些方法不能适应显著对象的外观的变化。在本文中，我们提出了一种新的实时视觉跟踪方法，即采用一个目的自适应LSTM网络有效地捕捉视频顺序依赖性和自适应学习对象的外观的变化。对于高计算效率，我们还提出了一种快速建议选择策略，其利用基于匹配追踪方法预先估计密提案和选择高品质的那些，以进料LSTM网络进行分类。这种策略有效地过滤掉一些不相关的建议，并避免了特征提取，这使得我们的方法比传统的基于分类的跟踪方法更快地操作冗余计算。此外，处理样品不足和不平衡类在线跟踪过程中的问题，我们采用了基于创成对抗性网络（GAN）的数据增强技术，以方便LSTM网络的培训。在四个视觉跟踪基准广泛的实验表明在这两种跟踪准确性和速度，其表现出对视觉跟踪复发性结构的巨大潜力方面我们的方法的状态的最先进的性能。</font>
</div>


<hr>
<div id="paper24"> <b>24. Poisson Kernel Avoiding Self-Smoothing in Graph Convolutional Networks</b>  <a href="https://arxiv.org/pdf/2002.02589" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqing Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shoudong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Zhao</a><br>
<font size="3">
Abstract: Graph convolutional network (GCN) is now an effective tool to deal with non-Euclidean data, such as social networks in social behavior analysis, molecular structure analysis in the field of chemistry, and skeleton-based action recognition. Graph convolutional kernel is one of the most significant factors in GCN to extract nodes' feature, and some improvements of it have reached promising performance theoretically and experimentally. However, there is limited research about how exactly different data types and graph structures influence the performance of these kernels. Most existing methods used an adaptive convolutional kernel to deal with a given graph structure, which still not reveals the internal reasons. In this paper, we started from theoretical analysis of the spectral graph and studied the properties of existing graph convolutional kernels. While taking some designed datasets with specific parameters into consideration, we revealed the self-smoothing phenomenon of convolutional kernels. After that, we proposed the Poisson kernel that can avoid self-smoothing without training any adaptive kernel. Experimental results demonstrate that our Poisson kernel not only works well on the benchmark dataset where state-of-the-art methods work fine, but also is evidently superior to them in synthetic datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图形卷积网络（GCN）现在是处理非欧几里得数据，如社会行为分析社交网络，在化学领域的分子结构分析，以及基于骨架动作识别的有效工具。图卷积内核的GCN以提取节点的功能，最显著的因素之一，而它的一些改进，已经达到了理论和实验有前途的性能。然而，有关数据类型和图形结构究竟如何影响不同这些内核的性能有限的研究。大多数现有的方法中使用的自适应卷积内核来处理一个给定的图形结构，仍然没有揭示的内在原因。在本文中，我们从谱图的理论分析开始，研究了现有的图形内核卷积的性质。虽然采取了一些设计数据集以特定参数加以考虑，我们揭示了卷积核的自流平现象。在那之后，我们提出的泊松内核，可避免自平滑无任何训练适应核。实验结果表明，我们的泊松内核不仅行之有效的基准数据集，其中国家的最先进的方法，做工精细，而且是在合成数据集明显优于它们。</font>
</div>


<hr>
<div id="paper25"> <b>25. Learning Hyperspectral Feature Extraction and Classification with  ResNeXt Network</b>  <a href="https://arxiv.org/pdf/2002.02585" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nyasaka%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Divinah Nyasaka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tinega%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haron Tinega</a><br>
<font size="3">
Abstract: The Hyperspectral image (HSI) classification is a standard remote sensing task, in which each image pixel is given a label indicating the physical land-cover on the earth's surface. The achievements of image semantic segmentation and deep learning approaches on ordinary images have accelerated the research on hyperspectral image classification. Moreover, the utilization of both the spectral and spatial cues in hyperspectral images has shown improved classification accuracy in hyperspectral image classification. The use of only 3D Convolutional Neural Networks (3D-CNN) to extract both spatial and spectral cues from Hyperspectral images results in an explosion of parameters hence high computational cost. We propose network architecture called the MixedSN that utilizes the 3D convolutions to modeling spectral-spatial information in the early layers of the architecture and the 2D convolutions at the top layers which majorly deal with semantic abstraction. We constrain our architecture to ResNeXt block because of their performance and simplicity. Our model drastically reduced the number of parameters and achieved comparable classification performance with state-of-the-art methods on Indian Pine (IP) scene dataset, Pavia University scene (PU) dataset, Salinas (SA) Scene dataset, and Botswana (BW) dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：高光谱图像（HSI）分类是一个标准的远程感测任务，其中，每个图像像素被赋予了标签指示在地球表面上的物理土地覆盖。图像语义分割和深学习方法对普通图像的成就，加速了对高光谱影像分类研究。此外，光谱和空间线索两者在高光谱图像的利用率已经显示出在高光谱图像分类改进的分类精度。仅使用三维卷积神经网络（3D-CNN）的提取从高光谱图像的结果的空间和频谱线索在的参数因此具有高的计算成本爆炸。我们建议网络架构，名为利用三维回旋在顶层这majorly处理语义抽象建模架构的早期层和二维卷积谱空间信息MixedSN。我们限制，因为它们的性能和简单了系统架构以ResNeXt块。我们的模型大大减少参数的数量，并实现与印度的松树（IP）的场景数据集的国家的最先进的方法相同的分类性能，帕维亚大学场景（PU）的数据集，萨利纳斯（SA）场景的数据集，以及博茨瓦纳（BW ）数据集。</font>
</div>


<hr>
<div id="paper26"> <b>26. Impact of ImageNet Model Selection on Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2002.02559" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youshan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Davison%2C+B+D" target="_blank" rel="noopener" style="color:#0000EE;">Brian D. Davison</a><br>
<font size="3">
Abstract: Deep neural networks are widely used in image classification problems. However, little work addresses how features from different deep neural networks affect the domain adaptation problem. Existing methods often extract deep features from one ImageNet model, without exploring other neural networks. In this paper, we investigate how different ImageNet models affect transfer accuracy on domain adaptation problems. We extract features from sixteen distinct pre-trained ImageNet models and examine the performance of twelve benchmarking methods when using the features. Extensive experimental results show that a higher accuracy ImageNet model produces better features, and leads to higher accuracy on domain adaptation problems (with a correlation coefficient of up to 0.95). We also examine the architecture of each neural network to find the best layer for feature extraction. Together, performance from our features exceeds that of the state-of-the-art in three benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络被广泛应用于图像分类问题。然而，很少工作地址是如何从不同的深层神经网络的功能影响领域适应性问题。现有的方法常从一个ImageNet模型深的特点，没有探索其他神经网络。在本文中，我们研究了不同型号ImageNet如何影响域的适应问题传递的准确性。我们提取从16不同的预先训练ImageNet机型的功能和使用功能检查时，十二基准方法的性能。广泛的实验结果表明，较高的精度ImageNet模型产生更好的功能，并导致更高的准确度上域的适应的问题（与最多的相关系数0.95）。我们还检查每个神经网络的体系结构，以找到特征提取的最佳层。总之，从我们的特色性能超过了国家的最先进的三个地基准数据集。</font>
</div>


<hr>
<div id="paper27"> <b>27. Opposite Structure Learning for Semi-supervised Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2002.02545" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Can Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lichen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianqian Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun Fu</a><br>
<font size="3">
Abstract: Current adversarial adaptation methods attempt to align the cross-domain features whereas two challenges remain unsolved: 1) conditional distribution mismatch between different domains and 2) the bias of decision boundary towards the source domain. To solve these challenges, we propose a novel framework for semi-supervised domain adaptation by unifying the learning of opposite structures (UODA). UODA consists of a generator and two classifiers (i.e., the source-based and the target-based classifiers respectively) which are trained with opposite forms of losses for a unified object. The target-based classifier attempts to cluster the target features to improve intra-class density and enlarge inter-class divergence. Meanwhile, the source-based classifier is designed to scatter the source features to enhance the smoothness of decision boundary. Through the alternation of source-feature expansion and target-feature clustering procedures, the target features are well-enclosed within the dilated boundary of the corresponding source features. This strategy effectively makes the cross-domain features precisely aligned. To overcome the model collapse through training, we progressively update the measurement of distance and the feature representation on both domains via an adversarial training paradigm. Extensive experiments on the benchmarks of DomainNet and Office-home datasets demonstrate the effectiveness of our approach over the state-of-the-art method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当前对抗性适应方法试图对准跨域特征而两个挑战仍然没有解决：1）不同的结构域和2之间条件分布不匹配）决策边界的偏置朝向源域。为了解决这些难题，我们通过统一相反的结构（UODA）的学习提出了半监督领域适应一个新的框架。 UODA由发电机和两个分类器（即，基于源和分别与基于目标的分类器），其与一个统一的对象损失相对形式的训练。基于目标的分类器试图群集目标功能，以提高的类内的密度和放大级间发散性。同时，基于源代码的分类被设计成散射源功能，以提高决策边界的平滑度。通过源极 - 功能扩展和目标特征聚类程序的交替，所述目标特征的对应的源特征的扩张型边界内孔封闭。这种策略有效地使交叉域特征精确地对准。为了克服通过培训模式崩溃，我们不断更新的距离的测量，并通过对抗性训练模式在两个域的特征表示。在DomainNet和Office家庭数据集的基准广泛的实验，证明了我们在国家的最先进的方法，该方法的有效性。</font>
</div>


<hr>
<div id="paper28"> <b>28. Continuous Geodesic Convolutions for Learning on 3D Shapes</b>  <a href="https://arxiv.org/pdf/2002.02506" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhangsihao Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Litany%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Or Litany</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Birdal%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tolga Birdal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sridhar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Srinath Sridhar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guibas%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leonidas Guibas</a><br>
<font size="3">
Abstract: The majority of descriptor-based methods for geometric processing of non-rigid shape rely on hand-crafted descriptors. Recently, learning-based techniques have been shown effective, achieving state-of-the-art results in a variety of tasks. Yet, even though these methods can in principle work directly on raw data, most methods still rely on hand-crafted descriptors at the input layer. In this work, we wish to challenge this practice and use a neural network to learn descriptors directly from the raw mesh. To this end, we introduce two modules into our neural architecture. The first is a local reference frame (LRF) used to explicitly make the features invariant to rigid transformations. The second is continuous convolution kernels that provide robustness to sampling. We show the efficacy of our proposed network in learning on raw meshes using two cornerstone tasks: shape matching, and human body parts segmentation. Our results show superior results over baseline methods that use hand-crafted descriptors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数的非刚性形状的几何处理基于描述符的方法依赖于手工制作的描述符。近年来，基于学习的技术已被证明有效，实现多种任务的国家的最先进的成果。然而，尽管这些方法可以直接在原始数据的原理工作的，大多数方法还是依靠在输入层手工制作的描述符。在这项工作中，我们要挑战这一做法，并用神经网络直接从原网学习描述。为此，我们引入两个模块到我们的神经结构。第一种是用于显式地使功能不变的刚性变换的本地参考帧（LRF）。第二个是连续卷积核，要采样提供鲁棒性。我们发现在学习上使用两个基石任务原料网我们提出的网络的功效：人体部位分割形状匹配和。我们的研究结果表明在基线的方法是用手工制作的描述效果出众。</font>
</div>


<hr>
<div id="paper29"> <b>29. Activation Density driven Energy-Efficient Pruning in Training</b>  <a href="https://arxiv.org/pdf/2002.02949" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Foldy-Porto%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timothy Foldy-Porto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Panda%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Priyadarshini Panda</a><br>
<font size="3">
Abstract: The process of neural network pruning with suitable fine-tuning and retraining can yield networks with considerably fewer parameters than the original with comparable degrees of accuracy. Typically, pruning methods require large, pre-trained networks as a starting point from which they perform a time-intensive iterative pruning and retraining algorithm. We propose a novel pruning in-training method that prunes a network real-time during training, reducing the overall training time to achieve an optimal compressed network. To do so, we introduce an activation density based analysis that identifies the optimal relative sizing or compression for each layer of the network. Our method removes the need for pre-training and is architecture agnostic, allowing it to be employed on a wide variety of systems. For VGG-19 and ResNet18 on CIFAR-10, CIFAR-100, and TinyImageNet, we obtain exceedingly sparse networks (up to 200x reduction in parameters and >60x reduction in inference compute operations in the best case) with comparable accuracies (up to 2%-3% loss with respect to the baseline network). By reducing the network size periodically during training, we achieve total training times that are shorter than those of previously proposed pruning methods. Furthermore, training compressed networks at different epochs with our proposed method yields considerable reduction in training compute complexity (1.6x -3.2x lower) at near iso-accuracy as compared to a baseline network trained entirely from scratch. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：与合适的微调神经网络修剪和再培训可以产生网络具有比具有可比较的精确度的原始参数相当少的方法。典型地，修剪方法需要大的，预训练的网络与其所执行时间密集的迭代修剪和再培训算法的起点。我们提出了一个新的修剪在训练方法训练李子期间网络的实时性，降低整体的训练时间，以达到最佳的压缩网络。要做到这一点，我们引入一个激活基于密度分析标识所述最佳相对尺寸或压缩为网络的每个层。我们的方法消除了对预训练的必要性和架构是不可知的，允许它被在各种各样的系统中采用。为VGG-19和ResNet18上CIFAR-10，CIFAR-100，和TinyImageNet，我们得到极其稀疏的网络（高达参数200X减少和> 60倍的减少在推理计算操作在最佳情况下）具有可比较的精度（最多2个％-3相对于基线网络％的损失）。通过培训期间定期降低了网络规模，我们实现了总的训练时间是比那些先前提出的修剪方法更短。此外，相比于完全从头培养了基线网络训练在与我们在接近异精度提出的方法的产率显着降低在训练计算复杂度（1.6倍-3.2x降低）不同时期压缩网络。</font>
</div>


<hr>
<div id="paper30"> <b>30. AnimePose: Multi-person 3D pose estimation and animation</b>  <a href="https://arxiv.org/pdf/2002.02792" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumarapu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laxman Kumarapu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prerana Mukherjee</a><br>
<font size="3">
Abstract: 3D animation of humans in action is quite challenging as it involves using a huge setup with several motion trackers all over the person's body to track the movements of every limb. This is time-consuming and may cause the person discomfort in wearing exoskeleton body suits with motion sensors. In this work, we present a trivial yet effective solution to generate 3D animation of multiple persons from a 2D video using deep learning. Although significant improvement has been achieved recently in 3D human pose estimation, most of the prior works work well in case of single person pose estimation and multi-person pose estimation is still a challenging problem. In this work, we firstly propose a supervised multi-person 3D pose estimation and animation framework namely AnimePose for a given input RGB video sequence. The pipeline of the proposed system consists of various modules: i) Person detection and segmentation, ii) Depth Map estimation, iii) Lifting 2D to 3D information for person localization iv) Person trajectory prediction and human pose tracking. Our proposed system produces comparable results on previous state-of-the-art 3D multi-person pose estimation methods on publicly available datasets MuCo-3DHP and MuPoTS-3D datasets and it also outperforms previous state-of-the-art human pose tracking methods by a significant margin of 11.7% performance gain on MOTA score on Posetrack 2018 dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在行动人的3D动画是相当具有挑战性的，因为它涉及到使用一个巨大的设置与几个运动跟踪器遍布人的身体来跟踪每一个肢体的运动。这是耗时的，并且可能导致人不适穿着外骨骼紧身衣与运动传感器。在这项工作中，我们提出了一个平凡而有效的解决方案以使用深度学习从2D视频多人的3D动画。虽然显著的改善已经在3D人体姿势估计最近取得，大部分之前的作品在一个人的姿态估计和多方人士的姿势估计的情况下很好地工作仍然是一个具有挑战性的问题。在这项工作中，我们首先提出了一个给定的输入RGB视频序列的监督多人3D姿态估计和动画框架，即AnimePose。所提出的系统的流水线由各种模块组成：i）人检测和分割，ⅱ）深度图估计，ⅲ）起重2D到3D信息用于人本地化ⅳ）人轨迹预测和人类姿态跟踪。我们所提出的系统产生的可公开获得的数据集以前的状态的最先进的3D多人姿势估计方法粘膜3DHP和MuPoTS-3D数据集比较的结果，同时也优于国家的最先进的前面的人体姿势的跟踪方法通过对MOTA 11.7％的性能增益显著保证金得分Posetrack 2018集。</font>
</div>


<hr>
<div id="paper31"> <b>31. Trust Your Model: Iterative Label Improvement and Robust Training by  Confidence Based Filtering and Dataset Partitioning</b>  <a href="https://arxiv.org/pdf/2002.02705" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Haase-Sch%C3%BCtz%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Haase-Schütz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stal%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rainer Stal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hertlein%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heinz Hertlein</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sick%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernhard Sick</a><br>
<font size="3">
Abstract: State-of-the-art, high capacity deep neural networks not only require large amounts of labelled training data, they are also highly susceptible to label errors in this data, typically resulting in large efforts and costs and therefore limiting the applicability of deep learning. To alleviate this issue, we propose a novel meta training and labelling scheme that is able to use inexpensive unlabelled data by taking advantage of the generalization power of deep neural networks. We show experimentally that by solely relying on one network architecture and our proposed scheme of iterative training and prediction steps, both label quality and resulting model accuracy can be improved significantly. Our method achieves state-of-the-art results, while being architecture agnostic and therefore broadly applicable. Compared to other methods dealing with erroneous labels, our approach does neither require another network to be trained, nor does it necessarily need an additional, highly accurate reference label set. Instead of removing samples from a labelled set, our technique uses additional sensor data without the need for manual labelling. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：国家的最先进的，高容量的深层神经网络，不仅需要大量的标记的训练数据，他们也很容易受到标签错误，在此数据，通常会生成大量的努力和成本，并因此限制的适用性深度学习。为了缓解这一问题，我们提出了一个新颖元的培训和标签计划，能够通过利用深层神经网络的推广力量的优势，使用廉价的未标记的数据。我们实验表明，单纯依靠一个网络架构和我们所提出的迭代训练和预测的步骤方案，无论是标签质量和得到的模型精度可以提高显著。我们的方法实现状态的最先进的结果，而被架构无关，因此广泛适用的。相比于处理错误标签的其他方法，我们的做法既没有要求其他网络进行训练，也不一定需要一个额外的，高度准确的参考符号集。而不是从标记组取出样品，我们的技术使用附加的传感器数据，而无需手动标记。</font>
</div>


<hr>
<div id="paper32"> <b>32. Optimization of Structural Similarity in Mathematical Imaging</b>  <a href="https://arxiv.org/pdf/2002.02657" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/math?searchtype=author&query=Otero%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">D. Otero</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=La+Torre%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">D. La Torre</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Michailovich%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">O. Michailovich</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Vrscay%2C+E+R" target="_blank" rel="noopener" style="color:#0000EE;">E.R. Vrscay</a><br>
<font size="3">
Abstract: It is now generally accepted that Euclidean-based metrics may not always adequately represent the subjective judgement of a human observer. As a result, many image processing methodologies have been recently extended to take advantage of alternative visual quality measures, the most prominent of which is the Structural Similarity Index Measure (SSIM). The superiority of the latter over Euclidean-based metrics have been demonstrated in several studies. However, being focused on specific applications, the findings of such studies often lack generality which, if otherwise acknowledged, could have provided a useful guidance for further development of SSIM-based image processing algorithms. Accordingly, instead of focusing on a particular image processing task, in this paper, we introduce a general framework that encompasses a wide range of imaging applications in which the SSIM can be employed as a fidelity measure. Subsequently, we show how the framework can be used to cast some standard as well as original imaging tasks into optimization problems, followed by a discussion of a number of novel numerical strategies for their solution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现在人们普遍认为，基于欧几里得度量可以不总是充分代表人类观察者的主观判断。其结果是，许多图像处理方法已经扩展最近采取的另类视觉质量的措施，其中最突出的是结构相似度指数度量（SSIM）的优势。后者通过基于欧几里得度量的优越性已被证明在一些研究。然而，被集中在特定的应用程序，这些研究的结果往往缺乏，如果其它方法确认，可能会对基于SSIM图像处理算法的进一步发展提供了有益的指导普遍性。因此，而不是集中在一个特定的图像处理任务，在本文中，我们引入包括宽范围的，其中，SSIM可以用作一个保真度测度成像应用的一般框架。随后，我们展示了框架如何可以用来施放一些标准以及原始成像任务为优化问题，其次是一些对他们的解决方案的新的数字战略的讨论。</font>
</div>


<hr>
<div id="paper33"> <b>33. Quantifying the Value of Lateral Views in Deep Learning for Chest X-rays</b>  <a href="https://arxiv.org/pdf/2002.02582" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Hashir%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Hashir</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bertrand%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hadrien Bertrand</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cohen%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Joseph Paul Cohen</a><br>
<font size="3">
Abstract: Most deep learning models in chest X-ray prediction utilize the posteroanterior (PA) view due to the lack of other views available. PadChest is a large-scale chest X-ray dataset that has almost 200 labels and multiple views available. In this work, we use PadChest to explore multiple approaches to merging the PA and lateral views for predicting the radiological labels associated with the X-ray image. We find that different methods of merging the model utilize the lateral view differently. We also find that including the lateral view increases performance for 32 labels in the dataset, while being neutral for the others. The increase in overall performance is comparable to the one obtained by using only the PA view with twice the amount of patients in the training set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：胸片预测最深刻的学习模型，利用后前（PA）视图由于缺乏可用的其他意见。 PadChest的是，有近200个标签和多视图提供一个大型的胸部X射线数据集。在这项工作中，我们使用PadChest探索多种方法来合并PA和横向视图预测与X射线图像有关的放射性标签。我们发现合并模型利用横向视图不同的，不同的方法。我们还发现，包括横向视图提高性能，在数据集32级的标签，同时保持中立的人。整体性能的增加是与通过仅使用PA视图与在训练集中两次患者的量而得到的一个。</font>
</div>


<hr>
<div id="paper34"> <b>34. Closing the Dequantization Gap: PixelCNN as a Single-Layer Flow</b>  <a href="https://arxiv.org/pdf/2002.02547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nielsen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Didrik Nielsen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Winther%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Ole Winther</a><br>
<font size="3">
Abstract: Flow models have recently made great progress at modeling quantized sensor data such as images and audio. Due to the continuous nature of flow models, dequantization is typically applied when using them for such quantized data. In this paper, we propose subset flows, a class of flows which can tractably transform subsets of the input space in one pass. As a result, they can be applied directly to quantized data without the need for dequantization. Based on this class of flows, we present a novel interpretation of several existing autoregressive models, including WaveNet and PixelCNN, as single-layer flow models defined through an invertible transformation between uniform noise and data samples. This interpretation suggests that these existing models, 1) admit a latent representation of data and 2) can be stacked in multiple flow layers. We demonstrate this by exploring the latent space of a PixelCNN and by stacking PixelCNNs in multiple flow layers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：流模型建模时量化传感器数据，如图像和音频最近取得了很大进展。由于流模型的连续性质，使用它们用于这种量化的数据去量化时典型地施加。在本文中，我们提议子集流动，一类可tractably将输入空间的子集在一个通流。其结果是，它们可以被直接应用到量化数据，而不需要反量化。基于此类流中，我们提出的几种现有的自回归模型，包括WaveNet和PixelCNN，作为单层流模型通过均匀噪声和数据样本之间的可逆变换中定义的新的解释。这种解释表明，这些现有的模型，1）承认数据和2的潜表示）可以在多个流动层堆叠。我们通过探讨PixelCNN的潜在空间，并通过在多个流程层层积PixelCNNs证明这一点。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-10</title>
    <url>/2020/02/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-10/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> A Multilingual View of Unsupervised Machine Translation <a href="https://arxiv.org/pdf/2002.02955" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> BERT-of-Theseus: Compressing BERT by Progressive Module Replacing <a href="https://arxiv.org/pdf/2002.02925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><div id="title3">
<b>3.</b> Neural Machine Translation System of Indic Languages -- An Attention  based Approach <a href="https://arxiv.org/pdf/2002.02758" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>


<div id="title4">
<b>4.</b> On-Device Information Extraction from SMS using Hybrid Hierarchical  Classification <a href="https://arxiv.org/pdf/2002.02755" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Incorporating Visual Semantics into Sentence Representations within a  Grounded Space <a href="https://arxiv.org/pdf/2002.02734" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Multimodal Matching Transformer for Live Commenting <a href="https://arxiv.org/pdf/2002.02649" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Translating Web Search Queries into Natural Language Questions <a href="https://arxiv.org/pdf/2002.02631" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Introducing Aspects of Creativity in Automatic Poetry Generation <a href="https://arxiv.org/pdf/2002.02511" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Goal-Oriented Multi-Task BERT-Based Dialogue State Tracker <a href="https://arxiv.org/pdf/2002.02450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> I love your chain mail! Making knights smile in a fantasy game world:  Open-domain goal-orientated dialogue agents <a href="https://arxiv.org/pdf/2002.02878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Unsupervised pretraining transfers well across languages <a href="https://arxiv.org/pdf/2002.02848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Depressed individuals express more distorted thinking on social media <a href="https://arxiv.org/pdf/2002.02800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> LEAP System for SRE19 Challenge -- Improvements and Error Analysis <a href="https://arxiv.org/pdf/2002.02735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Transformer Transducer: A Streamable Speech Recognition Model with  Transformer Encoders and RNN-T Loss <a href="https://arxiv.org/pdf/2002.02562" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Robust Multi-channel Speech Recognition using Frequency Aligned Network <a href="https://arxiv.org/pdf/2002.02520" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Consistency of a Recurrent Language Model With Respect to Incomplete  Decoding <a href="https://arxiv.org/pdf/2002.02492" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. A Multilingual View of Unsupervised Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.02955" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xavier Garcia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Foret%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre Foret</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thibault Sellam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P" target="_blank" rel="noopener" style="color:#0000EE;">Ankur P. Parikh</a><br>
<font size="3">
Abstract: We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one language in the (source, target) pair is not associated with any parallel data, but there may exist auxiliary parallel data that contains the other. This auxiliary data can naturally be utilized in our probabilistic framework via a novel cross-translation loss term. Empirically, we show that our approach results in higher BLEU scores over state-of-the-art unsupervised models on the WMT'14 English-French, WMT'16 English-German, and WMT'16 English-Romanian datasets in most directions. In particular, we obtain a +1.65 BLEU advantage over the best-performing unsupervised model in the Romanian-English direction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了多语种神经机器翻译概率框架，包括监管和监督的设置，注重监督的翻译。除了仅存在单语数据可用的研究香草情况下，我们提出了其中在（源，目标）一种语言对不与任何并行数据相关联的新的设置，但也有可能存在包含其它辅助的并行数据。该辅助数据可以自然地在我们的概率框架通过一种新颖的横翻译损耗项利用。根据经验，我们表明，我们的方法得到更高的分数BLEU在国家的最先进的无人监督的车型上WMT'14英法，WMT'16英语 - 德语和英语WMT'16  - 罗马尼亚数据集在大部分方向。特别是，我们获得了在罗马尼亚英语方向表现最好的无监督模型1.65 BLEU优势。</font>
</div>


<hr>
<div id="paper2"> <b>2. BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</b>  <a href="https://arxiv.org/pdf/2002.02925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Canwen Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangchunshu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models, and smooths the training process. Compared to the previous knowledge distillation approaches for BERT compression, our approach leverages only one loss function and one hyper-parameter, liberating human effort from hyper-parameter tuning. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们建议逐步模块更换一个新的模型的压缩方式，有效压缩BERT。我们的方法首先将原始BERT分成几个模块，并建立其紧凑的替代品。然后，我们随机与他们的替代品代替原来的模块的紧凑型模块训练到原来模块的模仿行为。我们不断通过培训提高替代的可能性。这样一来，我们的方法所带来的原始和紧凑车型之间的相互作用更深层次的，和平滑的训练过程。相较于以前的知识蒸馏方法用于BERT压缩，我们的方法利用只有一个损失函数和一个超参数，释放从高参数整定人的努力。我们的方法比现有的知识蒸馏方法胶水标杆，展示模型压缩的一个新的视角。</font>
</div>


<hr>
<div id="paper3"> <b>3. Neural Machine Translation System of Indic Languages -- An Attention  based Approach</b>  <a href="https://arxiv.org/pdf/2002.02758" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Parth Shah</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bakrola%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vishvajit Bakrola</a><br>
<font size="3">
Abstract: Neural machine translation (NMT) is a recent and effective technique which led to remarkable improvements in comparison of conventional machine translation techniques. Proposed neural machine translation model developed for the Gujarati language contains encoder-decoder with attention mechanism. In India, almost all the languages are originated from their ancestral language Sanskrit. They are having inevitable similarities including lexical and named entity similarity. Translating into Indic languages is always be a challenging task. In this paper, we have presented the neural machine translation system (NMT) that can efficiently translate Indic languages like Hindi and Gujarati that together covers more than 58.49 percentage of total speakers in the country. We have compared the performance of our NMT model with automatic evaluation matrices such as BLEU, perplexity and TER matrix. The comparison of our network with Google translate is also presented where it outperformed with a margin of 6 BLEU score on English-Gujarati translation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经机器翻译（NMT）是最近的和有效的技术，其导致显着改善在常规机器翻译技术相比。在古吉拉特语语言开发的建议神经机器翻译模型包含编码器，解码器，注意机制。在印度，几乎所有的语言都源于他们祖先的语言梵语。他们有着必然的相似，包括词汇和命名实体的相似性。翻译成印度语始终是一项艰巨的任务。在本文中，我们提出了神经机器翻译系统（NMT），可以有效地翻译印度语像印地文和古吉拉特一起覆盖全国总扬声器超过58.49百分比。我们比较我们与自动评估NMT模型的性能矩阵如BLEU，困惑和TER矩阵。还提出了我们与谷歌翻译网络的比较在那里与6 BLEU得分上英语翻译古吉拉特语保证金跑赢。</font>
</div>


<hr>
<div id="paper4"> <b>4. On-Device Information Extraction from SMS using Hybrid Hierarchical  Classification</b>  <a href="https://arxiv.org/pdf/2002.02755" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Vatsal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shubham Vatsal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Purre%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naresh Purre</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moharana%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sukumar Moharana</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramena%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gopi Ramena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohanty%2C+D+P" target="_blank" rel="noopener" style="color:#0000EE;">Debi Prasanna Mohanty</a><br>
<font size="3">
Abstract: Cluttering of SMS inbox is one of the serious problems that users today face in the digital world where every online login, transaction, along with promotions generate multiple SMS. This problem not only prevents users from searching and navigating messages efficiently but often results in users missing out the relevant information associated with the corresponding SMS like offer codes, payment reminders etc. In this paper, we propose a unique architecture to organize and extract the appropriate information from SMS and further display it in an intuitive template. In the proposed architecture, we use a Hybrid Hierarchical Long Short Term Memory (LSTM)-Convolutional Neural Network (CNN) to categorize SMS into multiple classes followed by a set of entity parsers used to extract the relevant information from the classified message. The architecture using its preprocessing techniques not only takes into account the enormous variations observed in SMS data but also makes it efficient for its on-device (mobile phone) functionalities in terms of inference timing and size. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：短信收件箱的杂波环境下是严重的问题之一是用户面对今天的数字世界里，所有的在线登录，交易，以得到提拔生成多个短信。这个问题不仅防止用户搜索和浏览效率消息，但通常会导致用户错过了与像优惠代码相应的SMS相关联的相关信息，催款等。在本文中，我们提出了一个独特的体系结构来组织和提取相应的以直观的模板从SMS，并进一步显示它的信息。在所提出的架构中，我们使用了基于分层长短期记忆（LSTM）-Convolutional神经网络（CNN）归类短信到多个类，然后一组用于提取分类信息相关的信息实体解析器。使用它的预处理技术的架构不仅考虑到了SMS数据中观察到的巨大的变化，但也使得有效用于推理定时和尺寸方面及其对设备（移动电话）的功能。</font>
</div>


<hr>
<div id="paper5"> <b>5. Incorporating Visual Semantics into Sentence Representations within a  Grounded Space</b>  <a href="https://arxiv.org/pdf/2002.02734" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bordes%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Bordes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zablocki%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eloi Zablocki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soulier%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laure Soulier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Piwowarski%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin Piwowarski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gallinari%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Gallinari</a><br>
<font size="3">
Abstract: Language grounding is an active field aiming at enriching textual representations with visual information. Generally, textual and visual elements are embedded in the same representation space, which implicitly assumes a one-to-one correspondence between modalities. This hypothesis does not hold when representing words, and becomes problematic when used to learn sentence representations --- the focus of this paper --- as a visual scene can be described by a wide variety of sentences. To overcome this limitation, we propose to transfer visual information to textual representations by learning an intermediate representation space: the grounded space. We further propose two new complementary objectives ensuring that (1) sentences associated with the same visual content are close in the grounded space and (2) similarities between related elements are preserved across modalities. We show that this model outperforms the previous state-of-the-art on classification and semantic relatedness tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言接地是一个活跃的领域，旨在丰富文本表示视觉信息。一般地，文本和视觉元素嵌入在相同的表示空间，这隐含地假设模态之间的一对一的对应关系。代表句话的时候这个假设不成立，并且在使用时要学会一句表述---本文的重点---作为一个视觉场景可以通过各种各样的句子来描述成为问题。为了克服这种局限性，我们提出通过学习中间表示空间的视觉信息传递到文本表示：接地的空间。我们进一步提出了两种新补充的目标，确保用相同的视觉内容相关：（1）句子接近接地的空间和（2）的相关要素之间的相似跨形式保留。我们表明，这种模型优于以前的分类和语义相关任务的国家的最先进的。</font>
</div>


<hr>
<div id="paper6"> <b>6. Multimodal Matching Transformer for Live Commenting</b>  <a href="https://arxiv.org/pdf/2002.02649" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaoqun Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Cui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuming Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Conghui Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tiejun Zhao</a><br>
<font size="3">
Abstract: Automatic live commenting aims to provide real-time comments on videos for viewers. It encourages users engagement on online video sites, and is also a good benchmark for video-to-text generation. Recent work on this task adopts encoder-decoder models to generate comments. However, these methods do not model the interaction between videos and comments explicitly, so they tend to generate popular comments that are often irrelevant to the videos. In this work, we aim to improve the relevance between live comments and videos by modeling the cross-modal interactions among different modalities. To this end, we propose a multimodal matching transformer to capture the relationships among comments, vision, and audio. The proposed model is based on the transformer framework and can iteratively learn the attention-aware representations for each modality. We evaluate the model on a publicly available live commenting dataset. Experiments show that the multimodal matching transformer model outperforms the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动活评论旨在对影片为观众提供实时评论。它鼓励对在线视频网站的用户参与，并且也是视频到文本生成一个很好的标杆。此任务最近的工作，采用编码器，解码器模型来生成评论。然而，这些方法没有视频和评论之间的相互作用明确建模，因此他们往往会产生流行的评论说，往往无关的视频。在这项工作中，我们的目标是通过模拟不同方式之间的跨模态的相互作用，以提高现场评论和视频之间的相关性。为此，我们提出了一种多模式匹配变压器捕捉到的意见，视觉和音频之间的关系。该模型是基于变压器的框架，并可以反复学习注意力感知表示每个模式。我们评估在公开的现场评论数据集模型。实验表明，该多模态匹配变压器模型优于国家的最先进的方法。</font>
</div>


<hr>
<div id="paper7"> <b>7. Translating Web Search Queries into Natural Language Questions</b>  <a href="https://arxiv.org/pdf/2002.02631" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adarsh Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dandapat%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sandipan Dandapat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chordia%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sushil Chordia</a><br>
<font size="3">
Abstract: Users often query a search engine with a specific question in mind and often these queries are keywords or sub-sentential fragments. For example, if the users want to know the answer for "What's the capital of USA", they will most probably query "capital of USA" or "USA capital" or some keyword-based variation of this. For example, for the user entered query "capital of USA", the most probable question intent is "What's the capital of USA?". In this paper, we are proposing a method to generate well-formed natural language question from a given keyword-based query, which has the same question intent as the query. Conversion of keyword-based web query into a well-formed question has lots of applications, with some of them being in search engines, Community Question Answering (CQA) website and bots communication. We found a synergy between query-to-question problem with standard machine translation(MT) task. We have used both Statistical MT (SMT) and Neural MT (NMT) models to generate the questions from the query. We have observed that MT models perform well in terms of both automatic and human evaluation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用户经常查询与具体问题的搜索引擎在心中，往往这些查询的关键字或子句子片段。例如，如果用户想知道的答案“什么是美国的首都”，他们将最有可能的查询“美国资本”或“美国资本”或一些这方面的基于关键字的变化。例如，用户输入查询“美国资本”，最有可能的问题，目的是“什么是美国的首都呢？”。在本文中，我们提议从给定的基于关键字的查询，其中有意向的询问同样的问题，良好的自然语言问题的方法。基于关键字的网页查询转换成一个结构良好的问题有很多的应用，在搜索引擎中的一些人是社区问答（CQA）的网站和漫游通信。我们发现查询到问题的问题，标准的机器翻译（MT）的任务之间的协同作用。我们都用了统计MT（SMT）和神经MT（NMT）模型来生成从查询的问题。我们观察到，MT车型在自动和人工评估方面表现良好。</font>
</div>


<hr>
<div id="paper8"> <b>8. Introducing Aspects of Creativity in Automatic Poetry Generation</b>  <a href="https://arxiv.org/pdf/2002.02511" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bena%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brendan Bena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jugal Kalita</a><br>
<font size="3">
Abstract: Poetry Generation involves teaching systems to automatically generate text that resembles poetic work. A deep learning system can learn to generate poetry on its own by training on a corpus of poems and modeling the particular style of language. In this paper, we propose taking an approach that fine-tunes GPT-2, a pre-trained language model, to our downstream task of poetry generation. We extend prior work on poetry generation by introducing creative elements. Specifically, we generate poems that express emotion and elicit the same in readers, and poems that use the language of dreams---called dream poetry. We are able to produce poems that correctly elicit the emotions of sadness and joy 87.5 and 85 percent, respectively, of the time. We produce dreamlike poetry by training on a corpus of texts that describe dreams. Poems from this model are shown to capture elements of dream poetry with scores of no less than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for all our poems. We also make use of the Coh-Metrix tool, outlining metrics we use to gauge the quality of text generated. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：诗歌生成涉及教学系统自动生成的文本类似于诗的工作。深学习系统可以学习在诗的语料库培训和建模语言的特殊风格产生对自己的诗歌。在本文中，我们建议采取的做法，微调GPT-2，预先训练的语言模型，我们的诗歌产生的下游任务。我们通过引入创意元素延长诗代前期工作。具体而言，我们产生表达情感和引发相同的读者，用梦想的语言---所谓的梦想诗诗和诗歌。我们能够产生诗歌分别是正确引起的时间悲伤和喜悦87.5％和85％，的情绪。我们通过描述梦想文本语料库培训产生梦幻般的诗意。从这个模型诗被示出为与在李克特量表的不小于3.2的分数梦想诗歌捕获元件。我们进行众包的人评价为我们所有的诗。我们还利用COH-Metrix的工具，概述我们用衡量生成的文本的质量指标。</font>
</div>


<hr>
<div id="paper9"> <b>9. Goal-Oriented Multi-Task BERT-Based Dialogue State Tracker</b>  <a href="https://arxiv.org/pdf/2002.02450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gulyaev%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pavel Gulyaev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Elistratova%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eugenia Elistratova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Konovalov%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasily Konovalov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuratov%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Kuratov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pugachev%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leonid Pugachev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Burtsev%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mikhail Burtsev</a><br>
<font size="3">
Abstract: Dialogue State Tracking (DST) is a core component of virtual assistants such as Alexa or Siri. To accomplish various tasks, these assistants need to support an increasing number of services and APIs. The Schema-Guided State Tracking track of the 8th Dialogue System Technology Challenge highlighted the DST problem for unseen services. The organizers introduced the Schema-Guided Dialogue (SGD) dataset with multi-domain conversations and released a zero-shot dialogue state tracking model. In this work, we propose a GOaL-Oriented Multi-task BERT-based dialogue state tracker (GOLOMB) inspired by architectures for reading comprehension question answering systems. The model "queries" dialogue history with descriptions of slots and services as well as possible values of slots. This allows to transfer slot values in multi-domain dialogues and have a capability to scale to unseen slot types. Our model achieves a joint goal accuracy of 53.97% on the SGD dataset, outperforming the baseline model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对话状态跟踪（DST）是虚拟助理如Alexa或锡里的核心部件。要完成各种任务，这些助手需要支持服务和API的越来越多。第八对话系统技术挑战赛的模式制导状态跟踪轨迹突出了DST问题的看不见的服务。主办方引入了多领域的对话架构制导对话（SGD）数据集，并发布了零射门的对话状态跟踪模型。在这项工作中，我们建议架构的启发基于BERT面向目标的多任务对话状态追踪器（哥伦布）阅读理解问答系统。该模型“查询”对话的历史与插槽的说明和服务，以及插槽的可能值。这允许在多域的对话能力转移槽值并具有刻度以看不见的插槽类型。我们的模型实现了对SGD数据集的53.97％的合资目标的准确性，跑赢基准模型。</font>
</div>


<hr>
<div id="paper10"> <b>10. I love your chain mail! Making knights smile in a fantasy game world:  Open-domain goal-orientated dialogue agents</b>  <a href="https://arxiv.org/pdf/2002.02878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Prabhumoye%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shrimai Prabhumoye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Margaret Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Urbanek%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jack Urbanek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dinan%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emily Dinan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Douwe Kiela</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weston%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Weston</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Szlam%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arthur Szlam</a><br>
<font size="3">
Abstract: Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a straightforward learning signal. Humans effortlessly combine the two, for example engaging in chit-chat with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning against an imitation-learned ``chit-chat'' model with two approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-K utterances from the chit-chat model. We show that both models outperform an inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对话研究倾向于闲聊和面向目标的任务区分。前者无疑是更自然，并具有广泛应用的语言，后者有更清晰的指标和一个简单的学习用信号。人类毫不费力地将二者结合起来，例如在闲聊从事与交换信息或引发特异性反应的目标。在这里，我们弥补了丰富的基于文本的多玩家幻想环境的设置这两个领域，其中代理和人类从事这两个动作和对话之间的鸿沟。具体来说，我们训练与强化学习面向目标的模型对模仿学习的``闲聊'模型方法有两种：政策要么学会选择一个主题或学会挑给从顶部-K话语的话语在闲聊模型。我们发现，这两种模式超越逆模型基线和为了达到目标，可以与他们的对话伙伴自然交谈。</font>
</div>


<hr>
<div id="paper11"> <b>11. Unsupervised pretraining transfers well across languages</b>  <a href="https://arxiv.org/pdf/2002.02848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Rivi%C3%A8re%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Morgane Rivière</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Joulin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Armand Joulin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mazar%C3%A9%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre-Emmanuel Mazaré</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dupoux%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emmanuel Dupoux</a><br>
<font size="3">
Abstract: Cross-lingual and multi-lingual training of Automatic Speech Recognition (ASR) has been extensively investigated in the supervised setting. This assumes the existence of a parallel corpus of speech and orthographic transcriptions. Recently, contrastive predictive coding (CPC) algorithms have been proposed to pretrain ASR systems with unlabelled data. In this work, we investigate whether unsupervised pretraining transfers well across languages. We show that a slight modification of the CPC pretraining extracts features that transfer well to other languages, being on par or even outperforming supervised pretraining. This shows the potential of unsupervised methods for languages with few linguistic resources. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨语言和自动语音识别（ASR）的多语种培训的监督设置了广泛的研究。这是假设的语音和正字改编的平行语料库的存在。近日，对比预测编码（CPC）算法被提出来与未标记的数据pretrain ASR系统。在这项工作中，我们调查是否无监督的训练前转移以及跨语言。我们表明，训练前中共提取物的稍微修改的特点是传输以及其他语言，是媲美甚至超越监督训练前。这显示了与一些语言资源语言的无监督方法的潜力。</font>
</div>


<hr>
<div id="paper12"> <b>12. Depressed individuals express more distorted thinking on social media</b>  <a href="https://arxiv.org/pdf/2002.02800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bathina%2C+K+C" target="_blank" rel="noopener" style="color:#0000EE;">Krishna C. Bathina</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thij%2C+M+t" target="_blank" rel="noopener" style="color:#0000EE;">Marijn ten Thij</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lorenzo-Luaces%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lorenzo Lorenzo-Luaces</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rutter%2C+L+A" target="_blank" rel="noopener" style="color:#0000EE;">Lauren A. Rutter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bollen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Johan Bollen</a><br>
<font size="3">
Abstract: Depression is a leading cause of disability worldwide, but is often under-diagnosed and under-treated. One of the tenets of cognitive-behavioral therapy (CBT) is that individuals who are depressed exhibit distorted modes of thinking, so-called cognitive distortions, which can negatively affect their emotions and motivation. Here, we show that individuals with a self-reported diagnosis of depression on social media express higher levels of distorted thinking than a random sample. Some types of distorted thinking were found to be more than twice as prevalent in our depressed cohort, in particular Personalizing and Emotional Reasoning. This effect is specific to the distorted content of the expression and can not be explained by the presence of specific topics, sentiment, or first-person pronouns. Our results point towards the detection, and possibly mitigation, of patterns of online language that are generally deemed depressogenic. They may also provide insight into recent observations that social media usage can have a negative impact on mental health. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：抑郁症是全世界残疾的主要原因，但往往没有得到诊断和治疗不足。一个认知行为疗法（CBT）的原则之一是，谁是抑郁个体表现出扭曲的思维方式，所谓的认知扭曲，可自己的情绪和动机产生负面影响。在这里，我们表明，抑郁对社交媒体的自我报告诊断的个体表达较高水平的扭曲的思维不是随机抽样的。发现某些类型的扭曲的思维方式是在我们的沮丧人群普遍两倍以上，尤其是个性化和情感推理。这种效果是特定于表达的失真内容，并且不能由特定的主题，情绪，或第一人称代词的存在来解释。我们的研究结果指向了检测，并可能减缓，那一般都认为depressogenic在线语言模式。他们还可以提供洞察到最近的观察，社交媒体的使用会对心理健康产生负面影响。</font>
</div>


<hr>
<div id="paper13"> <b>13. LEAP System for SRE19 Challenge -- Improvements and Error Analysis</b>  <a href="https://arxiv.org/pdf/2002.02735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Ramoji%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shreyas Ramoji</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Krishnan%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prashant Krishnan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mysore%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bhargavram Mysore</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Singh%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prachi Singh</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ganapathy%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sriram Ganapathy</a><br>
<font size="3">
Abstract: The NIST Speaker Recognition Evaluation - Conversational Telephone Speech (CTS) challenge 2019 was an open evaluation for the task of speaker verification in challenging conditions. In this paper, we provide a detailed account of the LEAP SRE system submitted to the CTS challenge focusing on the novel components in the back-end system modeling. All the systems used the time-delay neural network (TDNN) based x-vector embeddings. The x-vector system in our SRE19 submission used a large pool of training speakers (about 14k speakers). Following the x-vector extraction, we explored a neural network approach to backend score computation that was optimized for a speaker verification cost. The system combination of generative and neural PLDA models resulted in significant improvements for the SRE evaluation dataset. We also found additional gains for the SRE systems based on score normalization and calibration. Subsequent to the evaluations, we have performed a detailed analysis of the submitted systems. The analysis revealed the incremental gains obtained for different training dataset combinations as well as the modeling methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：NIST说话人识别评估 - 会话电话语音（CTS）挑战2019是为在艰难条件下的说话人确认的任务一个开放的评价。在本文中，我们提供了一个详细的帐户提交CTS挑战着眼于后端系统建模的新组件的LEAP SRE系统。所有的系统中使用的时间延迟神经网络（TDNN）基于X的矢量的嵌入。在我们SRE19提交的X-载体系统使用的培训扬声器（约14K扬声器）的大型游泳池。继X向量提取，我们探讨了神经网络的方法来后端分数计算这是该扬声器核查成本优化。生成和神经PLDA模型的系统组合导致的SRE评估数据集显著的改善。我们还发现基于分数标准化和校准SRE系统的额外收益。继评估，我们已经完成了提交系统的详细分析。分析揭示了不同的训练数据集组合以及建模方法获得的增量收益。</font>
</div>


<hr>
<div id="paper14"> <b>14. Transformer Transducer: A Streamable Speech Recognition Model with  Transformer Encoders and RNN-T Loss</b>  <a href="https://arxiv.org/pdf/2002.02562" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Lu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sak%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hasim Sak</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tripathi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anshuman Tripathi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=McDermott%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erik McDermott</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Koo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Koo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shankar Kumar</a><br>
<font size="3">
Abstract: In this paper we present an end-to-end speech recognition model with Transformer encoders that can be used in a streaming speech recognition system. Transformer computation blocks based on self-attention are used to encode both audio and label sequences independently. The activations from both audio and label encoders are combined with a feed-forward layer to compute a probability distribution over the label space for every combination of acoustic frame position and label history. This is similar to the Recurrent Neural Network Transducer (RNN-T) model, which uses RNNs for information encoding instead of Transformer encoders. The model is trained with a monotonic RNN-T loss well-suited to frame-synchronous, streaming decoding. We present results on the LibriSpeech dataset showing that limiting the left context for self-attention in the Transformer layers makes decoding computationally tractable for streaming, with only a slight degradation in accuracy. We also show that the full attention version of our model achieves competitive performance compared to existing LibriSpeech benchmarks for attention-based models trained with cross-entropy loss. Our results also show that we can bridge the gap between full attention and limited attention versions of our model by attending to a limited number of future frames. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了具有可在流式语音识别系统中使用的变压器编码器的终端到终端的语音识别模型。基于自我关注变压器计算块用于独立编码音频和标签序列。从音频和标签编码器的激活相结合，与前馈层，以计算在所述标签空间上的概率分布的声学帧位置和标签历史的每个组合。这是类似于回归神经网络传感器（RNN-T）模型，它使用RNNs用于编码代替变压器的编码器的信息。该模型被训练以单调RNN-T损耗非常适用于帧同步，流解码。我们上显示，限制自我关注的左上下文变压器层使得解码流媒体，只有在准确度稍有下降，易于计算的LibriSpeech数据集目前的结果。我们还表明，相对于现有的LibriSpeech基准注意力基础的模式与交叉熵损失训练的我们的模型的充分重视版本实现了有竞争力的表现。我们的研究结果还表明我们可以通过参加未来的帧数量有限弥合充分重视和关注有限的版本我们的模型之间的差距。</font>
</div>


<hr>
<div id="paper15"> <b>15. Robust Multi-channel Speech Recognition using Frequency Aligned Network</b>  <a href="https://arxiv.org/pdf/2002.02520" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taejin Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumatani%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kenichi Kumatani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minhua Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sundaram%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiva Sundaram</a><br>
<font size="3">
Abstract: Conventional speech enhancement technique such as beamforming has known benefits for far-field speech recognition. Our own work in frequency-domain multi-channel acoustic modeling has shown additional improvements by training a spatial filtering layer jointly within an acoustic model. In this paper, we further develop this idea and use frequency aligned network for robust multi-channel automatic speech recognition (ASR). Unlike an affine layer in the frequency domain, the proposed frequency aligned component prevents one frequency bin influencing other frequency bins. We show that this modification not only reduces the number of parameters in the model but also significantly and improves the ASR performance. We investigate effects of frequency aligned network through ASR experiments on the real-world far-field data where users are interacting with an ASR system in uncontrolled acoustic environments. We show that our multi-channel acoustic model with a frequency aligned network shows up to 18% relative reduction in word error rate. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：传统的语音增强技术，如波束赋形已经知道好处远场语音识别。我们自己的在频域多通道声学建模工作已经由声学模型内共同培养了空间滤波层示出的额外的改进。在本文中，我们进一步发展为强大的多通道自动语音识别（ASR）这个想法，并使用频率对准网络。不像在频域中的仿射层，所提出的频率对准部件防止一个频率窗口影响其它频率仓。我们表明，这种修改不仅显著减少了参数的数量模型，而且，提高了ASR性能。我们调查通过ASR实验上，用户与失控的声学环境ASR系统交互的真实世界的远场数据的频率对准网络的影响。我们表明，在字差错率我们与频率对准网络显示多通道声学模型高达18％的相对减少。</font>
</div>


<hr>
<div id="paper16"> <b>16. Consistency of a Recurrent Language Model With Respect to Incomplete  Decoding</b>  <a href="https://arxiv.org/pdf/2002.02492" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Welleck%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sean Welleck</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kulikov%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ilia Kulikov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaedeok Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R+Y" target="_blank" rel="noopener" style="color:#0000EE;">Richard Yuanzhe Pang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kyunghyun Cho</a><br>
<font size="3">
Abstract: Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管在各种任务的强大的性能，具有最大似然训练的神经序列模型已显示表现出的问题，如长度偏差和退化重复。我们研究使用常见的解码算法时，从经常性的语言模型接收无限长序列的相关问题。为了分析这个问题，我们首先定义的解码算法的不一致，这意味着该算法可以产生一个具有模型下零概率无限长度的序列。我们证明了常用的不完整的解码算法 - 贪婪搜索，波束搜索，前k个采样，和细胞核采样 - 不一致，尽管事实上，经常性的语言模型被训练来有限长度的生产序列。根据这些分析，我们提出了两种补救措施，地址不一致：前k和核取样，并自终止复发语言模型的一致变种。实证结果表明，发生矛盾的做法，而且所提出的方法防止不一致。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>综述类论文</title>
    <url>/2020/02/08/%E7%BB%BC%E8%BF%B0%E7%B1%BB%E8%AE%BA%E6%96%87/</url>
    <content><![CDATA[<h1 id="综述类论文"><a href="#综述类论文" class="headerlink" title="综述类论文"></a>综述类论文</h1><h2 id="领域自适应翻译"><a href="#领域自适应翻译" class="headerlink" title="领域自适应翻译"></a>领域自适应翻译</h2><ul>
<li>A Survey of Domain Adaptation for Neural Machine Translation. Chenhui Chu, Rui Wang. COLING 2018. <a href="https://arxiv.org/pdf/1806.00258" target="_blank" rel="noopener">[PDF]</a></li>
</ul><h2 id="篇章翻译"><a href="#篇章翻译" class="headerlink" title="篇章翻译"></a>篇章翻译</h2><ul>
<li>A Survey on Document-level Machine Translation: Methods and Evaluation. Sameen Maruf, Fahimeh Saleh, Gholamreza Haffari. arXiv 1912. <a href="https://arxiv.org/pdf/1912.08494" target="_blank" rel="noopener">[PDF]</a></li>
</ul><a id="more"></a>

<h2 id="多语言翻译"><a href="#多语言翻译" class="headerlink" title="多语言翻译"></a>多语言翻译</h2><ul>
<li>A Comprehensive Survey of Multilingual Neural Machine Translation. Raj Dabre, Chenhui Chu, Anoop Kunchukuttan. arXiv 2001. <a href="https://arxiv.org/pdf/2001.01115" target="_blank" rel="noopener">[PDF]</a></li>
</ul>
]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-07</title>
    <url>/2020/02/07/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-07/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Irony Detection in a Multilingual Context <a href="https://arxiv.org/pdf/2002.02427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Conversational Structure Aware and Context Sensitive Topic Model for  Online Discussions <a href="https://arxiv.org/pdf/2002.02353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Citation Data of Czech Apex Courts <a href="https://arxiv.org/pdf/2002.02224" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Related Tasks can Share! A Multi-task Framework for Affective language <a href="https://arxiv.org/pdf/2002.02154" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> A Neural Topical Expansion Framework for Unstructured Persona-oriented  Dialogue Generation <a href="https://arxiv.org/pdf/2002.02153" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Multilingual acoustic word embedding models for processing zero-resource  languages <a href="https://arxiv.org/pdf/2002.02109" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Attractive or Faithful? Popularity-Reinforced Learning for Inspired  Headline Generation <a href="https://arxiv.org/pdf/2002.02095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Aligning the Pretraining and Finetuning Objectives of Language Models <a href="https://arxiv.org/pdf/2002.02000" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B,  Phase-B <a href="https://arxiv.org/pdf/2002.01984" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Zero-Shot Activity Recognition with Videos <a href="https://arxiv.org/pdf/2002.02265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Understanding Car-Speak: Replacing Humans in Dealerships <a href="https://arxiv.org/pdf/2002.02070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Stimulating Creativity with FunLines: A Case Study of Humor Generation  in Headlines <a href="https://arxiv.org/pdf/2002.02031" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Irony Detection in a Multilingual Context</b>  <a href="https://arxiv.org/pdf/2002.02427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghanem%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bilal Ghanem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Karoui%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jihen Karoui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Benamara%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Farah Benamara</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosso%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paolo Rosso</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moriceau%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Véronique Moriceau</a><br>
<font size="3">
Abstract: This paper proposes the first multilingual (French, English and Arabic) and multicultural (Indo-European languages vs. less culturally close languages) irony detection system. We employ both feature-based models and neural architectures using monolingual word representation. We compare the performance of these systems with state-of-the-art systems to identify their capabilities. We show that these monolingual models trained separately on different languages using multilingual word representation or text-based features can open the door to irony detection in languages that lack of annotated data for irony. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一个多语种（法语，英语和阿拉伯语）和多元文化（印欧语言与文化少接近语言）具有讽刺意味的检测系统。我们采用使用单语单词表示既基于特征的模型和神经结构。我们比较这些系统与国家的最先进的系统的性能，以确定自己的能力。我们发现，这些单语车型上使用多语言的单词表示或基于文本的功能，可以打开在缺乏讽刺注释数据的语言门讽刺检测不同的语言单独训练。</font>
</div>


<hr>
<div id="paper2"> <b>2. Conversational Structure Aware and Context Sensitive Topic Model for  Online Discussions</b>  <a href="https://arxiv.org/pdf/2002.02353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingcheng Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Loparo%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kenneth Loparo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kolacinski%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Kolacinski</a><br>
<font size="3">
Abstract: Millions of online discussions are generated everyday on social media platforms. Topic modelling is an efficient way of better understanding large text datasets at scale. Conventional topic models have had limited success in online discussions, and to overcome their limitations, we use the discussion thread tree structure and propose a "popularity" metric to quantify the number of replies to a comment to extend the frequency of word occurrences, and the "transitivity" concept to characterize topic dependency among nodes in a nested discussion thread. We build a Conversational Structure Aware Topic Model (CSATM) based on popularity and transitivity to infer topics and their assignments to comments. Experiments on real forum datasets are used to demonstrate improved performance for topic extraction with six different measurements of coherence and impressive accuracy for topic assignments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数以百万计的在线讨论的是在社会化媒体平台上产生的每一天。主题造型是在规模更好地理解大数据集文字的有效方式。传统主题模型曾在网上讨论有限的成功，并克服其局限性，我们用话题树形结构，并提出了“人气”指标，以回复的数量量化为一个注释，延长词出现的频率，和“及物”的概念来描述话题依赖嵌套话题节点之间。我们建立一个基于普及和传递来推断主题和他们的任务，以评论的会话结构感知主题模型（CSATM）。真实数据集论坛的实验来证明与连贯性和令人印象深刻的精度为主题分配六种不同的测量话题提取改进性能。</font>
</div>


<hr>
<div id="paper3"> <b>3. Citation Data of Czech Apex Courts</b>  <a href="https://arxiv.org/pdf/2002.02224" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hara%C5%A1ta%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jakub Harašta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Novotn%C3%A1%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tereza Novotná</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=%C5%A0avelka%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaromír Šavelka</a><br>
<font size="3">
Abstract: In this paper, we introduce the citation data of the Czech apex courts (Supreme Court, Supreme Administrative Court and Constitutional Court). This dataset was automatically extracted from the corpus of texts of Czech court decisions - CzCDC 1.0. We obtained the citation data by building the natural language processing pipeline for extraction of the court decision identifiers. The pipeline included the (i) document segmentation model and the (ii) reference recognition model. Furthermore, the dataset was manually processed to achieve high-quality citation data as a base for subsequent qualitative and quantitative analyses. The dataset will be made available to the general public. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们介绍了捷克顶点法院的引用数据（最高法院，最高行政法院和宪法法院）。 CzCDC 1.0  - 这个数据集自动从捷克法院判决文本的语料库中提取。我们通过建立自然语言处理管道的法院判决标识符萃取得到的引文数据。该管道包括第（i）文件分割模型和（ⅱ）参考识别模型。此外，该数据集被人工处理，以实现高品质的引用数据作为后续定性和定量分析的位置。该数据集将提供给广大市民。</font>
</div>


<hr>
<div id="paper4"> <b>4. Related Tasks can Share! A Multi-task Framework for Affective language</b>  <a href="https://arxiv.org/pdf/2002.02154" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Deep%2C+K+S" target="_blank" rel="noopener" style="color:#0000EE;">Kumar Shikhar Deep</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Akhtar%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Md Shad Akhtar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ekbal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asif Ekbal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pushpak Bhattacharyya</a><br>
<font size="3">
Abstract: Expressing the polarity of sentiment as 'positive' and 'negative' usually have limited scope compared with the intensity/degree of polarity. These two tasks (i.e. sentiment classification and sentiment intensity prediction) are closely related and may offer assistance to each other during the learning process. In this paper, we propose to leverage the relatedness of multiple tasks in a multi-task learning framework. Our multi-task model is based on convolutional-Gated Recurrent Unit (GRU) framework, which is further assisted by a diverse hand-crafted feature set. Evaluation and analysis suggest that joint-learning of the related tasks in a multi-task framework can outperform each of the individual tasks in the single-task frameworks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：表达情绪的极性为“正”和“负”通常与强度/程度的极性相比具有有限的范围。这两个任务（即情感分类和情感强度预测）紧密相关，并在学习过程中可以互相提供协助。在本文中，我们提出了利用多任务的关联性在多任务学习框架。我们的多任务模式是基于卷积门控重复单元（GRU）的框架，这是一个多元化的手工制作的功能集进一步协助。评估和分析表明，联合学习在多任务框架的相关任务可以在单任务框架超越每个单独的任务。</font>
</div>


<hr>
<div id="paper5"> <b>5. A Neural Topical Expansion Framework for Unstructured Persona-oriented  Dialogue Generation</b>  <a href="https://arxiv.org/pdf/2002.02153" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghong Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piji Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoran Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengjie Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhaochun Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhumin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Ma</a><br>
<font size="3">
Abstract: Unstructured Persona-oriented Dialogue Systems (UPDS) has been demonstrated effective in generating persona consistent responses by utilizing predefined natural language user persona descriptions (e.g., "I am a vegan"). However, the predefined user persona descriptions are usually short and limited to only a few descriptive words, which makes it hard to correlate them with the dialogues. As a result, existing methods either fail to use the persona description or use them improperly when generating persona consistent responses. To address this, we propose a neural topical expansion framework, namely Persona Exploration and Exploitation (PEE), which is able to extend the predefined user persona description with semantically correlated content before utilizing them to generate dialogue responses. PEE consists of two main modules: persona exploration and persona exploitation. The former learns to extend the predefined user persona description by mining and correlating with existing dialogue corpus using a variational auto-encoder (VAE) based topic model. The latter learns to generate persona consistent responses by utilizing the predefined and extended user persona description. In order to make persona exploitation learn to utilize user persona description more properly, we also introduce two persona-oriented loss functions: Persona-oriented Matching (P-Match) loss and Persona-oriented Bag-of-Words (P-BoWs) loss which respectively supervise persona selection in encoder and decoder. Experimental results show that our approach outperforms state-of-the-art baselines, in terms of both automatic and human evaluations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：非结构化假面为本对话系统（UPDS）已经通过利用预定义的自然语言用户个性描述（例如，“我是素食主义者”）证明有效生成人物一致响应。然而，预定义用户的人物角色描述通常是短且仅限于一些描述性词语，这使得它很难将它们与对话相关联。其结果是，现有方法要么不能使用的人物角色描述或生成人物一致响应时不正确地使用它们。为了解决这个问题，我们提出了一个神经局部扩展的框架，即假面勘探和开采（PEE），这是能够利用它们来生成对话响应之前延长与语义相关的内容的预定义的用户角色的描述。 PEE包括两个主要模块：人物的勘探和开采的人物。前者学习到挖掘扩展预定义的用户角色的描述和使用基于主题模型，变分自动编码器（VAE）与现有的对话语料库相关。后者学会生成通过利用预定义的和扩展的用户个性描述人物一致响应。为了使人物开采学会更合理的利用用户的人物角色描述，我们还推出两款面向角色损功能：假面为本匹配（P-匹配）的损失和人物角色的导向一袋字（P-弓）损失分别在监督编码器和解码器的人物的选择。实验结果表明，我们的方法优于国家的最先进的基线，在自动和人的评估方面。</font>
</div>


<hr>
<div id="paper6"> <b>6. Multilingual acoustic word embedding models for processing zero-resource  languages</b>  <a href="https://arxiv.org/pdf/2002.02109" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Herman Kamper</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Matusevych%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yevgen Matusevych</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldwater%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sharon Goldwater</a><br>
<font size="3">
Abstract: Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. In settings where unlabelled speech is the only available resource, such embeddings can be used in "zero-resource" speech search, indexing and discovery systems. Here we propose to train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. For this transfer learning approach, we consider two multilingual recurrent neural network models: a discriminative classifier trained on the joint vocabularies of all training languages, and a correspondence autoencoder trained to reconstruct word pairs. We test these using a word discrimination task on six target zero-resource languages. When trained on seven well-resourced languages, both models perform similarly and outperform unsupervised models trained on the zero-resource languages. With just a single training language, the second model works better, but performance depends more on the particular training--testing language pair. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：声字的嵌入被固定维可变长度的语音段的表示。在设置里未标记的讲话是唯一可用的资源，这样的嵌入可在“零资源”的声音检索，索引和发现系统中使用。在这里，我们提出培养从多个资源充足的语言标记数据的单一监督嵌入模型，然后把它应用到看不见的零资源的语言。对于这种转移的学习方法，我们考虑两个多语种回归神经网络模型：辨别分类培训了所有训练语言的词汇联合，培养重建的单词对对应的自动编码。我们这些使用上的六个标靶零资源语言文字辨别任务测试。当七，资源丰富语言的训练，这两款车型同样执行和超越训练有素的零资源语言的无监督模型。只是一个单一的语言训练，第二个模型更好地工作，但性能更依赖于特定的训练 - 测试语言对。</font>
</div>


<hr>
<div id="paper7"> <b>7. Attractive or Faithful? Popularity-Reinforced Learning for Inspired  Headline Generation</b>  <a href="https://arxiv.org/pdf/2002.02095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun-Zhu Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shuai%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hong-Han Shuai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sung-Lin Yeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi-Lun Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ku%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lun-Wei Ku</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen-Chih Peng</a><br>
<font size="3">
Abstract: With the rapid proliferation of online media sources and published news, headlines have become increasingly important for attracting readers to news articles, since users may be overwhelmed with the massive information. In this paper, we generate inspired headlines that preserve the nature of news articles and catch the eye of the reader simultaneously. The task of inspired headline generation can be viewed as a specific form of Headline Generation (HG) task, with the emphasis on creating an attractive headline from a given news article. To generate inspired headlines, we propose a novel framework called POpularity-Reinforced Learning for inspired Headline Generation (PORL-HG). PORL-HG exploits the extractive-abstractive architecture with 1) Popular Topic Attention (PTA) for guiding the extractor to select the attractive sentence from the article and 2) a popularity predictor for guiding the abstractor to rewrite the attractive sentence. Moreover, since the sentence selection of the extractor is not differentiable, techniques of reinforcement learning (RL) are utilized to bridge the gap with rewards obtained from a popularity score predictor. Through quantitative and qualitative experiments, we show that the proposed PORL-HG significantly outperforms the state-of-the-art headline generation models in terms of attractiveness evaluated by both human (71.03%) and the predictor (at least 27.60%), while the faithfulness of PORL-HG is also comparable to the state-of-the-art generation model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着网络媒体来源和公布的消息迅速扩散，标题已成为吸引读者的新闻文章越来越重要，因为用户可能会用大量的信息所淹没。在本文中，我们产生灵感的头条新闻保持新闻报道的本质，同时吸引读者的眼球。启发标题一代人的任务，可以被看作是头条代（HG）任务的具体形式，并把重点放在建立从给定的新闻文章的标题吸引人。为了产生灵感的头条新闻，我们提出了一个所谓的流行，增强学习的启发标题代（PORL-HG）的新框架。 PORL-HG利用与1）热门话题注意（PTA）萃取-抽象体系结构用于引导所述提取器从物品和2）的流行度预测器用于引导提取器重写吸引力句子中选择有吸引力的句子。此外，由于提取的例句选择是不可微的，（RL）强化学习的技术用于桥接与从普及的分数的预测获得奖励的间隙。通过定量和定性实验，我们表明，该PORL-HG显著优于国家的最先进的标题代车型由两个人（71.03％）和预测（至少27.60％）评估吸引力方面，而PORL-HG的信实也比得上状态的最先进的生成模型。</font>
</div>


<hr>
<div id="paper8"> <b>8. Aligning the Pretraining and Finetuning Objectives of Language Models</b>  <a href="https://arxiv.org/pdf/2002.02000" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pierse%2C+N+W" target="_blank" rel="noopener" style="color:#0000EE;">Nuo Wang Pierse</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingwen Lu</a><br>
<font size="3">
Abstract: We demonstrate that explicitly aligning the pretraining objectives to the finetuning objectives in language model training significantly improves the finetuning task performance and reduces the minimum amount of finetuning examples required. The performance margin gained from objective alignment allows us to build language models with smaller sizes for tasks with less available training data. We provide empirical evidence of these claims by applying objective alignment to concept-of-interest tagging and acronym detection tasks. We found that, with objective alignment, our 768 by 3 and 512 by 3 transformer language models can reach accuracy of 83.9%/82.5% for concept-of-interest tagging and 73.8%/70.2% for acronym detection using only 200 finetuning examples per task, outperforming the 768 by 3 model pretrained without objective alignment by +4.8%/+3.4% and +9.9%/+6.3%. We name finetuning small language models in the presence of hundreds of training examples or less "Few Example learning". In practice, Few Example Learning enabled by objective alignment not only saves human labeling costs, but also makes it possible to leverage language models in more real-time applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们证明，明确对准训练前的目标的目标细化和微调在语言模型训练显著提高了任务细化和微调性能和降低微调所需的例子的最低金额。从客观比对所获得的性能裕量使我们能够建立语言模型尺寸较小与较少的可用训练数据的任务。我们通过将目标对准概念的兴趣标记和缩写检测任务提供这些说法的经验证据。我们发现，与目标定位，我们768 3和512 3变压器的语言模型可以达到83.9％/ 82.5％，准确度概念的兴趣标签和73.8％/ 70.2％的首字母缩写，检测只用200元细化和微调的例子任务，表现优于768由3模型由4.8％/ + 3.4％和9.9％/ + 6.3％没有客观对准预训练。我们的名字在数百个训练范例以下“几个示例学习”的存在微调小语言模型。在实践中，能够通过客观对准几个示例学习不仅节约了人工标识的成本，而且还能够利用语言模型在多个实时应用。</font>
</div>


<hr>
<div id="paper9"> <b>9. UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B,  Phase-B</b>  <a href="https://arxiv.org/pdf/2002.01984" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Telukuntla%2C+S+K" target="_blank" rel="noopener" style="color:#0000EE;">Sai Krishna Telukuntla</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kapri%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aditya Kapri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zadrozny%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wlodek Zadrozny</a><br>
<font size="3">
Abstract: In this paper, we detail our submission to the 2019, 7th year, BioASQ competition. We present our approach for Task-7b, Phase B, Exact Answering Task. These Question Answering (QA) tasks include Factoid, Yes/No, List Type Question answering. Our system is based on a contextual word embedding model. We have used a Bidirectional Encoder Representations from Transformers(BERT) based system, fined tuned for biomedical question answering task using BioBERT. In the third test batch set, our system achieved the highest MRR score for Factoid Question Answering task. Also, for List type question answering task our system achieved the highest recall score in the fourth test batch set. Along with our detailed approach, we present the results for our submissions, and also highlight identified downsides for our current approach and ways to improve them in our future experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们详细介绍了提交给2019年，第7年，BioASQ竞争。我们提出我们的任务-7B的方法，B相，精确应答任务。这些问题回答（QA）任务包括FACTOID，是/否，列表类型答疑。我们的系统是基于上下文的单词嵌入模型。我们使用来自变形金刚双向编码表示（BERT）的系统，罚款调整为使用BioBERT生物医学问题回答任务。在第三个试验批次设置，我们的系统取得了最高MRR得分事实型询问应答任务。另外，对于列表类型问答任务我们的系统实现了在第四一批测试集最高得分召回。随着我们的详细的方法，我们目前的结果为我们的意见，并且还强调确定了我们目前的方法和途径，以提高他们在我们未来的实验缺点。</font>
</div>


<hr>
<div id="paper10"> <b>10. Zero-Shot Activity Recognition with Videos</b>  <a href="https://arxiv.org/pdf/2002.02265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ornek%2C+E+P" target="_blank" rel="noopener" style="color:#0000EE;">Evin Pinar Ornek</a><br>
<font size="3">
Abstract: In this paper, we examined the zero-shot activity recognition task with the usage of videos. We introduce an auto-encoder based model to construct a multimodal joint embedding space between the visual and textual manifolds. On the visual side, we used activity videos and a state-of-the-art 3D convolutional action recognition network to extract the features. On the textual side, we worked with GloVe word embeddings. The zero-shot recognition results are evaluated by top-n accuracy. Then, the manifold learning ability is measured by mean Nearest Neighbor Overlap. In the end, we provide an extensive discussion over the results and the future directions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们研究了零次活动识别任务与视频的使用。我们引入了自动编码器基于模型的构建视觉和文本歧管之间的多模式联合嵌入空间。在可视侧，我们使用活动视频和一个国家的最先进的三维卷积动作识别网络来提取特征。在文字方面，我们曾与手套字的嵌入。零射门的识别结果被顶n准确评估。然后，歧管学习能力通过平均最近邻重叠测量。最后，我们提供对结果和未来的发展方向进行了广泛讨论。</font>
</div>


<hr>
<div id="paper11"> <b>11. Understanding Car-Speak: Replacing Humans in Dealerships</b>  <a href="https://arxiv.org/pdf/2002.02070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hooshmand%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Habeeb Hooshmand</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caverlee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Caverlee</a><br>
<font size="3">
Abstract: A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in "car-speak". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在美国的购车体验的很大一部分涉及在汽车经销店的互动。在经销店，汽车买方中继其销售代表的需求。然而，大多数汽车购买者只拥有他们所需要的车辆的抽象描述。因此，他们只能够描述自己理想中的车“汽车说话。”租车发言是抽象的语言，涉及到汽车的物理属性。在本文中，我们定义汽车发言。我们还致力于策划的车讲的语言合理的数据集。最后，我们培养几个分类，以分类车说话。</font>
</div>


<hr>
<div id="paper12"> <b>12. Stimulating Creativity with FunLines: A Case Study of Humor Generation  in Headlines</b>  <a href="https://arxiv.org/pdf/2002.02031" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hossain%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nabil Hossain</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krumm%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Krumm</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sajed%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tanvir Sajed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kautz%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Henry Kautz</a><br>
<font size="3">
Abstract: Building datasets of creative text, such as humor, is quite challenging. We introduce FunLines, a competitive game where players edit news headlines to make them funny, and where they rate the funniness of headlines edited by others. FunLines makes the humor generation process fun, interactive, collaborative, rewarding and educational, keeping players engaged and providing humor data at a very low cost compared to traditional crowdsourcing approaches. FunLines offers useful performance feedback, assisting players in getting better over time at generating and assessing humor, as our analysis shows. This helps to further increase the quality of the generated dataset. We show the effectiveness of this data by training humor classification models that outperform a previous benchmark, and we release this dataset to the public. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：创作文本的建筑数据集，如幽默，极具挑战性。我们介绍FunLines，有竞争力的游戏，玩家编辑新闻标题，使他们逗的，在那里他们率他人编辑头条funniness。 FunLines使得幽默生成过程的乐趣，互动，合作，奖励，教育，保持玩家参与，并与传统的众包接近以非常低的成本提供幽默的数据。 FunLines提供了有用的绩效反馈，帮助玩家在发电渐入佳境随着时间的推移和评估幽默，因为我们的分析显示。这有助于进一步提高所产生的数据集的质量。我们表明，该数据由超越以前的基准培训幽默分类模型的有效性，以及我们发布这个数据集给公众。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-07</title>
    <url>/2020/02/07/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-07/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Reliability Validation of Learning Enabled Vehicle Tracking <a href="https://arxiv.org/pdf/2002.02424" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Lane Boundary Geometry Extraction from Satellite Imagery <a href="https://arxiv.org/pdf/2002.02362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><div id="title3">
<b>3.</b> Random VLAD based Deep Hashing for Efficient Image Retrieval <a href="https://arxiv.org/pdf/2002.02333" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>


<div id="title4">
<b>4.</b> Fine-Grained Urban Flow Inference <a href="https://arxiv.org/pdf/2002.02318" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Person Re-identification by Contour Sketch under Moderate Clothing  Change <a href="https://arxiv.org/pdf/2002.02295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Zero-Shot Activity Recognition with Videos <a href="https://arxiv.org/pdf/2002.02265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Looking GLAMORous: Vehicle Re-Id in Heterogeneous Cameras Networks with  Global and Local Attention <a href="https://arxiv.org/pdf/2002.02256" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Unsupervised Bidirectional Cross-Modality Adaptation via Deeply  Synergistic Image and Feature Alignment for Medical Image Segmentation <a href="https://arxiv.org/pdf/2002.02255" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> RGB-based Semantic Segmentation Using Self-Supervised Depth Pre-Training <a href="https://arxiv.org/pdf/2002.02200" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Joint Deep Learning of Facial Expression Synthesis and Recognition <a href="https://arxiv.org/pdf/2002.02194" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Pose-Aware Instance Segmentation Framework from Cone Beam CT Images for  Tooth Segmentation <a href="https://arxiv.org/pdf/2002.02143" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> An Information-rich Sampling Technique over Spatio-Temporal CNN for  Classification of Human Actions in Videos <a href="https://arxiv.org/pdf/2002.02100" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Forensic Scanner Identification Using Machine Learning <a href="https://arxiv.org/pdf/2002.02079" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Driver Gaze Estimation in the Real World: Overcoming the Eyeglass  Challenge <a href="https://arxiv.org/pdf/2002.02077" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Residual-Recursion Autoencoder for Shape Illustration Images <a href="https://arxiv.org/pdf/2002.02063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Rotation-invariant Mixed Graphical Model Network for 2D Hand Pose  Estimation <a href="https://arxiv.org/pdf/2002.02033" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> 3DPIFCM Segmentation Algorithm for brain MRI <a href="https://arxiv.org/pdf/2002.01985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Parallel 3DPIFCM Algorithm for Noisy Brain MRI Images <a href="https://arxiv.org/pdf/2002.01981" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> StegColNet: Steganalysis based on an ensemble colorspace approach <a href="https://arxiv.org/pdf/2002.02413" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Covering the News with (AI) Style <a href="https://arxiv.org/pdf/2002.02369" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> VGAI: A Vision-Based Decentralized Controller Learning Framework for  Robot Swarms <a href="https://arxiv.org/pdf/2002.02308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> From Data to Actions in Intelligent Transportation Systems: a  Prescription of Functional Requirements for Model Actionability <a href="https://arxiv.org/pdf/2002.02210" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Unbalanced GANs: Pre-training the Generator of Generative Adversarial  Network using Variational Autoencoder <a href="https://arxiv.org/pdf/2002.02112" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Brain Tumor Segmentation by Cascaded Deep Neural Networks Using Multiple  Image Scales <a href="https://arxiv.org/pdf/2002.01975" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Crowdsourcing the Perception of Machine Teaching <a href="https://arxiv.org/pdf/2002.01618" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-06</title>
    <url>/2020/02/06/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-06/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Rapid Adaptation of BERT for Information Extraction on Domain-Specific  Business Documents <a href="https://arxiv.org/pdf/2002.01861" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Automatic Location Type Classification From Social-Media Posts <a href="https://arxiv.org/pdf/2002.01846" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Discontinuous Constituent Parsing with Pointer Networks <a href="https://arxiv.org/pdf/2002.01824" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters <a href="https://arxiv.org/pdf/2002.01808" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and  Manual Correction <a href="https://arxiv.org/pdf/2002.01761" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Parsing as Pretraining <a href="https://arxiv.org/pdf/2002.01685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Identification of Indian Languages using Ghost-VLAD pooling <a href="https://arxiv.org/pdf/2002.01664" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Lightweight Convolutional Representations for On-Device Natural Language  Processing <a href="https://arxiv.org/pdf/2002.01535" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Generalizing meanings from partners to populations: Hierarchical  inference supports convention formation on networks <a href="https://arxiv.org/pdf/2002.01510" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> If I Hear You Correctly: Building and Evaluating Interview Chatbots with  Active Listening Skills <a href="https://arxiv.org/pdf/2002.01862" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Rapid Adaptation of BERT for Information Extraction on Domain-Specific  Business Documents</b>  <a href="https://arxiv.org/pdf/2002.01861" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruixue Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luyun Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengkai Tu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuqing Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihang Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhao Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luchen Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Xiong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jimmy Lin</a><br>
<font size="3">
Abstract: Techniques for automatically extracting important content elements from business documents such as contracts, statements, and filings have the potential to make business operations more efficient. This problem can be formulated as a sequence labeling task, and we demonstrate the adaption of BERT to two types of business documents: regulatory filings and property lease agreements. There are aspects of this problem that make it easier than "standard" information extraction tasks and other aspects that make it more difficult, but on balance we find that modest amounts of annotated data (less than 100 documents) are sufficient to achieve reasonable accuracy. We integrate our models into an end-to-end cloud platform that provides both an easy-to-use annotation interface as well as an inference interface that allows users to upload documents and inspect model outputs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：技术的自动提取业务文档的重要内容元素，如合同，报表和申报必须让企业运营更为有效的潜力。这个问题可以配制成序列标注任务，我们证明BERT的两种类型商务文档的适应：监管机构备案和物业租赁协议。有迹象表明，使它比“标准”信息提取任务等各个方面，使之更加困难，更容易，但总的来说，我们发现，适量的注释数据（小于100个文件）的足以实现合理的准确性这个问题的各个方面。我们我们的模型集成到一个终端到终端的云平台，同时提供了一个易于使用的界面注释以及推理接口，允许用户上传文档并检查模型输出。</font>
</div>


<hr>
<div id="paper2"> <b>2. Automatic Location Type Classification From Social-Media Posts</b>  <a href="https://arxiv.org/pdf/2002.01846" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kravi%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elad Kravi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kimelfeld%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benny Kimelfeld</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kanza%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaron Kanza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reichart%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roi Reichart</a><br>
<font size="3">
Abstract: We introduce the problem of Automatic Location Type Classification from social media posts. Our goal is to correctly associate a set of messages posted in a small radius around a given location with their corresponding location type, e.g., school, church, restaurant or museum. We provide a dataset of locations associated with tweets posted in close geographical proximity. We explore two approaches to the problem: (a) a pipeline approach where each message is first classified, and then the location associated with the message set is inferred from the individual message labels; and (b) a joint approach where the individual messages are simultaneously processed to yield the desired location type. Our results demonstrate the superiority of the joint approach. Moreover, we show that due to the unique structure of the problem, where weakly-related messages are jointly processed to yield a single final label, simpler linear classifiers outperform deep neural network alternatives that have shown superior in previous text classification tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们从社交媒体文章介绍了自动定位类型划分的问题。我们的目标是正确的一组贴在小半径围绕给定的位置，其对应的位置类型，例如，学校，教堂，餐馆或博物馆的消息联系起来。我们提供的与贴在缘相近微博相关联的位置的数据集。我们探索两种方法的问题：（1）管线方法，其中每个消息首先分类，然后与消息集相关联的位置被从单独的消息标签推断;和（b）在各个消息被同时处理的联合方法，得到所需的位置类型。我们的研究结果表明共同方法的优越性。此外，我们表明，由于问题，在弱相关的消息被联合处理，以产生一个最终标签的独特结构，简单的线性分类跑赢大盘已显示出在以前的文本分类任务优良的深神经网络的替代品。</font>
</div>


<hr>
<div id="paper3"> <b>3. Discontinuous Constituent Parsing with Pointer Networks</b>  <a href="https://arxiv.org/pdf/2002.01824" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fern%C3%A1ndez-Gonz%C3%A1lez%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Fernández-González</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Gómez-Rodríguez</a><br>
<font size="3">
Abstract: One of the most complex syntactic representations used in computational linguistics and NLP are discontinuous constituent trees, crucial for representing all grammatical phenomena of languages such as German. Recent advances in dependency parsing have shown that Pointer Networks excel in efficiently parsing syntactic relations between words in a sentence. This kind of sequence-to-sequence models achieve outstanding accuracies in building non-projective dependency trees, but its potential has not been proved yet on a more difficult task. We propose a novel neural network architecture that, by means of Pointer Networks, is able to generate the most accurate discontinuous constituent representations to date, even without the need of Part-of-Speech tagging information. To do so, we internally model discontinuous constituent structures as augmented non-projective dependency structures. The proposed approach achieves state-of-the-art results on the two widely-used NEGRA and TIGER benchmarks, outperforming previous work by a wide margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一个在计算语言学和自然语言处理中使用的最复杂的句法表征的是不连续的组成部分树木，为代表的语言的所有语法现象，如德国的关键。在依存分析的最新进展表明，指针网络高强高效地分析词与词之间句法关系的句子。这种顺序对序列模型的实现建立非投影依赖树出色的精度，但它的潜力还没有得到一个更艰巨的任务尚未证实。我们提出了一种新的神经网络结构，通过指针网络的手段，能够产生最准确的不连续的组成表示到目前为止，即使没有需要的部分，词性标注信息。要做到这一点，我们在内部不连续的成分结构建模为增强非投影依赖结构。所提出的方法实现对两种广泛使用的NEGRA和TIGER基准国家的先进成果，大幅跑赢以前的工作。</font>
</div>


<hr>
<div id="paper4"> <b>4. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</b>  <a href="https://arxiv.org/pdf/2002.01808" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruize Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duyu Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongyu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuanjing Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=ji%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianshu ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cuihong Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daxin Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they may suffer from the problem of catastrophic forgetting. To address this, we propose K-Adapter, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus different adapters are efficiently trained in a distributed way. We inject two kinds of knowledge, including factual knowledge obtained from automatically aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge obtained from dependency parsing. Results on three knowledge-driven tasks (total six datasets) including relation classification, entity typing and question answering demonstrate that each adapter improves the performance, and the combination of both adapters brings further improvements. Probing experiments further show that K-Adapter captures richer factual and commonsense knowledge than RoBERTa. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们研究的知识注入到大预先训练的车型，如BERT和罗伯塔的问题。现有的方法通常注射知识时更新的预先训练模型的原始参数。然而，当多个种类的知识注入，它们可以从灾难性遗忘的问题的困扰。为了解决这个问题，我们提出了K-适配器，这仍然是预先训练模型固定和支持持续的知识灌输的原始参数。以罗伯塔作为预先训练模型，K-适配器有各种灌输的知识的神经适配器，就像连接到一个罗伯塔插件。有不同的适配器，从而不同的适配器以分布式方式有效地训练之间没有信息流。我们注入两种知识，包括自动对齐文本三联维基百科和维基数据获得的实际知识，并从依赖分析获得的语言知识。三个知识驱动型任务（共六集），包括有关分类，实体打字和答疑结果表明，每个适配器提高性能，并且这两个适配器的组合带来了进一步的改进。探测实验进一步表明，K-适配器捕捉比罗伯塔更丰富的事实和常识性知识。</font>
</div>


<hr>
<div id="paper5"> <b>5. Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and  Manual Correction</b>  <a href="https://arxiv.org/pdf/2002.01761" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingchen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zili Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanna Wang</a><br>
<font size="3">
Abstract: Princeton WordNet (PWN) is a lexicon-semantic network based on cognitive linguistics, which promotes the development of natural language processing. Based on PWN, five Chinese wordnets have been developed to solve the problems of syntax and semantics. They include: Northeastern University Chinese WordNet (NEW), Sinica Bilingual Ontological WordNet (BOW), Southeast University Chinese WordNet (SEW), Taiwan University Chinese WordNet (CWN), Chinese Open WordNet (COW). By using them, we found that these word networks have low accuracy and coverage, and cannot completely portray the semantic network of PWN. So we decided to make a new Chinese wordnet called Multi-Fusion Chinese Wordnet (MCW) to make up those shortcomings. The key idea is to extend the SEW with the help of Oxford bilingual dictionary and Xinhua bilingual dictionary, and then correct it. More specifically, we used machine learning and manual adjustment in our corrections. Two standards were formulated to help our work. We conducted experiments on three tasks including relatedness calculation, word similarity and word sense disambiguation for the comparison of lemma's accuracy, at the same time, coverage also was compared. The results indicate that MCW can benefit from coverage and accuracy via our method. However, it still has room for improvement, especially with lemmas. In the future, we will continue to enhance the accuracy of MCW and expand the concepts in it. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：普林斯顿的WordNet（PWN）是一种基于认知语言学的词汇，语义网络，促进自然语言处理的发展。基于PWN，五个中国词汇网络已发展到解决语法和语义的问题。它们包括：东北大学中国共发现（NEW），报双语本体共发现（BOW），东南大学中国共发现（SEW），台湾大学中国共发现（CWN），中国公开赛共发现（COW）。通过使用它们，我们发现，这些字网络具有低精度和覆盖范围，并不能完全刻画PWN的语义网络。所以我们决定称为Multi-融合中国WORDNET（MCW）新中国共发现来弥补这些缺陷。其核心思想是将与牛津双解词典和新华双语词典的帮助延长SEW，然后纠正它。更具体地讲，我们用机器学习和手动调节我们的更正。两个标准配制，以帮助我们的工作。我们三个任务，包括相关性计算，词语相似度和引理的精度比较多义进行了实验，在同一时间，范围也进行了比较。结果表明，MCW可以从覆盖范围和精度通过我们的方法中受益。然而，它仍然有改进的余地，尤其是与引理。今后，我们将继续加强MCW的准确性和扩大它的概念。</font>
</div>


<hr>
<div id="paper6"> <b>6. Parsing as Pretraining</b>  <a href="https://arxiv.org/pdf/2002.01685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Vilares%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Vilares</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Strzyz%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michalina Strzyz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=S%C3%B8gaard%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anders Søgaard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Gómez-Rodríguez</a><br>
<font size="3">
Abstract: Recent analyses suggest that encoders pretrained for language modeling capture certain morpho-syntactic structure. However, probing frameworks for word vectors still do not report results on standard setups such as constituent and dependency parsing. This paper addresses this problem and does full parsing (on English) relying only on pretraining architectures -- and no decoding. We first cast constituent and dependency parsing as sequence tagging. We then use a single feed-forward layer to directly map word vectors to labels that encode a linearized tree. This is used to: (i) see how far we can reach on syntax modelling with just pretrained encoders, and (ii) shed some light about the syntax-sensitivity of different word vectors (by freezing the weights of the pretraining network during training). For evaluation, we use bracketing F1-score and LAS, and analyze in-depth differences across representations for span lengths and dependency displacements. The overall results surpass existing sequence tagging parsers on the PTB (93.5%) and end-to-end EN-EWT UD (78.8%). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的分析表明预训练的语言模型捕捉特定的形态句法结构的编码器。然而，对于词矢量探测框架仍然不报告的标准设置，如成分和依存分析结果。本文将解决这个问题，不完全解析（英语）只在训练前的架构依赖 - 没有解码。首先，我们投的组成和依赖解析为序列标记。然后，我们使用一个单一的前馈层直接字矢量映射到编码的线性化树标签。这被用来：（ⅰ）见多远我们可以语法建模与刚刚预训练的编码器达到，和（ii）棚约不同字向量的语法灵敏度一些光（由训练期间冻结预训练网络的权重） 。对于评价，我们采用包围F1-得分和LAS，并分析跨表示深度的差异跨度长度和依赖位移。总的结果超过上PTB（93.5％）和端至端EN-EWT UD（78.8％）现有的序列标记的解析器。</font>
</div>


<hr>
<div id="paper7"> <b>7. Identification of Indian Languages using Ghost-VLAD pooling</b>  <a href="https://arxiv.org/pdf/2002.01664" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=N%2C+K+D" target="_blank" rel="noopener" style="color:#0000EE;">Krishna D N</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Patil%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankita Patil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Raj%2C+M+S+P" target="_blank" rel="noopener" style="color:#0000EE;">M.S.P Raj</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=S%2C+S+P+H" target="_blank" rel="noopener" style="color:#0000EE;">Sai Prasad H S</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garapati%2C+P+A" target="_blank" rel="noopener" style="color:#0000EE;">Prabhu Aashish Garapati</a><br>
<font size="3">
Abstract: In this work, we propose a new pooling strategy for language identification by considering Indian languages. The idea is to obtain utterance level features for any variable length audio for robust language recognition. We use the GhostVLAD approach to generate an utterance level feature vector for any variable length input audio by aggregating the local frame level features across time. The generated feature vector is shown to have very good language discriminative features and helps in getting state of the art results for language identification task. We conduct our experiments on 635Hrs of audio data for 7 Indian languages. Our method outperforms the previous state of the art x-vector [11] method by an absolute improvement of 1.88% in F1-score and achieves 98.43% F1-score on the held-out test data. We compare our system with various pooling approaches and show that GhostVLAD is the best pooling approach for this task. We also provide visualization of the utterance level embeddings generated using Ghost-VLAD pooling and show that this method creates embeddings which has very good language discriminative features. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们考虑印度语提出了语言识别新的合并策略。这样做是为了获得话语级功能为强大的语言识别的任何可变长度的音频。我们使用GhostVLAD方法，通过在时间上聚集所述本地帧级特征，以生成用于任何可变长度的输入音频发声水平特征向量。所生成的特征向量显示出具有很好的语言辨别功能，并获得艺术效果的语言识别任务的状态有所帮助。我们进行了对7种印度语言的音频数据的635Hrs我们的实验。我们的方法由1.88％的F1-得分的绝对改进优于现有技术的x矢量[11]的方法的先前状态，并实现所保持的输出测试数据98.43％F1-得分。我们比较了各种池系统接近，并表明GhostVLAD是这个任务的最佳方式汇集。我们还提供了使用Ghost-VLAD汇集和展示所产生的话语层面的嵌入的可视化，这种方法可以创建具有很好的语言判别特征的嵌入。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-06</title>
    <url>/2020/02/06/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-06/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> TPPO: A Novel Trajectory Predictor with Pseudo Oracle <a href="https://arxiv.org/pdf/2002.01852" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Analyzing the Dependency of ConvNets on Spatial Information <a href="https://arxiv.org/pdf/2002.01827" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><div id="title3">
<b>3.</b> Geocoding of trees from street addresses and street-level images <a href="https://arxiv.org/pdf/2002.01708" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>


<div id="title4">
<b>4.</b> CHAIN: Concept-harmonized Hierarchical Inference Interpretation of Deep  Convolutional Neural Networks <a href="https://arxiv.org/pdf/2002.01660" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Solving Raven's Progressive Matrices with Neural Networks <a href="https://arxiv.org/pdf/2002.01646" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Enhancing Feature Invariance with Learned Image Transformations for  Image Retrieval <a href="https://arxiv.org/pdf/2002.01642" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Illumination adaptive person reid based on teacher-student model and  adversarial training <a href="https://arxiv.org/pdf/2002.01625" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Monocular 3D Object Detection with Decoupled Structured Polygon  Estimation and Height-Guided Depth Estimation <a href="https://arxiv.org/pdf/2002.01619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Generating Interpretable Poverty Maps using Object Detection in  Satellite Images <a href="https://arxiv.org/pdf/2002.01612" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Accelerating Object Detection by Erasing Background Activations <a href="https://arxiv.org/pdf/2002.01609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Anomaly Detection by Latent Regularized Dual Adversarial Networks <a href="https://arxiv.org/pdf/2002.01607" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Unsupervised Community Detection with a Potts Model Hamiltonian, an  Efficient Algorithmic Solution, and Applications in Digital Pathology <a href="https://arxiv.org/pdf/2002.01599" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Ego-Lane Estimation by Modelling Lanes and Sensor Failures <a href="https://arxiv.org/pdf/2002.01913" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> A neural network model that learns differences in diagnosis strategies  among radiologists has an improved area under the curve for aneurysm status  classification in magnetic resonance angiography image series <a href="https://arxiv.org/pdf/2002.01891" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Proximity Preserving Binary Code using Signed Graph-Cut <a href="https://arxiv.org/pdf/2002.01793" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Human Posture Recognition and Gesture Imitation with a Humanoid Robot <a href="https://arxiv.org/pdf/2002.01779" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Feature-map-level Online Adversarial Knowledge Distillation <a href="https://arxiv.org/pdf/2002.01775" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Entropy Minimization vs. Diversity Maximization for Domain Adaptation <a href="https://arxiv.org/pdf/2002.01690" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Concept Whitening for Interpretable Image Recognition <a href="https://arxiv.org/pdf/2002.01650" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>python 设置超时退出</title>
    <url>/2020/02/04/python-%E8%AE%BE%E7%BD%AE%E8%B6%85%E6%97%B6%E9%80%80%E5%87%BA/</url>
    <content><![CDATA[<p>使用<strong>eventlet</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> eventlet</span><br><span class="line">eventlet.monkey_patch()</span><br><span class="line"><span class="keyword">with</span> eventlet.Timeout(<span class="number">10</span>,<span class="literal">False</span>):<span class="comment">#设置超时时间为10秒</span></span><br><span class="line">	time.sleep(<span class="number">20</span>) </span><br><span class="line">	print(<span class="string">'1'</span>)</span><br><span class="line">print(<span class="string">'2'</span>)</span><br></pre></td></tr></table></figure><p>上面程序只输出</p><a id="more"></a>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-03</title>
    <url>/2020/02/03/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-03/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Pretrained Transformers for Simple Question Answering over Knowledge  Graphs <a href="https://arxiv.org/pdf/2001.11985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> An efficient automated data analytics approach to large scale  computational comparative linguistics <a href="https://arxiv.org/pdf/2001.11899" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Hybrid Tiled Convolutional Neural Networks for Text Sentiment  Classification <a href="https://arxiv.org/pdf/2001.11857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Break It Down: A Question Understanding Benchmark <a href="https://arxiv.org/pdf/2001.11770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Teaching Machines to Converse <a href="https://arxiv.org/pdf/2001.11701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Pseudo-Bidirectional Decoding for Local Sequence Transduction <a href="https://arxiv.org/pdf/2001.11694" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Self-Adversarial Learning with Comparative Discrimination for Text  Generation <a href="https://arxiv.org/pdf/2001.11691" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Augmenting Visual Question Answering with Semantic Frame Information in  a Multitask Learning Approach <a href="https://arxiv.org/pdf/2001.11673" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Enhancement of Short Text Clustering by Iterative Classification <a href="https://arxiv.org/pdf/2001.11631" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Unwanted Advances in Higher Education: Uncovering Sexual Harassment  Experiences in Academia with Text Mining <a href="https://arxiv.org/pdf/2001.11552" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Pretrained Transformers for Simple Question Answering over Knowledge  Graphs</b>  <a href="https://arxiv.org/pdf/2001.11985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lukovnikov%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">D. Lukovnikov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fischer%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">A. Fischer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lehmann%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">J. Lehmann</a><br>
<font size="3">
Abstract: Answering simple questions over knowledge graphs is a well-studied problem in question answering. Previous approaches for this task built on recurrent and convolutional neural network based architectures that use pretrained word embeddings. It was recently shown that finetuning pretrained transformer networks (e.g. BERT) can outperform previous approaches on various natural language processing tasks. In this work, we investigate how well BERT performs on SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based models in datasparse scenarios. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在知识图回答简单的问题，在问答充分研究的问题。以前的方法完成这个任务建立在使用预训练字的嵌入复发和卷积神经网络基础架构。这是最近表明，微调预训练的变压器网络（例如BERT）可以超越各种自然语言处理任务，以前的方法。在这项工作中，我们探讨SimpleQuestions如何BERT执行和datasparse场景同时提供BERT和基于BiLSTM的模型的评估。</font>
</div>


<hr>
<div id="paper2"> <b>2. An efficient automated data analytics approach to large scale  computational comparative linguistics</b>  <a href="https://arxiv.org/pdf/2001.11899" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mikulyte%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabija Mikulyte</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gilbert%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Gilbert</a><br>
<font size="3">
Abstract: This research project aimed to overcome the challenge of analysing human language relationships, facilitate the grouping of languages and formation of genealogical relationship between them by developing automated comparison techniques. Techniques were based on the phonetic representation of certain key words and concept. Example word sets included numbers 1-10 (curated), large database of numbers 1-10 and sheep counting numbers 1-10 (other sources), colours (curated), basic words (curated). To enable comparison within the sets the measure of Edit distance was calculated based on Levenshtein distance metric. This metric between two strings is the minimum number of single-character edits, operations including: insertions, deletions or substitutions. To explore which words exhibit more or less variation, which words are more preserved and examine how languages could be grouped based on linguistic distances within sets, several data analytics techniques were involved. Those included density evaluation, hierarchical clustering, silhouette, mean, standard deviation and Bhattacharya coefficient calculations. These techniques lead to the development of a workflow which was later implemented by combining Unix shell scripts, a developed R package and SWI Prolog. This proved to be computationally efficient and permitted the fast exploration of large language sets and their analysis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本研究项目旨在克服分析人类语言的关系的挑战，通过开发自动比较技术促进语言和形成它们之间的关系家谱分组。技术是基于某些关键词和概念的语音表示。例如字组包括编号1-10（策划），大数据库编号1-10和羊计数编号1-10（其它来源），颜色（策划）的，基本字（策划）。为了使基于Levenshtein距离度量计算编辑距离的测量集合中的比较。两个字符串之间的度量是单字符编辑，操作，包括的最小数目：插入，缺失或取代。探讨其中的话表现出或多或少的变化，这词更保存和研究如何可以语言基于集内的语言距离进行分组，几个数据分析技术的参与。这些问题包括浓度评价，层次聚类，侧影，平均值，标准偏差和查亚系数的计算。这些技术导致后来被合并的Unix shell脚本，一个开发[R包，SWI Prolog的实现工作流的发展。事实证明，这是计算效率和许可的大型语言组和他们的分析快速探索。</font>
</div>


<hr>
<div id="paper3"> <b>3. Hybrid Tiled Convolutional Neural Networks for Text Sentiment  Classification</b>  <a href="https://arxiv.org/pdf/2001.11857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Trusca%2C+M+M" target="_blank" rel="noopener" style="color:#0000EE;">Maria Mihaela Trusca</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Spanakis%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerasimos Spanakis</a><br>
<font size="3">
Abstract: The tiled convolutional neural network (tiled CNN) has been applied only to computer vision for learning invariances. We adjust its architecture to NLP to improve the extraction of the most salient features for sentiment analysis. Knowing that the major drawback of the tiled CNN in the NLP field is its inflexible filter structure, we propose a novel architecture called hybrid tiled CNN that applies a filter only on the words that appear in the similar contexts and on their neighbor words (a necessary step for preventing the loss of some n-grams). The experiments on the datasets of IMDB movie reviews and SemEval 2017 demonstrate the efficiency of the hybrid tiled CNN that performs better than both CNN and tiled CNN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：平铺卷积神经网络（CNN平铺）已经只适用于计算机视觉学习不变性。我们调整公司架构，以NLP提高最显着的特征为情感分析提取。明知平铺CNN在NLP领域的主要缺点是其不灵活的过滤器结构，我们提出了一种新的架构称为混合平铺CNN说，仅在出现在相似的背景和他们的邻居的话的话应用过滤器（必要步骤，用于防止一些的n-gram的损失）。对IMDB电影评论和SemEval 2017年的数据集上的实验证明了混合动力的效率平铺CNN说，比CNN都和瓷砖CNN性能更好。</font>
</div>


<hr>
<div id="paper4"> <b>4. Break It Down: A Question Understanding Benchmark</b>  <a href="https://arxiv.org/pdf/2001.11770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wolfson%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomer Wolfson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Geva%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mor Geva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankit Gupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gardner%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matt Gardner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoav Goldberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deutch%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Deutch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Berant</a><br>
<font size="3">
Abstract: Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：理解自然语言问题需要一个问题分解成用于计算其答案的必要步骤的能力。在这项工作中，我们介绍的问题一个问题分解含义表示（QDMR）。 QDMR构成的步骤，通过自然语言来表达，所必需的回答问题的有序列表。我们开发了一个众包管道，显示出质量QDMRs可以大规模进行标注，并释放中断的数据集，包含超过83K对遇到的问题进行QDMRs。我们通过展示（一），它可以被用来改善对HotpotQA数据集开放域问答，（B），可以确定性地转换成伪SQL形式语言，它可以在语义缓解标注证明QDMR的效用解析应用。最后，我们使用中断训练序列到序列模型复制，它分析问题到QDMR结构，并表明它大幅优于几种天然基线。</font>
</div>


<hr>
<div id="paper5"> <b>5. Teaching Machines to Converse</b>  <a href="https://arxiv.org/pdf/2001.11701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwei Li</a><br>
<font size="3">
Abstract: The ability of a machine to communicate with humans has long been associated with the general success of AI. This dates back to Alan Turing's epoch-making work in the early 1950s, which proposes that a machine's intelligence can be tested by how well it, the machine, can fool a human into believing that the machine is a human through dialogue conversations. Many systems learn generation rules from a minimal set of authored rules or labels on top of hand-coded rules or templates, and thus are both expensive and difficult to extend to open-domain scenarios. Recently, the emergence of neural network models the potential to solve many of the problems in dialogue learning that earlier systems cannot tackle: the end-to-end neural frameworks offer the promise of scalability and language-independence, together with the ability to track the dialogue state and then mapping between states and dialogue actions in a way not possible with conventional systems. On the other hand, neural systems bring about new challenges: they tend to output dull and generic responses; they lack a consistent or a coherent persona; they are usually optimized through single-turn conversations and are incapable of handling the long-term success of a conversation; and they are not able to take the advantage of the interactions with humans. This dissertation attempts to tackle these challenges: Contributions are two-fold: (1) we address new challenges presented by neural network models in open-domain dialogue generation systems; (2) we develop interactive question-answering dialogue systems by (a) giving the agent the ability to ask questions and (b) training a conversation agent through interactions with humans in an online fashion, where a bot improves through communicating with humans and learning from the mistakes that it makes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一台机器与人沟通的能力一直与AI的普遍成功有关。这可以追溯到50年代初阿兰·图灵的划时代的工作，这提出了一个机器的智能可以通过如何，将本机，可以欺骗一个人相信该机器是通过对话谈话人进行测试。许多系统学习生成规则从一组上的手工编码的规则或模板顶部撰写规则或标签最小，从而既昂贵又难以扩展到开放域场景。最近，神经网络模型的出现的可能性，解决了许多在对话学习的问题，早期的系统无法应对：终端到终端的神经框架提供的可扩展性和语言独立性的承诺，与跟踪的能力一起对话状态，并与传统的系统不可能的方式国和对话的行动之间的映射，然后。在另一方面，神经系统带来了新的挑战：他们往往输出沉闷和通用的应对措施;他们缺乏一致或连贯的角色;他们通常是通过单圈的谈话进行了优化，不能处理的对话的长期成功;他们不能够采取互动的优势与人类。本文试图解决这些挑战：捐款有两方面：（1）我们解决在开放领域对话发电系统的神经网络模型提出了新的挑战; （2）我们开发给代理提问以在线的方式，其中一个机器人通过与人类和学习交流提高训练的对话代理通过互动与人类的能力，和（b）通过互动答疑对话系统（一）从它使错误。</font>
</div>


<hr>
<div id="paper6"> <b>6. Pseudo-Bidirectional Decoding for Local Sequence Transduction</b>  <a href="https://arxiv.org/pdf/2001.11694" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangchunshu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Xu</a><br>
<font size="3">
Abstract: Local sequence transduction (LST) tasks are sequence transduction tasks where there exists massive overlapping between the source and target sequences, such as Grammatical Error Correction (GEC) and spell or OCR correction. Previous work generally tackles LST tasks with standard sequence-to-sequence (seq2seq) models that generate output tokens from left to right and suffer from the issue of unbalanced outputs. Motivated by the characteristic of LST tasks, in this paper, we propose a simple but versatile approach named Pseudo-Bidirectional Decoding (PBD) for LST tasks. PBD copies the corresponding representation of source tokens to the decoder as pseudo future context to enable the decoder to attends to its bi-directional context. In addition, the bidirectional decoding scheme and the characteristic of LST tasks motivate us to share the encoder and the decoder of seq2seq models. The proposed PBD approach provides right side context information for the decoder and models the inductive bias of LST tasks, reducing the number of parameters by half and providing good regularization effects. Experimental results on several benchmark datasets show that our approach consistently improves the performance of standard seq2seq models on LST tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本地序列转导（LST）任务是在存在源和目标序列，例如语法纠错（GEC）和拼写或OCR校正之间大量重叠序列转导的任务。以前的工作通常铲球与生成输出令牌由左到右，并从非平衡输出的问题遭受标准序列对序列（seq2seq）模型LST任务。通过LST任务特性的启发，在本文中，我们提出了一个简单而通用的命名伪双向解码（PBD）为LST任务的方法。 PBD拷贝源的相应表示令牌给解码器作为伪未来上下文，以使解码器能够照顾到其双向上下文。此外，双向解码方案和LST任务的特点促使我们分享编码器和seq2seq车型的解码器。所提出的PBD方法提供了解码器和模型的LST任务归纳偏置，减少一半的参数的数量和提供良好的正规化效果右侧的上下文信息。在几个基准数据集的实验结果表明，该方法可以始终如一提高标准seq2seq车型上LST任务的性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. Self-Adversarial Learning with Comparative Discrimination for Text  Generation</b>  <a href="https://arxiv.org/pdf/2001.11691" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangchunshu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality between a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This self-improvement reward mechanism allows the model to receive credits more easily and avoid collapsing towards the limited number of real samples, which not only helps alleviate the reward sparsity issue but also reduces the risk of mode collapse. Experiments on text generation benchmark datasets show that our proposed approach substantially improves both the quality and the diversity, and yields more stable performance compared to the previous GANs for text generation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：传统的生成性对抗性网络（甘斯）的文本生成往往具有影响的质量和产生的样本的多样性奖励稀疏和模式崩溃的问题。为了解决这个问题，我们提出了一个新的自我对抗学习（SAL）为提高文本生成甘斯的表现模式。与此相反使用二元分类器作为它的鉴别器以预测样品是否是真实的还是生成的标准甘斯，SAL采用比较鉴别器，其是用于在一对样品之间比较所述文本质量成对分类器。在训练期间，如果其目前产生的句子被认为比其以前生成的样本更好SAL奖励发电机。这种自强不息的奖励机制，使模型更容易获得信贷，并避免对数量有限的实际样品，这不仅有助于缓解奖励稀疏问题倒塌，但也降低了模式崩溃的风险。在文本生成基准数据集的实验表明，该方法显着提高的质量和多样性，并产生更稳定的性能相比之前的甘斯的文本生成。</font>
</div>


<hr>
<div id="paper8"> <b>8. Augmenting Visual Question Answering with Semantic Frame Information in  a Multitask Learning Approach</b>  <a href="https://arxiv.org/pdf/2001.11673" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Alizadeh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehrdad Alizadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Di+Eugenio%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Barbara Di Eugenio</a><br>
<font size="3">
Abstract: Visual Question Answering (VQA) concerns providing answers to Natural Language questions about images. Several deep neural network approaches have been proposed to model the task in an end-to-end fashion. Whereas the task is grounded in visual processing, if the question focuses on events described by verbs, the language understanding component becomes crucial. Our hypothesis is that models should be aware of verb semantics, as expressed via semantic role labels, argument types, and/or frame elements. Unfortunately, no VQA dataset exists that includes verb semantic information. Our first contribution is a new VQA dataset (imSituVQA) that we built by taking advantage of the imSitu annotations. The imSitu dataset consists of images manually labeled with semantic frame elements, mostly taken from FrameNet. Second, we propose a multitask CNN-LSTM VQA model that learns to classify the answers as well as the semantic frame elements. Our experiments show that semantic frame element classification helps the VQA system avoid inconsistent responses and improves performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉答疑（VQA）的担忧提供了回答有关图像自然语言问题。一些深层神经网络方法被提出来的任务结束到终端的时装模特。尽管任务是在视觉处理接地，如果问题集中在事件描述由动词，理解组件的语言变得至关重要。我们的假设是模型应该知道动词语义的，如通过语义角色标签，参数类型，和/或框架元素表示。不幸的是，没有VQA数据集存在，包括动词语义信息。我们的第一个贡献是一个新的VQA的数据集（imSituVQA），我们通过采取imSitu注释的优势构建。数据集由具有语义框架元件手动标记的图像的imSitu，大多是从框架网络服用。其次，我们提出了一个多任务CNN-LSTM VQA模型学会的答案，以及语义框架内容进行分类。我们的实验表明，语义框架元素的分类有助于VQA系统避免不一致的响应和提高性能。</font>
</div>


<hr>
<div id="paper9"> <b>9. Enhancement of Short Text Clustering by Iterative Classification</b>  <a href="https://arxiv.org/pdf/2001.11631" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rakib%2C+M+R+H" target="_blank" rel="noopener" style="color:#0000EE;">Md Rashadul Hasan Rakib</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeh%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Norbert Zeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jankowska%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Magdalena Jankowska</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milios%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evangelos Milios</a><br>
<font size="3">
Abstract: Short text clustering is a challenging task due to the lack of signal contained in such short texts. In this work, we propose iterative classification as a method to b o ost the clustering quality (e.g., accuracy) of short texts. Given a clustering of short texts obtained using an arbitrary clustering algorithm, iterative classification applies outlier removal to obtain outlier-free clusters. Then it trains a classification algorithm using the non-outliers based on their cluster distributions. Using the trained classification model, iterative classification reclassifies the outliers to obtain a new set of clusters. By repeating this several times, we obtain a much improved clustering of texts. Our experimental results show that the proposed clustering enhancement method not only improves the clustering quality of different clustering methods (e.g., k-means, k-means--, and hierarchical clustering) but also outperforms the state-of-the-art short text clustering methods on several short text datasets by a statistically significant margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：短文本聚类是一个具有挑战性的任务，由于包含在如此短的文字缺乏的信号。在这项工作中，我们提出的迭代分类为B○OST短文本的聚类质量（例如，准确度）的方法。鉴于使用任意的聚类算法获得的短文本的聚类，分类迭代适用异常值去除以获得离群-空闲簇。然后训练使用基于其集群分布的非离群的分类算法。利用训练的分类模型，迭代分类重新分类离群获得一组新的集群。通过重复几次，我们得到的文本大大改善群集。我们的实验结果表明，所提出的聚类增强方法不仅提高了的不同的聚类方法（例如，k均值，K-指：，和层次聚类）聚类质量也优于状态的最先进的短文本有统计显著保证金聚类在几个简短的文本数据集的方法。</font>
</div>


<hr>
<div id="paper10"> <b>10. Unwanted Advances in Higher Education: Uncovering Sexual Harassment  Experiences in Academia with Text Mining</b>  <a href="https://arxiv.org/pdf/2001.11552" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/physics?searchtype=author&query=Karami%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amir Karami</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=White%2C+C+N" target="_blank" rel="noopener" style="color:#0000EE;">Cynthia Nicole White</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Ford%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kayla Ford</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Swan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suzanne Swan</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Spinel%2C+M+Y" target="_blank" rel="noopener" style="color:#0000EE;">Melek Yildiz Spinel</a><br>
<font size="3">
Abstract: Sexual harassment in academia is often a hidden problem because victims are usually reluctant to report their experiences. Recently, a web survey was developed to provide an opportunity to share thousands of sexual harassment experiences in academia. Using an efficient approach, this study collected and investigated more than 2,000 sexual harassment experiences to better understand these unwanted advances in higher education. This paper utilized text mining to disclose hidden topics and explore their weight across three variables: harasser gender, institution type, and victim's field of study. We mapped the topics on five themes drawn from the sexual harassment literature and found that more than 50% of the topics were assigned to the unwanted sexual attention theme. Fourteen percent of the topics were in the gender harassment theme, in which insulting, sexist, or degrading comments or behavior was directed towards women. Five percent of the topics involved sexual coercion (a benefit is offered in exchange for sexual favors), 5% involved sex discrimination, and 7% of the topics discussed retaliation against the victim for reporting the harassment, or for simply not complying with the harasser. Findings highlight the power differential between faculty and students, and the toll on students when professors abuse their power. While some topics did differ based on type of institution, there were no differences between the topics based on gender of harasser or field of study. This research can be beneficial to researchers in further investigation of this paper's dataset, and to policymakers in improving existing policies to create a safe and supportive environment in academia. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在学术界性骚扰往往是一个隐藏的问题，因为受害者往往不愿意报告自己的经历。最近，网络调查的开发提供共享成千上万的性骚扰经历学术界的机会。使用一种有效的方法，这项研究收集和调查2000余和性骚扰的经验，以更好地了解高等教育这些不必要的进步。本文利用文本挖掘透露隐藏的主题和跨越三个变量探索自己的体重：骚扰者性别，机构类型和研究的受害人的领域。我们映射从性骚扰文献中提取，发现的主题超过50％被分配到不必要的性关注主题五个主题的主题。的主题十四％的人在性别骚扰主题，在这种侮辱，性别歧视，或侮辱性的评论或行为针对妇女。的主题百分之五参与性胁迫（一个好处是提供以换取性方面的好处），5％涉及性别歧视，以及主题7％讨论对受害者报复举报骚扰，或者干脆不与骚扰符合。发现突出的教师和学生，以及学生的收费之间的功率差时，教授滥用职权。虽然有些题目确实有所不同根据类型的机构，有基于研究的骚扰或领域的性别主题之间没有差异。这项研究可以在本文的数据集的进一步调查研究有利，对政策制定者改善现有的政策，创造学术界安全和支持的环境。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>pip 安装提示空间不足</title>
    <url>/2020/01/21/pip-%E5%AE%89%E8%A3%85%E6%8F%90%E7%A4%BA%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3/</url>
    <content><![CDATA[<p>pip 安装提示空间不足</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Could not install packages due to an EnvironmentError: [Errno 28] No space left on device</span><br></pre></td></tr></table></figure><p>这是服务器上的/tmp空间不足，可以在自己的根目录下简历~/tmp代替 /tmp</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir ~/tmp</span><br><span class="line">export TMPDIR=$HOME/tmp</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-01-20</title>
    <url>/2020/01/20/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-01-20/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Unsupervised Learning of Camera Pose with Compositional Re-estimation <a href="https://arxiv.org/pdf/2001.06479" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Combining PRNU and noiseprint for robust and efficient device source  identification <a href="https://arxiv.org/pdf/2001.06440" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> TailorGAN: Making User-Defined Fashion Designs <a href="https://arxiv.org/pdf/2001.06427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Subjective Annotation for a Frame Interpolation Benchmark using Artifact  Amplification <a href="https://arxiv.org/pdf/2001.06409" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> GraphBGS: Background Subtraction via Recovery of Graph Signals <a href="https://arxiv.org/pdf/2001.06404" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Latency-Aware Differentiable Neural Architecture Search <a href="https://arxiv.org/pdf/2001.06392" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> BigEarthNet Deep Learning Models with A New Class-Nomenclature for  Remote Sensing Image Understanding <a href="https://arxiv.org/pdf/2001.06372" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Efficient Facial Feature Learning with Wide Ensemble-based Convolutional  Neural Networks <a href="https://arxiv.org/pdf/2001.06338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Vision Meets Drones: Past, Present and Future <a href="https://arxiv.org/pdf/2001.06303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Predicting the Physical Dynamics of Unseen 3D Objects <a href="https://arxiv.org/pdf/2001.06291" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Review: deep learning on 3D point clouds <a href="https://arxiv.org/pdf/2001.06280" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Compounding the Performance Improvements of Assembled Techniques in a  Convolutional Neural Network <a href="https://arxiv.org/pdf/2001.06268" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On <a href="https://arxiv.org/pdf/2001.06265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Two-Phase Object-Based Deep Learning for Multi-temporal SAR Image Change  Detection <a href="https://arxiv.org/pdf/2001.06252" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Registration made easy -- standalone orthopedic navigation with HoloLens <a href="https://arxiv.org/pdf/2001.06209" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> FPCR-Net: Feature Pyramidal Correlation and Residual Reconstruction for  Semi-supervised Optical Flow Estimation <a href="https://arxiv.org/pdf/2001.06171" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Interpreting Galaxy Deblender GAN from the Discriminator's Perspective <a href="https://arxiv.org/pdf/2001.06151" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Learning to Augment Expressions for Few-shot Fine-grained Facial  Expression Recognition <a href="https://arxiv.org/pdf/2001.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Spatio-Temporal Ranked-Attention Networks for Video Captioning <a href="https://arxiv.org/pdf/2001.06127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Automatic Discovery of Political Meme Genres with Diverse Appearances <a href="https://arxiv.org/pdf/2001.06122" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> On- Device Information Extraction from Screenshots in form of tags <a href="https://arxiv.org/pdf/2001.06094" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Tracking of Micro Unmanned Aerial Vehicles: A Comparative Study <a href="https://arxiv.org/pdf/2001.06066" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Increasing the robustness of DNNs against image corruptions by playing  the Game of Noise <a href="https://arxiv.org/pdf/2001.06057" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Modality-Balanced Models for Visual Dialogue <a href="https://arxiv.org/pdf/2001.06354" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Tethered Aerial Visual Assistance <a href="https://arxiv.org/pdf/2001.06347" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> DeepSUM++: Non-local Deep Neural Network for Super-Resolution of  Unregistered Multitemporal Images <a href="https://arxiv.org/pdf/2001.06342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Detection Method Based on Automatic Visual Shape Clustering for  Pin-Missing Defect in Transmission Lines <a href="https://arxiv.org/pdf/2001.06236" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Sideways: Depth-Parallel Training of Video Models <a href="https://arxiv.org/pdf/2001.06232" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> FedVision: An Online Visual Object Detection Platform Powered by  Federated Learning <a href="https://arxiv.org/pdf/2001.06202" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless  Approach <a href="https://arxiv.org/pdf/2001.06175" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> An adversarial learning framework for preserving users' anonymity in  face-based emotion recognition <a href="https://arxiv.org/pdf/2001.06103" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Code-Bridged Classifier (CBC): A Low or Negative Overhead Defense for  Making a CNN Classifier Robust Against Adversarial Attacks <a href="https://arxiv.org/pdf/2001.06099" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Curriculum Labeling: Self-paced Pseudo-Labeling for Semi-Supervised  Learning <a href="https://arxiv.org/pdf/2001.06001" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Unsupervised Learning of Camera Pose with Compositional Re-estimation</b>  <a href="https://arxiv.org/pdf/2001.06479" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nabavi%2C+S+S" target="_blank" rel="noopener" style="color:#0000EE;">Seyed Shahabeddin Nabavi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hosseinzadeh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehrdad Hosseinzadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fahimi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ramin Fahimi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Wang</a><br>
<font size="3">
Abstract: We consider the problem of unsupervised camera pose estimation. Given an input video sequence, our goal is to estimate the camera pose (i.e. the camera motion) between consecutive frames. Traditionally, this problem is tackled by placing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We propose an alternative approach that utilizes a compositional re-estimation process for camera pose estimation. Given an input, we first estimate a depth map. Our method then iteratively estimates the camera motion based on the estimated depth map. Our approach significantly improves the predicted camera motion both quantitatively and visually. Furthermore, the re-estimation resolves the problem of out-of-boundaries pixels in a novel and simple way. Another advantage of our approach is that it is adaptable to other camera pose estimation approaches. Experimental analysis on KITTI benchmark dataset demonstrates that our method outperforms existing state-of-the-art approaches in unsupervised camera ego-motion estimation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们认为监督的相机姿态估计的问题。给定的输入视频序列，我们的目标是估计连续帧之间的摄像机姿态（即，照相机运动）。传统上，这个问题是通过将严格的约束的转化载体或通过一个复杂的管道结合光流解决。我们建议，利用相机姿势估计的成分重新估计过程的替代方法。给定一个输入，我们首先估计深度图。然后，我们的迭代算法估计基于估计的深度地图上的摄像机运动。我们的方法在数量上和视觉上显著提高了预测的摄像机运动。此外，重新估计解决了一种新颖和简单的方式外的边界像素的问题。我们的方法的另一个优点是，它是适用于其他相机姿态估计方法。上KITTI基准数据集试验分析表明，我们现有的最先进的国家的方法优于在无监督照相机自运动估计方法。</font>
</div>


<hr>
<div id="paper2"> <b>2. Combining PRNU and noiseprint for robust and efficient device source  identification</b>  <a href="https://arxiv.org/pdf/2001.06440" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cozzolino%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Cozzolino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marra%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Francesco Marra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gragnaniello%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diego Gragnaniello</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poggi%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giovanni Poggi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Verdoliva%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luisa Verdoliva</a><br>
<font size="3">
Abstract: PRNU-based image processing is a key asset in digital multimedia forensics. It allows for reliable device identification and effective detection and localization of image forgeries, in very general conditions. However, performance impairs significantly in challenging conditions involving low quality and quantity of data. These include working on compressed and cropped images, or estimating the camera PRNU pattern based on only a few images. To boost the performance of PRNU-based analyses in such conditions we propose to leverage the image noiseprint, a recently proposed camera-model fingerprint that has proved effective for several forensic tasks. Numerical experiments on datasets widely used for source identification prove that the proposed method ensures a significant performance improvement in a wide range of challenging situations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于PRNU图像处理是数字多媒体取证的重要资产。它允许可靠的装置识别和有效的检测和图像伪造的定位，在很一般的条件。然而，性能也妨碍显著在挑战包括低质量和数据量的条件。这些包括工作压缩和裁切图像，或估计基于只有少数图像中的相机PRNU图案。为了提高在这样的条件下基于PRNU-分析的性能，我们提出了利用图像noiseprint，已被证明有效的几个法医任务的最近提出的相机型号的指纹。上的数据集广泛用于源识别数值实验证明，该方法确保在广泛的挑战的情况一显著性能改进。</font>
</div>


<hr>
<div id="paper3"> <b>3. TailorGAN: Making User-Defined Fashion Designs</b>  <a href="https://arxiv.org/pdf/2001.06427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lele Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guo Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheng-Haw Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=King%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erh-Kan King</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kuan-Ting Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hsieh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shao-Hang Hsieh</a><br>
<font size="3">
Abstract: Attribute editing has become an important and emerging topic of computer vision. In this paper, we consider a task: given a reference garment image A and another image B with target attribute (collar/sleeve), generate a photo-realistic image which combines the texture from reference A and the new attribute from reference B. The highly convoluted attributes and the lack of paired data are the main challenges to the task. To overcome those limitations, we propose a novel self-supervised model to synthesize garment images with disentangled attributes (e.g., collar and sleeves) without paired data. Our method consists of a reconstruction learning step and an adversarial learning step. The model learns texture and location information through reconstruction learning. And, the model's capability is generalized to achieve single-attribute manipulation by adversarial learning. Meanwhile, we compose a new dataset, named GarmentSet, with annotation of landmarks of collars and sleeves on clean garment images. Extensive experiments on this dataset and real-world samples demonstrate that our method can synthesize much better results than the state-of-the-art methods in both quantitative and qualitative comparisons. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：属性编辑已经成为计算机视觉的一个重要和新兴的话题。在本文中，我们考虑一个任务：给定一个参考服装图像A和与目标属性（领/套筒）另一图像B，生成结合了从参考点A的质地和从参考B的新的属性的照片般逼真的图像高度令人费解的属性和缺乏配对数据是任务的主要挑战。为了克服这些限制，我们提出了一种新型的自监督模型以合成服装图像与解缠结的属性（例如，领子和袖子），而不配对数据。我们的方法包括一个重建学习步骤和敌对学习步骤的。该模型通过学习学习重建质地和位置信息。而且，该模型的能力是广义的对抗学习，实现单属性操作。同时，我们组成一个新的数据集，名为GarmentSet，用干净的服装图像领子和袖子的地标标注。在此数据集和真实世界的样本大量的实验表明，我们的方法可以合成比国家的最先进的方法，定量和定性的比较更好的结果。</font>
</div>


<hr>
<div id="paper4"> <b>4. Subjective Annotation for a Frame Interpolation Benchmark using Artifact  Amplification</b>  <a href="https://arxiv.org/pdf/2001.06409" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Men%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Men</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hosu%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vlad Hosu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanhe Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bruhn%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrés Bruhn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saupe%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dietmar Saupe</a><br>
<font size="3">
Abstract: Current benchmarks for optical flow algorithms evaluate the estimation either directly by comparing the predicted flow fields with the ground truth or indirectly by using the predicted flow fields for frame interpolation and then comparing the interpolated frames with the actual frames. In the latter case, objective quality measures such as the mean squared error are typically employed. However, it is well known that for image quality assessment, the actual quality experienced by the user cannot be fully deduced from such simple measures. Hence, we conducted a subjective quality assessment crowdscouring study for the interpolated frames provided by one of the optical flow benchmarks, the Middlebury benchmark. It contains interpolated frames from 155 methods applied to each of 8 contents. We collected forced choice paired comparisons between interpolated images and corresponding ground truth. To increase the sensitivity of observers when judging minute difference in paired comparisons we introduced a new method to the field of full-reference quality assessment, called artifact amplification. From the crowdsourcing data we reconstructed absolute quality scale values according to Thurstone's model. As a result, we obtained a re-ranking of the 155 participating algorithms w.r.t. the visual quality of the interpolated frames. This re-ranking not only shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks, the results also provide the ground truth for designing novel image quality assessment (IQA) methods dedicated to perceptual quality of interpolated images. As a first step, we proposed such a new full-reference method, called WAE-IQA. By weighing the local differences between an interpolated image and its ground truth WAE-IQA performed slightly better than the currently best FR-IQA approach from the literature. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用于光学流算法电流基准通过与地面实况地或间接地通过使用所预测的流场为帧内插，然后比较实际帧中的内插帧进行比较的预测的流场评价了估计直接。在后者的情况下，客观质量的措施，如均方误差，通常采用。但是，众所周知，对于图像质量评价，用户体验到的实际质量不能完全从这些简单的措施推出。因此，我们进行了由所述光流基准之一，所述明德基准提供的内插帧主观质量评估crowdscouring研究。它包含从施加到每个8项内容155点的方法的内插帧。我们收集了强制插入图片和相应的地面实况之间选择配对比较。为了增加观察员的灵敏度，当在配对比较判断分差，我们引入了一个新的方法来全参考质量评估领域，被称为神器放大。从众包数据，我们根据瑟斯顿模型重建质量绝对刻度值。其结果是，我们获得了155种参与算法的重新排名w.r.t.内插帧的视觉质量。这个重新排序不仅示出了视觉质量评估作为另一个评价度量光流和帧插值基准的必要性，该结果也提供了设计新的图像质量评价地面实况（IQA）的方法专用于内插图像的感知质量。作为第一步，我们提出了这样一个新的全参考方法，称为WAE-IQA。通过称重插入图像和地面实况WAE-IQA之间的局部差异不是从文献中目前最好的FR-IQA方法表现稍好。</font>
</div>


<hr>
<div id="paper5"> <b>5. GraphBGS: Background Subtraction via Recovery of Graph Signals</b>  <a href="https://arxiv.org/pdf/2001.06404" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Giraldo%2C+J+H" target="_blank" rel="noopener" style="color:#0000EE;">Jhony H. Giraldo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bouwmans%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thierry Bouwmans</a><br>
<font size="3">
Abstract: Graph-based algorithms have been successful approaching the problems of unsupervised and semi-supervised learning. Recently, the theory of graph signal processing and semi-supervised learning have been combined leading to new developments and insights in the field of machine learning. In this paper, concepts of recovery of graph signals and semi-supervised learning are introduced in the problem of background subtraction. We propose a new algorithm named GraphBGS, this method uses a Mask R-CNN for instances segmentation; temporal median filter for background initialization; motion, texture, color, and structural features for representing the nodes of a graph; k-nearest neighbors for the construction of the graph; and finally a semi-supervised method inspired from the theory of recovery of graph signals to solve the problem of background subtraction. The method is evaluated on the publicly available change detection, and scene background initialization databases. Experimental results show that GraphBGS outperforms unsupervised background subtraction algorithms in some challenges of the change detection dataset. And most significantly, this method outperforms generative adversarial networks in unseen videos in some sequences of the scene background initialization database. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于图的算法已经成功逼近无监督和半监督学习的问题。近日，图形信号处理和半监督学习的理论已被合并导致新的进展和见解，在机器学习领域。在本文中，图形信号及半监督学习的恢复的概念背景减除的问题进行了介绍。我们提出了一种新的算法命名GraphBGS，这种方法使用面膜R-CNN的情况下，分割;时间中值滤波器，用于背景初始化;运动，纹理，颜色和用于表示图中的节点的结构特征; k最近的图的构造的邻居;最后一个半监督方法从图信号的恢复的理论启发解决背景减除的问题。该方法在可公开获得的变化检测评价，并现场后台初始化数据库。实验结果表明，在变化检测数据集的一些挑战GraphBGS性能优于无人监督的背景减除算法。而最显著，这种方法优于在场景后台初始化数据库的一些序列看不见的视频生成对抗性的网络。</font>
</div>


<hr>
<div id="paper6"> <b>6. Latency-Aware Differentiable Neural Architecture Search</b>  <a href="https://arxiv.org/pdf/2001.06392" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhui Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingxi Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaopeng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bowen Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongkai Xiong</a><br>
<font size="3">
Abstract: Differentiable neural architecture search methods became popular in automated machine learning, mainly due to their low search costs and flexibility in designing the search space. However, these methods suffer the difficulty in optimizing network, so that the searched network is often unfriendly to hardware. This paper deals with this problem by adding a differentiable latency loss term into optimization, so that the search process can tradeoff between accuracy and latency with a balancing coefficient. The core of latency prediction is to encode each network architecture and feed it into a multi-layer regressor, with the training data being collected from randomly sampling a number of architectures and evaluating them on the hardware. We evaluate our approach on NVIDIA Tesla-P100 GPUs. With 100K sampled architectures (requiring a few hours), the latency prediction module arrives at a relative error of lower than 10\%. Equipped with this module, the search method can reduce the latency by 20% meanwhile preserving the accuracy. Our approach also enjoys the ability of being transplanted to a wide range of hardware platforms with very few efforts, or being used to optimizing other non-differentiable factors such as power consumption. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：微的神经结构的搜索方法成为自动化机器学习流行，主要是由于其较低的搜寻成本和设计的搜索空间的灵活性。然而，这些方法在遭受网络优化的难度，使网络搜索往往是不友好的硬件。这与这个问题论文涉及通过添加微延迟损失项为优化，使之与平衡系数精度和延迟之间的搜索过程可以权衡。延迟预测的核心是编码每个网络结构和它送入多层回归，与从随机抽样的数架构和硬件评估他们被收集训练数据。我们评估我们对NVIDIA的Tesla-P100 GPU的方法。用100K采样架构（需要几个小时），等待时间预测模块到达的低于10 \％的相对误差。配备该模块，搜索方法可以通过20％的同时保持准确度降低延迟。我们的方法也享有被移植到了广泛的硬件平台用很少的努力，或者被用来优化其它非微因素，例如功耗的能力。</font>
</div>


<hr>
<div id="paper7"> <b>7. BigEarthNet Deep Learning Models with A New Class-Nomenclature for  Remote Sensing Image Understanding</b>  <a href="https://arxiv.org/pdf/2001.06372" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sumbul%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gencer Sumbul</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Kang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kreuziger%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tristan Kreuziger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marcelino%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Filipe Marcelino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Costa%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hugo Costa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Benevides%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Benevides</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caetano%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mario Caetano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Demir%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Begüm Demir</a><br>
<font size="3">
Abstract: Success of deep neural networks in the framework of remote sensing (RS) image analysis depends on the availability of a high number of annotated images. BigEarthNet is a new large-scale Sentinel-2 benchmark archive that has been recently introduced in RS to advance deep learning (DL) studies. Each image patch in BigEarthNet is annotated with multi-labels provided by the CORINE Land Cover (CLC) map of 2018 based on its most thematic detailed Level-3 class nomenclature. BigEarthNet has enabled data-hungry DL algorithms to reach high performance in the context of multi-label RS image retrieval and classification. However, initial research demonstrates that some CLC classes are challenging to be accurately described by considering only (single-date) Sentinel-2 images. To further increase the effectiveness of BigEarthNet, in this paper we introduce an alternative class-nomenclature to allow DL models for better learning and describing the complex spatial and spectral information content of the Sentinel-2 images. This is achieved by interpreting and arranging the CLC Level-3 nomenclature based on the properties of Sentinel-2 images in a new nomenclature of 19 classes. Then, the new class-nomenclature of BigEarthNet is used within state-of-the-art DL models (namely VGG model at the depth of 16 and 19 layers [VGG16 and VGG19] and ResNet model at the depth of 50, 101 and 152 layers [ResNet50, ResNet101, ResNet152] as well as K-Branch CNN model) in the context of multi-label classification. Experimental results show that the models trained from scratch on BigEarthNet outperform those pre-trained on ImageNet, especially in relation to some complex classes including agriculture and other vegetated and natural environments. All DL models are made publicly available, offering an important resource to guide future progress on content based image retrieval and scene classification problems in RS. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：遥感（RS）图像分析的框架深神经网络的成功取决于大量的注释的图像的可用性。 BigEarthNet是已在RS最近推出深处前进学习（DL）研究提供了新的大型哨兵-2基准存档。在BigEarthNet每个图像补丁标注有基于其最详细的专题级别3级命名的2018年CORINE土地覆盖（CLC）地图提供多标签。 BigEarthNet已使大量数据的DL算法，以达到多标签遥感影像检索和分类的情况下的高性能。然而，最初的研究表明，一些CLC类是具有挑战性的通过仅考虑（单日期）被精确地描述的Sentinel-2的图像。为了进一步提高BigEarthNet的有效性，本文介绍一种替代类的术语来允许DL模型更好的学习和描述哨兵2图像的复杂的空间和光谱信息的内容。这是通过解释和布置基于哨兵-2图像的在19类的新命名法的属性CLC 3级命名法来实现的。然后，BigEarthNet的新的类命名法是国家的最先进的DL模型（即VGG模型内以16层19的层[VGG16和VGG19]和RESNET模型的深度使用在50，101和152的深度层[ResNet50，ResNet101，ResNet152]以及K-科CNN模型）在多标签分类的上下文。实验结果表明，从头开始培训了BigEarthNet跑赢车型的预先训练上ImageNet，特别是涉及到一些复杂的类，包括农业和其他植被和自然环境。所有DL型号都公之于众，提供指导在RS基于内容的图像检索及场景分类问题未来发展的重要资源。</font>
</div>


<hr>
<div id="paper8"> <b>8. Efficient Facial Feature Learning with Wide Ensemble-based Convolutional  Neural Networks</b>  <a href="https://arxiv.org/pdf/2001.06338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Siqueira%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Henrique Siqueira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Magg%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sven Magg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wermter%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Wermter</a><br>
<font size="3">
Abstract: Ensemble methods, traditionally built with independently trained de-correlated models, have proven to be efficient methods for reducing the remaining residual generalization error, which results in robust and accurate methods for real-world applications. In the context of deep learning, however, training an ensemble of deep networks is costly and generates high redundancy which is inefficient. In this paper, we present experiments on Ensembles with Shared Representations (ESRs) based on convolutional networks to demonstrate, quantitatively and qualitatively, their data processing efficiency and scalability to large-scale datasets of facial expressions. We show that redundancy and computational load can be dramatically reduced by varying the branching level of the ESR without loss of diversity and generalization power, which are both important for ensemble performance. Experiments on large-scale datasets suggest that ESRs reduce the remaining residual generalization error on the AffectNet and FER+ datasets, reach human-level performance, and outperform state-of-the-art methods on facial expression recognition in the wild using emotion and affect concepts. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：集成方法，传统上与​​独立的培训去相关模型构建，已被证明是减少残留的剩余泛化误差，有效的方法，这导致对现实世界的应用健全和准确的方法。在深学习的情况下，然而，培养深网络的集合是昂贵的，并且产生高冗余这是低效的。在本文中，我们对合奏基于卷积网络证明，定量和定性地共享交涉（的ESR）本实验中，它们的数据处理效率和可扩展性的面部表情的大规模数据集。我们发现可以通过改变ESR的无分集和概括断电分支水平，这既是对合奏表演重要的急剧减少了冗余和计算负载。在大型数据集的实验表明，的ESR减少对AffectNet和FER +数据集，达到人类水平的性能，以及使用情感上的野生面部表情识别跑赢大市的国家的最先进的方法，直接影响概念的剩余的残留泛化的错误。</font>
</div>


<hr>
<div id="paper9"> <b>9. Vision Meets Drones: Past, Present and Future</b>  <a href="https://arxiv.org/pdf/2001.06303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Longyin Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dawei Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bian%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Bian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghua Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haibin Ling</a><br>
<font size="3">
Abstract: Drones, or general UAVs, equipped with cameras have been fast deployed with a wide range of applications, including agriculture, aerial photography, fast delivery, and surveillance. Consequently, automatic understanding of visual data collected from drones becomes highly demanding, bringing computer vision and drones more and more closely. To promote and track the developments of object detection and tracking algorithms, we have organized two challenge workshops in conjunction with European Conference on Computer Vision (ECCV) 2018, and IEEE International Conference on Computer Vision (ICCV) 2019, attracting more than 100 teams around the world. We provide a large-scale drone captured dataset, VisDrone, which includes four tracks, i.e., (1) image object detection, (2) video object detection, (3) single object tracking, and (4) multi-object tracking. This paper first presents a thorough review of object detection and tracking datasets and benchmarks, and discuss the challenges of collecting large-scale drone-based object detection and tracking datasets with fully manual annotations. After that, we describe our VisDrone dataset, which is captured over various urban/suburban areas of $14$ different cities across China from North to South. Being the largest such dataset ever published, VisDrone enables extensive evaluation and investigation of visual analysis algorithms on the drone platform. We provide a detailed analysis of the current state of the field of large-scale object detection and tracking on drones, and conclude the challenge as well as propose future directions and improvements. We expect the benchmark largely boost the research and development in video analysis on drone platforms. All the datasets and experimental results can be downloaded from the website: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无人机或者一般的无人机，配备摄像头已经迅速部署具有广泛的应用，包括农业，航空摄影，交货快，和监视。因此，从无人机采集的视频数据的自动理解变得极高，将计算机视觉和无人驾驶飞机越来越紧密。为了促进和跟踪目标检测与跟踪算法的发展，我们已经组织了一起2次挑战研讨会，欧洲会议计算机视觉（ECCV）2018，以及计算机视觉（ICCV）2019 IEEE国际会议，吸引了100多个团队世界。我们提供了一个大型雄蜂捕获数据集，VisDrone，它包括四个磁道，即，（1）图像对象检测，（2）视频对象检测，（3）单目标跟踪，和（4）的多目标跟踪。本文首先介绍目标检测与跟踪数据集和基准进行彻底审查，并讨论收集大型无人机基于体检测，并与全手动注释跟踪数据集的挑战。在那之后，我们描述了我们VisDrone数据集，这是超过$ $ 14在中国不同的城市，从南到北各个城市/郊区抓获。作为最大的此类数据集出版过的，VisDrone使广泛的评估和无人机平台上的视觉分析算法调查。我们提供大型物体检测和跟踪在无人机领域的现状进行了详细分析，并得出结论以及提出未来的发展方向和改进的挑战。我们预计恒生很大程度上提高对无人机平台在视频分析的研究和开发。此HTTPS URL：所有数据集和实验结果可以从网站上下载。</font>
</div>


<hr>
<div id="paper10"> <b>10. Predicting the Physical Dynamics of Unseen 3D Objects</b>  <a href="https://arxiv.org/pdf/2001.06291" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rempe%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davis Rempe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sridhar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Srinath Sridhar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">He Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guibas%2C+L+J" target="_blank" rel="noopener" style="color:#0000EE;">Leonidas J. Guibas</a><br>
<font size="3">
Abstract: Machines that can predict the effect of physical interactions on the dynamics of previously unseen object instances are important for creating better robots and interactive virtual worlds. In this work, we focus on predicting the dynamics of 3D objects on a plane that have just been subjected to an impulsive force. In particular, we predict the changes in state - 3D position, rotation, velocities, and stability. Different from previous work, our approach can generalize dynamics predictions to object shapes and initial conditions that were unseen during training. Our method takes the 3D object's shape as a point cloud and its initial linear and angular velocities as input. We extract shape features and use a recurrent neural network to predict the full change in state at each time step. Our model can support training with data from both a physics engine or the real world. Experiments show that we can accurately predict the changes in state for unseen object geometries and initial conditions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器，可以预测在以前看不见的对象实例的动态物理相互作用的作用是创造更好的机器人和互动的虚拟世界重要。在这项工作中，我们侧重于预测对刚刚经受冲击力的平面3D对象的动态。特别是，我们预测状态的变化 - 三维位置，旋转，速度和稳定性。从以前的工作不同的是，我们的方法可以概括的动态预测到物体的形状和初始条件的培训过程中看不见。我们的方法利用该3D对象的形状为点云和它的初始线速度和角速度作为输入。我们提取形状特征和使用回归神经网络在每个时间步来预测状态充满变化。我们的模型可以支持从两个物理引擎或现实世界的数据训练。实验结果表明，我们可以精确地预测为看不见的对象的几何形状和初始条件状态中的变化。</font>
</div>


<hr>
<div id="paper11"> <b>11. Review: deep learning on 3D point clouds</b>  <a href="https://arxiv.org/pdf/2001.06280" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bello%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Saifullahi Aminu Bello</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shangshu Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheng Wang</a><br>
<font size="3">
Abstract: Point cloud is point sets defined in 3D metric space. Point cloud has become one of the most significant data format for 3D representation. Its gaining increased popularity as a result of increased availability of acquisition devices, such as LiDAR, as well as increased application in areas such as robotics, autonomous driving, augmented and virtual reality. Deep learning is now the most powerful tool for data processing in computer vision, becoming the most preferred technique for tasks such as classification, segmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, point cloud, on the other hand, is unstructured. The unstructuredness of point clouds makes use of deep learning for its processing directly very challenging. Earlier approaches overcome this challenge by preprocessing the point cloud into a structured grid format at the cost of increased computational cost or lost of depth information. Recently, however, many state-of-the-arts deep learning techniques that directly operate on point cloud are being developed. This paper contains a survey of the recent state-of-the-art deep learning techniques that mainly focused on point cloud data. We first briefly discussed the major challenges faced when using deep learning directly on point cloud, we also briefly discussed earlier approaches which overcome the challenges by preprocessing the point cloud into a structured grid. We then give the review of the various state-of-the-art deep learning approaches that directly process point cloud in its unstructured form. We introduced the popular 3D point cloud benchmark datasets. And we also further discussed the application of deep learning in popular 3D vision tasks including classification, segmentation and detection. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：点云是3D度量空间定义的点集。点云已成为3D表示最显著数据格式之一。它获得越来越多的受欢迎程度增加采集设备，如激光雷达的可用性，以及在诸如机器人，自动驾驶等领域加强应用，增强和虚拟现实的结果。现在深学习是数据在计算机视觉处理的最有力的工具，成为任务，如分类，细分和检测的最优选的技术。虽然深学习技术主要应用于数据与结构化网格，点云，在另一方面，是非结构化的。该unstructuredness点云的利用深度学习的其直接处理非常具有挑战性。早期的方法通过预处理点云成结构化的网格格式以增加计算成本的成本或丢失的深度信息克服这一挑战。然而，最近深学习直接对点云操作的技术的许多艺术国家的正在开发中。本文件包含了一个调查国家的最先进的深得知主要集中在点云数据的技术，最近的。我们首先简要讨论了使用深直接在点云学习时所面临的重大挑战，我们还简要讨论克服通过预处理点云成结构化网格的挑战，早期的方法。然后，我们给国家的最先进的各种深学习的复习方法直接处理点云中的非结构化的形式。我们引进了当前流行的三维点云标准数据集。我们还进一步讨论在流行的3D视觉任务，包括分类，分割和检测深度学习的应用。</font>
</div>


<hr>
<div id="paper12"> <b>12. Compounding the Performance Improvements of Assembled Techniques in a  Convolutional Neural Network</b>  <a href="https://arxiv.org/pdf/2001.06268" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jungkyu Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Won%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taeryun Won</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kiho Hong</a><br>
<font size="3">
Abstract: Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to a basic CNN model in combination can improve the accuracy and robustness of the model while minimizing the loss of throughput. For example, our proposed ResNet-50 shows an improvement in top-1 accuracy from 76.3% to 82.78%, and an mCE improvement from 76.0% to 48.9%, on the ImageNet ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. The resulting model significantly outperforms state-of-the-art models with similar accuracy in terms of mCE and inference throughput. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several open datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019, and the source code and trained models are available at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在图像分类最近的研究表明多种用于改善卷积神经网络（细胞神经网络）的表现技法。不过，对现有技术结合起来，创造一个实际的模型仍屡见不鲜。在这项研究中，我们进行了广泛的实验，以验证仔细组装这些技术并将它们应用到结合的基本模式CNN能提高模型的精确度和耐用性，同时最大限度地减少产量损失。例如，我们所提出的RESNET-50示出了在顶部-1精度的提高，从76.3％到82.78％，和从76.0％的MCE改善48.9％，对ImageNet ILSVRC2012验证集。有了这些改进，推理可以通过仅降低从536到312得到的模型显著优于状态的最先进的模型具有类似的精度在MCE和推理吞吐量方面。为了验证迁移学习，细粒分类和图像检索任务的性能改进上几个开放的数据集进行了测试，结果表明，提高骨干网络的性能提升传输学习表现显著。我们的方法在iFood比赛细粒度的视觉识别在CVPR 2019获得第一名，源代码和训练的模型可在此HTTPS URL</font>
</div>


<hr>
<div id="paper13"> <b>13. SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On</b>  <a href="https://arxiv.org/pdf/2001.06265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jandial%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Surgan Jandial</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chopra%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ayush Chopra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ayush%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kumar Ayush</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hemani%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mayur Hemani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhijeet Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krishnamurthy%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Balaji Krishnamurthy</a><br>
<font size="3">
Abstract: Image-based virtual try-on for fashion has gained considerable attention recently. The task requires trying on a clothing item on a target model image. An efficient framework for this is composed of two stages: (1) warping (transforming) the try-on cloth to align with the pose and shape of the target model, and (2) a texture transfer module to seamlessly integrate the warped try-on cloth onto the target model image. Existing methods suffer from artifacts and distortions in their try-on output. In this work, we present SieveNet, a framework for robust image-based virtual try-on. Firstly, we introduce a multi-stage coarse-to-fine warping network to better model fine-grained intricacies (while transforming the try-on cloth) and train it with a novel perceptual geometric matching loss. Next, we introduce a try-on cloth conditioned segmentation mask prior to improve the texture transfer network. Finally, we also introduce a dueling triplet loss strategy for training the texture translation network which further improves the quality of the generated try-on results. We present extensive qualitative and quantitative evaluations of each component of the proposed pipeline and show significant performance improvements against the current state-of-the-art method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于映像的虚拟试穿时装最近获得了相当大的关注。任务需要试穿的目标模型图像上的衣物。这种高效的框架由两个阶段组成：（1）翘曲（变换）的试穿布与目标模型的姿态和形状对齐，和（2）的纹理传送模块无缝集成翘曲试戴上布到目标模型图像。现有的方法从它们的试穿输出文物和扭曲痛苦。在这项工作中，我们提出SieveNet，对于稳健的基于图像的虚拟试穿的框架。首先，我们引入一个多级粗到细的翘曲网络，以更好地模型细粒度错综复杂（同时改造试穿布），并用新的知觉几何匹配损耗训练它。接下来，我们引入一个试穿改善质地传递网络之前布空调分割掩码。最后，我们还引进了决斗三重损失的策略训练纹理翻译网络，进一步提高了产生试穿结果的质量。我们提出了广泛的定性和建议的管道中各组分的定量评估，显示对当前国家的最先进的方法显著的性能改进。</font>
</div>


<hr>
<div id="paper14"> <b>14. Two-Phase Object-Based Deep Learning for Multi-temporal SAR Image Change  Detection</b>  <a href="https://arxiv.org/pdf/2001.06252" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinzheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guo Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ce Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Atkinson%2C+P+M" target="_blank" rel="noopener" style="color:#0000EE;">Peter M Atkinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoheng Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jian%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Jian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xichuan Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongming Li</a><br>
<font size="3">
Abstract: Change detection is one of the fundamental applications of synthetic aperture radar (SAR) images. However, speckle noise presented in SAR images has a much negative effect on change detection. In this research, a novel two-phase object-based deep learning approach is proposed for multi-temporal SAR image change detection. Compared with traditional methods, the proposed approach brings two main innovations. One is to classify all pixels into three categories rather than two categories: unchanged pixels, changed pixels caused by strong speckle (false changes), and changed pixels formed by real terrain variation (real changes). The other is to group neighboring pixels into segmented into superpixel objects (from pixels) such as to exploit local spatial context. Two phases are designed in the methodology: 1) Generate objects based on the simple linear iterative clustering algorithm, and discriminate these objects into changed and unchanged classes using fuzzy c-means (FCM) clustering and a deep PCANet. The prediction of this Phase is the set of changed and unchanged superpixels. 2) Deep learning on the pixel sets over the changed superpixels only, obtained in the first phase, to discriminate real changes from false changes. SLIC is employed again to achieve new superpixels in the second phase. Low rank and sparse decomposition are applied to these new superpixels to suppress speckle noise significantly. A further clustering step is applied to these new superpixels via FCM. A new PCANet is then trained to classify two kinds of changed superpixels to achieve the final change maps. Numerical experiments demonstrate that, compared with benchmark methods, the proposed approach can distinguish real changes from false changes effectively with significantly reduced false alarm rates, and achieve up to 99.71% change detection accuracy using multi-temporal SAR imagery. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：变化检测是合成孔径雷达（SAR）图像的基本应用中的一个。然而，斑点的SAR图像噪声提出了变化检测更负面的影响。在这项研究中，一种新型的两相基于对象的深度学习方法提出了多时相SAR图像变化检测。与传统方法相比，该方法带来了两个主要的创新。一是所有像素分为三类，而不是两类：不变的像素，改变像素造成强烈的斑点（假的变化），并改变了像素的实际地形的变化（真正的变化）而形成。另一种是相邻像素到分割成超像素的对象（从像素），如以利用局部空间上下文组。两个相被设计成在该方法：1）基于该简单的线性迭代聚类算法的目的，并区分这些对象到使用模糊c均值（FCM）聚类和深PCANet变与不变类。这个阶段的预测是一组变与不变的超像素。 2）上的像素集在所述改变仅超像素，在第一阶段中获得的，深学习辨别从虚假变化的实际变化。 SLIC被再次用来实现第二阶段的新的超像素。低等级和稀疏分解的噪音显著应用到这些新的超像素来抑制斑点。进一步的聚类步骤被施加到通过FCM这些新的超像素。然后，新的PCANet被训练2种改变超级像素的分类，以实现最终的变化图。数值结果表明，与基准方法相比，该方法可以区分有效地降低了显著的误报率假的变化真正的变化，实现了利用多时相SAR影像99.71％的变化检测精度。</font>
</div>


<hr>
<div id="paper15"> <b>15. Registration made easy -- standalone orthopedic navigation with HoloLens</b>  <a href="https://arxiv.org/pdf/2001.06209" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liebmann%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florentin Liebmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roner%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Roner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Atzigen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marco von Atzigen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wanivenhaus%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Wanivenhaus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neuhaus%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Caroline Neuhaus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Spirig%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">José Spirig</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scaramuzza%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Scaramuzza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sutter%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Reto Sutter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Snedeker%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jess Snedeker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Farshad%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mazda Farshad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=F%C3%BCrnstahl%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philipp Fürnstahl</a><br>
<font size="3">
Abstract: In surgical navigation, finding correspondence between preoperative plan and intraoperative anatomy, the so-called registration task, is imperative. One promising approach is to intraoperatively digitize anatomy and register it with the preoperative plan. State-of-the-art commercial navigation systems implement such approaches for pedicle screw placement in spinal fusion surgery. Although these systems improve surgical accuracy, they are not gold standard in clinical practice. Besides economical reasons, this may be due to their difficult integration into clinical workflows and unintuitive navigation feedback. Augmented Reality has the potential to overcome these limitations. Consequently, we propose a surgical navigation approach comprising intraoperative surface digitization for registration and intuitive holographic navigation for pedicle screw placement that runs entirely on the Microsoft HoloLens. Preliminary results from phantom experiments suggest that the method may meet clinical accuracy requirements. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在手术导航，术前计划及术中解剖，所谓的注册任务之间找到对应，势在必行。一个可行的方法是手术中数字化解剖，并与术前计划注册。国家的最先进的商用导航系统实现了对脊柱融合术椎弓根螺钉放置这些方法。虽然这些系统提高手术准确性，他们不是在临床实践中的金标准。除了经济上的原因，这可能是由于他们难以融入临床工作流程和直观的导航反馈。增强现实必须克服这些局限性的潜力。因此，我们提出了一种外科手术导航的方法，包括用于登记和直观的全息术中的导航表面的数字化椎弓根螺钉放置的是完全在Microsoft HoloLens运行。从幻像实验的初步结果表明，该方法可满足临床的精度要求。</font>
</div>


<hr>
<div id="paper16"> <b>16. FPCR-Net: Feature Pyramidal Correlation and Residual Reconstruction for  Semi-supervised Optical Flow Estimation</b>  <a href="https://arxiv.org/pdf/2001.06171" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingyu Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cuiling Lan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenjun Zeng</a><br>
<font size="3">
Abstract: Optical flow estimation is an important yet challenging problem in the field of video analytics. The features of different semantics levels/layers of a convolutional neural network can provide information of different granularity. To exploit such flexible and comprehensive information, we propose a semi-supervised Feature Pyramidal Correlation and Residual Reconstruction Network (FPCR-Net) for optical flow estimation from frame pairs. It consists of two main modules: pyramid correlation mapping and residual reconstruction. The pyramid correlation mapping module takes advantage of the multi-scale correlations of global/local patches by aggregating features of different scales to form a multi-level cost volume. The residual reconstruction module aims to reconstruct the sub-band high-frequency residuals of finer optical flow in each stage. Based on the pyramid correlation mapping, we further propose a correlation-warping-normalization (CWN) module to efficiently exploit the correlation dependency. Experiment results show that the proposed scheme achieves the state-of-the-art performance, with improvement by 0.80, 1.15 and 0.10 in terms of average end-point error (AEE) against competing baseline methods - FlowNet2, LiteFlowNet and PWC-Net on the Final pass of Sintel dataset, respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：光流估计是视频分析领域的一个重要而具有挑战性的问题。不同的语义等级的特征/卷积神经网络的层可提供不同粒度的信息。为了利用这种柔性和全面的信息，我们提出了从帧双光流估计一个半监督功能锥体相关和残差重建网络（FPCR-净）。它包括两个主要模块：金字塔相关映射和残差重建。金字塔相关映射模块通过聚合不同尺度的特征，以形成多级成本体积利用全局/局部贴片的多尺度相关的。将残余的重建模块目标以重建在每个阶段中更精细的光流的子带的高频残差。基于金字塔的相关性映射，我们进一步提出的相关扭曲规范化（CWN）模块，以有效地利用的相关性依赖。实验结果表明，该方案由0.80，1.15和0.10，平均终点误差（AEE）来实现国家的最先进的性能，提高同台竞技基线方法 -  FlowNet2，LiteFlowNet和PWC-Net的上辛特尔数据集的最终道次，分别。</font>
</div>


<hr>
<div id="paper17"> <b>17. Interpreting Galaxy Deblender GAN from the Discriminator's Perspective</b>  <a href="https://arxiv.org/pdf/2001.06151" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heyi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuewei Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mueller%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Klaus Mueller</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Xu</a><br>
<font size="3">
Abstract: Generative adversarial networks (GANs) are well known for their unsupervised learning capabilities. A recent success in the field of astronomy is deblending two overlapping galaxy images via a branched GAN model. However, it remains a significant challenge to comprehend how the network works, which is particularly difficult for non-expert users. This research focuses on behaviors of one of the network's major components, the Discriminator, which plays a vital role but is often overlooked, Specifically, we enhance the Layer-wise Relevance Propagation (LRP) scheme to generate a heatmap-based visualization. We call this technique Polarized-LRP and it consists of two parts i.e. positive contribution heatmaps for ground truth images and negative contribution heatmaps for generated images. Using the Galaxy Zoo dataset we demonstrate that our method clearly reveals attention areas of the Discriminator when differentiating generated galaxy images from ground truth images. To connect the Discriminator's impact on the Generator, we visualize the gradual changes of the Generator across the training process. An interesting result we have achieved there is the detection of a problematic data augmentation procedure that would else have remained hidden. We find that our proposed method serves as a useful visual analytical tool for a deeper understanding of GAN models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：创成对抗网络（甘斯）是众所周知的无监督的学习能力。在天文学领域最近的成功经由支GAN模型去混合两个重叠的星系图像。然而，它仍然是一个挑战显著理解如何在网络的作品，这对非专业用户特别困难。这项研究的重点是网络的主要组成部分之一的行为，鉴别，它起着至关重要的作用，但往往被忽视，特别是，我们提高了逐层关联传播（LRP）方案来生成一个基于热图可视化。我们称这种技术偏光LRP，它由两个部分组成，即积极的贡献热图的地面真理图像和生成的图像负贡献热图。利用星系动物园的数据集，我们证明了我们的方法区分从地面实况图像生成星系图像时，清楚地表明鉴别的关注的领域。要连接鉴别对发电机的影响，我们可以形象地发电机的整个训练过程中逐渐变化。我们已经实现了有一个有趣的结果是，将其他仍然隐藏着一个问题的数据增高过程的检测。我们发现，我们提出的方法作为一个有用的可视化分析工具，GAN模式有更深的了解。</font>
</div>


<hr>
<div id="paper18"> <b>18. Learning to Augment Expressions for Few-shot Fine-grained Facial  Expression Recognition</b>  <a href="https://arxiv.org/pdf/2001.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenxuan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanwei Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenjie Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqi Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoqiang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu-Gang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyang Xue</a><br>
<font size="3">
Abstract: Affective computing and cognitive theory are widely used in modern human-computer interaction scenarios. Human faces, as the most prominent and easily accessible features, have attracted great attention from researchers. Since humans have rich emotions and developed musculature, there exist a lot of fine-grained expressions in real-world applications. However, it is extremely time-consuming to collect and annotate a large number of facial images, of which may even require psychologists to correctly categorize them. To the best of our knowledge, the existing expression datasets are only limited to several basic facial expressions, which are not sufficient to support our ambitions in developing successful human-computer interaction systems. To this end, a novel Fine-grained Facial Expression Database - F2ED is contributed in this paper, and it includes more than 200k images with 54 facial expressions from 119 persons. Considering the phenomenon of uneven data distribution and lack of samples is common in real-world scenarios, we further evaluate several tasks of few-shot expression learning by virtue of our F2ED, which are to recognize the facial expressions given only few training instances. These tasks mimic human performance to learn robust and general representation from few examples. To address such few-shot tasks, we propose a unified task-driven framework Compositional Generative Adversarial Network (Comp-GAN) learning to synthesize facial images and thus augmenting the instances of few-shot expression classes. Extensive experiments are conducted on F2ED and existing facial expression datasets, i.e., JAFFE and FER2013, to validate the efficacy of our F2ED in pre-training facial expression recognition network and the effectiveness of our proposed approach Comp-GAN to improve the performance of few-shot recognition tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：情感计算和认知理论被广泛应用于现代的人机交互场景。人脸，作为最突出和方便的特点，从研究者的高度关注。由于人类具有丰富的情感和发达的肌肉，还存在很多现实世界的应用细粒度的表情。然而，这是非常耗时的收集和注释了大量面部图像，这甚至可能需要心理学家正确分类。据我们所知，现有的表达数据仅限于几个基本的面部表情，这是不足以支持我们的野心开发成功的人机交互系统。为此，一种新的细粒度面部表情数据库 -  F2ED在本文提供的，它包括超过200K的图像与来自119分的人54个的面部表情。考虑不均匀分布数据的现象，缺乏样品是现实世界的情景一样，我们还凭借我们F2ED，这是认识到只给出几个训练实例面部表情的评价几拍表达式学习的几个任务。这些任务模拟人类的表现从几个例子学习强大和一般的表示。为了解决这样的一些次任务，我们提出了一个统一的任务驱动的框架组成剖成对抗性网络（压缩 -  GAN）学习合成面部图像，从而增强几炮表达类的实例。大量的实验是在F2ED和现有的面部表情的数据集，即JAFFE和FER2013进行，以验证我们F2ED的功效在训练前的面部表情识别网络和我们提出的方法比较-GaN的有效性，提高few-性能镜头识别任务。</font>
</div>


<hr>
<div id="paper19"> <b>19. Spatio-Temporal Ranked-Attention Networks for Video Captioning</b>  <a href="https://arxiv.org/pdf/2001.06127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cherian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anoop Cherian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jue Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hori%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chiori Hori</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marks%2C+T+K" target="_blank" rel="noopener" style="color:#0000EE;">Tim K. Marks</a><br>
<font size="3">
Abstract: Generating video descriptions automatically is a challenging task that involves a complex interplay between spatio-temporal visual features and language models. Given that videos consist of spatial (frame-level) features and their temporal evolutions, an effective captioning model should be able to attend to these different cues selectively. To this end, we propose a Spatio-Temporal and Temporo-Spatial (STaTS) attention model which, conditioned on the language state, hierarchically combines spatial and temporal attention to videos in two different orders: (i) a spatio-temporal (ST) sub-model, which first attends to regions that have temporal evolution, then temporally pools the features from these regions; and (ii) a temporo-spatial (TS) sub-model, which first decides a single frame to attend to, then applies spatial attention within that frame. We propose a novel LSTM-based temporal ranking function, which we call ranked attention, for the ST model to capture action dynamics. Our entire framework is trained end-to-end. We provide experiments on two benchmark datasets: MSVD and MSR-VTT. Our results demonstrate the synergy between the ST and TS modules, outperforming recent state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生成视频描述自动是一个具有挑战性的任务，涉及到时空视觉特征和语言模型之间的复杂的相互作用。鉴于影片由空间（帧级）的功能及其时间的演化，有效的字幕模型应该能够参加到这些不同的线索选择性。为此，我们提出了时空和时间空间（STATS）注意模型，该模型，条件上的语言状态，分层结合的空间和时间关注到视频中两个不同的顺序：（I）的时空（ST）子模型，该第一照顾到具有时间演变，区域然后在时间上从池这些区域的特征;和（ii）一个时间空间（TS）的子模型，该模型首先决定单个帧出席，然后应用于的帧内的空间的关注。我们提出了一个新的基于LSTM-时间排序功能，我们称之为排名的重视，对于ST模型捕捉行动力度。我们的整个框架的培训结束到终端。我们提供了两个标准数据集实验：MSVD和MSR-VTT。我们的结果证明了ST和TS模块之间的协同作用，优于国家的最先进的最近的方法。</font>
</div>


<hr>
<div id="paper20"> <b>20. Automatic Discovery of Political Meme Genres with Diverse Appearances</b>  <a href="https://arxiv.org/pdf/2001.06122" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Theisen%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Theisen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brogan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joel Brogan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+P+B" target="_blank" rel="noopener" style="color:#0000EE;">Pamela Bilo Thomas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moreira%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Moreira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Phoa%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pascal Phoa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weninger%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tim Weninger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scheirer%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Walter Scheirer</a><br>
<font size="3">
Abstract: Forms of human communication are not static --- we expect some evolution in the way information is conveyed over time because of advances in technology. One example of this phenomenon is the image-based meme, which has emerged as a dominant form of political messaging in the past decade. While originally used to spread jokes on social media, memes are now having an outsized impact on public perception of world events. A significant challenge in automatic meme analysis has been the development of a strategy to match memes from within a single genre when the appearances of the images vary. Such variation is especially common in memes exhibiting mimicry. For example, when voters perform a common hand gesture to signal their support for a candidate. In this paper we introduce a scalable automated visual recognition pipeline for discovering political meme genres of diverse appearance. This pipeline can ingest meme images from a social network, apply computer vision-based techniques to extract local features and index new images into a database, and then organize the memes into related genres. To validate this approach, we perform a large case study on the 2019 Indonesian Presidential Election using a new dataset of over two million images collected from Twitter and Instagram. Results show that this approach can discover new meme genres with visually diverse images that share common stylistic elements, paving the way forward for further work in semantic analysis and content attribution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类的沟通方式不是一成不变的---我们期待的方式获取信息的一些变化传送随着时间的推移，因为技术的进步。这种现象的一个例子是基于图像的米姆，这已经成为过去十年政治信息的主要形式。虽然原本是用来传播的笑话在社会化媒体，模因现在不得不对世界事件的公众认知的丰厚影响。在自动梅梅分析的显著挑战是一项战略，从单一的体裁内匹配模因时图像的外观变化的发展。这种变化是中模仿记因尤其常见。例如，当执行选民一个共同的手势的信号其用于候选的支持。在本文中，我们介绍了用于发现不同外观的政治米姆流派一个可扩展的自动化视觉识别管道。这条管道可以从社交网络梅梅摄取图像，应用计算机基于视觉的技术来提取局部特征和指数新的图像到一个数据库，然后整理成模因相关流派。为了验证这种方法，我们使用从Twitter和Instagram的收集超过两百万图像的新的数据集上的2019印尼总统选举的一个大案例。结果表明，该方法可以发现新的米姆风格与有着共同的风格元素在视觉上不同的图像，铺平了道路前进为语义分析和内容属性的进一步工作。</font>
</div>


<hr>
<div id="paper21"> <b>21. On- Device Information Extraction from Screenshots in form of tags</b>  <a href="https://arxiv.org/pdf/2001.06094" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sumit Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramena%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gopi Ramena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manoj Goyal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohanty%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Debi Mohanty</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankur Agarwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Changmai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benu Changmai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moharana%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sukumar Moharana</a><br>
<font size="3">
Abstract: We propose a method to make mobile screenshots easily searchable. In this paper, we present the workflow in which we: 1) preprocessed a collection of screenshots, 2) identified script presentin image, 3) extracted unstructured text from images, 4) identifiedlanguage of the extracted text, 5) extracted keywords from the text, 6) identified tags based on image features, 7) expanded tag set by identifying related keywords, 8) inserted image tags with relevant images after ranking and indexed them to make it searchable on device. We made the pipeline which supports multiple languages and executed it on-device, which addressed privacy concerns. We developed novel architectures for components in the pipeline, optimized performance and memory for on-device computation. We observed from experimentation that the solution developed can reduce overall user effort and improve end user experience while searching, whose results are published. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议让移动截图易于搜索的方法。在本文中，我们提出我们在其中工作流：1）预处理截图的集合，2）识别的脚本presentin图像，3）提取从图像非结构化文本，4）提取的文本的identifiedlanguage，5）提取的关键词从文本，6）的基础上的图像特征识别的标签，7）膨胀通过识别相关的关键字标签集，8）与相关图像插入的图像标签的排名后和索引他们，使其可检索在设备上。我们做了哪些支持多种语言流水线开始执行它的设备，其中涉及隐私问题。我们开发新的架构在管线，优化的性能和内存设备上的计算组件。我们从实验观察到，解决方案开发可降低整体用户的努力和改善最终用户体验，同时搜索，其结果公布。</font>
</div>


<hr>
<div id="paper22"> <b>22. Tracking of Micro Unmanned Aerial Vehicles: A Comparative Study</b>  <a href="https://arxiv.org/pdf/2001.06066" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B6k%C3%A7e%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fatih Gökçe</a><br>
<font size="3">
Abstract: Micro unmanned aerial vehicles (mUAV) became very common in recent years. As a result of their widespread usage, when they are flown by hobbyists illegally, crucial risks are imposed and such mUAVs need to be sensed by security systems. Furthermore, the sensing of mUAVs are essential for also swarm robotics research where the individuals in a flock of robots require systems to sense and localize each other for coordinated operation. In order to obtain such systems, there are studies to detect mUAVs utilizing different sensing mediums, such as vision, infrared and sound signals, and small-scale radars. However, there are still challenges that awaits to be handled in this field such as integrating tracking approaches to the vision-based detection systems to enhance accuracy and computational complexity. For this reason, in this study, we combine various tracking approaches to a vision-based mUAV detection system available in the literature, in order to evaluate different tracking approaches in terms of accuracy and as well as investigate the effect of such integration to the computational cost. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：微型无人机（mUAV）在最近几年变得很普遍。由于其广泛使用的结果，当他们被非法爱好者飞行，关键的风险强加的，这样mUAVs需要通过安全系统进行检测。此外，还群机器人研究其中机器人的羊群个人要求系统意识和本地化相互协调运行mUAVs的检测是必不可少的。为了得到这样的系统，也有研究，以检测使用不同的感测介质，如视觉，红外线和声音信号，以及小规模雷达mUAVs。然而，仍然有挑战等待着在这一领域，如集成的跟踪方法，以基于视觉的检测系统，以提高精度和计算复杂性进行处理。为此，在本研究中，我们结合各种跟踪方法，以文献中的基于视觉的mUAV检测系统，以评估不同的跟踪方法在准确性方面和以及调查这种整合的计算效果成本。</font>
</div>


<hr>
<div id="paper23"> <b>23. Increasing the robustness of DNNs against image corruptions by playing  the Game of Noise</b>  <a href="https://arxiv.org/pdf/2001.06057" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rusak%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evgenia Rusak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schott%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lukas Schott</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zimmermann%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roland Zimmermann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bitterwolf%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julian Bitterwolf</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bringmann%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oliver Bringmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bethge%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Bethge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brendel%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wieland Brendel</a><br>
<font size="3">
Abstract: The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the previous state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against uncorrelated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类视觉系统对宽范围的天然存在的变型和损坏等雨或雪非常健壮。相比之下，现代的图像识别模型的性能上前所未见的损坏进行评估时，强烈地下降。在这里，我们证明了一个简单的，但适当调整训练加性高斯和斑点噪声推广出奇地好于看不见的腐败，很容易达到艺术的腐败基准ImageNet-C（含ResNet50）对以前的状态和MNIST-C。我们依靠这些强大的基准结果的顶部，并表明对不相关的最坏情况下的噪声分布引线识别模型的对抗性训练，在性能上的额外增加。这正可以进一步改进先前提出的防御方法相结合。</font>
</div>


<hr>
<div id="paper24"> <b>24. Modality-Balanced Models for Visual Dialogue</b>  <a href="https://arxiv.org/pdf/2001.06354" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyounghun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Bansal</a><br>
<font size="3">
Abstract: The Visual Dialog task requires a model to exploit both image and conversational context information to generate the next response to the dialogue. However, via manual analysis, we find that a large number of conversational questions can be answered by only looking at the image without any access to the context history, while others still need the conversation context to predict the correct answers. We demonstrate that due to this reason, previous joint-modality (history and image) models over-rely on and are more prone to memorizing the dialogue history (e.g., by extracting certain keywords or patterns in the context information), whereas image-only models are more generalizable (because they cannot memorize or extract keywords from history) and perform substantially better at the primary normalized discounted cumulative gain (NDCG) task metric which allows multiple correct answers. Hence, this observation encourages us to explicitly maintain two models, i.e., an image-only model and an image-history joint model, and combine their complementary abilities for a more balanced multimodal model. We present multiple methods for this integration of the two models, via ensemble and consensus dropout fusion with shared parameters. Empirically, our models achieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG and high balance across metrics), and substantially outperform the winner of the Visual Dialog challenge 2018 on most metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可视对话任务需要一个模型，同时利用图像和会话的上下文信息来生成到对话的下一个响应。然而，通过人工分析，我们发现了大量的对话问题只能由看图像，而不到上下文历史上的任何访问来回答，而其他人还需要对话上下文来预测正确的答案。我们表明，由于这个原因，以往合资模式（史和图像）模式过分依赖，而且更容易记住的对话记录（例如，通过上下文信息提取的关键字或模式），而只有图象模型更加普及（因为他们无法记住或者从历史中提取的关键字），并在主要贴现归累计收益（NDCG）任务指标，它允许多个正确答案大幅更好地履行。因此，这种观察鼓励我们要明确地保持两种模式，即只有一个影像的模型和图像的历史关节模型，并结合它们的互补能力，为一个更加平衡的多模式模型。我们提出了这种整合两个模型的多种方法，通过与共享参数合奏和共识辍学融合。根据经验，我们的模型实现对视觉对话挑战2019（关于NDCG和整个指标高平衡等级3）强劲的业绩，并基本跑赢视觉对话框挑战2018的大多数指标的赢家。</font>
</div>


<hr>
<div id="paper25"> <b>25. Tethered Aerial Visual Assistance</b>  <a href="https://arxiv.org/pdf/2001.06347" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuesu Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dufek%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Dufek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Murphy%2C+R+R" target="_blank" rel="noopener" style="color:#0000EE;">Robin R. Murphy</a><br>
<font size="3">
Abstract: In this paper, an autonomous tethered Unmanned Aerial Vehicle (UAV) is developed into a visual assistant in a marsupial co-robots team, collaborating with a tele-operated Unmanned Ground Vehicle (UGV) for robot operations in unstructured or confined environments. These environments pose extreme challenges to the remote tele-operator due to the lack of sufficient situational awareness, mostly caused by the unstructuredness and confinement, stationary and limited field-of-view and lack of depth perception from the robot's onboard cameras. To overcome these problems, a secondary tele-operated robot is used in current practices, who acts as a visual assistant and provides external viewpoints to overcome the perceptual limitations of the primary robot's onboard sensors. However, a second tele-operated robot requires extra manpower and teamwork demand between primary and secondary operators. The manually chosen viewpoints tend to be subjective and sub-optimal. Considering these intricacies, we develop an autonomous tethered aerial visual assistant in place of the secondary tele-operated robot and operator, to reduce human robot ratio from 2:2 to 1:2. Using a fundamental viewpoint quality theory, a formal risk reasoning framework, and a newly developed tethered motion suite, our visual assistant is able to autonomously navigate to good-quality viewpoints in a risk-aware manner through unstructured or confined spaces with a tether. The developed marsupial co-robots team could improve tele-operation efficiency in nuclear operations, bomb squad, disaster robots, and other domains with novel tasks or highly occluded environments, by reducing manpower and teamwork demand, and achieving better visual assistance quality with trustworthy risk-aware motion. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种自主拴无人机（UAV）的发展成为有袋动物共同的机器人团队视觉助理，具有远程操作的无人地面车辆（UGV），用于非结构化或狭窄的环境中机器人进行作业协作。这些环境造成由于缺乏足够的态势感知能力，主要由unstructuredness和约束，固定和有限领域的视图造成极端挑战远程远程操作，缺乏从机器人的车载摄像机的景深感知。为了克服这些问题，二次远程操作机器人在当前的实践，谁充当视觉辅助，并提供外部视点克服初级机器人的机载传感器的感知限制使用。然而，第二个远程操作机器人需要初级和次级运营商之间的额外的人力和团队需求。手动选择视点趋于主观和次优的。考虑到这些复杂性，我们开发代替二次远程操作机器人和操作员的一个自治系留空中视觉助理，从2减少人类机器人比为1:2至1:2。使用基本视点质量理论，正式的风险推理框架，和新开发的系绳运动套件，我们的视觉助手是能够通过与系绳非结构化或密闭空间自主导航至在风险意识的方式高质量的观点。所开发的有袋动物共同的机器人团队可以提高核作战远程操作效率，拆弹小组，灾难机器人，并与新的任务或非常闭塞的环境中，通过减少人力和团队协作需求，并实现更好的视觉援助质量值得信赖的风险其他领域知晓运动。</font>
</div>


<hr>
<div id="paper26"> <b>26. DeepSUM++: Non-local Deep Neural Network for Super-Resolution of  Unregistered Multitemporal Images</b>  <a href="https://arxiv.org/pdf/2001.06342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Molini%2C+A+B" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Bordone Molini</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Valsesia%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diego Valsesia</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fracastoro%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giulia Fracastoro</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Magli%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Enrico Magli</a><br>
<font size="3">
Abstract: Deep learning methods for super-resolution of a remote sensing scene from multiple unregistered low-resolution images have recently gained attention thanks to a challenge proposed by the European Space Agency. This paper presents an evolution of the winner of the challenge, showing how incorporating non-local information in a convolutional neural network allows to exploit self-similar patterns that provide enhanced regularization of the super-resolution problem. Experiments on the dataset of the challenge show improved performance over the state-of-the-art, which does not exploit non-local information. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：超分辨率从多个未注册的低分辨率图像的遥感场景的深度学习方法最近获得了感谢关注欧洲航天局提出了挑战。本文介绍了挑战冠军，展示了如何在卷积神经网络将非本地信息的发展允许利用自相似的模式，提供了增强的超分辨率问题的正规化。对挑战的数据集实验表明在国家的最先进的，它并没有利用非本地信息更好的性能。</font>
</div>


<hr>
<div id="paper27"> <b>27. Detection Method Based on Automatic Visual Shape Clustering for  Pin-Missing Defect in Transmission Lines</b>  <a href="https://arxiv.org/pdf/2001.06236" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenbing Zhao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Qi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongyu Qi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Qi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yincheng Qi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhai%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongjie Zhai</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenqing Zhao</a><br>
<font size="3">
Abstract: Bolts are the most numerous fasteners in transmission lines and are prone to losing their split pins. How to realize the automatic pin-missing defect detection for bolts in transmission lines so as to achieve timely and efficient trouble shooting is a difficult problem and the long-term research target of power systems. In this paper, an automatic detection model called Automatic Visual Shape Clustering Network (AVSCNet) for pin-missing defect is constructed. Firstly, an unsupervised clustering method for the visual shapes of bolts is proposed and applied to construct a defect detection model which can learn the difference of visual shape. Next, three deep convolutional neural network optimization methods are used in the model: the feature enhancement, feature fusion and region feature extraction. The defect detection results are obtained by applying the regression calculation and classification to the regional features. In this paper, the object detection model of different networks is used to test the dataset of pin-missing defect constructed by the aerial images of transmission lines from multiple locations, and it is evaluated by various indicators and is fully verified. The results show that our method can achieve considerably satisfactory detection effect. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：螺栓是输电线路最众多的紧固件，而且容易失去自己的开口销。如何实现对输电线路的螺栓自动销缺失的缺陷检测，从而及时实现高效的故障排除是一个困难的问题，电力系统的长期研究目标。在本文中，一种自动检测模型称为自动视觉形状聚类网络（AVSCNet）为销缺失缺陷构造。首先，对于螺栓的视觉形状的无监督聚类方法，并应用于构建其可以学习视觉形状的差异的缺陷检测模型。接下来，在模型中使用了三个深卷积神经网络优化方法：增强功能，特征融合和区域特征提取。缺陷检测结果通过将回归计算和分类区域特征获得。在本文中，不同网络的物体检测模型用于测试的通过的从多个位置传输线架空图像构建销缺失缺陷的数据集，并且它是由各种指示器评估并且被充分验证。结果表明，我们的方法可以达到相当满意的检测效果。</font>
</div>


<hr>
<div id="paper28"> <b>28. Sideways: Depth-Parallel Training of Video Models</b>  <a href="https://arxiv.org/pdf/2001.06232" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Malinowski%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mateusz Malinowski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Swirszcz%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Grzegorz Swirszcz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carreira%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joao Carreira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Patraucean%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Viorica Patraucean</a><br>
<font size="3">
Abstract: We propose Sideways, an approximate backpropagation scheme for training video models. In standard backpropagation, the gradients and activations at every computation step through the model are temporally synchronized. The forward activations need to be stored until the backward pass is executed, preventing inter-layer (depth) parallelization. However, can we leverage smooth, redundant input streams such as videos to develop a more efficient training scheme? Here, we explore an alternative to backpropagation; we overwrite network activations whenever new ones, i.e., from new frames, become available. Such a more gradual accumulation of information from both passes breaks the precise correspondence between gradients and activations, leading to theoretically more noisy weight updates. Counter-intuitively, we show that Sideways training of deep convolutional video networks not only still converges, but can also potentially exhibit better generalization compared to standard synchronized backpropagation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出侧身，培训视频机型的大致反向传播方案。在标准反向传播，在通过所述模型中的每个计算步骤中的梯度和激活在时间上同步。正向激活需要被存储，直到执行向后通，从而防止层间（深度）并行化。然而，我们可以利用平滑，冗余输入流，如视频，开发更有效的培训计划？在这里，我们探索反向传播的替代;我们覆盖的网络激活，每当新的，即由新的框架，变得可用。这样的来自两个信息更渐进累积通断梯度和激活之间的确切的对应，从而导致理论上更嘈杂重量的更新。与直觉相反，我们表明，与标准同步反向传播侧身培训深卷积视频网络不仅仍然收敛的，但也有可能表现出较好的泛化。</font>
</div>


<hr>
<div id="paper29"> <b>29. FedVision: An Online Visual Object Detection Platform Powered by  Federated Learning</b>  <a href="https://arxiv.org/pdf/2001.06202" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anbu Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">He Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youzhi Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanyuan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lican Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianjian Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Yang</a><br>
<font size="3">
Abstract: Visual object detection is a computer vision-based artificial intelligence (AI) technique which has many practical applications (e.g., fire hazard monitoring). However, due to privacy concerns and the high cost of transmitting video data, it is highly challenging to build object detection models on centrally stored large training datasets following the current approach. Federated learning (FL) is a promising approach to resolve this challenge. Nevertheless, there currently lacks an easy to use tool to enable computer vision application developers who are not experts in federated learning to conveniently leverage this technology and apply it in their systems. In this paper, we report FedVision - a machine learning engineering platform to support the development of federated learning powered computer vision applications. The platform has been deployed through a collaboration between WeBank and Extreme Vision to help customers develop computer vision-based safety monitoring solutions in smart city applications. Over four months of usage, it has achieved significant efficiency improvement and cost reduction while removing the need to transmit sensitive data for three major corporate customers. To the best of our knowledge, this is the first real application of FL in computer vision-based tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉对象检测是具有许多实际应用（例如，火灾监视）一个基于计算机视觉的人工智能（AI）技术。然而，由于隐私问题和传输视频数据的成本高，这是非常具有挑战性的集中存储大量训练数据构建物体检测模式下的电流的方法。联合学习（FL）是一种很有前途的方法来解决这一难题。尽管如此，目前缺乏一个易于使用的工具，使计算机视觉应用开发商谁是不是在联合学习专家能够方便地利用这一技术，并在他们的系统应用它。在本文中，我们报告FedVision  - 机器学习技术平台支持的联合学习动力的计算机视觉应用的开发。该平台已通过帮助客户WeBank和极端视觉之间的合作开发部署在智能城市应用基于计算机视觉的安全监控解决方案。四个多月的使用，它已经取得了显著提高效率和降低成本，同时消除需要发送的敏感数据有三个主要的企业客户。据我们所知，这是计算机基于视觉的任务FL的第一个真正的应用。</font>
</div>


<hr>
<div id="paper30"> <b>30. Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless  Approach</b>  <a href="https://arxiv.org/pdf/2001.06175" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chanoh Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moghadam%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peyman Moghadam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Soohwan Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sridharan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sridha Sridharan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fookes%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Clinton Fookes</a><br>
<font size="3">
Abstract: The demand for multimodal sensing systems for robotics is growing due to the increase in robustness, reliability and accuracy offered by these systems. These systems also need to be spatially and temporally co-registered to be effective. In this paper, we propose a targetless and structureless spatiotemporal camera-LiDAR calibration method. Our method combines a closed-form solution with a modified structureless bundle adjustment where the coarse-to-fine approach does not {require} an initial guess on the spatiotemporal parameters. Also, as 3D features (structure) are calculated from triangulation only, there is no need to have a calibration target or to match 2D features with the 3D point cloud which provides flexibility in the calibration process and sensor configuration. We demonstrate the accuracy and robustness of the proposed method through both simulation and real data experiments using multiple sensor payload configurations mounted to hand-held, aerial and legged robot systems. Also, qualitative results are given in the form of a colorized point cloud visualization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多传感系统对机器人的需求正在不断增长，由于这些系统提供的耐用性，可靠性和精确度的提高。这些系统还需要在空间和时间上处于同一注册是有效的。在本文中，我们提出了一个无标的和无结构的时空相机，激光雷达校准方法。我们的方法结合了改性无结构束调整，其中粗到细的方法不要求{}上的时空参数的初始猜测的闭合形式解。另外，作为三维特征（结构）从三角测量计算只，没有必要有一个校准目标或匹配2D与3D点云，其提供在校准过程和传感器配置的灵活性的特点。我们证明了该方法的准确度和鲁棒性通过使用多个传感器的有效载荷的配置模拟和实际数据实验安装到手持式，空中和腿式机器人系统。此外，定性的结果以彩色点云可视化的形式给出。</font>
</div>


<hr>
<div id="paper31"> <b>31. An adversarial learning framework for preserving users' anonymity in  face-based emotion recognition</b>  <a href="https://arxiv.org/pdf/2001.06103" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Narula%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vansh Narula</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhangyang" target="_blank" rel="noopener" style="color:#0000EE;">Zhangyang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang" target="_blank" rel="noopener" style="color:#0000EE;">Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaspari%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Theodora Chaspari</a><br>
<font size="3">
Abstract: Image and video-capturing technologies have permeated our every-day life. Such technologies can continuously monitor individuals' expressions in real-life settings, affording us new insights into their emotional states and transitions, thus paving the way to novel well-being and healthcare applications. Yet, due to the strong privacy concerns, the use of such technologies is met with strong skepticism, since current face-based emotion recognition systems relying on deep learning techniques tend to preserve substantial information related to the identity of the user, apart from the emotion-specific information. This paper proposes an adversarial learning framework which relies on a convolutional neural network (CNN) architecture trained through an iterative procedure for minimizing identity-specific information and maximizing emotion-dependent information. The proposed approach is evaluated through emotion classification and face identification metrics, and is compared against two CNNs, one trained solely for emotion recognition and the other trained solely for face identification. Experiments are performed using the Yale Face Dataset and Japanese Female Facial Expression Database. Results indicate that the proposed approach can learn a convolutional transformation for preserving emotion recognition accuracy and degrading face identity recognition, providing a foundation toward privacy-aware emotion recognition technologies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图像和视频捕捉技术已经渗透到我们每一天的生活。这种技术可连续监测在现实生活中设置个人的表现，获得了我们新的见解他们的情感状态和转换，从而铺平了道路新的福祉和医疗应用。然而，由于强烈的隐私问题，使用这种技术时遭到强烈的怀疑态度，因为当前面为基础的情感识别系统依托深学习技术倾向于从情感保存有关用户的身份基本信息，除了-具体信息。本文提出了一种对抗性的学习框架，它依赖于通过最小化身份的具体信息，并最大限度地提高情绪相关的信息的迭代过程，培养了卷积神经网络（CNN）架构。所提出的方法是通过情感分类和面部识别指标评估，并针对两种细胞神经网络，另一个只卖情感识别训练和其他专为面部识别训练的比较。实验使用的是Yale人脸数据集和日本女性表情数据库进行。结果表明，该方法可以学习卷积转变为维护情感识别的准确性和有辱人格的脸身份识别情况，提供秘密感知情感识别技术奠定了基础。</font>
</div>


<hr>
<div id="paper32"> <b>32. Code-Bridged Classifier (CBC): A Low or Negative Overhead Defense for  Making a CNN Classifier Robust Against Adversarial Attacks</b>  <a href="https://arxiv.org/pdf/2001.06099" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Behnia%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Farnaz Behnia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mirzaeian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Mirzaeian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabokrou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Sabokrou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manoj%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sai Manoj</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohsenin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tinoosh Mohsenin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khasawneh%2C+K+N" target="_blank" rel="noopener" style="color:#0000EE;">Khaled N. Khasawneh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Homayoun%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Houman Homayoun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sasan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avesta Sasan</a><br>
<font size="3">
Abstract: In this paper, we propose Code-Bridged Classifier (CBC), a framework for making a Convolutional Neural Network (CNNs) robust against adversarial attacks without increasing or even by decreasing the overall models' computational complexity. More specifically, we propose a stacked encoder-convolutional model, in which the input image is first encoded by the encoder module of a denoising auto-encoder, and then the resulting latent representation (without being decoded) is fed to a reduced complexity CNN for image classification. We illustrate that this network not only is more robust to adversarial examples but also has a significantly lower computational complexity when compared to the prior art defenses. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出代码桥接分类（CBC），用于进行卷积神经网络（细胞神经网络）相对抗强大的攻击不增加，甚至通过降低整体模型的计算复杂性的框架。更具体地，我们提出了一种层叠的编码器卷积模型，其中，所述输入图像首先被去噪的自动编码器的编码器模块编码，然后将得到的潜表示（没有被解码）被馈送到降低复杂度的CNN为图像分类。我们表明，该网络不仅更加坚固，以对抗的例子，但也有显著较低的计算复杂性相比，现有技术抗辩。</font>
</div>


<hr>
<div id="paper33"> <b>33. Curriculum Labeling: Self-paced Pseudo-Labeling for Semi-Supervised  Learning</b>  <a href="https://arxiv.org/pdf/2001.06001" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cascante-Bonilla%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paola Cascante-Bonilla</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fuwen Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanjun Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ordonez%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vicente Ordonez</a><br>
<font size="3">
Abstract: Semi-supervised learning aims to take advantage of a large amount of unlabeled data to improve the accuracy of a model that only has access to a small number of labeled examples. We propose curriculum labeling, an approach that exploits pseudo-labeling for propagating labels to unlabeled samples in an iterative and self-paced fashion. This approach is surprisingly simple and effective and surpasses or is comparable with the best methods proposed in the recent literature across all the standard benchmarks for image classification. Notably, we obtain 94.91% accuracy on CIFAR-10 using only 4,000 labeled samples, and 88.56% top-5 accuracy on Imagenet-ILSVRC using 128,000 labeled samples. In contrast to prior works, our approach shows improvements even in a more realistic scenario that leverages out-of-distribution unlabeled data samples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：半监督学习的目标采取了大量的未标记数据的优势，提高了一个模型，只获得了少量的标识样本的准确性。我们建议的课程标签，它利用伪标签用于在迭代和自学的方式传播标签的未标记样本的方法。这种方法是非常简单和有效，超过或者是在最近的文献在所有标准的基准图像分类提出的最佳方法相媲美。值得注意的是，我们使用128000个标记的样品获得关于Imagenet-ILSVRC上CIFAR-10 94.91％的准确度仅使用4000标记的样品，以及88.56％顶5的精度。相较于之前的作品，我们的做法显示了改善，即使在更现实的情况下，充分利用外的分布未标记的数据样本。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-20</title>
    <url>/2020/01/20/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-20/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> A Common Semantic Space for Monolingual and Cross-Lingual  Meta-Embeddings <a href="https://arxiv.org/pdf/2001.06381" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Modality-Balanced Models for Visual Dialogue <a href="https://arxiv.org/pdf/2001.06354" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><div id="title3">
<b>3.</b> A Hybrid Solution to Learn Turn-Taking in Multi-Party Service-based Chat  Groups <a href="https://arxiv.org/pdf/2001.06350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>


<div id="title4">
<b>4.</b> RobBERT: a Dutch RoBERTa-based Language Model <a href="https://arxiv.org/pdf/2001.06286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue  System <a href="https://arxiv.org/pdf/2001.06206" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Plato Dialogue System: A Flexible Conversational AI Research Platform <a href="https://arxiv.org/pdf/2001.06463" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Supervised Speaker Embedding De-Mixing in Two-Speaker Environment <a href="https://arxiv.org/pdf/2001.06397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> On- Device Information Extraction from Screenshots in form of tags <a href="https://arxiv.org/pdf/2001.06094" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> User-in-the-loop Adaptive Intent Detection for Instructable Digital  Assistant <a href="https://arxiv.org/pdf/2001.06007" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. A Common Semantic Space for Monolingual and Cross-Lingual  Meta-Embeddings</b>  <a href="https://arxiv.org/pdf/2001.06381" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Garc%C3%ADa%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Iker García</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agerri%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rodrigo Agerri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rigau%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">German Rigau</a><br>
<font size="3">
Abstract: This paper presents a new technique for creating monolingual and cross-lingual meta-embeddings. Our method integrates multiple word embeddings created from complementary techniques, textual sources, knowledge bases and languages. Existing word vectors are projected to a common semantic space using linear transformations and averaging. With our method the resulting meta-embeddings maintain the dimensionality of the original embeddings without losing information while dealing with the out-of-vocabulary problem. An extensive empirical evaluation demonstrates the effectiveness of our technique with respect to previous work on various intrinsic and extrinsic multilingual evaluations, obtaining competitive results for Semantic Textual Similarity and state-of-the-art performance for word similarity and POS tagging (English and Spanish). The resulting cross-lingual meta-embeddings also exhibit excellent cross-lingual transfer learning capabilities. In other words, we can leverage pre-trained source embeddings from a resource-rich language in order to improve the word representations for under-resourced languages. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了创建单语和跨语言间的嵌入的新技术。我们的方法整合了互补技术，文本来源，知识库和语言创建多个字的嵌入。现有字矢量投影到使用线性变换和平均共同语义空间。随着我们的方法所产生的荟萃的嵌入保持原有的嵌入的维度，而不会丢失信息，在处理外的词汇的问题。广泛的实证评价表明了我们的技术相对于各种内在和外在的多语种评估先前的工作成效，获得了语义文本相似性和国家的最先进的性能竞争的结果词语相似度和词性标注（英语和西班牙语） 。产生的跨语种元的嵌入也表现出优异的跨语言迁移学习能力。换句话说，我们可以利用从资源丰富的语言预先训练源的嵌入，以提高资源不足的语言文字表述。</font>
</div>


<hr>
<div id="paper2"> <b>2. Modality-Balanced Models for Visual Dialogue</b>  <a href="https://arxiv.org/pdf/2001.06354" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyounghun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Bansal</a><br>
<font size="3">
Abstract: The Visual Dialog task requires a model to exploit both image and conversational context information to generate the next response to the dialogue. However, via manual analysis, we find that a large number of conversational questions can be answered by only looking at the image without any access to the context history, while others still need the conversation context to predict the correct answers. We demonstrate that due to this reason, previous joint-modality (history and image) models over-rely on and are more prone to memorizing the dialogue history (e.g., by extracting certain keywords or patterns in the context information), whereas image-only models are more generalizable (because they cannot memorize or extract keywords from history) and perform substantially better at the primary normalized discounted cumulative gain (NDCG) task metric which allows multiple correct answers. Hence, this observation encourages us to explicitly maintain two models, i.e., an image-only model and an image-history joint model, and combine their complementary abilities for a more balanced multimodal model. We present multiple methods for this integration of the two models, via ensemble and consensus dropout fusion with shared parameters. Empirically, our models achieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG and high balance across metrics), and substantially outperform the winner of the Visual Dialog challenge 2018 on most metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可视对话任务需要一个模型，同时利用图像和会话的上下文信息来生成到对话的下一个响应。然而，通过人工分析，我们发现了大量的对话问题只能由看图像，而不到上下文历史上的任何访问来回答，而其他人还需要对话上下文来预测正确的答案。我们表明，由于这个原因，以往合资模式（史和图像）模式过分依赖，而且更容易记住的对话记录（例如，通过上下文信息提取的关键字或模式），而只有图象模型更加普及（因为他们无法记住或者从历史中提取的关键字），并在主要贴现归累计收益（NDCG）任务指标，它允许多个正确答案大幅更好地履行。因此，这种观察鼓励我们要明确地保持两种模式，即只有一个影像的模型和图像的历史关节模型，并结合它们的互补能力，为一个更加平衡的多模式模型。我们提出了这种整合两个模型的多种方法，通过与共享参数合奏和共识辍学融合。根据经验，我们的模型实现对视觉对话挑战2019（关于NDCG和整个指标高平衡等级3）强劲的业绩，并基本跑赢视觉对话框挑战2018的大多数指标的赢家。</font>
</div>


<hr>
<div id="paper3"> <b>3. A Hybrid Solution to Learn Turn-Taking in Multi-Party Service-based Chat  Groups</b>  <a href="https://arxiv.org/pdf/2001.06350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Bayser%2C+M+G" target="_blank" rel="noopener" style="color:#0000EE;">Maira Gatti de Bayser</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guerra%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Melina Alberio Guerra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cavalin%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paulo Cavalin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pinhanez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Claudio Pinhanez</a><br>
<font size="3">
Abstract: To predict the next most likely participant to interact in a multi-party conversation is a difficult problem. In a text-based chat group, the only information available is the sender, the content of the text and the dialogue history. In this paper we present our study on how these information can be used on the prediction task through a corpus and architecture that integrates turn-taking classifiers based on Maximum Likelihood Expectation (MLE), Convolutional Neural Networks (CNN) and Finite State Automata (FSA). The corpus is a synthetic adaptation of the Multi-Domain Wizard-of-Oz dataset (MultiWOZ) to a multiple travel service-based bots scenario with dialogue errors and was created to simulate user's interaction and evaluate the architecture. We present experimental results which show that the CNN approach achieves better performance than the baseline with an accuracy of 92.34%, but the integrated solution with MLE, CNN and FSA achieves performance even better, with 95.65%. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为了预测下一个最有可能的参与者进行互动的多方通话是一个棘手的问题。在基于文本的聊天群，唯一可用的信息是发送者，文本和对话历史的内容。在本文中，我们介绍如何将这些信息可以在预测任务中使用通过语料库和架构，集成了转向回吐基于最大似然期望（MLE），卷积神经网络（CNN）和有限状态自动分类（我们的研究FSA ）。该语料库是多域向导的盎司数据集（MultiWOZ）与对话错误多个旅游服务为主的机器人场景的合成适应和创建来模拟用户的交互和评估体系结构。我们这表明，CNN方法实现比92.34％的准确度基准更好的性能，但与MLE，CNN和FSA集成的解决方案实现性能更为出色，有95.65％目前的实验结果。</font>
</div>


<hr>
<div id="paper4"> <b>4. RobBERT: a Dutch RoBERTa-based Language Model</b>  <a href="https://arxiv.org/pdf/2001.06286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Delobelle%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pieter Delobelle</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Winters%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Winters</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Berendt%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bettina Berendt</a><br>
<font size="3">
Abstract: Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT (Bi-directional Encoders for Transformers), which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies showed that BERT models trained on a single language significantly outperform the multilingual results. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train their Dutch BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We show that RobBERT improves state of the art results in Dutch-specific language tasks, and also outperforms other existing Dutch BERT-based models in sentiment analysis. These results indicate that RobBERT is a powerful pre-trained model for fine-tuning for a large variety of Dutch language tasks. We publicly release this pre-trained model in hope of supporting further downstream Dutch NLP applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预先训练语言模型已经主宰自然语言处理领域在最近几年，并导致显著的性能提升各种复杂的自然语言的任务。其中最突出的预先训练语言模型是BERT（变形金刚双向编码器），它被发布了作为一个英语和一个多语种的版本。虽然多语种BERT执行以及对许多任务，最近的研究显示，培训了一个单一的语言，BERT模型显著跑赢多种语言的结果。培训荷兰BERT模型因而具有广泛的荷兰NLP任务很大的潜力。虽然以前的方法已使用BERT的早期实现培养他们的荷兰BERT，我们使用了罗伯塔，一个稳健优化BERT的方法，培养所谓的RobBERT荷兰语言模型。我们发现，RobBERT改善状态荷兰人特有的语言任务的艺术效果，而且在情感分析优于其他现有的基于BERT荷模型。这些结果表明，RobBERT是微调功能强大的预先训练的模型种类繁多的荷兰语任务。我们在公开支持进一步的下游荷兰NLP应用希望释放此预先训练模式。</font>
</div>


<hr>
<div id="paper5"> <b>5. Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue  System</b>  <a href="https://arxiv.org/pdf/2001.06206" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun-Wei Chu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kuan-Yen Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chao-Chun Hsu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ku%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lun-Wei Ku</a><br>
<font size="3">
Abstract: Understanding dynamic scenes and dialogue contexts in order to converse with users has been challenging for multimodal dialogue systems. The 8-th Dialog System Technology Challenge (DSTC8) proposed an Audio Visual Scene-Aware Dialog (AVSD) task, which contains multiple modalities including audio, vision, and language, to evaluate how dialogue systems understand different modalities and response to users. In this paper, we proposed a multi-step joint-modality attention network (JMAN) based on recurrent neural network (RNN) to reason on videos. Our model performs a multi-step attention mechanism and jointly considers both visual and textual representations in each reasoning process to better integrate information from the two different modalities. Compared to the baseline released by AVSD organizers, our model achieves a relative 12.1% and 22.4% improvement over the baseline on ROUGE-L score and CIDEr score. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：了解动态场景和对话的上下文，以便与用户交谈已具有挑战性的多模态对话系统。 8个对话系统技术挑战（DSTC8）提出了一个视听场景感知对话框（AVSD）任务，其中包含多种方式，包括音频，视觉和语言，以评估对话系统是如何理解不同的方式和响应用户。在本文中，我们提出了一种基于递归神经网络（RNN）一个多步骤的联合方式关注网络（JMAN）理性上的视频。我们的模型进行多步注意机制，共同考虑在每个推理过程视觉和文本表示，以更好的信息从两种不同的方式进行整合。相比于通过AVSD主办方公布的基线，我们的模型实现了对ROUGE-L分和苹果酒得分基线相对12.1％和22.4％的改善。</font>
</div>


<hr>
<div id="paper6"> <b>6. Plato Dialogue System: A Flexible Conversational AI Research Platform</b>  <a href="https://arxiv.org/pdf/2001.06463" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Papangelis%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandros Papangelis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Namazifar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahdi Namazifar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khatri%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chandra Khatri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi-Chia Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Molino%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piero Molino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tur%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gokhan Tur</a><br>
<font size="3">
Abstract: As the field of Spoken Dialogue Systems and Conversational AI grows, so does the need for tools and environments that abstract away implementation details in order to expedite the development process, lower the barrier of entry to the field, and offer a common test-bed for new ideas. In this paper, we present Plato, a flexible Conversational AI platform written in Python that supports any kind of conversational agent architecture, from standard architectures to architectures with jointly-trained components, single- or multi-party interactions, and offline or online training of any conversational agent component. Plato has been designed to be easy to understand and debug and is agnostic to the underlying learning frameworks that train each component. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：口语对话系统和会话人工智能领域的增长，确实需要工具和环境，为了加快开发进程，降低进入该领域的障碍，并提供一个共同的测试 - 抽象掉的实施细则床上躺了新的思路。在本文中，我们目前柏拉图，灵活的对话AI平台用Python编写的，它支持任何类型的会话代理架构，从标准架构与联合训练的成分，单或多方互动，以及离线或在线培训体系任何会话代理组件。柏拉图已经被设计成易于理解和调试，并是不可知的是培养每个组件的基础学习框架。</font>
</div>


<hr>
<div id="paper7"> <b>7. Supervised Speaker Embedding De-Mixing in Two-Speaker Environment</b>  <a href="https://arxiv.org/pdf/2001.06397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanpei Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hain%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Hain</a><br>
<font size="3">
Abstract: In this work, a speaker embedding de-mixing approach is proposed. Instead of separating two-speaker signal in signal space like speech source separation, the proposed approach separates different speaker properties from two-speaker signal in embedding space. The proposed approach contains two steps. In step one, the clean speaker embeddings are learned and collected by a residual TDNN based network. In step two, the two-speaker signal and the embedding of one of the speakers are input to a speaker embedding de-mixing network. The de-mixing network is trained to generate the embedding of the other speaker of the by reconstruction loss. Speaker identification accuracy on the de-mixed speaker embeddings is used to evaluate the quality of the obtained embeddings. Experiments are done in two kind of data: artificial augmented two-speaker data (TIMIT) and real world recording of two-speaker data (MC-WSJ). Six diffident speaker embedding de-mixing architectures are investigated. Comparing with the speaker identification accuracy on the clean speaker embeddings (98.5%), the obtained results show that one of the speaker embedding de-mixing architectures obtain close performance, reaching 96.9% test accuracy on TIMIT when the SNR between the target speaker and interfering speaker is 5 dB. More surprisingly, we found choosing a simple subtraction as the embedding de-mixing function could obtain the second best performance, reaching 95.2% test accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，扬声器嵌入脱混合方法提出。代替在如语音源分离的信号分离空间两个扬声器信号的，所提出的方法分离两个扬声器信号在嵌入空间中的不同扬声器的特性。所提出的方法包括两个步骤。在第一步中，清洁扬声器的嵌入被学习和由残余基于TDNN网络收集。在步骤2中，两个扬声器信号和扬声器中的一个的嵌入被输入到扬声器中嵌入解混合网络。所述去混合网络进行训练，以产生的另一个扬声器的由重建丢失的嵌入。对解混合扬声器的嵌入扬声器识别精度被用于评估所获得的嵌入的质量。实验以两种类型的数据来完成：人工增强双扬声器数据（TIMIT）和双扬声器数据的真实世界记录（MC-WSJ）。六个心虚音箱嵌入脱混合体系结构进行了研究。与在干净的扬声器的嵌入扬声器识别精度（98.5％）相比较，所获得的结果表明，该扬声器中的一个嵌入脱混合架构获得紧密的性能，上TIMIT达到96.9％测试精度当目标讲话者和干扰之间的SNR扬声器为5dB。更令人惊讶的，我们发现选择一个简单的减法作为嵌入脱混合功能可以得到第二最佳性能，达到95.2％的测试精度。</font>
</div>


<hr>
<div id="paper8"> <b>8. On- Device Information Extraction from Screenshots in form of tags</b>  <a href="https://arxiv.org/pdf/2001.06094" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sumit Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramena%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gopi Ramena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manoj Goyal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohanty%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Debi Mohanty</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankur Agarwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Changmai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benu Changmai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moharana%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sukumar Moharana</a><br>
<font size="3">
Abstract: We propose a method to make mobile screenshots easily searchable. In this paper, we present the workflow in which we: 1) preprocessed a collection of screenshots, 2) identified script presentin image, 3) extracted unstructured text from images, 4) identifiedlanguage of the extracted text, 5) extracted keywords from the text, 6) identified tags based on image features, 7) expanded tag set by identifying related keywords, 8) inserted image tags with relevant images after ranking and indexed them to make it searchable on device. We made the pipeline which supports multiple languages and executed it on-device, which addressed privacy concerns. We developed novel architectures for components in the pipeline, optimized performance and memory for on-device computation. We observed from experimentation that the solution developed can reduce overall user effort and improve end user experience while searching, whose results are published. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议让移动截图易于搜索的方法。在本文中，我们提出我们在其中工作流：1）预处理截图的集合，2）识别的脚本presentin图像，3）提取从图像非结构化文本，4）提取的文本的identifiedlanguage，5）提取的关键词从文本，6）的基础上的图像特征识别的标签，7）膨胀通过识别相关的关键字标签集，8）与相关图像插入的图像标签的排名后和索引他们，使其可检索在设备上。我们做了哪些支持多种语言流水线开始执行它的设备，其中涉及隐私问题。我们开发新的架构在管线，优化的性能和内存设备上的计算组件。我们从实验观察到，解决方案开发可降低整体用户的努力和改善最终用户体验，同时搜索，其结果公布。</font>
</div>


<hr>
<div id="paper9"> <b>9. User-in-the-loop Adaptive Intent Detection for Instructable Digital  Assistant</b>  <a href="https://arxiv.org/pdf/2001.06007" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lair%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Lair</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Delgrange%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Clément Delgrange</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mugisha%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Mugisha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dussoux%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-Michel Dussoux</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oudeyer%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre-Yves Oudeyer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dominey%2C+P+F" target="_blank" rel="noopener" style="color:#0000EE;">Peter Ford Dominey</a><br>
<font size="3">
Abstract: People are becoming increasingly comfortable using Digital Assistants (DAs) to interact with services or connected objects. However, for non-programming users, the available possibilities for customizing their DA are limited and do not include the possibility of teaching the assistant new tasks. To make the most of the potential of DAs, users should be able to customize assistants by instructing them through Natural Language (NL). To provide such functionalities, NL interpretation in traditional assistants should be improved: (1) The intent identification system should be able to recognize new forms of known intents, and to acquire new intents as they are expressed by the user. (2) In order to be adaptive to novel intents, the Natural Language Understanding module should be sample efficient, and should not rely on a pretrained model. Rather, the system should continuously collect the training data as it learns new intents from the user. In this work, we propose AidMe (Adaptive Intent Detection in Multi-Domain Environments), a user-in-the-loop adaptive intent detection framework that allows the assistant to adapt to its user by learning his intents as their interaction progresses. AidMe builds its repertoire of intents and collects data to train a model of semantic similarity evaluation that can discriminate between the learned intents and autonomously discover new forms of known intents. AidMe addresses two major issues - intent learning and user adaptation - for instructable digital assistants. We demonstrate the capabilities of AidMe as a standalone system by comparing it with a one-shot learning system and a pretrained NLU module through simulations of interactions with a user. We also show how AidMe can smoothly integrate to an existing instructable digital assistant. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人们使用数字助理（DAS）与服务或连接的对象进行交互变得越来越舒适。然而，对于非编程的用户，定制自己的DA可用的可能性是有限的，不包括教学助理新任务的可能性。为了充分利用的DA的潜力，用户应该能够通过自然语言（NL），指示他们定制的助手。为了提供这样的功能，在传统的助理NL解释应加以改进：（1）意图识别系统应该能够识别已知的意图的新形式，因为它们是由用户表达了收购意向新。 （2）为了适应新的意图，所述自然语言理解模块应该是样品高效，并且不应该依赖于预训练的模型。相反，因为它学习来自用户的新意图，系统应不断收集训练数据。在这项工作中，我们提出AidMe（在多域环境自适应意图检测），用户在半实物自适应意图检测框架，允许助手通过学习他的意图及其互进步，以适应其用户。 AidMe建立其意图和收集数据的剧目来训练语义相似性评价的模型，可以和所学意图区分自主发现已知意图的新形式。 AidMe地址两大问题 - 意向学习和适应用户 - 对于造说明数字助理。我们通过将其与一次性学习系统，并通过与用户的交互的模拟预训练NLU模块比较表明AidMe的能力，作为一个独立的系统。我们还表明AidMe如何平滑地集成到现有的造说明数字助理。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>python gtts 文本转语音</title>
    <url>/2020/01/19/python-gtts-%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3/</url>
    <content><![CDATA[<h1 id="安装gtts"><a href="#安装gtts" class="headerlink" title="安装gtts"></a>安装gtts</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install gTTS</span><br></pre></td></tr></table></figure><h1 id="文本转语音"><a href="#文本转语音" class="headerlink" title="文本转语音"></a>文本转语音</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gtts <span class="keyword">import</span> gTTS</span><br><span class="line">tts = gTTS(text=<span class="string">"Hello World"</span>, lang=<span class="string">'en'</span>)</span><br><span class="line">tts.save(<span class="string">"helloworld.mp3"</span>)</span><br></pre></td></tr></table></figure><h1 id="播放语音"><a href="#播放语音" class="headerlink" title="播放语音"></a>播放语音</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.system(<span class="string">"start helloworld.mp3"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>tts</tag>
      </tags>
  </entry>
  <entry>
    <title>python 调用谷歌翻译接口</title>
    <url>/2020/01/19/python-%E8%B0%83%E7%94%A8%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91%E6%8E%A5%E5%8F%A3/</url>
    <content><![CDATA[<p>googletrans 是一个封装了谷歌翻译接口的python代码库，可以通过googletrans实现免费、无限制调用谷歌翻译接口。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install googletrans</span><br></pre></td></tr></table></figure><h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> googletrans <span class="keyword">import</span> Translator</span><br><span class="line">translator = Translator(service_urls=[</span><br><span class="line">      <span class="string">'translate.google.cn'</span>,</span><br><span class="line">      <span class="string">'translate.google.com'</span>])</span><br><span class="line">trans=translator.translate(<span class="string">'Hello World'</span>, src=<span class="string">'en'</span>, dest=<span class="string">'zh-cn'</span>)</span><br><span class="line"><span class="comment"># 原文</span></span><br><span class="line">print(trans.origin)</span><br><span class="line"><span class="comment"># 译文</span></span><br><span class="line">print(trans.text)</span><br></pre></td></tr></table></figure><a id="more"></a>




<h1 id="语种识别"><a href="#语种识别" class="headerlink" title="语种识别"></a>语种识别</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">detection=translator.detect(<span class="string">'All with Love'</span>)</span><br><span class="line">print(detection.lang)</span><br></pre></td></tr></table></figure>

<h1 id="语种缩略表示"><a href="#语种缩略表示" class="headerlink" title="语种缩略表示"></a>语种缩略表示</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">LANGUAGES = &#123;</span><br><span class="line">    <span class="string">'af'</span>: <span class="string">'afrikaans'</span>,</span><br><span class="line">    <span class="string">'sq'</span>: <span class="string">'albanian'</span>,</span><br><span class="line">    <span class="string">'am'</span>: <span class="string">'amharic'</span>,</span><br><span class="line">    <span class="string">'ar'</span>: <span class="string">'arabic'</span>,</span><br><span class="line">    <span class="string">'hy'</span>: <span class="string">'armenian'</span>,</span><br><span class="line">    <span class="string">'az'</span>: <span class="string">'azerbaijani'</span>,</span><br><span class="line">    <span class="string">'eu'</span>: <span class="string">'basque'</span>,</span><br><span class="line">    <span class="string">'be'</span>: <span class="string">'belarusian'</span>,</span><br><span class="line">    <span class="string">'bn'</span>: <span class="string">'bengali'</span>,</span><br><span class="line">    <span class="string">'bs'</span>: <span class="string">'bosnian'</span>,</span><br><span class="line">    <span class="string">'bg'</span>: <span class="string">'bulgarian'</span>,</span><br><span class="line">    <span class="string">'ca'</span>: <span class="string">'catalan'</span>,</span><br><span class="line">    <span class="string">'ceb'</span>: <span class="string">'cebuano'</span>,</span><br><span class="line">    <span class="string">'ny'</span>: <span class="string">'chichewa'</span>,</span><br><span class="line">    <span class="string">'zh-cn'</span>: <span class="string">'chinese (simplified)'</span>,</span><br><span class="line">    <span class="string">'zh-tw'</span>: <span class="string">'chinese (traditional)'</span>,</span><br><span class="line">    <span class="string">'co'</span>: <span class="string">'corsican'</span>,</span><br><span class="line">    <span class="string">'hr'</span>: <span class="string">'croatian'</span>,</span><br><span class="line">    <span class="string">'cs'</span>: <span class="string">'czech'</span>,</span><br><span class="line">    <span class="string">'da'</span>: <span class="string">'danish'</span>,</span><br><span class="line">    <span class="string">'nl'</span>: <span class="string">'dutch'</span>,</span><br><span class="line">    <span class="string">'en'</span>: <span class="string">'english'</span>,</span><br><span class="line">    <span class="string">'eo'</span>: <span class="string">'esperanto'</span>,</span><br><span class="line">    <span class="string">'et'</span>: <span class="string">'estonian'</span>,</span><br><span class="line">    <span class="string">'tl'</span>: <span class="string">'filipino'</span>,</span><br><span class="line">    <span class="string">'fi'</span>: <span class="string">'finnish'</span>,</span><br><span class="line">    <span class="string">'fr'</span>: <span class="string">'french'</span>,</span><br><span class="line">    <span class="string">'fy'</span>: <span class="string">'frisian'</span>,</span><br><span class="line">    <span class="string">'gl'</span>: <span class="string">'galician'</span>,</span><br><span class="line">    <span class="string">'ka'</span>: <span class="string">'georgian'</span>,</span><br><span class="line">    <span class="string">'de'</span>: <span class="string">'german'</span>,</span><br><span class="line">    <span class="string">'el'</span>: <span class="string">'greek'</span>,</span><br><span class="line">    <span class="string">'gu'</span>: <span class="string">'gujarati'</span>,</span><br><span class="line">    <span class="string">'ht'</span>: <span class="string">'haitian creole'</span>,</span><br><span class="line">    <span class="string">'ha'</span>: <span class="string">'hausa'</span>,</span><br><span class="line">    <span class="string">'haw'</span>: <span class="string">'hawaiian'</span>,</span><br><span class="line">    <span class="string">'iw'</span>: <span class="string">'hebrew'</span>,</span><br><span class="line">    <span class="string">'hi'</span>: <span class="string">'hindi'</span>,</span><br><span class="line">    <span class="string">'hmn'</span>: <span class="string">'hmong'</span>,</span><br><span class="line">    <span class="string">'hu'</span>: <span class="string">'hungarian'</span>,</span><br><span class="line">    <span class="string">'is'</span>: <span class="string">'icelandic'</span>,</span><br><span class="line">    <span class="string">'ig'</span>: <span class="string">'igbo'</span>,</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'indonesian'</span>,</span><br><span class="line">    <span class="string">'ga'</span>: <span class="string">'irish'</span>,</span><br><span class="line">    <span class="string">'it'</span>: <span class="string">'italian'</span>,</span><br><span class="line">    <span class="string">'ja'</span>: <span class="string">'japanese'</span>,</span><br><span class="line">    <span class="string">'jw'</span>: <span class="string">'javanese'</span>,</span><br><span class="line">    <span class="string">'kn'</span>: <span class="string">'kannada'</span>,</span><br><span class="line">    <span class="string">'kk'</span>: <span class="string">'kazakh'</span>,</span><br><span class="line">    <span class="string">'km'</span>: <span class="string">'khmer'</span>,</span><br><span class="line">    <span class="string">'ko'</span>: <span class="string">'korean'</span>,</span><br><span class="line">    <span class="string">'ku'</span>: <span class="string">'kurdish (kurmanji)'</span>,</span><br><span class="line">    <span class="string">'ky'</span>: <span class="string">'kyrgyz'</span>,</span><br><span class="line">    <span class="string">'lo'</span>: <span class="string">'lao'</span>,</span><br><span class="line">    <span class="string">'la'</span>: <span class="string">'latin'</span>,</span><br><span class="line">    <span class="string">'lv'</span>: <span class="string">'latvian'</span>,</span><br><span class="line">    <span class="string">'lt'</span>: <span class="string">'lithuanian'</span>,</span><br><span class="line">    <span class="string">'lb'</span>: <span class="string">'luxembourgish'</span>,</span><br><span class="line">    <span class="string">'mk'</span>: <span class="string">'macedonian'</span>,</span><br><span class="line">    <span class="string">'mg'</span>: <span class="string">'malagasy'</span>,</span><br><span class="line">    <span class="string">'ms'</span>: <span class="string">'malay'</span>,</span><br><span class="line">    <span class="string">'ml'</span>: <span class="string">'malayalam'</span>,</span><br><span class="line">    <span class="string">'mt'</span>: <span class="string">'maltese'</span>,</span><br><span class="line">    <span class="string">'mi'</span>: <span class="string">'maori'</span>,</span><br><span class="line">    <span class="string">'mr'</span>: <span class="string">'marathi'</span>,</span><br><span class="line">    <span class="string">'mn'</span>: <span class="string">'mongolian'</span>,</span><br><span class="line">    <span class="string">'my'</span>: <span class="string">'myanmar (burmese)'</span>,</span><br><span class="line">    <span class="string">'ne'</span>: <span class="string">'nepali'</span>,</span><br><span class="line">    <span class="string">'no'</span>: <span class="string">'norwegian'</span>,</span><br><span class="line">    <span class="string">'ps'</span>: <span class="string">'pashto'</span>,</span><br><span class="line">    <span class="string">'fa'</span>: <span class="string">'persian'</span>,</span><br><span class="line">    <span class="string">'pl'</span>: <span class="string">'polish'</span>,</span><br><span class="line">    <span class="string">'pt'</span>: <span class="string">'portuguese'</span>,</span><br><span class="line">    <span class="string">'pa'</span>: <span class="string">'punjabi'</span>,</span><br><span class="line">    <span class="string">'ro'</span>: <span class="string">'romanian'</span>,</span><br><span class="line">    <span class="string">'ru'</span>: <span class="string">'russian'</span>,</span><br><span class="line">    <span class="string">'sm'</span>: <span class="string">'samoan'</span>,</span><br><span class="line">    <span class="string">'gd'</span>: <span class="string">'scots gaelic'</span>,</span><br><span class="line">    <span class="string">'sr'</span>: <span class="string">'serbian'</span>,</span><br><span class="line">    <span class="string">'st'</span>: <span class="string">'sesotho'</span>,</span><br><span class="line">    <span class="string">'sn'</span>: <span class="string">'shona'</span>,</span><br><span class="line">    <span class="string">'sd'</span>: <span class="string">'sindhi'</span>,</span><br><span class="line">    <span class="string">'si'</span>: <span class="string">'sinhala'</span>,</span><br><span class="line">    <span class="string">'sk'</span>: <span class="string">'slovak'</span>,</span><br><span class="line">    <span class="string">'sl'</span>: <span class="string">'slovenian'</span>,</span><br><span class="line">    <span class="string">'so'</span>: <span class="string">'somali'</span>,</span><br><span class="line">    <span class="string">'es'</span>: <span class="string">'spanish'</span>,</span><br><span class="line">    <span class="string">'su'</span>: <span class="string">'sundanese'</span>,</span><br><span class="line">    <span class="string">'sw'</span>: <span class="string">'swahili'</span>,</span><br><span class="line">    <span class="string">'sv'</span>: <span class="string">'swedish'</span>,</span><br><span class="line">    <span class="string">'tg'</span>: <span class="string">'tajik'</span>,</span><br><span class="line">    <span class="string">'ta'</span>: <span class="string">'tamil'</span>,</span><br><span class="line">    <span class="string">'te'</span>: <span class="string">'telugu'</span>,</span><br><span class="line">    <span class="string">'th'</span>: <span class="string">'thai'</span>,</span><br><span class="line">    <span class="string">'tr'</span>: <span class="string">'turkish'</span>,</span><br><span class="line">    <span class="string">'uk'</span>: <span class="string">'ukrainian'</span>,</span><br><span class="line">    <span class="string">'ur'</span>: <span class="string">'urdu'</span>,</span><br><span class="line">    <span class="string">'uz'</span>: <span class="string">'uzbek'</span>,</span><br><span class="line">    <span class="string">'vi'</span>: <span class="string">'vietnamese'</span>,</span><br><span class="line">    <span class="string">'cy'</span>: <span class="string">'welsh'</span>,</span><br><span class="line">    <span class="string">'xh'</span>: <span class="string">'xhosa'</span>,</span><br><span class="line">    <span class="string">'yi'</span>: <span class="string">'yiddish'</span>,</span><br><span class="line">    <span class="string">'yo'</span>: <span class="string">'yoruba'</span>,</span><br><span class="line">    <span class="string">'zu'</span>: <span class="string">'zulu'</span>,</span><br><span class="line">    <span class="string">'fil'</span>: <span class="string">'Filipino'</span>,</span><br><span class="line">    <span class="string">'he'</span>: <span class="string">'Hebrew'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>官方手册: <a href="https://py-googletrans.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://py-googletrans.readthedocs.io/en/latest/</a></p>
]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>谷歌翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-17</title>
    <url>/2020/01/18/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-17/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Lexical Sememe Prediction using Dictionary Definitions by Capturing  Local Semantic Correspondence <a href="https://arxiv.org/pdf/2001.05954" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Speech Emotion Recognition Based on Multi-feature and Multi-lingual  Fusion <a href="https://arxiv.org/pdf/2001.05908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Comparing Rule-based, Feature-based and Deep Neural Methods for  De-identification of Dutch Medical Records <a href="https://arxiv.org/pdf/2001.05714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> A Pilot Study on Multiple Choice Machine Reading Comprehension for  Vietnamese Texts <a href="https://arxiv.org/pdf/2001.05687" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> AandP: Utilizing Prolog for converting between active sentence and  passive sentence with three-steps conversion <a href="https://arxiv.org/pdf/2001.05672" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Schema2QA: Answering Complex Queries on the Structured Web with a Neural  Model <a href="https://arxiv.org/pdf/2001.05609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Stereotypical Bias Removal for Hate Speech Detection Task using  Knowledge-based Generalizations <a href="https://arxiv.org/pdf/2001.05495" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> AggressionNet: Generalised Multi-Modal Deep Temporal and Sequential  Learning for Aggression Identification <a href="https://arxiv.org/pdf/2001.05493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> #MeToo on Campus: Studying College Sexual Assault at Scale Using Data  Reported on Social Media <a href="https://arxiv.org/pdf/2001.05970" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Show, Recall, and Tell: Image Captioning with Recall Mechanism <a href="https://arxiv.org/pdf/2001.05876" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> "Why is 'Chicago' deceptive?" Towards Building Model-Driven Tutorials  for Humans <a href="https://arxiv.org/pdf/2001.05871" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Ensemble based discriminative models for Visual Dialog Challenge 2018 <a href="https://arxiv.org/pdf/2001.05865" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Discoverability in Satellite Imagery: A Good Sentence is Worth a  Thousand Pictures <a href="https://arxiv.org/pdf/2001.05839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Document Network Projection in Pretrained Word Embedding Space <a href="https://arxiv.org/pdf/2001.05727" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Delving Deeper into the Decoder for Video Captioning <a href="https://arxiv.org/pdf/2001.05614" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Insertion-Deletion Transformer <a href="https://arxiv.org/pdf/2001.05540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Lexical Sememe Prediction using Dictionary Definitions by Capturing  Local Semantic Correspondence</b>  <a href="https://arxiv.org/pdf/2001.05954" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaju Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fanchao Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maosong Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyuan Liu</a><br>
<font size="3">
Abstract: Sememes, defined as the minimum semantic units of human languages in linguistics, have been proven useful in many NLP tasks. Since manual construction and update of sememe knowledge bases (KBs) are costly, the task of automatic sememe prediction has been proposed to assist sememe annotation. In this paper, we explore the approach of applying dictionary definitions to predicting sememes for unannotated words. We find that sememes of each word are usually semantically matched to different words in its dictionary definition, and we name this matching relationship local semantic correspondence. Accordingly, we propose a Sememe Correspondence Pooling (SCorP) model, which is able to capture this kind of matching to predict sememes. We evaluate our model and baseline methods on a famous sememe KB HowNet and find that our model achieves state-of-the-art performance. Moreover, further quantitative analysis shows that our model can properly learn the local semantic correspondence between sememes and words in dictionary definitions, which explains the effectiveness of our model. The source codes of this paper can be obtained from this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：义位，定义为语言学人类语言的语义的最小单位，已在许多自然语言处理的任务被证明是有用的。由于人工建设和义素知识库（KBS）更新是昂贵的，自动义原预测的任务已经提出，以协助义原注释。在本文中，我们将探讨采用字典的定义为预测未注释词义位的方法。我们发现，每个词的义位通常是在语义上在其字典上的定义匹配不同的话，我们命名此匹配关系当地语义对应。因此，我们提出了一个义位对应池（SCORP）模型，它能够捕捉到这种匹配预测义原。我们评估在一个著名的义原KB知网我们的模型和基线的方法和发现，我们的模型实现了国家的最先进的性能。此外，进一步的定量分析表明，我们的模型能够正确地学习字典定义义位与词之间的本地语义对应，这说明我们的模型的有效性。本文的源代码可以从该HTTPS URL来获得。</font>
</div>


<hr>
<div id="paper2"> <b>2. Speech Emotion Recognition Based on Multi-feature and Multi-lingual  Fusion</b>  <a href="https://arxiv.org/pdf/2001.05908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunyi Wang</a><br>
<font size="3">
Abstract: A speech emotion recognition algorithm based on multi-feature and Multi-lingual fusion is proposed in order to resolve low recognition accuracy caused by lack of large speech dataset and low robustness of acoustic features in the recognition of speech emotion. First, handcrafted and deep automatic features are extracted from existing data in Chinese and English speech emotions. Then, the various features are fused respectively. Finally, the fused features of different languages are fused again and trained in a classification model. Distinguishing the fused features with the unfused ones, the results manifest that the fused features significantly enhance the accuracy of speech emotion recognition algorithm. The proposed solution is evaluated on the two Chinese corpus and two English corpus, and is shown to provide more accurate predictions compared to original solution. As a result of this study, the multi-feature and Multi-lingual fusion algorithm can significantly improve the speech emotion recognition accuracy when the dataset is small. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于多特征和多语种的融合语音情感识别算法是为了解决由于缺乏大型数据集的讲话和在识别语音情感的声学特征低稳健的低识别精度提出。首先，手工和自动深特征在中国和英语演讲情绪现有的数据中提取。于是，各种功能都融合分别。最后，不同语言的熔断特性再次融合，并在分类模型训练。判定，非融合的，结果清单中的融合功能，融合功能显著增强语音情感识别算法的精度。提出的解决方案是在两名中国语料库和两个英语语料库进行评估，并显示相对于原来的解决方案，以提供更精确的预测。作为这项研究的结果是，多特征和多语种的融合算法可以显著提高语音情感识别的准确性如果数据集小。</font>
</div>


<hr>
<div id="paper3"> <b>3. Comparing Rule-based, Feature-based and Deep Neural Methods for  De-identification of Dutch Medical Records</b>  <a href="https://arxiv.org/pdf/2001.05714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Trienes%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Trienes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Trieschnigg%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dolf Trieschnigg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Seifert%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christin Seifert</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hiemstra%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Djoerd Hiemstra</a><br>
<font size="3">
Abstract: Unstructured information in electronic health records provide an invaluable resource for medical research. To protect the confidentiality of patients and to conform to privacy regulations, de-identification methods automatically remove personally identifying information from these medical records. However, due to the unavailability of labeled data, most existing research is constrained to English medical text and little is known about the generalizability of de-identification methods across languages and domains. In this study, we construct a varied dataset consisting of the medical records of 1260 patients by sampling data from 9 institutes and three domains of Dutch healthcare. We test the generalizability of three de-identification methods across languages and domains. Our experiments show that an existing rule-based method specifically developed for the Dutch language fails to generalize to this new data. Furthermore, a state-of-the-art neural architecture performs strongly across languages and domains, even with limited training data. Compared to feature-based and rule-based methods the neural method requires significantly less configuration effort and domain-knowledge. We make all code and pre-trained de-identification models available to the research community, allowing practitioners to apply them to their datasets and to enable future benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在电子健康记录的非结构化信息提供医学研究的宝贵资源。为了保护病人的保密性和符合隐私法规，去识别方法自动删除的个人识别这些医疗记录信息。然而，由于标签的数据，大多数现有的研究被限制在英国的医疗文本和小的不可有人知道的跨语言和领域去识别方法的普遍性。在这项研究中，我们构建了一个不同的数据集从9个院所和荷兰医疗保健的三个域采样数据组成的1260例患者的医疗记录。我们测试的跨语言，跨域三个去识别方法的普遍性。我们的实验表明，专门为荷兰语言开发现有的基于规则的方法不能推广到这个新的数据。此外，一个国家的最先进的神经结构进行强烈跨语言和域，即使在有限的训练数据。相比于基于规则的基于特征和方法，神经方法需要显著较少配置工作和领域的知识。我们让所有的代码和预先训练去标识模型提供给研究界，让从业者将它们应用到自己的数据集，以使未来的基准。</font>
</div>


<hr>
<div id="paper4"> <b>4. A Pilot Study on Multiple Choice Machine Reading Comprehension for  Vietnamese Texts</b>  <a href="https://arxiv.org/pdf/2001.05687" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Nguyen%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kiet Van Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+K+V" target="_blank" rel="noopener" style="color:#0000EE;">Khiem Vinh Tran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luu%2C+S+T" target="_blank" rel="noopener" style="color:#0000EE;">Son T. Luu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+A+G" target="_blank" rel="noopener" style="color:#0000EE;">Anh Gia-Tuan Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+N+L" target="_blank" rel="noopener" style="color:#0000EE;">Ngan Luu-Thuy Nguyen</a><br>
<font size="3">
Abstract: Machine Reading Comprehension (MRC) is the task of natural language processing which studies the ability to read and understand unstructured texts and then find the correct answers for questions. Until now, we have not yet had any MRC dataset for such a low-resource language as Vietnamese. In this paper, we introduce ViMMRC, a challenging machine comprehension corpus with multiple-choice questions, intended for research on the machine comprehension of Vietnamese text. This corpus includes 2,783 multiple-choice questions and answers based on a set of 417 Vietnamese texts used for teaching reading comprehension for 1st to 5th graders. Answers may be extracted from the contents of single or multiple sentences in the corresponding reading text. A thorough analysis of the corpus and experimental results in this paper illustrate that our corpus ViMMRC demands reasoning abilities beyond simple word matching. We proposed the method of Boosted Sliding Window (BSW) that improves 5.51% in accuracy over the best baseline method. We also measured human performance on the corpus and compared it to our MRC models. The performance gap between humans and our best experimental model indicates that significant progress can be made on Vietnamese machine reading comprehension in further research. The corpus is freely available at our website for research purposes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机阅读理解（MRC）是自然语言处理的哪些研究阅读和理解非结构化的文本，然后找到问题的正确答案的能力的任务。到现在为止，我们还没有过任何MRC数据集这样的低资源语言越南。在本文中，我们介绍ViMMRC，一个具有挑战性的机器理解语料库与多项选择题，供越南文本的机器理解研究。该文集包括基于一套用于教学阅读理解的1日至5年级学生417个越南文2783多项选择题及答案。答案可以从在相应的阅读文本单个或多个句子的内容被提取。本文的语料和实验结果的深入分析表明我们的语料库ViMMRC要求超出了简单的词语匹配的推理能力。我们提出的提振推拉窗（BSW）的，超过最佳基线法提高了精度5.51％的方法。我们还测量了语料库人的表现和它相比，我们的MRC模型。人类和我们最好的实验模型之间的性能差距表明，显著的进展可以在进一步研究越南机器阅读理解进行。该语料库是免费提供的，在我们的网站用于研究目的。</font>
</div>


<hr>
<div id="paper5"> <b>5. AandP: Utilizing Prolog for converting between active sentence and  passive sentence with three-steps conversion</b>  <a href="https://arxiv.org/pdf/2001.05672" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+T+Q" target="_blank" rel="noopener" style="color:#0000EE;">Trung Q. Tran</a><br>
<font size="3">
Abstract: I introduce a simple but efficient method to solve one of the critical aspects of English grammar which is the relationship between active sentence and passive sentence. In fact, an active sentence and its corresponding passive sentence express the same meaning, but their structure is different. I utilized Prolog [4] along with Definite Clause Grammars (DCG) [5] for doing the conversion between active sentence and passive sentence. Some advanced techniques were also used such as Extra Arguments, Extra Goals, Lexicon, etc. I tried to solve a variety of cases of active and passive sentences such as 12 English tenses, modal verbs, negative form, etc. More details and my contributions will be presented in the following sections. The source code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我介绍一个简单的，但要解决的英语语法的关键方面是主动句和被动句之间的关系的一个有效的方法。事实上，一个主动句和其对应的被动句表达同一个意思，但它们的结构是不同的。我使用的Prolog [4]与定条款文法（DCG）[5]这样做主动句和被动句子之间的转换沿。一些先进的技术，还使用了诸如额外的参数，额外的目标，词汇，等我试图解决各种主动和被动句等12个英文时态，情态动词，否定形式等更多细节和我的贡献的情况下将在下面的章节中介绍。源代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper6"> <b>6. Schema2QA: Answering Complex Queries on the Structured Web with a Neural  Model</b>  <a href="https://arxiv.org/pdf/2001.05609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Silei Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Campagna%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giovanni Campagna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Monica S. Lam</a><br>
<font size="3">
Abstract: Virtual assistants today require every website to submit skills individually into their proprietary repositories. The skill consists of a fixed set of supported commands and the formal representation of each command. The assistants use the contributed data to create a proprietary linguistic interface, typically using an intent classifier. This paper proposes an open-source toolkit, called Schema2QA, that leverages the this http URL markup found in many websites to automatically build skills. Schema2QA has several advantages: (1) Schema2QA handles compositional queries involving multiple fields automatically, such as "find the Italian restaurant around here with the most reviews", or "what W3C employees on LinkedIn went to Oxford"; (2) Schema2QA translates natural language into executable queries on the up-to-date data from the website; (3) natural language training can be applied to one domain at a time to handle multiple websites using the same this http URL representations. We apply Schema2QA to two different domains, showing that the skills we built can answer useful queries with little manual effort. Our skills achieve an overall accuracy between 74% and 78%, and can answer questions that span three or more properties with 65% accuracy. We also show that a new domain can be supported by transferring knowledge. The open-source Schema2QA lets each website create and own its linguistic interface. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虚拟助理都要求每一个网站提交技巧单独为他们的专利库。技能由一组固定的支持的命令和各命令的正式表示。助手用提供的数据以创建一个专有的语言接口，通常使用的意图分类器。本文提出了一种开放源代码工具包，叫做Schema2QA，即利用了这个在很多网站上找到的自动构建技术HTTP URL标记。 Schema2QA有以下几个优点：（1）Schema2QA自动处理涉及多个领域组成的查询，如“找到意大利餐厅这里与大多数评论围绕”或“去牛津大学在LinkedIn什么W3C员工”; （2）Schema2QA转换自然语言转换为可执行的查询从网站上的最新数据; （3）自然语言培训可以同时被应用到一个域中使用相同的这个HTTP URL交涉处理多个网站。我们应用Schema2QA于两个不同的领域，显示出我们建立了技能可以回答很少的手动工作有用的查询。我们的技能达到74％和78％之间的整体精度，并能回答这个跨越65％的准确率三个或更多的性能问题。我们还表明，一个新的域可以通过知识转移的支持。开源Schema2QA让每个网站创建和拥有自己的语言界面。</font>
</div>


<hr>
<div id="paper7"> <b>7. Stereotypical Bias Removal for Hate Speech Detection Task using  Knowledge-based Generalizations</b>  <a href="https://arxiv.org/pdf/2001.05495" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Badjatiya%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pinkesh Badjatiya</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manish Gupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Varma%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasudeva Varma</a><br>
<font size="3">
Abstract: With the ever-increasing cases of hate spread on social media platforms, it is critical to design abuse detection mechanisms to proactively avoid and control such incidents. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. Bias removal has been traditionally studied for structured datasets, but we aim at bias mitigation from unstructured text data. In this paper, we make two important contributions. First, we systematically design methods to quantify the bias for any model and propose algorithms for identifying the set of words which the model stereotypes. Second, we propose novel methods leveraging knowledge-based generalizations for bias-free learning. Knowledge-based generalization provides an effective way to encode knowledge because the abstraction they provide not only generalizes content but also facilitates retraction of information from the hate speech detection classifier, thereby reducing the imbalance. We experiment with multiple knowledge generalization policies and analyze their effect on general performance and in mitigating bias. Our experiments with two real-world datasets, a Wikipedia Talk Pages dataset (WikiDetox) of size ~96k and a Twitter dataset of size ~24k, show that the use of knowledge-based generalizations results in better performance by forcing the classifier to learn from generalized content. Our methods utilize existing knowledge-bases and can easily be extended to other tasks </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着不断增加的社会化媒体平台上传播仇恨的情况下，它是设计滥用检测手段，积极主动规避和控制此类事件的关键。虽然存在仇恨言论的检测方法，他们刻板印象的话，因此从本质上偏向训练受到影响。偏置消除历来被研究了结构化数据集，但我们的目标是从非结构化的文本数据缓解偏差。在本文中，我们提出两个重要的贡献。首先，我们系统的设计方法，以量化的任何模型的偏差，提出的算法识别词集该模型定型。第二，我们提出了新的方法利用知识为基础的概括为无偏差的学习。基于知识的推广提供了一个有效的方式来编码知识，因为他们提供的不只是抽象概括的内容，但也有利于信息回缩从仇恨言论检测分类，从而减少不平衡。我们与多个知识推广政策实验和分析整体性能和减轻他们的偏见的影响。我们有两个现实世界的尺寸〜96K的数据集，维基百科对话页数据集（WikiDetox）和大小的Twitter的数据集〜24K，表明通过强制分类在更好的性能使用基于知识的概括的结果，从学习实验广义的内容。我们的方法利用现有的知识基地，可以很容易地扩展到其他任务</font>
</div>


<hr>
<div id="paper8"> <b>8. AggressionNet: Generalised Multi-Modal Deep Temporal and Sequential  Learning for Aggression Identification</b>  <a href="https://arxiv.org/pdf/2001.05493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anant Khandelwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Niraj Kumar</a><br>
<font size="3">
Abstract: Wide usage of social media platforms has increased the risk of aggression, which results in mental stress and affects the lives of people negatively like psychological agony, fighting behavior, and disrespect to others. Majority of such conversations contains code-mixed languages[28]. Additionally, the way used to express thought or communication style also changes from one social media plat-form to another platform (e.g., communication styles are different in twitter and Facebook). These all have increased the complexity of the problem. To solve these problems, we have introduced a unified and robust multi-modal deep learning architecture which works for English code-mixed dataset and uni-lingual English dataset both.The devised system, uses psycho-linguistic features and very ba-sic linguistic features. Our multi-modal deep learning architecture contains, Deep Pyramid CNN, Pooled BiLSTM, and Disconnected RNN(with Glove and FastText embedding, both). Finally, the system takes the decision based on model averaging. We evaluated our system on English Code-Mixed TRAC 2018 dataset and uni-lingual English dataset obtained from Kaggle. Experimental results show that our proposed system outperforms all the previous approaches on English code-mixed dataset and uni-lingual English dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：社会化媒体平台，用途广泛增加侵略的风险，这会导致精神压力和负面影响的人们的生活就像心理上的痛苦，战斗行为，和不尊重他人。这样的对话的大多数包含代码混合语言[28]。此外，该方法用来表达思想或沟通方式也从一个社交媒体平台，改变到另一个平台（例如，沟通方式是在Twitter和Facebook有所不同）。这些都增加了问题的复杂性。为了解决这些问题，我们引入了一个统一和强大的多模态深度学习架构，适用于英语代码混合数据集和单语种英语数据集both.The设计系统，采用心理语言特征和非常BA-SIC语言特征。我们的多模态深度学习架构包含，深金字塔CNN，汇集BiLSTM，并断开RNN（带手套和FastText嵌入，两者）。最后，该系统采用基于模型平均的决定。我们评估了英语代码混合TRAC 2018数据集，并从Kaggle获得单语种英语数据集我们的系统。实验结果表明，该系统优于所有英语代码混合数据集和单语种英语数据集以前的方法。</font>
</div>


<hr>
<div id="paper9"> <b>9. #MeToo on Campus: Studying College Sexual Assault at Scale Using Data  Reported on Social Media</b>  <a href="https://arxiv.org/pdf/2001.05970" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Duong%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Viet Duong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phu Pham</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bose%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ritwik Bose</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiebo Luo</a><br>
<font size="3">
Abstract: Recently, the emergence of the #MeToo trend on social media has empowered thousands of people to share their own sexual harassment experiences. This viral trend, in conjunction with the massive personal information and content available on Twitter, presents a promising opportunity to extract data driven insights to complement the ongoing survey based studies about sexual harassment in college. In this paper, we analyze the influence of the #MeToo trend on a pool of college followers. The results show that the majority of topics embedded in those #MeToo tweets detail sexual harassment stories, and there exists a significant correlation between the prevalence of this trend and official reports on several major geographical regions. Furthermore, we discover the outstanding sentiments of the #MeToo tweets using deep semantic meaning representations and their implications on the affected users experiencing different types of sexual harassment. We hope this study can raise further awareness regarding sexual misconduct in academia. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，在社会化媒体的#MeToo趋势的出现已经授权成千上万的人分享自己的性骚扰经历。这种病毒发展趋势，结合大量的个人信息，并在Twitter上可用内容，提出了一个有前途的机会抽取数据的深入分析，以补充有关大学性骚扰正在进行的调查为基础的研究。在本文中，我们分析了对高校追随者池#MeToo趋势的影响。结果显示，大部分嵌入在这些#MeToo主题的鸣叫细节性骚扰的故事，并且存在几大地理区域这一趋势，官方报告的患病率之间的相关性显著。此外，我们发现使用深层语义表述及其对受影响的用户体验不同类型的性骚扰影响的#MeToo鸣叫的优秀情绪。我们希望这项研究能提高公众对学术界的性行为不端进一步的认识。</font>
</div>


<hr>
<div id="paper10"> <b>10. Show, Recall, and Tell: Image Captioning with Recall Mechanism</b>  <a href="https://arxiv.org/pdf/2001.05876" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zechen Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yonghua Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongtao Lu</a><br>
<font size="3">
Abstract: Generating natural and accurate descriptions in image cap-tioning has always been a challenge. In this paper, we pro-pose a novel recall mechanism to imitate the way human con-duct captioning. There are three parts in our recall mecha-nism : recall unit, semantic guide (SG) and recalled-wordslot (RWS). Recall unit is a text-retrieval module designedto retrieve recalled words for images. SG and RWS are de-signed for the best use of recalled words. SG branch cangenerate a recalled context, which can guide the process ofgenerating caption. RWS branch is responsible for copyingrecalled words to the caption. Inspired by pointing mecha-nism in text summarization, we adopt a soft switch to balancethe generated-word probabilities between SG and RWS. Inthe CIDEr optimization step, we also introduce an individualrecalled-word reward (WR) to boost training. Our proposedmethods (SG+RWS+WR) achieve BLEU-4 / CIDEr / SPICEscores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 /129.1 / 22.4 with CIDEr optimization on MSCOCO Karpathytest split, which surpass the results of other state-of-the-artmethods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生成自然和图像帽tioning准确的描述一直是一个挑战。在本文中，我们亲姿势模仿的方式人类CON-管字幕一种新的召回机制。有三个部分在我们的回忆机甲-NISM：召回单位，语义指南（SG），并回顾-wordslot（RWS）。召回单元是文本的检索模块designedto检索图像召回的话。 SG和RWS被解签订了最好的使用被召回的话。 SG分支cangenerate一个回顾上下文，其可以引导过程ofgenerating字幕。 RWS分公司负责copyingrecalled字标题。通过指向文本摘要机甲-NISM启发，我们采用软切换至SG和RWS之间balancethe产生字概率。在矿井苹果酒优化步骤，我们还引入individualrecalled字奖励（WR），以提升培训。我们的proposedmethods（SG + RWS + WR）实现BLEU-4 /苹果酒/ 36.6 / 116.9 / 21.3与交叉熵损失和38.7 /129.1 /苹果酒优化上MSCOCO Karpathytest分裂22.4，这超越其他状态 - 的结果SPICEscores的最artmethods。</font>
</div>


<hr>
<div id="paper11"> <b>11. "Why is 'Chicago' deceptive?" Towards Building Model-Driven Tutorials  for Humans</b>  <a href="https://arxiv.org/pdf/2001.05871" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vivian Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenhao Tan</a><br>
<font size="3">
Abstract: To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a training phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically selected examples from training data with explanations. We use deceptive review detection as a testbed and conduct large-scale, randomized human-subject experiments to examine the effectiveness of such tutorials. We find that tutorials indeed improve human performance, with and without real-time assistance. In particular, although deep learning provides superior predictive performance than simple models, tutorials and explanations from simple models are more useful to humans. Our work suggests future directions for human-centered tutorials and explanations towards a synergy between humans and AI. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为支持与机器学习模型人的决策，我们经常需要嵌入是unsalient的，未知的，或者违反直觉的人类模型阐发模式。虽然现有的方法重点讲解机器的预测具有实时援助，我们探讨模型驱动的教程，以帮助人们了解一个训练阶段这些模式。我们认为，从科学论文，类似于科学传播的现行做法，并从解释训练数据自动选择的例子准则都教程。我们使用欺骗性的审查检测作为测试平台，并进行大规模，随机人体学科实验来检验这种教程的效果。我们发现，确实教程改善人类的性能，使用和不使用实时的援助。特别是，虽然深度学习不是简单的模型，教程和简单模型的解释提供了卓越的预测性能对人体更有益。我们的工作提出了以人为本对人类和人工智能之间的协同作用的教程和说明未来的发展方向。</font>
</div>


<hr>
<div id="paper12"> <b>12. Ensemble based discriminative models for Visual Dialog Challenge 2018</b>  <a href="https://arxiv.org/pdf/2001.05865" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shubham Agarwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raghav Goyal</a><br>
<font size="3">
Abstract: This manuscript describes our approach for the Visual Dialog Challenge 2018. We use an ensemble of three discriminative models with different encoders and decoders for our final submission. Our best performing model on 'test-std' split achieves the NDCG score of 55.46 and the MRR value of 63.77, securing third position in the challenge. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本稿件描述了我们的视觉对话挑战2018年我们使用三种判别模型不同的编码器和解码器为我们最后提交的集成方法。在“测试-STD”分裂我们的最佳表现模型达到NDCG得分55.46和63.77的MRR的价值，确保在挑战第三的位置。</font>
</div>


<hr>
<div id="paper13"> <b>13. Discoverability in Satellite Imagery: A Good Sentence is Worth a  Thousand Pictures</b>  <a href="https://arxiv.org/pdf/2001.05839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Noever%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Noever</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Regian%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wes Regian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ciolino%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matt Ciolino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kalin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Josh Kalin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hambrick%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dom Hambrick</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Blankenship%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaye Blankenship</a><br>
<font size="3">
Abstract: Small satellite constellations provide daily global coverage of the earth's landmass, but image enrichment relies on automating key tasks like change detection or feature searches. For example, to extract text annotations from raw pixels requires two dependent machine learning models, one to analyze the overhead image and the other to generate a descriptive caption. We evaluate seven models on the previously largest benchmark for satellite image captions. We extend the labeled image samples five-fold, then augment, correct and prune the vocabulary to approach a rough min-max (minimum word, maximum description). This outcome compares favorably to previous work with large pre-trained image models but offers a hundred-fold reduction in model size without sacrificing overall accuracy (when measured with log entropy loss). These smaller models provide new deployment opportunities, particularly when pushed to edge processors, on-board satellites, or distributed ground stations. To quantify a caption's descriptiveness, we introduce a novel multi-class confusion or error matrix to score both human-labeled test data and never-labeled images that include bounding box detection but lack full sentence captions. This work suggests future captioning strategies, particularly ones that can enrich the class coverage beyond land use applications and that lessen color-centered and adjacency adjectives ("green", "near", "between", etc.). Many modern language transformers present novel and exploitable models with world knowledge gleaned from training from their vast online corpus. One interesting, but easy example might learn the word association between wind and waves, thus enriching a beach scene with more than just color descriptions that otherwise might be accessed from raw pixels without text annotation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：小卫星星座提供地球陆地面积的每日全球覆盖，但图像富集依赖于自动化样改变检测或功能的搜索关键任务。例如，为了从原始像素提取文本注释需要两个依赖机器学习模型，一个分析俯视图像和其他，以产生描述性标题。我们评估对卫星图片说明以前最大的基准七款车型。我们扩展了标记图像样本五倍，然后扩充的，正确的和修剪词汇接近粗糙最小 - 最大（最小字，最大的描述）。这一结果相比毫不逊色与大预先训练的图像模型，但提供的模型大小百倍还原之前的工作不牺牲整体精度（当日志熵损失测量）。这些小模型提供了新的部署机会，特别是当推到边缘处理器，板载卫星，或分布式地面站。为了量化标题的描述性，我们引入了一个新的多类混淆或错误矩阵得分人类标记的测试数据，而不会标记的图像，包括边框检测，但缺乏完整的句子标题。这项工作表明未来字幕战略，特别是那些能充实类覆盖率超过土地使用的应用程序和减轻色心和邻接的形容词（“绿色”，“近”，“间”等）。许多现代语言的变压器存在新颖性和与世界的知识利用的模型从训练中收集来自其庞大的在线语料库。一个有趣的，但简单的例子可以学习乘风破浪的词语联想，从而丰富海滩场景比，否则可能从原始像素进行访问，而不文本注释只是颜色的描述更多。</font>
</div>


<hr>
<div id="paper14"> <b>14. Document Network Projection in Pretrained Word Embedding Space</b>  <a href="https://arxiv.org/pdf/2001.05727" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gourru%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antoine Gourru</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guille%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrien Guille</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Velcin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Velcin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jacques%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Jacques</a><br>
<font size="3">
Abstract: We present Regularized Linear Embedding (RLE), a novel method that projects a collection of linked documents (e.g. citation network) into a pretrained word embedding space. In addition to the textual content, we leverage a matrix of pairwise similarities providing complementary information (e.g., the network proximity of two documents in a citation graph). We first build a simple word vector average for each document, and we use the similarities to alter this average representation. The document representations can help to solve many information retrieval tasks, such as recommendation, classification and clustering. We demonstrate that our approach outperforms or matches existing document network embedding methods on node classification and link prediction tasks. Furthermore, we show that it helps identifying relevant keywords to describe document classes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出正则线性嵌入（RLE），一种新型的方法，其项目链接的文档（例如引网络）的集合到一个预训练的字嵌入空间。除了文本内容，我们利用成对的相似性提供补充信息（例如，在引用图两个文件的网络接近）的基质中。我们首先建立每个文档的简单词汇向量平均，而我们使用的相似改变这种平均表示。该文件表示可以帮助解决许多信息检索任务，如推荐，分类和聚类。我们证明我们的方法比或匹配现有的文档嵌入网络节点上的分类和链接预测任务的方法。此外，我们表明，它可以帮助识别相关关键字来描述文档类。</font>
</div>


<hr>
<div id="paper15"> <b>15. Delving Deeper into the Decoder for Video Captioning</b>  <a href="https://arxiv.org/pdf/2001.05614" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoran Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianmin Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Hu</a><br>
<font size="3">
Abstract: Video captioning is an advanced multi-modal task which aims to describe a video clip using a natural language sentence. The encoder-decoder framework is the most popular paradigm for this task in recent years. However, there still exist some non-negligible problems in the decoder of a video captioning model. We make a thorough investigation into the decoder and adopt three techniques to improve the performance of the model. First of all, a combination of variational dropout and layer normalization is embedded into a recurrent unit to alleviate the problem of overfitting. Secondly, a new method is proposed to evaluate the performance of a model on a validation set so as to select the best checkpoint for testing. Finally, a new training strategy called \textit{professional learning} is proposed which develops the strong points of a captioning model and bypasses its weaknesses. It is demonstrated in the experiments on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets that our model has achieved the best results evaluated by BLEU, CIDEr, METEOR and ROUGE-L metrics with significant gains of up to 11.7% on MSVD and 5% on MSR-VTT compared with the previous state-of-the-art models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频字幕是一种先进的多模态的任务，目的是描述使用自然语言句子的视频剪辑。编码器，解码器框架是在最近几年，这一任务最流行的范例。但是，仍然存在着一个视频字幕模型的解码器的一些不可忽视的问题。我们做一个彻底的调查，解码器，并采用三种技术来提高模型的性能。首先，变差和层正常化的组合被嵌入到一个重复单元，以减轻过拟合问题。其次，新方法，提出了在验证集评估模型的性能，以便选择最佳的检查点进行测试。最后，新的培训战略称为\ textit {专业学习}，提出了开发一个字幕模型的长处，避开其弱点。它证明了在微软研究院的视频描述语料库（MSVD）和MSR视频的实验文本（MSR-VTT）的数据集，我们的模型已经实现由BLEU，苹果酒，流星和ROUGE-L指标评估了显著收益最好的结果高达11.7％的MSVD和5％的MSR-VTT与以前国家的最先进的机型相比。</font>
</div>


<hr>
<div id="paper16"> <b>16. Insertion-Deletion Transformer</b>  <a href="https://arxiv.org/pdf/2001.05540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruis%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laura Ruis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mitchell Stern</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Proskurnia%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julia Proskurnia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Chan</a><br>
<font size="3">
Abstract: We propose the Insertion-Deletion Transformer, a novel transformer-based neural architecture and training method for sequence generation. The model consists of two phases that are executed iteratively, 1) an insertion phase and 2) a deletion phase. The insertion phase parameterizes a distribution of insertions on the current output hypothesis, while the deletion phase parameterizes a distribution of deletions over the current output hypothesis. The training method is a principled and simple algorithm, where the deletion model obtains its signal directly on-policy from the insertion model output. We demonstrate the effectiveness of our Insertion-Deletion Transformer on synthetic translation tasks, obtaining significant BLEU score improvement over an insertion-only model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出的插入缺失变压器，一种新型的基于变压器的神经结构和序列生成训练方法。该模型由被迭代执行两个阶段：1）的插入相位和2）的删除相。插入阶段参数化对电流输出假设插入的分布，而删除相位参数化缺失的过电流输出假设的分布。该训练方法是一个原则性和简单的算法，其中该删除模型直接获得关于策略从插入模型输出它的信号。我们证明我们的插入缺失变压器上合成翻译任务的有效性，通过一个只有插入模型取得显著BLEU评分改善。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>一些公司及高校在线翻译系统</title>
    <url>/2020/01/17/%E4%B8%80%E4%BA%9B%E5%85%AC%E5%8F%B8%E5%8F%8A%E9%AB%98%E6%A0%A1%E5%9C%A8%E7%BA%BF%E7%BF%BB%E8%AF%91%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h1 id="公司在线翻译系统"><a href="#公司在线翻译系统" class="headerlink" title="公司在线翻译系统"></a>公司在线翻译系统</h1><ul>
<li><a href="https://fanyi.baidu.com/" target="_blank" rel="noopener">百度翻译</a></li>
<li><a href="https://translate.google.cn/" target="_blank" rel="noopener">谷歌翻译</a></li>
<li><a href="http://fanyi.youdao.com/" target="_blank" rel="noopener">有道翻译</a></li>
<li><a href="https://fanyi.qq.com/" target="_blank" rel="noopener">腾讯翻译君</a></li>
<li><a href="https://fanyi.sogou.com/" target="_blank" rel="noopener">搜狗翻译</a></li>
<li><a href="https://niutrans.vip/trans" target="_blank" rel="noopener">小牛翻译</a></li>
<li><a href="https://cloudtranslation.com/online/" target="_blank" rel="noopener">云译</a></li>
</ul><h1 id="高校在线翻译系统"><a href="#高校在线翻译系统" class="headerlink" title="高校在线翻译系统"></a>高校在线翻译系统</h1><ul>
<li><a href="http://nmt.xmu.edu.cn/" target="_blank" rel="noopener">厦门大学</a></li>
</ul>]]></content>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-16</title>
    <url>/2020/01/17/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-16/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses <a href="https://arxiv.org/pdf/2001.05467" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> A BERT based Sentiment Analysis and Key Entity Detection Approach for  Online Financial Texts <a href="https://arxiv.org/pdf/2001.05326" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Authorship Attribution in Bangla literature using Character-level CNN <a href="https://arxiv.org/pdf/2001.05316" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> A Continuous Space Neural Language Model for Bengali Language <a href="https://arxiv.org/pdf/2001.05315" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Embedding Compression with Isotropic Iterative Quantization <a href="https://arxiv.org/pdf/2001.05314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Tensor Graph Convolutional Networks for Text Classification <a href="https://arxiv.org/pdf/2001.05313" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process  Approach to Linguistic Relationships <a href="https://arxiv.org/pdf/2001.05297" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Urdu-English Machine Transliteration using Neural Networks <a href="https://arxiv.org/pdf/2001.05296" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Language Models Are An Effective Patient Representation Learning  Technique For Electronic Health Record Data <a href="https://arxiv.org/pdf/2001.05295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> The empirical structure of word frequency distributions <a href="https://arxiv.org/pdf/2001.05292" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Exploring and Improving Robustness of Multi Task Deep Neural Networks  via Domain Agnostic Defenses <a href="https://arxiv.org/pdf/2001.05286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Detecting New Word Meanings: A Comparison of Word Embedding Models in  Spanish <a href="https://arxiv.org/pdf/2001.05285" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Improving Spoken Language Understanding By Exploiting ASR N-best  Hypotheses <a href="https://arxiv.org/pdf/2001.05284" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> FGN: Fusion Glyph Network for Chinese Named Entity Recognition <a href="https://arxiv.org/pdf/2001.05272" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation <a href="https://arxiv.org/pdf/2001.05139" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Parallel Machine Translation with Disentangled Context Transformer <a href="https://arxiv.org/pdf/2001.05136" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Robust Speaker Recognition Using Speech Enhancement And Attention Model <a href="https://arxiv.org/pdf/2001.05031" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> A Tree Adjoining Grammar Representation for Models Of Stochastic  Dynamical Systems <a href="https://arxiv.org/pdf/2001.05320" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Auto Completion of User Interface Layout Design Using Transformer-Based  Tree Decoders <a href="https://arxiv.org/pdf/2001.05308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Teddy: A System for Interactive Review Analysis <a href="https://arxiv.org/pdf/2001.05171" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Modeling Product Search Relevance in e-Commerce <a href="https://arxiv.org/pdf/2001.04980" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses</b>  <a href="https://arxiv.org/pdf/2001.05467" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Niu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Bansal</a><br>
<font size="3">
Abstract: Many sequence-to-sequence dialogue models tend to generate safe, uninformative responses. There have been various useful efforts on trying to eliminate them. However, these approaches either improve decoding algorithms during inference, rely on hand-crafted features, or employ complex models. In our work, we build dialogue models that are dynamically aware of what utterances or tokens are dull without any feature-engineering. Specifically, we start with a simple yet effective automatic metric, AvgOut, which calculates the average output probability distribution of all time steps on the decoder side during training. This metric directly estimates which tokens are more likely to be generated, thus making it a faithful evaluation of the model diversity (i.e., for diverse models, the token probabilities should be more evenly distributed rather than peaked at a few dull tokens). We then leverage this novel metric to propose three models that promote diversity without losing relevance. The first model, MinAvgOut, directly maximizes the diversity score through the output distributions of each batch; the second model, Label Fine-Tuning (LFT), prepends to the source sequence a label continuously scaled by the diversity score to control the diversity level; the third model, RL, adopts Reinforcement Learning and treats the diversity score as a reward signal. Moreover, we experiment with a hybrid model by combining the loss terms of MinAvgOut and RL. All four models outperform their base LSTM-RNN model on both diversity and relevance by a large margin, and are comparable to or better than competitive baselines (also verified via human evaluation). Moreover, our approaches are orthogonal to the base model, making them applicable as an add-on to other emerging better dialogue models in the future. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：许多序列对序列的对话模式容易产生安全，无信息的响应。已经有上试图消除他们各种有用的努力。然而，这些方法或者改进的推理过程中的解码算法，依靠手工制作的功能，或采用复杂的模型。在我们的工作，我们建立对话模型是动态意识到什么话语或令牌是没有任何功能的工程平淡。具体而言，我们先从一个简单而有效的自动度量，AvgOut，其在训练期间计算出的解码器侧的所有时间步长的平均输出概率分布。该指标直接估计令牌更容易产生，从而使得它的型号多样的忠实评价（即，对于不同的车型，令牌的概率应该是更均匀地分布，而不是在几个沉闷的令牌见顶）。然后，我们利用这一新的指标，提出促进多样性不失相关性三种模式。第一种模式，MinAvgOut，直接通过最大化每批的输出分布的分集比分;第二个模型，标签微调（LFT），前置到源序列通过分集比分来控制分集电平连续缩放的标签;第三种模式，RL，采用强化学习和对待多样性分数作为奖励信号。此外，我们结合MinAvgOut和RL的损失方面与混合动力模型试验。所有这四种型号跑赢上都多样性和实用性大幅度的基地LSTM-RNN模型，并比竞争基准（也可以通过人工评估验证）相当或更好。此外，我们的方法是正交的示范基地，使它们适用于作为一个附加在未来其他新兴更好的对话模式。</font>
</div>


<hr>
<div id="paper2"> <b>2. A BERT based Sentiment Analysis and Key Entity Detection Approach for  Online Financial Texts</b>  <a href="https://arxiv.org/pdf/2001.05326" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingyun Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinhao Zheng</a><br>
<font size="3">
Abstract: The emergence and rapid progress of the Internet have brought ever-increasing impact on financial domain. How to rapidly and accurately mine the key information from the massive negative financial texts has become one of the key issues for investors and decision makers. Aiming at the issue, we propose a sentiment analysis and key entity detection approach based on BERT, which is applied in online financial text mining and public opinion analysis in social media. By using pre-train model, we first study sentiment analysis, and then we consider key entity detection as a sentence matching or Machine Reading Comprehension (MRC) task in different granularity. Among them, we mainly focus on negative sentimental information. We detect the specific entity by using our approach, which is different from traditional Named Entity Recognition (NER). In addition, we also use ensemble learning to improve the performance of proposed approach. Experimental results show that the performance of our approach is generally higher than SVM, LR, NBM, and BERT for two financial sentiment analysis and key entity detection datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：出现和互联网的飞速进步也带来了金融领域不断增加的影响。如何快速，准确地矿从大量负面财务文本的关键信息已成为投资者和决策者的关键问题之一。针对这个问题，我们提出了一个情感分析和基于BERT，这是在网上金融文本挖掘和舆情分析社交媒体应用的关键实体检测方法。通过使用预火车模型，我们首先研究情感分析，然后我们考虑的关键实体检测在不同粒度的句子匹配或机器阅读理解（MRC）的任务。其中，我们主要集中在负感伤的信息。我们发现，通过使用我们的方法，这是从传统的命名实体识别（NER）不同的特定实体。此外，我们还可以使用集成学习，以提高该方法的性能。实验结果表明，我们的方法的性能一般比SVM，LR，NBM，和BERT较高的两个财务情绪分析和关键实体检测数据集。</font>
</div>


<hr>
<div id="paper3"> <b>3. Authorship Attribution in Bangla literature using Character-level CNN</b>  <a href="https://arxiv.org/pdf/2001.05316" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Khatun%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aisha Khatun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anisur Rahman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Islam%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Md. Saiful Islam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marium-E-Jannat" target="_blank" rel="noopener" style="color:#0000EE;">Marium-E-Jannat</a><br>
<font size="3">
Abstract: Characters are the smallest unit of text that can extract stylometric signals to determine the author of a text. In this paper, we investigate the effectiveness of character-level signals in Authorship Attribution of Bangla Literature and show that the results are promising but improvable. The time and memory efficiency of the proposed model is much higher than the word level counterparts but accuracy is 2-5% less than the best performing word-level models. Comparison of various word-based models is performed and shown that the proposed model performs increasingly better with larger datasets. We also analyze the effect of pre-training character embedding of diverse Bangla character set in authorship attribution. It is seen that the performance is improved by up to 10% on pre-training. We used 2 datasets from 6 to 14 authors, balancing them before training and compare the results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：字符是文本，可以提取stylometric信号来确定文本的作者的最小单位。在本文中，我们研究了字符级信号的孟加拉文学著作权归属的有效性，并表明其结果是有希望的，但改善的。该模型的时间和内存效率比字级同行高得多，但精度比表现最好的字级车型少2-5％。执行并显示各种基于词的模型比较，该模型执行越来越多地与更大的数据集更好。我们还分析了前培训字符著作权归属不同孟加拉字符集的嵌入的效果。可以看出，性能高达10％的预培训提高。我们使用的数据集2的6至14作家，训练前平衡他们并比较结果。</font>
</div>


<hr>
<div id="paper4"> <b>4. A Continuous Space Neural Language Model for Bengali Language</b>  <a href="https://arxiv.org/pdf/2001.05315" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+H+A" target="_blank" rel="noopener" style="color:#0000EE;">Hemayet Ahmed Chowdhury</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Imon%2C+M+A+H" target="_blank" rel="noopener" style="color:#0000EE;">Md. Azizul Haque Imon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anisur Rahman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khatun%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aisha Khatun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Islam%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Md. Saiful Islam</a><br>
<font size="3">
Abstract: Language models are generally employed to estimate the probability distribution of various linguistic units, making them one of the fundamental parts of natural language processing. Applications of language models include a wide spectrum of tasks such as text summarization, translation and classification. For a low resource language like Bengali, the research in this area so far can be considered to be narrow at the very least, with some traditional count based models being proposed. This paper attempts to address the issue and proposes a continuous-space neural language model, or more specifically an ASGD weight dropped LSTM language model, along with techniques to efficiently train it for Bengali Language. The performance analysis with some currently existing count based models illustrated in this paper also shows that the proposed architecture outperforms its counterparts by achieving an inference perplexity as low as 51.2 on the held out data set for Bengali. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言模型通常用来估计各种语言单位的概率分布，使其自然语言处理的基本组成部分之一。语言模型的应用包括任务，如文本摘要，翻译和分类的广泛。对于像孟加拉低资源的语言，在这方面至今也算是研究是起码狭窄，而提出了一些传统的基于计数模式。本文试图解决这个问题，并提出了一个连续的空间神经语言模型，或者更准确地说是ASGD体重也下降LSTM语言模型，用技术来有效地训练它的孟加拉语一起。本文所示还显示了一些目前存在的以计数为基础的模型的性能分析，提出的架构通过实现一个推论困惑低至51.2对孟加拉的伸出数据集优于其同行。</font>
</div>


<hr>
<div id="paper5"> <b>5. Embedding Compression with Isotropic Iterative Quantization</b>  <a href="https://arxiv.org/pdf/2001.05314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyu Liao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanzhi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinru Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Yuan</a><br>
<font size="3">
Abstract: Continuous representation of words is a standard component in deep learning-based NLP models. However, representing a large vocabulary requires significant memory, which can cause problems, particularly on resource-constrained platforms. Therefore, in this paper we propose an isotropic iterative quantization (IIQ) approach for compressing embedding vectors into binary ones, leveraging the iterative quantization technique well established for image retrieval, while satisfying the desired isotropic property of PMI based models. Experiments with pre-trained embeddings (i.e., GloVe and HDC) demonstrate a more than thirty-fold compression ratio with comparable and sometimes even improved performance over the original real-valued embedding vectors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：词的连续表示是基于深学习NLP车型的标准组件。然而，较大量的词汇需要显著的内存，这可能会导致问题，特别是在资源受限的平台。因此，在本文中，我们提出了压缩嵌入矢量成二进制的，利用图像检索完善的迭代量化技术，同时满足基于PMI模型所需的各向同性各向同性迭代量化（IIQ）的方法。与预训练的嵌入（即，手套和HDC）实验证实与在原始实值嵌入矢量可比有时甚至改善性能超过30倍的压缩比。</font>
</div>


<hr>
<div id="paper6"> <b>6. Tensor Graph Convolutional Networks for Text Classification</b>  <a href="https://arxiv.org/pdf/2001.05313" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xien Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=You%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinxin You</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Ji Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Ping Lv</a><br>
<font size="3">
Abstract: Compared to sequential learning models, graph-based neural networks exhibit some excellent properties, such as ability capturing global information. In this paper, we investigate graph-based neural networks for text classification problem. A new framework TensorGCN (tensor graph convolutional networks), is presented for this task. A text graph tensor is firstly constructed to describe semantic, syntactic, and sequential contextual information. Then, two kinds of propagation learning perform on the text graph tensor. The first is intra-graph propagation used for aggregating information from neighborhood nodes in a single graph. The second is inter-graph propagation used for harmonizing heterogeneous information between graphs. Extensive experiments are conducted on benchmark datasets, and the results illustrate the effectiveness of our proposed framework. Our proposed TensorGCN presents an effective way to harmonize and integrate heterogeneous information from different kinds of graphs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：相比于连续的学习模式，基于图形的神经网络表现出一些优异的性能，如能力捕捉全球信息。在本文中，我们研究了文本分类问题，基于图形的神经网络。一个新的框架TensorGCN（图张卷积网络），提出了这一任务。文本图形张量首先被构造来描述语义，语法，和顺序的上下文信息。然后，有两种传播学习上的文字图形张量执行。第一种是用于在单个图表聚集来自邻近节点的信息，图形的帧内传播。第二个是用于协调图之间异构信息曲线图间传播。大量的实验是在基准数据集进行，其结果说明我们提出的框架的有效性。我们提出的TensorGCN礼物协调和异构信息从不同类型的图形整合的有效途径。</font>
</div>


<hr>
<div id="paper7"> <b>7. Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process  Approach to Linguistic Relationships</b>  <a href="https://arxiv.org/pdf/2001.05297" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cathcart%2C+C+A" target="_blank" rel="noopener" style="color:#0000EE;">Chundra Aroor Cathcart</a><br>
<font size="3">
Abstract: This paper addresses a series of complex and unresolved issues in the historical phonology of West Iranian languages. The West Iranian languages (Persian, Kurdish, Balochi, and other languages) display a high degree of non-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to language contact; we argue, however, that an oversimplified view of the processes at work has prevailed in the literature on West Iranian dialectology, with specialists assuming that deviations from an expected outcome in a given non-Persian language are due to lexical borrowing from some chronological stage of Persian. It is demonstrated that this qualitative approach yields at times problematic conclusions stemming from the lack of explicit probabilistic inferences regarding the distribution of the data: Persian may not be the sole donor language; additionally, borrowing at the lexical level is not always the mechanism that introduces irregularity. In many cases, the possibility that West Iranian languages show different reflexes in different conditioning environments remains under-explored. We employ a novel Bayesian approach designed to overcome these problems and tease apart the different determinants of irregularity in patterns of West Iranian sound change. Our methodology allows us to provisionally resolve a number of outstanding questions in the literature on West Iranian dialectology concerning the dialectal affiliation of certain sound changes. We outline future directions for work of this sort. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文地址了一系列西伊朗语支的历史音韵复杂和悬而未决的问题。西伊朗语支（波斯，库尔德人，俾路支语等语种）表现出高度的非Lautgesetzlich行为。大多数这种不规则的无疑是由于语言接触;我们认为，但是，在工作流程的一个过于简单化的观点在西方的伊朗方言的文学盛行，与由于从一些时间阶段的词汇借用专家假设从给定的非波斯语的预期结果偏差波斯语。据证实，这种定性方法的产量有时有问题的结论，因为缺乏有关数据的分布概率明确推论的词干：波斯可能不是唯一的供体语言;另外，在词汇水平借用并不总是机制引入了不规则性。在许多情况下，西伊朗的语言说明了在不同的环境条件不同反射的可能性仍然充分开发。我们采用设计来克服这些问题，并梳理出不规则的西伊朗声音的变化模式的不同决定一种新的贝叶斯方法。我们的方法可以让我们暂时解决了许多文献对西方的伊朗方言有关的某些声音的变化方言隶属关系悬而未决的问题。我们为这种工作勾勒未来的发展方向。</font>
</div>


<hr>
<div id="paper8"> <b>8. Urdu-English Machine Transliteration using Neural Networks</b>  <a href="https://arxiv.org/pdf/2001.05296" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Din%2C+U+M+u" target="_blank" rel="noopener" style="color:#0000EE;">Usman Mohy ud Din</a><br>
<font size="3">
Abstract: Machine translation has gained much attention in recent years. It is a sub-field of computational linguistic which focus on translating text from one language to other language. Among different translation techniques, neural network currently leading the domain with its capabilities of providing a single large neural network with attention mechanism, sequence-to-sequence and long-short term modelling. Despite significant progress in domain of machine translation, translation of out-of-vocabulary words(OOV) which include technical terms, named-entities, foreign words are still a challenge for current state-of-art translation systems, and this situation becomes even worse while translating between low resource languages or languages having different structures. Due to morphological richness of a language, a word may have different meninges in different context. In such scenarios, translation of word is not only enough in order provide the correct/quality translation. Transliteration is a way to consider the context of word/sentence during translation. For low resource language like Urdu, it is very difficult to have/find parallel corpus for transliteration which is large enough to train the system. In this work, we presented transliteration technique based on Expectation Maximization (EM) which is un-supervised and language independent. Systems learns the pattern and out-of-vocabulary (OOV) words from parallel corpus and there is no need to train it on transliteration corpus explicitly. This approach is tested on three models of statistical machine translation (SMT) which include phrasebased, hierarchical phrase-based and factor based models and two models of neural machine translation which include LSTM and transformer model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器翻译已经获得了广泛关注，近年来。这是着眼于从一种语言到另一种语言翻译文本计算语言学的子领域。在不同的翻译技术，目前主导其提供与注意机制，序列对序列和长短期建模一个大的神经网络的功能域的神经网络。尽管在机器翻译，出词汇的词（OOV），其中包括技术术语，命名实体翻译的领域显著进步，外来词仍然是国家的最先进的电流转换系统的挑战，而且这种情况变得更而具有不同结构的低资源语言或语言之间的转换变得更糟。由于语言的形态丰富，一个字可以有不同的上下文不同的脑膜。在这种情况下，文字的翻译不仅足以为了提供正确的/翻译质量。音译是考虑在翻译过程中字/句子的上下文的方式。对于像乌尔都语低资源语言，它是很难有/找到音译平行语料库是足够大的训练系统。在这项工作中，我们提出了基于期望最大化（EM）的音译技术，它是无监督和语言无关。系统学习的模式，从平行语料库超出词汇（OOV）的话，也没有必要训练它音译语料库明确。这种方法是在三个模型的统计机器翻译（SMT），其中包括phrasebased的测试，分层phrasebased和基于因子模型和神经机器翻译的两款车型，其中包括LSTM和变压器模型。</font>
</div>


<hr>
<div id="paper9"> <b>9. Language Models Are An Effective Patient Representation Learning  Technique For Electronic Health Record Data</b>  <a href="https://arxiv.org/pdf/2001.05295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Steinberg%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ethan Steinberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ken Jung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fries%2C+J+A" target="_blank" rel="noopener" style="color:#0000EE;">Jason A. Fries</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Corbin%2C+C+K" target="_blank" rel="noopener" style="color:#0000EE;">Conor K. Corbin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pfohl%2C+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Stephen R. Pfohl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+N+H" target="_blank" rel="noopener" style="color:#0000EE;">Nigam H. Shah</a><br>
<font size="3">
Abstract: Widespread adoption of electronic health records (EHRs) has fueled development of clinical outcome models using machine learning. However, patient EHR data are complex, and how to optimally represent them is an open question. This complexity, along with often small training set sizes available to train these clinical outcome models, are two core challenges for training high quality models. In this paper, we demonstrate that learning generic representations from the data of all the patients in the EHR enables better performing prediction models for clinical outcomes, allowing for these challenges to be overcome. We adapt common representation learning techniques used in other domains and find that representations inspired by language models enable a 3.5% mean improvement in AUROC on five clinical outcomes compared to standard baselines, with the average improvement rising to 19% when only a small number of patients are available for training a prediction model for a given clinical outcome. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：广泛采用的电子健康记录（电子病历）具有利用机器学习临床结果模型的燃料的发展。然而，患者的电子病历数据是复杂的，如何最优地表示他们是一个悬而未决的问题。这种复杂性，经常与小的训练集以及尺寸，以训练这些临床结果的模型，是培养高素质模型两个核心挑战。在本文中，我们证明了学习所有的患者在电子病历的数据一般表示为临床结果可以实现更好的进行预测模型，从而不必在克服这些挑战。我们采用通用表示学习其他领域使用的技术，并找到语言模型的启发是表示能够在AUROC五个临床结果3.5％的平均改善比标准的基线，平均提高上升到19％时，只有少数患者可用于训练预测模型对于给定的临床结果。</font>
</div>


<hr>
<div id="paper10"> <b>10. The empirical structure of word frequency distributions</b>  <a href="https://arxiv.org/pdf/2001.05292" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramscar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Ramscar</a><br>
<font size="3">
Abstract: The frequencies at which individual words occur across languages follow power law distributions, a pattern of findings known as Zipf's law. A vast literature argues over whether this serves to optimize the efficiency of human communication, however this claim is necessarily post hoc, and it has been suggested that Zipf's law may in fact describe mixtures of other distributions. From this perspective, recent findings that Sinosphere first (family) names are geometrically distributed are notable, because this is actually consistent with information theoretic predictions regarding optimal coding. First names form natural communicative distributions in most languages, and I show that when analyzed in relation to the communities in which they are used, first name distributions across a diverse set of languages are both geometric and, historically, remarkably similar, with power law distributions only emerging when empirical distributions are aggregated. I then show this pattern of findings replicates in communicative distributions of English nouns and verbs. These results indicate that if lexical distributions support efficient communication, they do so because their functional structures directly satisfy the constraints described by information theory, and not because of Zipf's law. Understanding the function of these information structures is likely to be key to explaining humankind's remarkable communicative capacities. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在发生个别单词跨语言服从幂律分布的频率，被称为齐普夫定律发现的模式。大量文献论证了，这是否用于优化人力沟通的效率，但这种说法必然是事后，这已经表明齐普夫定律，实际上可能描述的其他分布的混合。从这个角度来看，最近的调查结果Sinosphere第一（家庭）名称几何分布是显着的，因为这是关于最优编码信息理论预测实际上是一致的。名字形成大多数语言自然交际分布，我表明，在关系分析，在一组不同的语言中使用它们的社区，第一个名称发行时都是几何和，从历史上看，非常相似，幂律分布只有当经验分布聚集出现。然后我显示英语名词和动词的交际分布发现重复的这种模式。这些结果表明，如果词汇分布支持有效的沟通，他们这样做是因为他们的功能结构直接满足信息理论中描述的约束，并没有因为齐普夫定律。了解这些信息结构的功能很可能是关键，解释人类的非凡能力，交际。</font>
</div>


<hr>
<div id="paper11"> <b>11. Exploring and Improving Robustness of Multi Task Deep Neural Networks  via Domain Agnostic Defenses</b>  <a href="https://arxiv.org/pdf/2001.05286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Murali%2C+K+C" target="_blank" rel="noopener" style="color:#0000EE;">Kashyap Coimbatore Murali</a><br>
<font size="3">
Abstract: In this paper, we explore the robustness of the Multi-Task Deep Neural Networks (MT-DNN) against non-targeted adversarial attacks across Natural Language Understanding (NLU) tasks as well as some possible ways to defend against them. Liu et al., have shown that the Multi-Task Deep Neural Network, due to the regularization effect produced when training as a result of its cross task data, is more robust than a vanilla BERT model trained only on one task (1.1%-1.5% absolute difference). We further show that although the MT-DNN has generalized better, making it easily transferable across domains and tasks, it can still be compromised as after only 2 attacks (1-character and 2-character) the accuracy drops by 42.05% and 32.24% for the SNLI and SciTail tasks. Finally, we propose a domain agnostic defense which restores the model's accuracy (36.75% and 25.94% respectively) as opposed to a general-purpose defense or an off-the-shelf spell checker. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们探索了多任务深层神经网络（MT-DNN）的稳健性对整个自然语言理解（NLU）任务以及一些可能的方式来抵御这些非目标对抗性攻击。 Liu等人，已经表明，多任务深层的神经网络中，由于正规化效应产生当训练作为其横任务数据的结果是，比只在一个任务（1.1％培养了香草BERT模型更健壮 - 1.5％的绝对差）。进一步的研究表明，虽然MT-DNN具有更好的推广，使得它很容易跨域和任务转让的，它仍然可以作出妥协，只有2次攻击（1个字符和2个字符）的准确度42.05％和32.24％，下降后对于SNLI和SciTail任务。最后，我们提出了一个未知的领域国防其恢复模型的精确度（36.75％和25.94分别％），而不是通用的防守还是关闭的，现成的拼写检查器。</font>
</div>


<hr>
<div id="paper12"> <b>12. Detecting New Word Meanings: A Comparison of Word Embedding Models in  Spanish</b>  <a href="https://arxiv.org/pdf/2001.05285" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Torres-Rivera%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrés Torres-Rivera</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torres-Moreno%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juan-Manuel Torres-Moreno</a><br>
<font size="3">
Abstract: Semantic neologisms (SN) are defined as words that acquire a new word meaning while maintaining their form. Given the nature of this kind of neologisms, the task of identifying these new word meanings is currently performed manually by specialists at observatories of neology. To detect SN in a semi-automatic way, we developed a system that implements a combination of the following strategies: topic modeling, keyword extraction, and word sense disambiguation. The role of topic modeling is to detect the themes that are treated in the input text. Themes within a text give clues about the particular meaning of the words that are used, for example: viral has one meaning in the context of computer science (CS) and another when talking about health. To extract keywords, we used TextRank with POS tag filtering. With this method, we can obtain relevant words that are already part of the Spanish lexicon. We use a deep learning model to determine if a given keyword could have a new meaning. Embeddings that are different from all the known meanings (or topics) indicate that a word might be a valid SN candidate. In this study, we examine the following word embedding models: Word2Vec, Sense2Vec, and FastText. The models were trained with equivalent parameters using Wikipedia in Spanish as corpora. Then we used a list of words and their concordances (obtained from our database of neologisms) to show the different embeddings that each model yields. Finally, we present a comparison of these outcomes with the concordances of each word to show how we can determine if a word could be a valid candidate for SN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语义新词（SN）被定义为获得一个新词，同时保持其形式意义的话。鉴于这种新词的性质，目前手动专家在旧词新的天文台进行识别这些新词的意义的任务。为了检测SN以半自动化的方式，我们开发了一个系统，该系统实现了以下策略的组合：主题建模，关键词提取，以及词义消歧。主题建模的作用是检测在输入文本处理的主题。文本中的主题提供有关的被使用，例如词的特殊含义的线索：病毒只有一种含义在计算机科学（CS）和其他的方面讲卫生的时候。要提取的关键字，我们使用TextRank与POS标签过滤。通过这种方法，我们可以得到与已是西班牙词汇的一部分相关的词。我们使用了深刻的学习模式，以确定是否一个给定的关键字可能有新的意义。嵌入物是所有已知的含义（或主题）不同的指示词可能是一个有效的SN候选人。在这项研究中，我们考察以下单词嵌入型号：Word2Vec，Sense2Vec和FastText。模特们在西班牙使用维基百科语料库等效参数训练。然后我们使用的单词的列表和他们的语词（从我们的新词的数据库中获得）来显示不同的嵌入每个型号的产量。最后，我们提出这些结果与每个单词的词汇索引的比较，以显示我们如何确定一个词可能是SN有效候选人。</font>
</div>


<hr>
<div id="paper13"> <b>13. Improving Spoken Language Understanding By Exploiting ASR N-best  Hypotheses</b>  <a href="https://arxiv.org/pdf/2001.05284" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingda Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weitong Ruan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyue Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soldaini%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luca Soldaini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hamza%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wael Hamza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengwei Su</a><br>
<font size="3">
Abstract: In a modern spoken language understanding (SLU) system, the natural language understanding (NLU) module takes interpretations of a speech from the automatic speech recognition (ASR) module as the input. The NLU module usually uses the first best interpretation of a given speech in downstream tasks such as domain and intent classification. However, the ASR module might misrecognize some speeches and the first best interpretation could be erroneous and noisy. Solely relying on the first best interpretation could make the performance of downstream tasks non-optimal. To address this issue, we introduce a series of simple yet efficient models for improving the understanding of semantics of the input speeches by collectively exploiting the n-best speech interpretations from the ASR module. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在现代口语理解（SLU）系统，自然语言理解（NLU）模块需要一个讲话的解释从自动语音识别（ASR）模块的输入。该NLU模块通常使用一个给定的讲话在下游任务，如域名和意图分类第一最好的诠释。然而，ASR模块可能误识别的一些讲话和第一最好的诠释可能是错误的和嘈杂。仅仅依靠第一最好的诠释可以使下游任务的性能最优的。为了解决这个问题，我们引入了一系列简单而有效的模型，通过集体从ASR模块利用N条最佳演讲诠释提高输入演讲的语义的理解。</font>
</div>


<hr>
<div id="paper14"> <b>14. FGN: Fusion Glyph Network for Chinese Named Entity Recognition</b>  <a href="https://arxiv.org/pdf/2001.05272" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xuan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenyu Xuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuyu Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shengyi Jiang</a><br>
<font size="3">
Abstract: Chinese NER is a challenging task. As pictographs, Chinese characters contain latent glyph information, which is often overlooked. We propose the FGN, Fusion Glyph Network for Chinese NER. This method may offer glyph information for fusion representation learning with BERT. The major innovations of FGN include: (1) a novel CNN structure called CGS-CNN is proposed to capture glyph information from both character graphs and their neighboring graphs. (2) we provide a method with sliding window and Slice-Attention to extract interactive information between BERT representation and glyph representation. Experiments are conducted on four NER datasets, showing that FGN with LSTM-CRF as tagger achieves new state-of-the-arts performance for Chinese NER. Further, more experiments are conducted to investigate the influences of various components and settings in FGN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：中国NER是一个具有挑战性的任务。作为象形文字，中国字符包含潜在的字形信息，这些信息往往被忽视。我们提出了FGN，融合雕文网中国ER。这种方法可以提供融合表示学习与BERT字形信息。 FGN的主要创新点包括：（1）所谓的CGS-CNN一种新颖的CNN结构，提出从两个字符图和其周边图形捕获字形信息。 （2）我们提供具有滑动窗口和切片-注意提取BERT表示和字形表示之间的交互信息的方法。实验是在四个NER数据集进行，显示为恶搞实现国家的最艺术的新的性能为中国NER与LSTM-CRF是FGN。此外，更多的实验以调查FGN的各种组件和设置的影响。</font>
</div>


<hr>
<div id="paper15"> <b>15. A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation</b>  <a href="https://arxiv.org/pdf/2001.05139" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Guan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhihao Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyan Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minlie Huang</a><br>
<font size="3">
Abstract: Story generation, namely generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we employ multi-task learning which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：故事的产生，即产生从一个领先的情况下合理的故事，是一个重要而艰巨的任务。尽管在模拟的流畅性和局部连贯，现有的神经语言生成模型的成功（例如，GPT-2）仍从重复，逻辑冲突受到影响，并且缺乏长程的在产生的故事的连贯性。我们推测，这是因为关联相关常识的知识，理解因果关系，并计划实体和事件的适当时间顺序的难度。在本文中，我们设计了常识性的故事，一代知识强化训练前的模式。我们建议利用来自外部的知识基础常识知识产生合理的故事。为了进一步捕获的因果关系，并在合理的故事句子之间的时间相关性，我们采用多任务学习相结合的具有区分客观区分微调过程中真实和虚假的故事。自动和手动评估表明，我们的模型能够产生更合理的故事，比国家的最先进的基线，特别是在逻辑和全球协调方面。</font>
</div>


<hr>
<div id="paper16"> <b>16. Parallel Machine Translation with Disentangled Context Transformer</b>  <a href="https://arxiv.org/pdf/2001.05136" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jungo Kasai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Cross</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marjan Ghazvininejad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiatao Gu</a><br>
<font size="3">
Abstract: State-of-the-art neural machine translation models generate a translation from left to right and every step is conditioned on the previously generated tokens. The sequential nature of this generation process causes fundamental latency in inference since we cannot generate multiple tokens in each sentence in parallel. We propose an attention-masking based model, called Disentangled Context (DisCo) transformer, that simultaneously generates all tokens given different contexts. The DisCo transformer is trained to predict every output token given an arbitrary subset of the other reference tokens. We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：国家的最先进的从左至右和每一步的前提是之前生成的令牌神经机器翻译模型生成翻译。这个生成过程的有序性导致的推论根本延迟，因为我们不能生成并行每个句子多个令牌。我们建议注意的遮蔽基于模型，称为迎刃而解上下文（迪斯科）变压器，能够同时生成给出不同的上下文中的所有令牌。迪斯科变压器训练以预测每个输出令牌给出的其它参考标记的任意子集。我们还开发并行易先推理算法，反复细化每个令牌并行，减少了所需的迭代次数。我们对7点的方向具有不同大小的数据大量的实验证明，如果没有更好的，性能比现有技术中的非自回归机器翻译的状态，而显著减少平均解码时间我们的模型实现了有竞争力的。</font>
</div>


<hr>
<div id="paper17"> <b>17. Robust Speaker Recognition Using Speech Enhancement And Attention Model</b>  <a href="https://arxiv.org/pdf/2001.05031" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanpei Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hain%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Hain</a><br>
<font size="3">
Abstract: In this paper, a novel architecture for speaker recognition is proposed by cascading speech enhancement and speaker processing. Its aim is to improve speaker recognition performance when speech signals are corrupted by noise. Instead of individually processing speech enhancement and speaker recognition, the two modules are integrated into one framework by a joint optimisation using deep neural networks. Furthermore, to increase robustness against noise, a multi-stage attention mechanism is employed to highlight the speaker related features learned from context information in time and frequency domain. To evaluate speaker identification and verification performance of the proposed approach, we test it on the dataset of VoxCeleb1, one of mostly used benchmark datasets. Moreover, the robustness of our proposed approach is also tested on VoxCeleb1 data when being corrupted by three types of interferences, general noise, music, and babble, at different signal-to-noise ratio (SNR) levels. The obtained results show that the proposed approach using speech enhancement and multi-stage attention models outperforms two strong baselines not using them in most acoustic conditions in our experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文对说话人识别一个新颖的架构通过级联语音增强和扬声器的处理建议。其目的是为了提高说话人识别性能时，语音信号被噪声干扰。而不是单独处理语音增强和说话人识别，这两个模块是通过使用深层神经网络的联合优化集成到一个框架。此外，为了增加可以有效抵抗噪声，采用多级注意机制，突出显示上下文信息在时间和频域学会了说话者相关的功能。为了评价说话人识别和建议的方法验证性能，我们测试它VoxCeleb1，大多采用标准数据集之一的数据集。此外，我们的建议的方法的稳健性上VoxCeleb1数据还测试由三种类型的干扰，一般噪声，音乐和多路重合，在不同的信噪比（SNR）水平被损坏时。得到的结果表明，该方法使用语音增强和多级车型的关注性能优于两周强的基线没有在我们的实验中最声学条件下使用它们。</font>
</div>


<hr>
<div id="paper18"> <b>18. A Tree Adjoining Grammar Representation for Models Of Stochastic  Dynamical Systems</b>  <a href="https://arxiv.org/pdf/2001.05320" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Khandelwal%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dhruv Khandelwal</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Schoukens%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maarten Schoukens</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=T%C3%B3th%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roland Tóth</a><br>
<font size="3">
Abstract: Model structure and complexity selection remains a challenging problem in system identification, especially for parametric non-linear models. Many Evolutionary Algorithm (EA) based methods have been proposed in the literature for estimating model structure and complexity. In most cases, the proposed methods are devised for estimating structure and complexity within a specified model class and hence these methods do not extend to other model structures without significant changes. In this paper, we propose a Tree Adjoining Grammar (TAG) for stochastic parametric models. TAGs can be used to generate models in an EA framework while imposing desirable structural constraints and incorporating prior knowledge. In this paper, we propose a TAG that can systematically generate models ranging from FIRs to polynomial NARMAX models. Furthermore, we demonstrate that TAGs can be easily extended to more general model classes, such as the non-linear Box-Jenkins model class, enabling the realization of flexible and automatic model structure and complexity selection via EA. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：模型的结构和复杂的选择仍然在系统识别一个具有挑战性的问题，尤其是对于参数非线性模型。许多进化算法（EA）为基础的方法在文献中已经提出了用于估计模型结构和复杂性。在大多数情况下，所提出的方法被设计为在指定模型类内估计结构和复杂性，并因此这些方法不延伸到其他模型结构而不显著变化。在本文中，我们提出了一个树连接语法（TAG）为随机参数模型。标签可以用于在EA框架来生成模型，同时施加理想的结构约束和结合先验知识。在本文中，我们提出了一个标记，可以系统地生成模型，从FIR的多项式NARMAX模型。此外，我们证明，标签可以容易地扩展到更一般的模型类，诸如非线性箱Jenkins模型类，可实现灵活和自动模型结构和复杂选择经由EA实现。</font>
</div>


<hr>
<div id="paper19"> <b>19. Auto Completion of User Interface Layout Design Using Transformer-Based  Tree Decoders</b>  <a href="https://arxiv.org/pdf/2001.05308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Amelot%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Amelot</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bengio%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samy Bengio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Si%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Si Si</a><br>
<font size="3">
Abstract: It has been of increasing interest in the field to develop automatic machineries to facilitate the design process. In this paper, we focus on assisting graphical user interface (UI) layout design, a crucial task in app development. Given a partial layout, which a designer has entered, our model learns to complete the layout by predicting the remaining UI elements with a correct position and dimension as well as the hierarchical structures. Such automation will significantly ease the effort of UI designers and developers. While we focus on interface layout prediction, our model can be generally applicable for other layout prediction problems that involve tree structures and 2-dimensional placements. Particularly, we design two versions of Transformer-based tree decoders: Pointer and Recursive Transformer, and experiment with these models on a public dataset. We also propose several metrics for measuring the accuracy of tree prediction and ground these metrics in the domain of user experience. These contribute a new task and methods to deep learning research. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一直在该领域越来越多的关注，开发自动机器方便的设计过程。在本文中，我们侧重于帮助图形用户界面（UI）布局设计，在应用发展的重要任务。鉴于部分的布局，设计师已经进入，我们的模型学会通过预测与正确的位置和尺寸，其余的UI元素以及分层结构完成全国布局。这样的自动化将显著缓解UI设计师和开发人员的努力。虽然我们专注于界面布局的预测，我们的模型可以普遍适用于涉及树形结构和二维展示位置等布局预报问题。特别是，我们设计了基于变压器的树解码器的两个版本：指针和递归变压器，并与公共数据集，这些模型进行试验。我们还提出几个指标，用于测量树预测的准确性，并在用户体验领域地这些指标。这些贡献了新的任务和方法，深度学习研究。</font>
</div>


<hr>
<div id="paper20"> <b>20. Teddy: A System for Interactive Review Analysis</b>  <a href="https://arxiv.org/pdf/2001.05171" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiong Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Engel%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Engel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Evensen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sara Evensen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuliang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Demiralp%2C+%C3%87" target="_blank" rel="noopener" style="color:#0000EE;">Çağatay Demiralp</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wang-Chiew Tan</a><br>
<font size="3">
Abstract: Reviews are integral to e-commerce services and products. They contain a wealth of information about the opinions and experiences of users, which can help better understand consumer decisions and improve user experience with products and services. Today, data scientists analyze reviews by developing rules and models to extract, aggregate, and understand information embedded in the review text. However, working with thousands of reviews, which are typically noisy incomplete text, can be daunting without proper tools. Here we first contribute results from an interview study that we conducted with fifteen data scientists who work with review text, providing insights into their practices and challenges. Results suggest data scientists need interactive systems for many review analysis tasks. In response we introduce Teddy, an interactive system that enables data scientists to quickly obtain insights from reviews and improve their extraction and modeling pipelines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：评论是不可或缺的电子商务服务和产品。它们包含了大量关于用户的意见和经验，这有助于更好地了解消费者的决策和提高产品和服务的用户体验信息。如今，科学家数据分析通过制定规则和模型来提取，汇总评价，并了解嵌入在审查文本信息。然而，成千上万的评论，这是典型的吵不完整的文本工作，可没有合适的工具望而生畏。在这里，我们首先从访谈研究，我们具有十五数据科学家谁的工作与评论文本进行，提供洞察到他们的做法和挑战作出贡献的结果。结果表明数据科学家需要对很多的评论分析任务的交互系统。在回应我们介绍泰迪，一个互动系统，使数据科学家能够迅速从审查获得洞察力和改善他们的提取和建模管道。</font>
</div>


<hr>
<div id="paper21"> <b>21. Modeling Product Search Relevance in e-Commerce</b>  <a href="https://arxiv.org/pdf/2001.04980" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Iyer%2C+R+R" target="_blank" rel="noopener" style="color:#0000EE;">Rahul Radhakrishnan Iyer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kohli%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rohan Kohli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prabhumoye%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shrimai Prabhumoye</a><br>
<font size="3">
Abstract: With the rapid growth of e-Commerce, online product search has emerged as a popular and effective paradigm for customers to find desired products and engage in online shopping. However, there is still a big gap between the products that customers really desire to purchase and relevance of products that are suggested in response to a query from the customer. In this paper, we propose a robust way of predicting relevance scores given a search query and a product, using techniques involving machine learning, natural language processing and information retrieval. We compare conventional information retrieval models such as BM25 and Indri with deep learning models such as word2vec, sentence2vec and paragraph2vec. We share some of our insights and findings from our experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着电子商务的快速发展，在线产品搜索已经成为一种流行和有效的模式，为客户找到所需的产品和从事网上购物。然而，仍然有客户真正渴望的产品的购买和相关性被提出以响应来自客户的查询产品之间有很大的差距。在本文中，我们提出了预测给定的搜索查询和产品的相关性分值，使用涉及机器学习，自然语言处理和信息检索技术的可靠方式。我们比较传统的信息检索模型如BM25和大狐猴与深度学习模式，如word2vec，sentence2vec和paragraph2vec。我们分享我们的一些见解和研究结果，从我们的实验。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-15</title>
    <url>/2020/01/16/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-15/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning <a href="https://arxiv.org/pdf/2001.04935" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Balancing the composition of word embeddings across heterogenous data  sets <a href="https://arxiv.org/pdf/2001.04693" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Bi-Decoder Augmented Network for Neural Machine Translation <a href="https://arxiv.org/pdf/2001.04586" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> On the Replicability of Combining Word Embeddings and Retrieval Models <a href="https://arxiv.org/pdf/2001.04484" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Detecting depression in dyadic conversations with multimodal narratives  and visualizations <a href="https://arxiv.org/pdf/2001.04809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> A (Simplified) Supreme Being Necessarily Exists -- Says the Computer! <a href="https://arxiv.org/pdf/2001.04701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Improved Robust ASR for Social Robots in Public Spaces <a href="https://arxiv.org/pdf/2001.04619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Faster Transformer Decoding: N-gram Masked Self-Attention <a href="https://arxiv.org/pdf/2001.04589" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning</b>  <a href="https://arxiv.org/pdf/2001.04935" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roei Schuster</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tal Schuster</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meri%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoav Meri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shmatikov%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vitaly Shmatikov</a><br>
<font size="3">
Abstract: Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word "meaning" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks. Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the "meaning" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another. An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：字的嵌入，即，低维向量表示，如手套和SGNS，编码字在这个意义上，词语向量之间的距离对应于它们的语义接近“意思是”。这使语义的迁移学习的各种自然语言处理任务。 Word中的嵌入通常是受过训练的大型公共语料库，如维基百科或Twitter。我们表明，攻击者谁可以修改其嵌入训练可以通过改变空间嵌入它们的位置控制的新的和现有的词“意义”的语料库。我们开发了语料库的特点，可作为单词之间距离的代理明确的表达，并建立自己的价值观和嵌入的距离之间的因果关系。然后，我们展示了如何使用两个敌对目标的这种关系：（1）做一个字一个字的世界排名第一的邻居，和（2）从一个语义集群移动一个字到另一个。在嵌入的攻击会影响不同的下游任务，这表明首次数据传输学习情境中毒的力量。我们使用这种攻击来操纵信息检索系统，如简历搜索查询扩展，使某些名字命名实体识别模型或多或少可见，并造成新词被翻译成特定的目标词无论使用什么语言。最后，我们展示了攻击者如何产生语言上可能语料库修改，从而欺骗试图难以置信的句子从使用语言模型的语料库过滤防御。</font>
</div>


<hr>
<div id="paper2"> <b>2. Balancing the composition of word embeddings across heterogenous data  sets</b>  <a href="https://arxiv.org/pdf/2001.04693" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Brandl%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephanie Brandl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lassner%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Lassner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alber%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maximilian Alber</a><br>
<font size="3">
Abstract: Word embeddings capture semantic relationships based on contextual information and are the basis for a wide variety of natural language processing applications. Notably these relationships are solely learned from the data and subsequently the data composition impacts the semantic of embeddings which arguably can lead to biased word vectors. Given qualitatively different data subsets, we aim to align the influence of single subsets on the resulting word vectors, while retaining their quality. In this regard we propose a criteria to measure the shift towards a single data subset and develop approaches to meet both objectives. We find that a weighted average of the two subset embeddings balances the influence of those subsets while word similarity performance decreases. We further propose a promising optimization approach to balance influences and quality of word embeddings. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于上下文信息的嵌入Word中捕捉语义关系，并且是各种各样的自然语言处理应用的基础。值得注意的是这些关系仅由数据并随后将数据组合物影响的语义的嵌入可论证可导致偏置字矢量的教训。考虑到质的不同数据子集，我们的目标是一致的最终的字向量单亚群的影响力，同时保持它们的质量。在这方面，我们提出了一个标准来衡量一个单一的数据子集的转变和发展途径，以满足这两个目标。我们发现，两个子集的嵌入的加权平均余额部分数据的影响，而单词类似性能降低。我们进一步提出了一个有前途的优化方法来平衡影响和字的嵌入质量。</font>
</div>


<hr>
<div id="paper3"> <b>3. Bi-Decoder Augmented Network for Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2001.04586" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Boyuan Pan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yazheng Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhou Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yueting Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deng Cai</a><br>
<font size="3">
Abstract: Neural Machine Translation (NMT) has become a popular technology in recent years, and the encoder-decoder framework is the mainstream among all the methods. It's obvious that the quality of the semantic representations from encoding is very crucial and can significantly affect the performance of the model. However, existing unidirectional source-to-target architectures may hardly produce a language-independent representation of the text because they rely heavily on the specific relations of the given language pairs. To alleviate this problem, in this paper, we propose a novel Bi-Decoder Augmented Network (BiDAN) for the neural machine translation task. Besides the original decoder which generates the target language sequence, we add an auxiliary decoder to generate back the source language sequence at the training time. Since each decoder transforms the representations of the input text into its corresponding language, jointly training with two target ends can make the shared encoder has the potential to produce a language-independent semantic space. We conduct extensive experiments on several NMT benchmark datasets and the results demonstrate the effectiveness of our proposed approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经机器翻译（NMT）已经成为一种流行的技术，近年来，和编码器，解码器的结构与第方法中所有的主流。很明显，从编码语义表示的质量是非常重要的，可以显著影响模型的性能。但是，现有的单向源到目标架构可以几乎不产生文本的语言无关的表示，因为他们在很大程度上依赖于特定的语言对的特定关系。为了缓解这一问题，在本文中，我们提出了一个新颖的双解码器增强网络（毕单）的神经机器翻译的任务。除了生成目标语言序列原有解码器，我们添加辅助解码器，以生成回到了训练时间的源语言序列。因为每个解码器将输入的文本的表示成其相应的语言，共同具有两个靶的端部的训练可以使共享编码器具有以产生独立于语言的语义空间的潜力。我们几个NMT基准数据集进行了广泛的实验，结果证明我们提出的方法的有效性。</font>
</div>


<hr>
<div id="paper4"> <b>4. On the Replicability of Combining Word Embeddings and Retrieval Models</b>  <a href="https://arxiv.org/pdf/2001.04484" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Papariello%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luca Papariello</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bampoulidis%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandros Bampoulidis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lupu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mihai Lupu</a><br>
<font size="3">
Abstract: We replicate recent experiments attempting to demonstrate an attractive hypothesis about the use of the Fisher kernel framework and mixture models for aggregating word embeddings towards document representations and the use of these representations in document classification, clustering, and retrieval. Specifically, the hypothesis was that the use of a mixture model of von Mises-Fisher (VMF) distributions instead of Gaussian distributions would be beneficial because of the focus on cosine distances of both VMF and the vector space model traditionally used in information retrieval. Previous experiments had validated this hypothesis. Our replication was not able to validate it, despite a large parameter scan space. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近期重复实验，试图证明有关使用费的内核架构和混合模型的聚集对文档表示字的嵌入和使用文档分类，聚类和检索这些表象的一个有吸引力的假说。具体而言，假设是使用冯米塞斯-Fisher分析（VMF）的混合物模型的分布，而不是高斯分布将是因为聚焦在两个VMF和信息检索传统上使用向量空间模型的余弦距离的有益的。以前的实验已经证实了这一假设。我们的复制无法验证它，尽管大的参数扫描空间。</font>
</div>


<hr>
<div id="paper5"> <b>5. Detecting depression in dyadic conversations with multimodal narratives  and visualizations</b>  <a href="https://arxiv.org/pdf/2001.04809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Joshua Y. Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G+Y" target="_blank" rel="noopener" style="color:#0000EE;">Greyson Y. Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yacef%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kalina Yacef</a><br>
<font size="3">
Abstract: Conversations contain a wide spectrum of multimodal information that gives us hints about the emotions and moods of the speaker. In this paper, we developed a system that supports humans to analyze conversations. Our main contribution is the identification of appropriate multimodal features and the integration of such features into verbatim conversation transcripts. We demonstrate the ability of our system to take in a wide range of multimodal information and automatically generated a prediction score for the depression state of the individual. Our experiments showed that this approach yielded better performance than the baseline model. Furthermore, the multimodal narrative approach makes it easy to integrate learnings from other disciplines, such as conversational analysis and psychology. Lastly, this interdisciplinary and automated approach is a step towards emulating how practitioners record the course of treatment as well as emulating how conversational analysts have been analyzing conversations by hand. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对话包含的多模式信息范围广泛，让我们有预兆说话者的情绪和心情。在本文中，我们开发了一个系统，支持人类分析的对话。我们的主要贡献是适当的多模式特征的识别和整合这些功能集成到逐字谈话笔录。我们证明我们的系统采取广泛的多模式信息，并自动生成个人的抑郁状态的预测得分的能力。我们的实验表明，这种方法取得了比基线模型更好的性能。此外，多模式的叙事方法，可以轻松集成到其他学科，如会话分析和心理学的学习收获。最后，这种跨学科的和自动化的方法是对模拟从业者如何记录治疗过程，以及如何模拟对话分析家一直用手分析对话的一个步骤。</font>
</div>


<hr>
<div id="paper6"> <b>6. A (Simplified) Supreme Being Necessarily Exists -- Says the Computer!</b>  <a href="https://arxiv.org/pdf/2001.04701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Benzm%C3%BCller%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christoph Benzmüller</a><br>
<font size="3">
Abstract: A simplified variant of Kurt Gödel's modal ontological argument is presented. Some of Gödel's, resp. Scott's, premises are modified, others are dropped, and modal collapse is avoided. The emended argument is shown valid already in quantified modal logic K. The presented simplifications have been computationally explored utilising latest knowledge representation and reasoning technology based on higher-order logic. The paper thus illustrates how modern symbolic AI technology can contribute new knowledge to formal philosophy and theology. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：哥德尔的模式本体论的简化变体显示。有些哥德尔，RESP的。斯科特的，房屋被修改，其他被丢弃，避免了模态崩溃。在仔细的校勘参数显示有效的已量化模态逻辑K.所提出的简化了计算研究利用最新的知识表示和基于高阶逻辑推理技术。因此，阐述了象征性的AI技术如何现代可以促进新知识的正式哲学和神学。</font>
</div>


<hr>
<div id="paper7"> <b>7. Improved Robust ASR for Social Robots in Public Spaces</b>  <a href="https://arxiv.org/pdf/2001.04619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Jankowski%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Charles Jankowski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mruthyunjaya%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vishwas Mruthyunjaya</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lin%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruixi Lin</a><br>
<font size="3">
Abstract: Social robots deployed in public spaces present a challenging task for ASR because of a variety of factors, including noise SNR of 20 to 5 dB. Existing ASR models perform well for higher SNRs in this range, but degrade considerably with more noise. This work explores methods for providing improved ASR performance in such conditions. We use the AiShell-1 Chinese speech corpus and the Kaldi ASR toolkit for evaluations. We were able to exceed state-of-the-art ASR performance with SNR lower than 20 dB, demonstrating the feasibility of achieving relatively high performing ASR with open-source toolkits and hundreds of hours of training data, which is commonly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：部署在公共场所的社交机器人目前由于多种因素的影响，其中包括20至5分贝的噪音信噪比ASR一项艰巨的任务。现有ASR模型表现良好在这个范围内的较高的信噪比，但更多的噪音大大降低。这项工作探索提供在这样的条件下改善ASR性能的方法。我们使用AiShell-1中国语料库和Kaldi ASR工具包的评估。我们能够超过信噪比国家的最先进的ASR性能大于20dB低，表明达到比较高的用开源工具包和数以百计的训练数据，这是通常可以利用的时间来完成ASR的可行性。</font>
</div>


<hr>
<div id="paper8"> <b>8. Faster Transformer Decoding: N-gram Masked Self-Attention</b>  <a href="https://arxiv.org/pdf/2001.04589" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ciprian Chelba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mia Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankur Bapna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Noam Shazeer</a><br>
<font size="3">
Abstract: Motivated by the fact that most of the information relevant to the prediction of target tokens is drawn from the source sentence $S=s_1, \ldots, s_S$, we propose truncating the target-side window used for computing self-attention by making an $N$-gram assumption. Experiments on WMT EnDe and EnFr data sets show that the $N$-gram masked self-attention model loses very little in BLEU score for $N$ values in the range $4, \ldots, 8$, depending on the task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事实上，大多数的相关目标令牌的预测信息从源句子$ S = S_1，\ ldots，S_S $绘制的启发，我们建议截断用于通过计算自我关注的目标侧窗做一个$ N $ -gram假设。在WMT恩德和EnFr数据集上的实验表明，$ N $ -gram掩盖自我注意模型的BLEU分数$ N $值的范围在$ 4 \ ldots，$ 8，根据任务非常小的损失。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-14</title>
    <url>/2020/01/15/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-14/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Multi-Source Domain Adaptation for Text Classification via  DistanceNet-Bandits <a href="https://arxiv.org/pdf/2001.04362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark  for Chinese <a href="https://arxiv.org/pdf/2001.04351" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural  Architecture Search <a href="https://arxiv.org/pdf/2001.04246" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Mining customer product reviews for product development: A summarization  process <a href="https://arxiv.org/pdf/2001.04200" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Joint Reasoning for Multi-Faceted Commonsense Knowledge <a href="https://arxiv.org/pdf/2001.04170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> ProphetNet: Predicting Future N-gram for Sequence-to-Sequence  Pre-training <a href="https://arxiv.org/pdf/2001.04063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Stochastic Natural Language Generation Using Dependency Information <a href="https://arxiv.org/pdf/2001.03897" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Rethinking Generalization of Neural Models: A Named Entity Recognition  Case Study <a href="https://arxiv.org/pdf/2001.03844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Revisiting Challenges in Data-to-Text Generation with Fact Grounding <a href="https://arxiv.org/pdf/2001.03830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Learning Cross-Context Entity Representations from Text <a href="https://arxiv.org/pdf/2001.03765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> PatentTransformer-2: Controlling Patent Text Generation by Structural  Metadata <a href="https://arxiv.org/pdf/2001.03708" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Does syntax need to grow on trees? Sources of hierarchical inductive  bias in sequence-to-sequence networks <a href="https://arxiv.org/pdf/2001.03632" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Reformer: The Efficient Transformer <a href="https://arxiv.org/pdf/2001.04451" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> LP-SparseMAP: Differentiable Relaxed Optimization for Sparse Structured  Prediction <a href="https://arxiv.org/pdf/2001.04437" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Negative Statements Considered Useful <a href="https://arxiv.org/pdf/2001.04425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Asymmetrical Hierarchical Networks with Attentive Interactions for  Interpretable Review-Based Recommendation <a href="https://arxiv.org/pdf/2001.04346" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Shareable Representations for Search Query Understanding <a href="https://arxiv.org/pdf/2001.04345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Improving Dysarthric Speech Intelligibility Using Cycle-consistent  Adversarial Training <a href="https://arxiv.org/pdf/2001.04260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Structural Decompositions of Epistemic Logic Programs <a href="https://arxiv.org/pdf/2001.04219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> A logic-based relational learning approach to relation extraction: The  OntoILPER system <a href="https://arxiv.org/pdf/2001.04192" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Retouchdown: Adding Touchdown to StreetLearn as a Shareable Resource for  Language Grounding Tasks in Street View <a href="https://arxiv.org/pdf/2001.03671" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Multi-Source Domain Adaptation for Text Classification via  DistanceNet-Bandits</b>  <a href="https://arxiv.org/pdf/2001.04362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pasunuru%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ramakanth Pasunuru</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Bansal</a><br>
<font size="3">
Abstract: Domain adaptation performance of a learning algorithm on a target domain is a function of its source domain error and a divergence measure between the data distribution of these two domains. We present a study of various distance-based measures in the context of NLP tasks, that characterize the dissimilarity between domains based on sample estimates. We first conduct analysis experiments to show which of these distance measures can best differentiate samples from same versus different domains, and are correlated with empirical results. Next, we develop a DistanceNet model which uses these distance measures, or a mixture of these distance measures, as an additional loss function to be minimized jointly with the task's loss function, so as to achieve better unsupervised domain adaptation. Finally, we extend this model to a novel DistanceNet-Bandit model, which employs a multi-armed bandit controller to dynamically switch between multiple source domains and allow the model to learn an optimal trajectory and mixture of domains for transfer to the low-resource target domain. We conduct experiments on popular sentiment analysis datasets with several diverse domains and show that our DistanceNet model, as well as its dynamic bandit variant, can outperform competitive baselines in the context of unsupervised domain adaptation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对目标域学习算法的域自适应性能是它的源域误差的函数和这两个结构域的数据分布之间的偏差度量。我们提出的在NLP任务范围内各种基于距离的测量，表征根据样本估计域间的差异性进行了研究。我们首先进行分析实验表明其中的这些距离措施最好的分化样本相同与不同的域，并与实验结果是相关的。接下来，我们开发出使用这些距离的措施，或者这些距离测量的混合物DistanceNet模型，作为额外的损失函数要与任务的损失函数共同最小化，从而达到更好的无监督的领域适应性。最后，我们扩展该模型以一种新颖的DistanceNet匪模型，其采用多臂老虎控制器到多个源域之间动态开关和允许模型学习域的最佳轨迹和混合物，然后转移到低资源目标域。我们进行了对流行的情感分析数据集实验与多个不同领域，并表明我们的模型DistanceNet，以及它的动态强盗变种，可以在无人监督的领域适应性的背景下跑赢大市的竞争基准。</font>
</div>


<hr>
<div id="paper2"> <b>2. CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark  for Chinese</b>  <a href="https://arxiv.org/pdf/2001.04351" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianqian Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cong Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yin Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weitang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuanwei Zhang</a><br>
<font size="3">
Abstract: In this paper, we introduce the NER dataset from CLUE organization (CLUENER2020), a well-defined fine-grained dataset for named entity recognition in Chinese. CLUENER2020 contains 10 categories. Apart from common labels like person, organization, and location, it contains more diverse categories. It is more challenging than current other Chinese NER datasets and could better reflect real-world applications. For comparison, we implement several state-of-the-art baselines as sequence labeling tasks and report human performance, as well as its analysis. To facilitate future work on fine-grained NER for Chinese, we release our dataset, baselines, and leader-board. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们将介绍从CLUE组织NER数据集（CLUENER2020），一个明确的细粒度数据集在中国命名实体识别。 CLUENER2020包含10个类别。除了像个人，组织和位置共同的标签，它包含了更多样化的类别。它比目前的其他中国NER数据集更具挑战性，更能反映现实世界的应用。为了便于比较，我们实现国家的最先进的一些基线为序列标注任务和报告人的表现，以及它的分析。为了方便日后对中国细粒度NER的工作，我们发布的数据集，基线和领袖板。</font>
</div>


<hr>
<div id="paper3"> <b>3. AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural  Architecture Search</b>  <a href="https://arxiv.org/pdf/2001.04246" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daoyuan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaliang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghui Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bofang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bolin Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongbo Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingren Zhou</a><br>
<font size="3">
Abstract: Large pre-trained language models such as BERT have shown their effectiveness in various natural language processing tasks. However, the huge parameter size makes them difficult to be deployed in real-time applications that require quick inference with limited resources. Existing methods compress BERT into small models while such compression is task-independent, i.e., the same compressed BERT for all different downstream tasks. Motivated by the necessity and benefits of task-oriented BERT compression, we propose a novel compression method, AdaBERT, that leverages differentiable Neural Architecture Search to automatically compress BERT into task-adaptive small models for specific tasks. We incorporate a task-oriented knowledge distillation loss to provide search hints and an efficiency-aware loss as search constraints, which enables a good trade-off between efficiency and effectiveness for task-adaptive BERT compression. We evaluate AdaBERT on several NLP tasks, and the results demonstrate that those task-adaptive compressed models are 12.7x to 29.3x faster than BERT in inference time and 11.5x to 17.0x smaller in terms of parameter size, while comparable performance is maintained. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型预训练的语言模型，如BERT表明它们在不同的自然语言处理任务的有效性。然而，巨大的参数尺寸使得它们很难在需要快速推断资源有限的实时应用进行部署。现有的方法压缩BERT为小机型，而这种压缩是任务无关，即对所有不同的下游任务相同的压缩BERT。由必要性和面向任务的BERT压缩的好处的启发，我们提出了一种新的压缩方法，AdaBERT，它利用微神经结构的搜索自动压缩成BERT任务自适应小型号为特定的任务。我们结合了面向任务的知识蒸馏损失提供搜索提示和效率意识的损失，搜索约束，这使得任务自适应BERT压缩一个很好的权衡效率和效益之间。我们评估几个NLP任务AdaBERT，结果表明，这些任务自适应压缩模型12.7倍至29.3x比推理时间和11.5倍BERT更快17.0x参数规模而言较小，而相当的性能得以维持。</font>
</div>


<hr>
<div id="paper4"> <b>4. Mining customer product reviews for product development: A summarization  process</b>  <a href="https://arxiv.org/pdf/2001.04200" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianjun Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yannou%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernard Yannou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leroy%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yann Leroy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poirson%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emilie Poirson</a><br>
<font size="3">
Abstract: This research set out to identify and structure from online reviews the words and expressions related to customers' likes and dislikes to guide product development. Previous methods were mainly focused on product features. However, reviewers express their preference not only on product features. In this paper, based on an extensive literature review in design science, the authors propose a summarization model containing multiples aspects of user preference, such as product affordances, emotions, usage conditions. Meanwhile, the linguistic patterns describing these aspects of preference are discovered and drafted as annotation guidelines. A case study demonstrates that with the proposed model and the annotation guidelines, human annotators can structure the online reviews with high inter-agreement. As high inter-agreement human annotation results are essential for automatizing the online review summarization process with the natural language processing, this study provides materials for the future study of automatization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本研究着手从网上评论识别和结构关系到客户的好恶词语来指导产品的开发。先前的方法主要集中在产品功能。然而，评论家表达自己的喜好，不仅在产品功能。在本文的基础上，设计科学的全面的文献，作​​者提出了一个包含用户偏好的倍数方面，如产品的可供性，情绪，利用状况的总结模式。同时，描述偏好这些方面的语言模式被发现并起草作为注解的指导方针。案例研究表明，与所提出的模型和注释指引，人工注释就可以构建高之间的协议网上审查。由于采用协议间的人类标注的结果是与自然语言处理automatizing在线审核汇总过程中必不可少的，这项研究提供了自动化的未来学习材料。</font>
</div>


<hr>
<div id="paper5"> <b>5. Joint Reasoning for Multi-Faceted Commonsense Knowledge</b>  <a href="https://arxiv.org/pdf/2001.04170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chalier%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yohan Chalier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Razniewski%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Razniewski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weikum%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerhard Weikum</a><br>
<font size="3">
Abstract: Commonsense knowledge (CSK) supports a variety of AI applications, from visual understanding to chatbots. Prior works on acquiring CSK, such as ConceptNet, have compiled statements that associate concepts, like everyday objects or activities, with properties that hold for most or some instances of the concept. Each concept is treated in isolation from other concepts, and the only quantitative measure (or ranking) of properties is a confidence score that the statement is valid. This paper aims to overcome these limitations by introducing a multi-faceted model of CSK statements and methods for joint reasoning over sets of inter-related statements. Our model captures four different dimensions of CSK statements: plausibility, typicality, remarkability and salience, with scoring and ranking along each dimension. For example, hyenas drinking water is typical but not salient, whereas hyenas eating carcasses is salient. For reasoning and ranking, we develop a method with soft constraints, to couple the inference over concepts that are related in in a taxonomic hierarchy. The reasoning is cast into an integer linear programming (ILP), and we leverage the theory of reduction costs of a relaxed LP to compute informative rankings. This methodology is applied to several large CSK collections. Our evaluation shows that we can consolidate these inputs into much cleaner and more expressive knowledge. Results are available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：常识知识（CSK）支持多种AI应用，从视觉理解聊天机器人。在获取CSK之前的作品，如ConceptNet，编译语句关联的概念，像日常生活中的物体或活动，性质搁置了大部分或概念的若干实例。每个概念隔离治疗与其他概念和属性的唯一定量测量（或排序）是置信得分的声明是有效的。本文旨在通过引入CSK语句和方法的多面模型在台相互关联的语句联合推理来克服这些限制。我们的模型捕获CSK报表的四个维度：合理性，典型性，remarkability和显着性，与得分和沿每个维度的排名。例如，鬣狗饮用水是典型的但不显着，而鬣狗吃尸体是显着的。推理和排名，我们开发了一个方法与软约束，耦合，而忽视了在一个分类层次结构相关的概念推理。推理铸造成整数线性规划（ILP），和我们利用的轻松LP的降低成本的理论来计算信息排名。这种方法适用于几个大的CSK集合。我们的评估显示，我们可以整合这些投入更清洁，更富有表现力的知识。结果可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper6"> <b>6. ProphetNet: Predicting Future N-gram for Sequence-to-Sequence  Pre-training</b>  <a href="https://arxiv.org/pdf/2001.04063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weizhen Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yeyun Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dayiheng Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiusheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruofei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: In this paper, we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism.Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step.The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB) respectively. Experimental results show ProphetNet achieves the best performance on both abstractive summarization and question generation tasks compared to the models using the same base scale pre-training dataset. For the large scale dataset pre-training, ProphetNet achieves new state-of-the-art results on Gigaword and comparable results on CNN/DailyMail using only about 1/5 pre-training epochs of the previous model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出名为ProphetNet一个新的序列到序列前的训练模式，它引入了一个新的自我监督的目标命名为将来的n-gram预测和建议的N流的自我关注的mechanism.Instead在传统的序列到序列模型中的一个步骤的提前预测的优化，ProphetNet由n-领先一步预测该预测下一个n各自时间step.The将来的n-gram在预测令牌同时基于先前上下文令牌明确地优化鼓励模型来规划未来的令牌，并防止过度拟合强大的本地相关性。我们使用碱规模的数据集（16GB）和分别大规模数据集（160GB）预列车ProphetNet。实验结果表明ProphetNet达到上相比，使用相同的基本预分训练数据集模型既抽象总结和询问生成任务的最佳性能。对于大规模数据集前培训，ProphetNet实现国家的最先进的新的Gigaword和使用CNN /每日邮报以前的型号只有约1/5前的训练时期比较的结果的结果。</font>
</div>


<hr>
<div id="paper7"> <b>7. Stochastic Natural Language Generation Using Dependency Information</b>  <a href="https://arxiv.org/pdf/2001.03897" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Seifossadat%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elham Seifossadat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sameti%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossein Sameti</a><br>
<font size="3">
Abstract: This article presents a stochastic corpus-based model for generating natural language text. Our model first encodes dependency relations from training data through a feature set, then concatenates these features to produce a new dependency tree for a given meaning representation, and finally generates a natural language utterance from the produced dependency tree. We test our model on nine domains from tabular, dialogue act and RDF format. Our model outperforms the corpus-based state-of-the-art methods trained on tabular datasets and also achieves comparable results with neural network-based approaches trained on dialogue act, E2E and WebNLG datasets for BLEU and ERR evaluation metrics. Also, by reporting Human Evaluation results, we show that our model produces high-quality utterances in aspects of informativeness and naturalness as well as quality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了生成自然语言文本随机基于语料库的模型。我们的模式首先编码的依赖性和通过功能训练数据集的关系，然后连接这些特征来产生一个给定的意思表示一个新的依赖关系树，最后产生从产生依赖关系树的自然语言语句。我们测试我们从表格，对话行为和RDF格式9个域模型。我们的模型优于训练有素的表格数据集基于语料库的国家的最先进的方法和也实现了比较的结果神经网络的基础上对话行为，E2E和WebNLG数据集的BLEU和ERR评价指标办法训练。此外，通过报告人的评价结果​​，我们表明，我们的模型在信息量和自然，以及质量方面的生产高品质的话语。</font>
</div>


<hr>
<div id="paper8"> <b>8. Rethinking Generalization of Neural Models: A Named Entity Recognition  Case Study</b>  <a href="https://arxiv.org/pdf/2001.03844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinlan Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuanjing Huang</a><br>
<font size="3">
Abstract: While neural network-based models have achieved impressive performance on a large body of NLP tasks, the generalization behavior of different models remains poorly understood: Does this excellent performance imply a perfect generalization model, or are there still some limitations? In this paper, we take the NER task as a testbed to analyze the generalization behavior of existing models from different perspectives and characterize the differences of their generalization abilities through the lens of our proposed measures, which guides us to better design models and training methods. Experiments with in-depth analyses diagnose the bottleneck of existing neural NER models in terms of breakdown performance analysis, annotation errors, dataset bias, and category relationships, which suggest directions for improvement. We have released the datasets: (ReCoNLL, PLONER) for the future research at our project page: this http URL. As a by-product of this paper, we have open-sourced a project that involves a comprehensive summary of recent NER papers and classifies them into different research topics: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管基于神经网络的模型已在大机构的NLP任务，取得了骄人的业绩，不同型号的遗体推广行为知之甚少：这是否出色表现意味着一个完美的泛化模型，还是有仍有一定的局限性？在本文中，我们采取了NER任务作为测试平台，分析从不同的角度现有车型的推广行为，并通过我们的建议措施的镜头，是指导我们更好地设计模型和训练方法表征其泛化能力的差异。在深入分析实验诊断现有的神经NER模型的瓶颈在击穿性能分析，标注错误，数据集偏见和类别的关系，其提出改进方向的术语。我们已经发布了数据集：（ReCoNLL，PLONER）对未来的研究，在我们的项目页面：这个HTTP URL。作为本文的副产品，我们有开源的，涉及到的最近NER文件，并将其分类，全面总结成不同的研究课题项目：该HTTPS URL。</font>
</div>


<hr>
<div id="paper9"> <b>9. Revisiting Challenges in Data-to-Text Generation with Fact Grounding</b>  <a href="https://arxiv.org/pdf/2001.03830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongmin Wang</a><br>
<font size="3">
Abstract: Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, hoping to attract more research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据到文本代车型面临参照正确的输入源，确保数据的保真度的挑战。在这方面，怀斯曼等激励研究。 （2017）介绍了从箱 - 和线路得分表中生成的NBA比赛的摘要语料库RotoWire。然而，有限的尝试已在这方面取得和挑战依然存在。我们观察到，其中的总结内容只有约60％可以接地的技术统计记录的语料库一个突出的瓶颈。这样的信息不足往往误导了条件语言模型制作无条件随机的事实，从而导致实际的幻觉。在这项工作中，我们恢复信息平衡和改造这个任务专注于事实接地数据到文本生成。我们引进一个纯化和大规模数据集，RotoWire-FG（实况接地），从今年2017年覆盖和丰富的输入表50％以上的数据，希望能吸引更多的研究集中在这个方向。此外，我们通过表重建的一种新形式的积分作为辅助任务，以提高生成质量实现对国家的最先进的模型改进的数据的保真度。</font>
</div>


<hr>
<div id="paper10"> <b>10. Learning Cross-Context Entity Representations from Text</b>  <a href="https://arxiv.org/pdf/2001.03765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeffrey Ling</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=FitzGerald%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicholas FitzGerald</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zifei Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soares%2C+L+B" target="_blank" rel="noopener" style="color:#0000EE;">Livio Baldini Soares</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=F%C3%A9vry%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thibault Févry</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weiss%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Weiss</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kwiatkowski%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Kwiatkowski</a><br>
<font size="3">
Abstract: Language modeling tasks, in which words, or word-pieces, are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases or human readable encyclopedias tend to be entity-centric, we investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the text contexts in which those entities were mentioned. We show that large scale training of neural models allows us to learn high quality entity representations, and we demonstrate successful results on four domains: (1) existing entity-level typing benchmarks, including a 64% error reduction over previous work on TypeNet (Murty et al., 2018); (2) a novel few-shot category reconstruction task; (3) existing entity linking benchmarks, where we match the state-of-the-art on CoNLL-Aida without linking-specific features and obtain a score of 89.8% on TAC-KBP 2010 without using any alias table, external knowledge base or in domain training data and (4) answering trivia questions, which uniquely identify entities. Our global entity representations encode fine-grained type categories, such as Scottish footballers, and can answer trivia questions such as: Who was the last inmate of Spandau jail in Berlin? </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言建模任务，其中词或字块，在本地范围内的基础上预测，一直是学习的嵌入词和短语的背景有关的表示是非常有效的。通过观察该努力的代码世界知识转化为机器可读的知识基础或人类可读的百科全书往往是实体为中心的推动下，我们研究使用填充式的空白任务的学习实体的情况下独立表示从文本在这些实体中提到的上下文。我们展示的神经模型的规模大的培训，让我们了解高品质实体交涉，我们证明在四个主要领域成功的结果：（1）现有的实体层面打字的基准，其中包括64％的误差减少了以前的工作在键入net（穆尔蒂。等人，2018）; （2）一种新的几拍重建类别任务; （3）现有的实体连接的基准，在那里我们匹配状态的最先进的上CoNLL-阿依无需关联的特定功能，将获得于2010年05 TAC-KBP得分为89.8％，而无需使用任何别名表，外部知识库或在域训练数据和（4）回答琐事问题，唯一标识实体。我们的全球实体表示编码细粒度类型类别，如苏格兰足球运动员，并且可以回答小问题，如：谁是施潘道监狱在柏林的最后一个犯人？</font>
</div>


<hr>
<div id="paper11"> <b>11. PatentTransformer-2: Controlling Patent Text Generation by Structural  Metadata</b>  <a href="https://arxiv.org/pdf/2001.03708" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jieh-Sheng Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hsiang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jieh Hsiang</a><br>
<font size="3">
Abstract: PatentTransformer is our codename for patent text generation based on Transformer-based models. Our goal is "Augmented Inventing." In this second version, we leverage more of the structural metadata in patents. The structural metadata includes patent title, abstract, and dependent claim, in addition to independent claim previously. Metadata controls what kind of patent text for the model to generate. Also, we leverage the relation between metadata to build a text-to-text generation flow, for example, from a few words to a title, the title to an abstract, the abstract to an independent claim, and the independent claim to multiple dependent claims. The text flow can go backward because the relation is trained bidirectionally. We release our GPT-2 models trained from scratch and our code for inference so that readers can verify and generate patent text on their own. As for generation quality, we measure it by both ROUGE and Google Universal Sentence Encoder. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：PatentTransformer是我们基于基于变压器的新型专利文本生成代号。我们的目标是“增强发明了。”在第二个版本中，我们利用更多的结构性元数据的专利。结构元数据包括专利标题，摘要，以及从属权利要求中，除了独立权利要求先前。元数据控制什么样的专利文本为模型来生成。此外，我们利用的元数据之间的关系，以建立一个文本到文本生成流，例如，从几话标题，标题为抽象，抽象到一个独立的权利要求，以及在独立权利要求到多个从属索赔。因为关系是双向训练文本流可以去落后。我们发布我们从头开始训练的GPT-2机型和我们推断代码，使读者可以验证并产生自己的专利文本。至于代的品质，我们双方ROUGE和谷歌万能句子编码器测量。</font>
</div>


<hr>
<div id="paper12"> <b>12. Does syntax need to grow on trees? Sources of hierarchical inductive  bias in sequence-to-sequence networks</b>  <a href="https://arxiv.org/pdf/2001.03632" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=McCoy%2C+R+T" target="_blank" rel="noopener" style="color:#0000EE;">R. Thomas McCoy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Frank</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Linzen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tal Linzen</a><br>
<font size="3">
Abstract: Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：暴露在同样的训练数据学习者可以概括不同，由于不同的感性偏见。在神经网络模型，感性的偏见在理论上可以从模型架构的任何方面引起的。我们调查其建筑因素影响训练的两个句法任务，英语问题的形成和英语时态reinflection神经序列到序列模型的推广行为。对于这两个任务，训练集是基于层次结构和基于线性顺序的推广泛化一致。所有的建筑因素，我们调查定性的影响模型如何推广，其中包括没有明确的连接层次结构的因素。例如，LSTMs越冬和显示本质上不同的感应偏压。然而，持续推动整个任务的分层偏见的唯一因素是使用一个树形结构的模型，而不是连续的复发模型，这表明类似人类的语法概括要求的建筑句法结构。</font>
</div>


<hr>
<div id="paper13"> <b>13. Reformer: The Efficient Transformer</b>  <a href="https://arxiv.org/pdf/2001.04451" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kitaev%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikita Kitaev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+%C5%81" target="_blank" rel="noopener" style="color:#0000EE;">Łukasz Kaiser</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Levskaya%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anselm Levskaya</a><br>
<font size="3">
Abstract: Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型变压器模型通常实现多项任务的国家的最先进的成果，但训练这些模型可能极其昂贵的，特别是在长序列。我们介绍了两种技术来提高变压器的效率。首先，我们通过一个使用局部性敏感散列，从O（$ L ^ 2 $）至O（$ L \材L $），其中$ L $是的长度改变其复杂性替代点积关注顺序。此外，我们使用可逆残渣层而不是标准的残差，其允许在训练过程中，而不是$ N $倍，其中$ N $是层的数目仅一次存储激活。将得到的模型，重整器，而被更内存效率和长序列快得多与Transformer模型看齐进行。</font>
</div>


<hr>
<div id="paper14"> <b>14. LP-SparseMAP: Differentiable Relaxed Optimization for Sparse Structured  Prediction</b>  <a href="https://arxiv.org/pdf/2001.04437" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Niculae%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vlad Niculae</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T" target="_blank" rel="noopener" style="color:#0000EE;">André F. T. Martins</a><br>
<font size="3">
Abstract: Structured prediction requires manipulating a large number of combinatorial structures, e.g., dependency trees or alignments, either as latent or output variables. Recently, the SparseMAP method has been proposed as a differentiable, sparse alternative to maximum a posteriori (MAP) and marginal inference. SparseMAP returns a combination of a small number of structures, a desirable property in some downstream applications. However, SparseMAP requires a tractable MAP inference oracle. This excludes, e.g., loopy graphical models or factor graphs with logic constraints, which generally require approximate inference. In this paper, we introduce LP-SparseMAP, an extension of SparseMAP that addresses this limitation via a local polytope relaxation. LP-SparseMAP uses the flexible and powerful domain specific language of factor graphs for defining and backpropagating through arbitrary hidden structure, supporting coarse decompositions, hard logic constraints, and higher-order correlations. We derive the forward and backward algorithms needed for using LP-SparseMAP as a hidden or output layer. Experiments in three structured prediction tasks show benefits compared to SparseMAP and Structured SVM. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结构化预测需要操纵大量组合结构，例如，依赖树木或比对，无论是作为潜在的或输出变量。最近，SparseMAP方法已经被提出作为一个微的，稀疏替代最大后验（MAP）和边际推理。 SparseMAP返回少量的结构，在一些下游应用的期望特性的组合。然而，SparseMAP需要一个听话的地图推断预言。这不包括，例如，多圈图形模型或因子图与逻辑约束，这通常需要近似推断。在本文中，我们介绍了LP-SparseMAP，SparseMAP的扩展，地址通过本地多面体放松这一限制。 LP-SparseMAP使用用于定义和通过任意隐藏结构backpropagating，支撑粗分解，硬逻辑约束，和更高阶的相关性因子图的灵活和强大的域专用语言。我们推导需要使用LP-SparseMAP作为隐藏或输出层中的向前和向后的算法。在三个结构预测任务实验表明相比SparseMAP和结构化SVM的好处。</font>
</div>


<hr>
<div id="paper15"> <b>15. Negative Statements Considered Useful</b>  <a href="https://arxiv.org/pdf/2001.04425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Arnaout%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiba Arnaout</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Razniewski%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Razniewski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weikum%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerhard Weikum</a><br>
<font size="3">
Abstract: Knowledge bases (KBs), pragmatic collections of knowledge about notable entities, are an important asset in applications such as search, question answering and dialogue. Rooted in a long tradition in knowledge representation, all popular KBs only store positive information, while they abstain from taking any stance towards statements not contained in them. In this paper, we make the case for explicitly stating interesting statements which are not true. Negative statements would be important to overcome current limitations of question answering, yet due to their potential abundance, any effort towards compiling them needs a tight coupling with ranking. We introduce two approaches towards compiling negative statements. (i) In peer-based statistical inferences, we compare entities with highly related entities in order to derive potential negative statements, which we then rank using supervised and unsupervised features. (ii) In query-log-based text extraction, we use a pattern-based approach for harvesting search engine query logs. Experimental results show that both approaches hold promising and complementary potential. Along with this paper, we publish the first datasets on interesting negative information, containing over 1.1M statements for 100K popular Wikidata entities. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：知识库（KBS），约著名的实体知识务实的集合，是在应用程序，如搜索，问答和对话的重要资产。在知识表示有着悠久的传统根深蒂固，所有流行的知识库系统只保存正面信息，而他们从迈出不包含在他们陈述的任何立场弃权。在本文中，我们做出明确说明有趣的声明不属实的情况。克服答疑的电流限制否定陈述将是重要的，但由于其潜在的丰富，对编译他们的任何努力，需要与排名的紧密耦合。我们引入对编译否定陈述两种方法。 （一）在对等的统计推断，我们比较具有高度相关实体的实体，以得出潜在的负面陈述，然后我们使用级监督和无监督的功能。 （二）在查询日志基于文本的提取，我们用收获的搜索引擎查询日志基于模式的方法。实验结果表明，这两种方法保持承诺和互补的潜力。除了本文中，我们公布有趣的负面信息的第一数据集，包含100K流行的维基数据实体超过1.1M的语句。</font>
</div>


<hr>
<div id="paper16"> <b>16. Asymmetrical Hierarchical Networks with Attentive Interactions for  Interpretable Review-Based Recommendation</b>  <a href="https://arxiv.org/pdf/2001.04346" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingchao Ni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengzhang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Zong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongjin Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanchi Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haifeng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Melo%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerard de Melo</a><br>
<font size="3">
Abstract: Recently, recommender systems have been able to emit substantially improved recommendations by leveraging user-provided reviews. Existing methods typically merge all reviews of a given user or item into a long document, and then process user and item documents in the same manner. In practice, however, these two sets of reviews are notably different: users' reviews reflect a variety of items that they have bought and are hence very heterogeneous in their topics, while an item's reviews pertain only to that single item and are thus topically homogeneous. In this work, we develop a novel neural network model that properly accounts for this important difference by means of asymmetric attentive modules. The user module learns to attend to only those signals that are relevant with respect to the target item, whereas the item module learns to extract the most salient contents with regard to properties of the item. Our multi-hierarchical paradigm accounts for the fact that neither are all reviews equally useful, nor are all sentences within each review equally pertinent. Extensive experimental results on a variety of real datasets demonstrate the effectiveness of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，推荐系统已经能够通过利用用户提供的评论发出显着改善的建议。现有的方法通常合并给定用户或项目的所有评价为长的文档，然后处理以同样的方式用户和项目的文件。然而在实践中，这两组的评论是显着不同：用户的评价反映的各种物品，他们已经买了，并因此在其主题非常庞杂，而项目的审查只涉及到单个项目，因此是局部均匀。在这项工作中，我们开发了妥善占不对称周到模块的方式这一重要区别一个新的神经网络模型。用户模块学会照顾只有那些相关的相对于目标项目的信号，而项目模块学会了关于该项目的属性提取最突出的内容。我们的多层次模式考虑的事实是，无论是全部评论同样有用，也不是每个评论中的所有语句同样相关。在各种真实数据集的大量实验结果证明了该方法的有效性。</font>
</div>


<hr>
<div id="paper17"> <b>17. Shareable Representations for Search Query Understanding</b>  <a href="https://arxiv.org/pdf/2001.04345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mukul Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youna Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Headden%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Will Headden</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goutam%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rahul Goutam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heran Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bing Yin</a><br>
<font size="3">
Abstract: Understanding search queries is critical for shopping search engines to deliver a satisfying customer experience. Popular shopping search engines receive billions of unique queries yearly, each of which can depict any of hundreds of user preferences or intents. In order to get the right results to customers it must be known queries like "inexpensive prom dresses" are intended to not only surface results of a certain product type but also products with a low price. Referred to as query intents, examples also include preferences for author, brand, age group, or simply a need for customer service. Recent works such as BERT have demonstrated the success of a large transformer encoder architecture with language model pre-training on a variety of NLP tasks. We adapt such an architecture to learn intents for search queries and describe methods to account for the noisiness and sparseness of search query data. We also describe cost effective ways of hosting transformer encoder models in context with low latency requirements. With the right domain-specific training we can build a shareable deep learning model whose internal representation can be reused for a variety of query understanding tasks including query intent identification. Model sharing allows for fewer large models needed to be served at inference time and provides a platform to quickly build and roll out new search query classifiers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：了解搜索查询是购物搜索引擎提供一个满意的客户体验至关重要。流行的购物搜索引擎获得数十亿年唯一的查询，每一个都可以描绘出任何数百个用户的偏好或意图的。为了得到正确的结果，必须知道像“便宜的舞会礼服”旨在不仅是某个产品类型的表面效果，也具有价格低的产品查询客户。称为查询意图，例子还包括作者，品牌，年龄组或只是需要为客户服务的偏好。最近的作品如BERT都展现了大型变压器编码器架构，拥有对各种NLP任务语言模型前培训的成功。我们采用这样的架构，以学习为搜索查询意图和描述的是占搜索查询数据的吵闹和稀疏。我们还描述在低延迟要求的背景下举办的变压器编码器模型的经济有效的方式。有了正确的特定领域的培训，我们可以建立其内部表示可以为多种查询理解任务，包括查询意图识别重复使用一个共享的深度学习模式。模型共享允许在需要推理时间送达较少的大型模型，并提供了一个平台快速构建并推出新的搜索查询的分类。</font>
</div>


<hr>
<div id="paper18"> <b>18. Improving Dysarthric Speech Intelligibility Using Cycle-consistent  Adversarial Training</b>  <a href="https://arxiv.org/pdf/2001.04260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+S+H" target="_blank" rel="noopener" style="color:#0000EE;">Seung Hee Yang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chung%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minhwa Chung</a><br>
<font size="3">
Abstract: Dysarthria is a motor speech impairment affecting millions of people. Dysarthric speech can be far less intelligible than those of non-dysarthric speakers, causing significant communication difficulties. The goal of our work is to develop a model for dysarthric to healthy speech conversion using Cycle-consistent GAN. Using 18,700 dysarthric and 8,610 healthy control Korean utterances that were recorded for the purpose of automatic recognition of voice keyboard in a previous study, the generator is trained to transform dysarthric to healthy speech in the spectral domain, which is then converted back to speech. Objective evaluation using automatic speech recognition of the generated utterance on a held-out test set shows that the recognition performance is improved compared with the original dysarthic speech after performing adversarial training, as the absolute WER has been lowered by 33.4%. It demonstrates that the proposed GAN-based conversion method is useful for improving dysarthric speech intelligibility. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：构音障碍是影响数百万人的电机语言障碍。构音障碍的言语可以比那些非构音障碍的扬声器远不如理解，造成显著沟通困难。我们工作的目标是开发用于构音障碍的使用周期一致甘健康语音转换模型。使用18700构音障碍，并且记录在先前的研究中自动识别语音键盘的目的8,610健康控制朝鲜的言论，发电机被训练在频域中，然后将其转换回语音转换构音障碍的健康讲话。客观评价使用上的保持输出测试组示出了识别性能与执行对抗性训练后的原始dysarthic语音相比得到改善，作为绝对WER已经被降低了33.4％的产生的话语的自动语音识别。这表明，所提出的基于GaN的转换方法是提高构音障碍的语音清晰度非常有用。</font>
</div>


<hr>
<div id="paper19"> <b>19. Structural Decompositions of Epistemic Logic Programs</b>  <a href="https://arxiv.org/pdf/2001.04219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hecher%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Markus Hecher</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morak%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Morak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Woltran%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Woltran</a><br>
<font size="3">
Abstract: Epistemic logic programs (ELPs) are a popular generalization of standard Answer Set Programming (ASP) providing means for reasoning over answer sets within the language. This richer formalism comes at the price of higher computational complexity reaching up to the fourth level of the polynomial hierarchy. However, in contrast to standard ASP, dedicated investigations towards tractability have not been undertaken yet. In this paper, we give first results in this direction and show that central ELP problems can be solved in linear time for ELPs exhibiting structural properties in terms of bounded treewidth. We also provide a full dynamic programming algorithm that adheres to these bounds. Finally, we show that applying treewidth to a novel dependency structure---given in terms of epistemic literals---allows to bound the number of ASP solver calls in typical ELP solving procedures. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：认知逻辑程序（电子学习）是标准的回答集编程（ASP）提供用于在语言中的推理在结果集的流行推广。这更丰富的形式主义来以较高的计算复杂性达到最高多项式层次的第四级的价格。然而，相对于标准的ASP，朝易处理专用的调查还没有进行呢。在本文中，我们让在这个方向的第一结果和表明中央ELP问题可以在线性时间内解决了在有界树宽的方面表现出结构性质电子学习。我们还提供一个完整的动态规划算法了符合这些界限。最后，我们表明，将树宽以一种新颖的依赖结构---在认识文字的形式给出---允许的ASP求解器的典型ELP解决过程的调用绑定的号码。</font>
</div>


<hr>
<div id="paper20"> <b>20. A logic-based relational learning approach to relation extraction: The  OntoILPER system</b>  <a href="https://arxiv.org/pdf/2001.04192" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lima%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rinaldo Lima</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Espinasse%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernard Espinasse</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Freitas%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fred Freitas</a><br>
<font size="3">
Abstract: Relation Extraction (RE), the task of detecting and characterizing semantic relations between entities in text, has gained much importance in the last two decades, mainly in the biomedical domain. Many papers have been published on Relation Extraction using supervised machine learning techniques. Most of these techniques rely on statistical methods, such as feature-based and tree-kernels-based methods. Such statistical learning techniques are usually based on a propositional hypothesis space for representing examples, i.e., they employ an attribute-value representation of features. This kind of representation has some drawbacks, particularly in the extraction of complex relations which demand more contextual information about the involving instances, i.e., it is not able to effectively capture structural information from parse trees without loss of information. In this work, we present OntoILPER, a logic-based relational learning approach to Relation Extraction that uses Inductive Logic Programming for generating extraction models in the form of symbolic extraction rules. OntoILPER takes profit of a rich relational representation of examples, which can alleviate the aforementioned drawbacks. The proposed relational approach seems to be more suitable for Relation Extraction than statistical ones for several reasons that we argue. Moreover, OntoILPER uses a domain ontology that guides the background knowledge generation process and is used for storing the extracted relation instances. The induced extraction rules were evaluated on three protein-protein interaction datasets from the biomedical domain. The performance of OntoILPER extraction models was compared with other state-of-the-art RE systems. The encouraging results seem to demonstrate the effectiveness of the proposed solution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：关系抽取（RE），检测和文本中的实体之间的表征语义关系的任务，获得了巨大的重要性在过去的二十年中，主要是在生物医学领域。许多论文已使用监督机器学习技术发表了关系抽取。这些技术大部分依赖于统计方法，如基于树的内核基于特征和方法。这样的统计学习的技术通常是基于用于表示实施例中，即一个命题假设空间，他们采用的特征的属性 - 值表示。这种表示法存在一些缺陷，特别是在复杂的关系，其中要求对涉及的情况下，即更多的上下文信息的提取，它不能有效地捕捉解析树结构信息不会丢失信息。在这项工作中，我们目前OntoILPER，一种基于逻辑的关系学习方法关系抽取使用归纳逻辑程序设计中的象征提取规则的形式产生的提取模式。 OntoILPER需要的例子丰富的关系表示，这可以减轻上述缺点的利润。拟议的关系的方式似乎更适合关系抽取比统计的人有几个原因，我们认为。此外，OntoILPER使用领域本体引导的背景知识生成处理，用于存储提取的关系实例。从生物医学域中的三个蛋白质 - 蛋白质相互作用数据集的感应提取规则进行评价。 OntoILPER提取模型的性能与国家的最先进的其它可再生能源系统进行了比较。令人鼓舞的结果似乎证明了该解决方案的有效性。</font>
</div>


<hr>
<div id="paper21"> <b>21. Retouchdown: Adding Touchdown to StreetLearn as a Shareable Resource for  Language Grounding Tasks in Street View</b>  <a href="https://arxiv.org/pdf/2001.03671" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mehta%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Harsh Mehta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Artzi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoav Artzi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baldridge%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Baldridge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ie%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eugene Ie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mirowski%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Mirowski</a><br>
<font size="3">
Abstract: The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both of the Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in Chen et al. (2019) and show that the panoramas we have added to StreetLearn fully support both Touchdown tasks and can be used effectively for further research and comparison. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：（Chen等，2019）着陆数据集由通过纽约市的街道导航人工注释，并在给定的位置，解决空间的描述提供了说明。为了使更广泛的研究团体与着陆任务有效地开展工作，我们公开发布的29K原街景全景图所需的触地得分。我们遵循用于StreetLearn数据发布过程（Mirowski等，2019），以检查全景的个人身份信息，模糊它们是必要的。这些已被添加到所述数据集StreetLearn并如前面对StreetLearn使用可以通过相同的过程来获得。我们还为双方的着陆任务提供一个参考实现：视觉和语言导航（VLN）和空间分辨率描述（SDR）。我们比较我们的模型结果与陈等人给出的。 （2019），并表明我们已经添加到StreetLearn全景图完全支持着陆任务，可以进一步研究和比较有效地使用。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-13</title>
    <url>/2020/01/14/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-13/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Towards Minimal Supervision BERT-based Grammar Error Correction <a href="https://arxiv.org/pdf/2001.03521" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Co-evolution of language and agents in referential games <a href="https://arxiv.org/pdf/2001.03361" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><div id="title3">
<b>3.</b> Machine Learning Approaches for Amharic Parts-of-speech Tagging <a href="https://arxiv.org/pdf/2001.03324" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>


<div id="title4">
<b>4.</b> Learning to Multi-Task Learn for Better Neural Machine Translation <a href="https://arxiv.org/pdf/2001.03294" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> A Scalable Chatbot Platform Leveraging Online Community Posts: A  Proof-of-Concept Study <a href="https://arxiv.org/pdf/2001.03278" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Simulating Lexical Semantic Change from Sense-Annotated Data <a href="https://arxiv.org/pdf/2001.03216" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Debate Dynamics for Human-comprehensible Fact-checking on Knowledge  Graphs <a href="https://arxiv.org/pdf/2001.03436" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Inductive Document Network Embedding with Topic-Word Attention <a href="https://arxiv.org/pdf/2001.03369" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Linking Social Media Posts to News with Siamese Transformers <a href="https://arxiv.org/pdf/2001.03303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Towards Minimal Supervision BERT-based Grammar Error Correction</b>  <a href="https://arxiv.org/pdf/2001.03521" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiyuan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonios Anastasopoulos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W" target="_blank" rel="noopener" style="color:#0000EE;">Alan W Black</a><br>
<font size="3">
Abstract: Current grammatical error correction (GEC) models typically consider the task as sequence generation, which requires large amounts of annotated data and limit the applications in data-limited settings. We try to incorporate contextual information from pre-trained language model to leverage annotation and benefit multilingual scenarios. Results show strong potential of Bidirectional Encoder Representations from Transformers (BERT) in grammatical error correction task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当前语法纠错（GEC）模型通常考虑的任务，因为序列产生，这需要大量的注释数据，并限制在数据有限的情况下的应用程序。我们尝试从预先训练语言模型来杠杆注释结合上下文信息，有利于多语言情景。结果表明，从变形金刚（BERT）的语法纠错任务双向编码器交涉的巨大潜力。</font>
</div>


<hr>
<div id="paper2"> <b>2. Co-evolution of language and agents in referential games</b>  <a href="https://arxiv.org/pdf/2001.03361" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dagan%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gautier Dagan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hupkes%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dieuwke Hupkes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bruni%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elia Bruni</a><br>
<font size="3">
Abstract: Referential games offer a grounded learning environment for neural agents, that accounts for the functional aspects of language. However, they fail to account for another fundamental aspect of human language: Because languages are transmitted from generation to generation, they have to be learnable by new language users, which makes them subject to cultural evolution. Recent work has shown that incorporating cultural evolution in referential game results in considerable improvements in the properties of the languages that emerge in the game. In this work, we first substantiate this claim with a different data set and a wider array of evaluation metrics. Then, drawing inspiration from linguistic theories of human language evolution, we consider a scenario in which not only cultural but also genetic evolution is integrated. As our core contribution, we introduce the Language Transmission Engine, in which cultural evolution of the language is combined with genetic evolution of the agents' architecture. We show that this co-evolution scenario leads to across-the-board improvements on all considered metrics. These results stress that cultural evolution is important for language emergence studies, but also the suitability of the architecture itself should be considered. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：参照游戏提供接地的学习环境，为神经剂，这占了语言的功能方面。然而，他们无法解释人类语言的另一个重要方面：由于语言从代代相传，他们必须通过新的语言的用户，这使得他们受到文化的演变可以学习的。最近的研究显示，在纳入参考的比赛结果在在游戏中出现的语言的性质相当大的改善文化的演变。在这项工作中，我们首先证实这一要求与不同的数据集和评价度量的更广泛的阵列。然后，从人类语言进化的语言学理论中汲取灵感，我们认为这不仅是文化，而且基因进化集成的场景。作为我们的核心贡献，我们介绍了语言传输引擎，其中语言文化演进与代理的架构的遗传进化相结合。我们表明，这种协同进化的情况导致对所有考虑的指标，全面的板的改进。这些结果强调的是文化进化是语言出现的研究很重要，而且建筑本身的适用性应予以考虑。</font>
</div>


<hr>
<div id="paper3"> <b>3. Machine Learning Approaches for Amharic Parts-of-speech Tagging</b>  <a href="https://arxiv.org/pdf/2001.03324" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gashaw%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ibrahim Gashaw</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shashirekha%2C+H+L" target="_blank" rel="noopener" style="color:#0000EE;">H L. Shashirekha</a><br>
<font size="3">
Abstract: Part-of-speech (POS) tagging is considered as one of the basic but necessary tools which are required for many Natural Language Processing (NLP) applications such as word sense disambiguation, information retrieval, information processing, parsing, question answering, and machine translation. Performance of the current POS taggers in Amharic is not as good as that of the contemporary POS taggers available for English and other European languages. The aim of this work is to improve POS tagging performance for the Amharic language, which was never above 91%. Usage of morphological knowledge, an extension of the existing annotated data, feature extraction, parameter tuning by applying grid search and the tagging algorithms have been examined and obtained significant performance difference from the previous works. We have used three different datasets for POS experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：部分的词类（POS）标记被认为是其所需的许多自然语言处理（NLP）的应用，如词义消歧，信息检索，信息处理，分析，问题解答基本而必要的工具之一，和机器翻译。在阿姆哈拉语当前POS标注器的性能还不如说可用于英语和其他欧洲语言的当代POS标注器的。这项工作的目的是为了改进为阿姆哈拉语，这是从来没有91％以上的词性标注的性能。形态的知识，现有的注解数据的扩展，特征提取，参数整定运用网格搜索和标记算法的使用已经被检查，并从以前的作品获得显著的性能差异。我们使用了三种不同的数据集用于POS实验。</font>
</div>


<hr>
<div id="paper4"> <b>4. Learning to Multi-Task Learn for Better Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2001.03294" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zaremoodi%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Poorya Zaremoodi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gholamreza Haffari</a><br>
<font size="3">
Abstract: Scarcity of parallel sentence pairs is a major challenge for training high quality neural machine translation (NMT) models in bilingually low-resource scenarios, as NMT is data-hungry. Multi-task learning is an elegant approach to inject linguistic-related inductive biases into NMT, using auxiliary syntactic and semantic tasks, to improve generalisation. The challenge, however, is to devise effective training schedules, prescribing when to make use of the auxiliary tasks during the training process to fill the knowledge gaps of the main translation task, a setting referred to as biased-MTL. Current approaches for the training schedule are based on hand-engineering heuristics, whose effectiveness vary in different MTL settings. We propose a novel framework for learning the training schedule, ie learning to multi-task learn, for the MTL setting of interest. We formulate the training schedule as a Markov decision process which paves the way to employ policy learning methods to learn the scheduling policy. We effectively and efficiently learn the training schedule policy within the imitation learning framework using an oracle policy algorithm that dynamically sets the importance weights of auxiliary tasks based on their contributions to the generalisability of the main NMT task. Experiments on low-resource NMT settings show the resulting automatically learned training schedulers are competitive with the best heuristics, and lead to up to +1.1 BLEU score improvements. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：平行句对稀缺是在双语低资源方案培养高素质神经机器翻译（NMT）车型的一大挑战，因为NMT是大量数据的。多任务学习是注入语言相关的感性偏见到NMT，利用辅助句法和语义的任务，以提高泛化一个优雅的方法。我们面临的挑战，但是，是制定有效的培训计划，开处方时，在训练过程中使用的辅助任务，填补了主要翻译任务的知识差距，设定被称为偏压MTL。对于训练计划目前的做法是基于手工工程启发式，其有效性在不同MTL设置而异。我们提出了学习培训计划，即学习多任务学习，感兴趣的MTL设置一个新的框架。我们制定的训练计划为马尔可夫决策过程，铺平了道路雇用政策的学习方法来学习调度策略。我们有效地学习使用Oracle策略算法，动态设置的基础上他们的主要任务NMT的普适性贡献辅助任务的重要性权重模仿学习框架内的培训计划政策。在低资源NMT设置实验表明所产生的自动学习训练调度与最好的启发式竞争力，并导致高达+1.​​1 BLEU得分的改善。</font>
</div>


<hr>
<div id="paper5"> <b>5. A Scalable Chatbot Platform Leveraging Online Community Posts: A  Proof-of-Concept Study</b>  <a href="https://arxiv.org/pdf/2001.03278" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sihyeon Jo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Im%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sangwon Im</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">SangWook Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S+H" target="_blank" rel="noopener" style="color:#0000EE;">Seung Hee Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hee-Eun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seong-Woo Kim</a><br>
<font size="3">
Abstract: The development of natural language processing algorithms and the explosive growth of conversational data are encouraging researches on the human-computer conversation. Still, getting qualified conversational data on a large scale is difficult and expensive. In this paper, we verify the feasibility of constructing a data-driven chatbot with processed online community posts by using them as pseudo-conversational data. We argue that chatbots for various purposes can be built extensively through the pipeline exploiting the common structure of community posts. Our experiment demonstrates that chatbots created along the pipeline can yield the proper responses. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自然语言处理算法的开发和会话数据的爆炸性增长是令人鼓舞的人机对话的研究。尽管如此，越来越大规模合格的会话数据难，看病贵。在本文中，我们核实使用它们作为伪会话数据建设有处理在线社区的帖子一个数据驱动的聊天机器人的可行性。我们认为，出于各种目的聊天机器人，可以通过管道利用社区帖子的共同结构广泛建立。我们的实验表明，沿管道创建聊天机器人能得到适当的回应。</font>
</div>


<hr>
<div id="paper6"> <b>6. Simulating Lexical Semantic Change from Sense-Annotated Data</b>  <a href="https://arxiv.org/pdf/2001.03216" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schlechtweg%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dominik Schlechtweg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Walde%2C+S+S+i" target="_blank" rel="noopener" style="color:#0000EE;">Sabine Schulte im Walde</a><br>
<font size="3">
Abstract: We present a novel procedure to simulate lexical semantic change from synchronic sense-annotated data, and demonstrate its usefulness for assessing lexical semantic change detection models. The induced dataset represents a stronger correspondence to empirically observed lexical semantic change than previous synthetic datasets, because it exploits the intimate relationship between synchronic polysemy and diachronic change. We publish the data and provide the first large-scale evaluation gold standard for LSC detection models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种新的方法来模拟从共时性意义标注的数据词汇语义变化，并展示其评估词汇语义变化检测模型有效性。感应数据集表示更强的对应于比以前的合成数据集经验观察词汇语义变化，因为它利用共时多义性和历时变化之间的亲密关系。我们发布的数据，并提供了LSC检测模型的首次大规模评估的黄金标准。</font>
</div>


<hr>
<div id="paper7"> <b>7. Debate Dynamics for Human-comprehensible Fact-checking on Knowledge  Graphs</b>  <a href="https://arxiv.org/pdf/2001.03436" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hildebrandt%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marcel Hildebrandt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Serna%2C+J+A+Q" target="_blank" rel="noopener" style="color:#0000EE;">Jorge Andres Quintero Serna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunpu Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ringsquandl%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Ringsquandl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Joblin%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mitchell Joblin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tresp%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Volker Tresp</a><br>
<font size="3">
Abstract: We propose a novel method for fact-checking on knowledge graphs based on debate dynamics. The underlying idea is to frame the task of triple classification as a debate game between two reinforcement learning agents which extract arguments -- paths in the knowledge graph -- with the goal to justify the fact being true (thesis) or the fact being false (antithesis), respectively. Based on these arguments, a binary classifier, referred to as the judge, decides whether the fact is true or false. The two agents can be considered as sparse feature extractors that present interpretable evidence for either the thesis or the antithesis. In contrast to black-box methods, the arguments enable the user to gain an understanding for the decision of the judge. Moreover, our method allows for interactive reasoning on knowledge graphs where the users can raise additional arguments or evaluate the debate taking common sense reasoning and external information into account. Such interactive systems can increase the acceptance of various AI applications based on knowledge graphs and can further lead to higher efficiency, robustness, and fairness. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了基于辩论动力学知识图其实检查的新方法。其基本思想是将框架三重分类的任务，两个加强学习剂，其提取参数之间辩论的游戏 - 在知识图上的路径 - 用进球来证明的事实是真实的（论文）或事实是假的（对立面），分别。根据这些参数，一个二元分类，简称判断，决定是否其实是真还是假。这两种药剂可以看作是稀疏的特征提取，对于无论是论文或对立面目前可解释的证据。相较于黑箱方法，参数使用户获得了法官的决定的理解。此外，我们的方法允许对知识图，其中用户可以提出额外的参数或评估的辩论采取常识推理和外部信息纳入考虑交互推理。这样的交互系统可以增加接受的基于知识的图表各种AI应用，并进一步导致更高的效率，稳健性和公平性。</font>
</div>


<hr>
<div id="paper8"> <b>8. Inductive Document Network Embedding with Topic-Word Attention</b>  <a href="https://arxiv.org/pdf/2001.03369" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Brochier%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robin Brochier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guille%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrien Guille</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Velcin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Velcin</a><br>
<font size="3">
Abstract: Document network embedding aims at learning representations for a structured text corpus i.e. when documents are linked to each other. Recent algorithms extend network embedding approaches by incorporating the text content associated with the nodes in their formulations. In most cases, it is hard to interpret the learned representations. Moreover, little importance is given to the generalization to new documents that are not observed within the network. In this paper, we propose an interpretable and inductive document network embedding method. We introduce a novel mechanism, the Topic-Word Attention (TWA), that generates document representations based on the interplay between word and topic representations. We train these word and topic vectors through our general model, Inductive Document Network Embedding (IDNE), by leveraging the connections in the document network. Quantitative evaluations show that our approach achieves state-of-the-art performance on various networks and we qualitatively show that our model produces meaningful and interpretable representations of the words, topics and documents. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文档在网络学习表示了结构化文本语料库即当文档相互链接嵌入目标。最近算法扩展网络嵌入通过将在它们的制剂中的节点相关联的文本内容接近。在大多数情况下，这是很难解释学表示。此外，小的重要性是考虑到泛化到未在网络内观察到新文档。在本文中，我们提出了一个解释和归纳文档网络嵌入方法。我们引入新的机制，主题字注意（TWA），其基于字和主题陈述之间的相互文档表示。我们培养这些词和话题载体通过我们的一般模型，归纳文档网络嵌入（IDNE），通过利用文档网络中的连接。定量评估表明，我们的方法实现了各种网络上的国家的最先进的性能和我们定性地表明，我们的模型产生的话，主题和文件有意义的，可解释的表示。</font>
</div>


<hr>
<div id="paper9"> <b>9. Linking Social Media Posts to News with Siamese Transformers</b>  <a href="https://arxiv.org/pdf/2001.03303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Danovitch%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jacob Danovitch</a><br>
<font size="3">
Abstract: Many computational social science projects examine online discourse surrounding a specific trending topic. These works often involve the acquisition of large-scale corpora relevant to the event in question to analyze aspects of the response to the event. Keyword searches present a precision-recall trade-off and crowd-sourced annotations, while effective, are costly. This work aims to enable automatic and accurate ad-hoc retrieval of comments discussing a trending topic from a large corpus, using only a handful of seed news articles. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：许多计算社会科学的研究项目围绕特定热门话题的在线话语。这些作品往往涉及收购有关问题的情况下大规模语料库的分析应对事件的各个方面。关键字搜索呈现精密召回权衡和人群来源的注解，而有效的，是昂贵的。这项工作的目的在于使的意见，从大语料库讨论一个热门话题自动精确的ad-hoc检索，仅使用种子的新闻报道屈指可数。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【论文笔记】Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context</title>
    <url>/2020/01/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Task-Oriented-Dialog-Systems-that-Consider-Multiple-Appropriate-Responses-under-the-Same-Context/</url>
    <content><![CDATA[<p><strong>Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context</strong>. Yichi Zhang, Zhijian Ou, Zhou Yu. <a href="https://arxiv.org/abs/1911.10484" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p><img src="/images/DADL1.jpg" alt></p><p><img src="/images/DADL2.jpg" alt></p><p>在对话中，对于同一句话，可以有多种回复。但是，现有模型往往趋于生成出现概率最高的回复，而忽视了概率较低的回复。本文通过数据增强的方法，使得模型具备生成多样化回复的能力。</p><a id="more"></a>



<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p><img src="/images/DADL4.jpg" alt><br>在数据预处理阶段，在整个数据集中，找出所有的dialogue state相同的system actions，作为ground truth的补充增强。</p>
<h2 id="整体方法"><a href="#整体方法" class="headerlink" title="整体方法"></a>整体方法</h2><p><img src="/images/DADL3.jpg" alt><br>训练过程中，所有可能的回复概率都要最大，而不只需要ground truth概率最大。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>1 encoder + 3 decoder<br><img src="/images/DADL5.jpg" alt></p>
<p>作者认为通过这样训练，模型就具备了生成多样性回复的能力，在测试的时候可以通过multi beam search、top-k等方式生成多样性回复。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者本次实验主要在数据集MultiWoZ进行。<br><img src="/images/DADL7.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Dialog System</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Integrating Relation Constraints with Neural Relation Extractors</title>
    <url>/2020/01/08/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Integrating-Relation-Constraints-with-Neural-Relation-Extractors/</url>
    <content><![CDATA[<p><strong>Integrating Relation Constraints with Neural Relation Extractors</strong>. Yuan Ye, Yansong Feng, Bingfeng Luo, Yuxuan Lai, Dongyan Zhao. AAAI 2020. <a href="https://arxiv.org/abs/1911.11493" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>在关系抽取任务中，<strong>某个关系的所有subject或者object属于同一种类型</strong>（如：在“母校”的所有subject都属于“人”），或者<strong>多个关系之间往往存在依赖关系</strong>（如“城市”和“地区”的subject都是地名），但是现有模型都没有考虑这个约束，只是单独考虑每一个关系。本文工作利用这种约束以提升关系抽取任务的效果。</p><a id="more"></a>

<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>本文从Coherent和Semantic两个角度出发，提出两种方法。</p>
<h2 id="Coherent"><a href="#Coherent" class="headerlink" title="Coherent"></a>Coherent</h2><p>一致性：满足约束的两个关系，预测概率要同时高。</p>
<p><img src="/images/IER1.jpg" alt></p>
<p>矩阵v表示关系约束C,如果关系i和关系j满足约束，则v_ij=1。</p>
<h2 id="Semantic"><a href="#Semantic" class="headerlink" title="Semantic"></a>Semantic</h2><p>语义性：符合约束中某个规则的两个实例，至少有一个实例满足规则中的某个关系。</p>
<p><img src="/images/IER2.jpg" alt></p>
<p>矩阵u表示约束C,如果关系j和关系k满足约束i，则v_ij=1,v_ik=1.</p>
<p>最后loss由两部分构成，Lo为原始的loss，Lc为约束loss。<br><img src="/images/IER3.jpg" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在ACNN和APCNN两个模型上进行验证，均获得了提升。<br><img src="/images/IER4.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Neural Relation Extraction</tag>
        <tag>Relation Constraints</tag>
      </tags>
  </entry>
  <entry>
    <title>智源社区2019年大会PPT分享 </title>
    <url>/2020/01/08/%E6%99%BA%E6%BA%90%E7%A4%BE%E5%8C%BA2019%E5%B9%B4%E5%A4%A7%E4%BC%9APPT%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p>获取方式 <a href="https://mp.weixin.qq.com/s/zqqQVwr16EhqA2zxYUQSWQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/zqqQVwr16EhqA2zxYUQSWQ</a></p>
<a id="more"></a>

<p><img src="/images/zhiyuan2019-1.jpg" alt=""></p>
<p><img src="/images/zhiyuan2019-2.jpg" alt=""></p>
]]></content>
  </entry>
  <entry>
    <title>【shell】批量删除除了某个文件外的其他所有文件</title>
    <url>/2020/01/08/%E3%80%90shell%E3%80%91%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E9%99%A4%E4%BA%86%E6%9F%90%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%96%E7%9A%84%E5%85%B6%E4%BB%96%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -f !(no_delete_file1|no_delete_file2)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls |grep -v no_delete_file |xargs rm -f</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>批量删除</tag>
      </tags>
  </entry>
  <entry>
    <title>AAAI2020 预讲会</title>
    <url>/2019/12/22/AAAI2020-%E9%A2%84%E8%AE%B2%E4%BC%9A/</url>
    <content><![CDATA[<p>AAAI2020 预讲会翻译对话与文本生成、文本分析与内容挖掘两个Session比较有意思的论文。</p><ul>
<li><p>Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation <a href="https://arxiv.org/abs/1911.09320" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N2-%E9%82%B5%E6%99%A8%E6%B3%BD-%E4%B8%AD%E7%A7%91%E9%99%A2%E8%AE%A1%E7%AE%97%E6%89%80.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Modeling Fluency and Faithfulness for Diverse Neural Machine Translation <a href="https://arxiv.org/abs/1912.00178" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N1-%E8%B0%A2%E5%A9%89%E8%8E%B9-%E4%B8%AD%E7%A7%91%E9%99%A2%E8%AE%A1%E7%AE%97%E6%89%80.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Task-Oriented Dialog Systems that Consider Multiple Appropriate Response under the Same Context <a href="https://arxiv.org/abs/1911.10484" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N3-%E5%BC%A0%E4%BA%A6%E5%BC%9B-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Neural Machine Translation with Joint Representation <a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N8-%E6%9D%8E%E7%82%8E%E6%B4%8B-%E4%B8%9C%E5%8C%97%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Multi-Scale Self-Attention for Text Classification <a href="https://arxiv.org/abs/1912.00544" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S2N1-%E9%83%AD%E7%90%A6%E9%B9%8F-%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Intergrating Relation Constraints with Neural Relation Extractors <a href="https://arxiv.org/abs/1911.11493" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S2N5-%E5%8F%B6%E5%85%83-%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Cross-Lingual Natural Language Generation via Pre-Training <a href="https://arxiv.org/abs/1909.10481" target="_blank" rel="noopener">[PDF]</a></p>
</li>
</ul>]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
      <tags>
        <tag>AAAI</tag>
      </tags>
  </entry>
  <entry>
    <title>国内一些NLP实验室及老师主页</title>
    <url>/2019/12/21/%E5%9B%BD%E5%86%85%E4%B8%80%E4%BA%9BNLP%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%8F%8A%E8%80%81%E5%B8%88%E4%B8%BB%E9%A1%B5/</url>
    <content><![CDATA[<p>以下列表只是我个人整理，随意排序，欢迎大家补充与指正。</p>
<ul>
<li><p><a href="http://nlp.csai.tsinghua.edu.cn/site2/" target="_blank" rel="noopener">清华大学自然语言处理与社会人文计算实验室</a></p>
<p><a href="http://www.cs.tsinghua.edu.cn/publish/cs/4616/2013/20130424103737386785027/20130424103737386785027_.html" target="_blank" rel="noopener">孙茂松</a> <a href="http://nlp.csai.tsinghua.edu.cn/~ly/index_cn.html" target="_blank" rel="noopener">刘洋</a> <a href="http://nlp.csai.tsinghua.edu.cn/~lzy/" target="_blank" rel="noopener">刘知远</a></p>
</li>
<li><p><a href="http://www.wict.pku.edu.cn/" target="_blank" rel="noopener">北京大学王选计算机研究所</a></p>
<p><a href="http://www.icst.pku.edu.cn/xztd/xztd_01/1222625.htm" target="_blank" rel="noopener">万小军</a> <a href="http://www.icst.pku.edu.cn/xztd/xztd_01/1222614.htm" target="_blank" rel="noopener">严睿</a> <a href="http://www.wict.pku.edu.cn/zhaodongyan/" target="_blank" rel="noopener">赵东岩</a> <a href="https://sites.google.com/site/ysfeng/home" target="_blank" rel="noopener">冯岩松</a> <a href="https://www.cs.uic.edu/~liub/" target="_blank" rel="noopener">刘兵</a></p>
</li>
<li><p><a href="http://www.cs.tsinghua.edu.cn/publish/cs/index.html" target="_blank" rel="noopener">清华大学计算机科学与技术系</a></p>
<p><a href="http://coai.cs.tsinghua.edu.cn/hml/" target="_blank" rel="noopener">黄民烈</a></p>
</li>
</ul>
<a id="more"></a>

<ul>
<li><p><a href="http://www.icip.org.cn/zh/homepage/" target="_blank" rel="noopener">中国科学院软件研究所中文信息处理实验室</a></p>
<p><a href="http://www.icip.org.cn/team/sunle/" target="_blank" rel="noopener">孙乐</a> <a href="http://www.icip.org.cn/team/hanxianpei/" target="_blank" rel="noopener">韩先培</a></p>
</li>
<li><p><a href="http://iip.ict.ac.cn/" target="_blank" rel="noopener">中国科学院计算技术研究所智能信息处理重点实验室</a></p>
<p><a href="http://sourcedb.ict.cas.cn/cn/jssrck/201709/t20170910_4857722.html" target="_blank" rel="noopener">冯洋</a></p>
</li>
<li><p><a href="http://www.nlplab.com/niuplan/niutrans.ch.html" target="_blank" rel="noopener">东北大学自然语言处理实验室</a></p>
<p><a href="http://www.nlplab.com/members/zhujingbo.html" target="_blank" rel="noopener">朱靖波</a> <a href="http://www.nlplab.com/members/xiaotong.html" target="_blank" rel="noopener">肖桐</a></p>
</li>
<li><p><a href="http://www.cs.fudan.edu.cn/" target="_blank" rel="noopener">复旦大学计算机科学技术学院</a></p>
<p><a href="https://xpqiu.github.io/" target="_blank" rel="noopener">邱锡鹏</a></p>
</li>
<li><p><a href="http://cs.tju.edu.cn/csweb/" target="_blank" rel="noopener">天津大学计算机科学与技术学院</a></p>
<p><a href="https://zhangmeishan.github.io/chn.html" target="_blank" rel="noopener">张梅山</a> <a href="http://cs.tju.edu.cn/csweb/admin_teacher/view?id=232" target="_blank" rel="noopener">熊德意</a></p>
</li>
<li><p><a href="http://nlp.xmu.edu.cn/index.html" target="_blank" rel="noopener">厦门大学自然语言处理实验室</a></p>
<p><a href="http://121.192.180.171:8080/" target="_blank" rel="noopener">史晓东</a></p>
</li>
<li><p><a href="https://cdmc.xmu.edu.cn/index.htm" target="_blank" rel="noopener">厦门大学数字媒体计算研究中心</a></p>
<p><a href="https://cdmc.xmu.edu.cn/info/1010/1054.htm" target="_blank" rel="noopener">苏劲松</a></p>
</li>
<li><p><a href="http://bcmi.sjtu.edu.cn/index.cn.html" target="_blank" rel="noopener">上海交通大学BCMI实验室</a></p>
<p><a href="http://bcmi.sjtu.edu.cn/~zhaohai/" target="_blank" rel="noopener">赵海</a></p>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>写作助手</title>
    <url>/2019/12/21/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/</url>
    <content><![CDATA[<ul>
<li><p><a href="https://www.overleaf.com/" target="_blank" rel="noopener">Overleaf</a><br>在线编辑Latex。</p>
</li>
<li><p><a href="https://www.grammarly.com/" target="_blank" rel="noopener">Grammarly</a><br>自动检测语法。</p>
</li>
<li><p><a href="http://www.esoda.org/" target="_blank" rel="noopener">易搜搭</a><br>词语搭配。</p>
</li>
</ul>
<a id="more"></a>]]></content>
      <tags>
        <tag>写作助手</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Improved Document Modelling with a Neural Discourse Parser</title>
    <url>/2019/12/21/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Improved-Document-Modelling-with-a-Neural-Discourse-Parser/</url>
    <content><![CDATA[<p><strong>Improved Document Modelling with a Neural Discourse Parser</strong>.Fajri Koto, Jey Han Lau, Timothy Baldwin. ArXiv 1911.<a href="https://arxiv.org/abs/1911.06919" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>使用篇章结构信息提高篇章建模。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>文章的关键有两点，篇章结构是什么？如何利用篇章结构？</p><a id="more"></a>


<h2 id="篇章结构是什么？"><a href="#篇章结构是什么？" class="headerlink" title="篇章结构是什么？"></a>篇章结构是什么？</h2><p><img src="/images/DMDP1.jpg" alt></p>
<p>本篇文章的篇章结构由RST分析得到，首先将篇章切分成EDU，然后再EDU基础上建立篇章分析树，树上的叶子结点为EDU，非叶子结点为其两个子节点的篇章关系，树上的边为对应子节点在该关系中的重要性。（具体可以去了解一下RST官网介绍和相关论文）</p>
<h2 id="如何利用篇章结构？"><a href="#如何利用篇章结构？" class="headerlink" title="如何利用篇章结构？"></a>如何利用篇章结构？</h2><p>如何利用篇章结构，首先是如何编码篇章结构，也就是如何抽取篇章分析树的特征。针对每个树根节点到每个叶子结点的路径，作者设计两类特征：Shallow Discourse Features 和 Latent Discourse Features。</p>
<h3 id="Shallow-Discourse-Features"><a href="#Shallow-Discourse-Features" class="headerlink" title="Shallow Discourse Features"></a>Shallow Discourse Features</h3><ul>
<li>叶子结点重要性分数</li>
</ul>
<p><img src="/images/DMDP2.jpg" alt></p>
<p>统计路径上Nucleus的比例，h(root)为根节点高度。</p>
<ul>
<li>关系重要性分数</li>
</ul>
<p><img src="/images/DMDP3.jpg" alt></p>
<p>统计路径上每个关系的加权比例，h(x)为节点x的高度。</p>
<ul>
<li><p>结点类别</p>
<p>  Nucleus or satellite</p>
<ul>
<li>兄弟结点</li>
</ul>
<h3 id="Latent-Discourse-Features"><a href="#Latent-Discourse-Features" class="headerlink" title="Latent Discourse Features"></a>Latent Discourse Features</h3></li>
</ul>
<p><img src="/images/DMDP6.jpg" alt></p>
<p>使用两个Bi-LSTM分别编码词序列和句法特征序列，avg-pool，然后拼接。</p>
<p><img src="/images/DMDP4.jpg" alt></p>
<p>拼接后的序列再过一个Bi-LSTM得到最终特征表示。</p>
<p><img src="/images/DMDP5.jpg" alt></p>
<h3 id="如何利用篇章特征"><a href="#如何利用篇章特征" class="headerlink" title="如何利用篇章特征"></a>如何利用篇章特征</h3><p> 得到两类特征后，要如何利用呢？本文提出了三种方法。</p>
<ul>
<li><p>拼接word embedding</p>
<p><img src="/images/DMDP7.jpg" alt></p>
</li>
<li><p>加一层Bi-LSTM</p>
<p><img src="/images/DMDP8.jpg" alt></p>
</li>
<li><p>作为解码attention的一个额外输入</p>
<p><img src="/images/DMDP9.jpg" alt></p>
</li>
</ul>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="Document-Summarizatoin"><a href="#Document-Summarizatoin" class="headerlink" title="Document Summarizatoin"></a>Document Summarizatoin</h2><p>  <img src="/images/DMDP10.jpg" alt></p>
<p>  第一种和第二种方法较好。</p>
<h2 id="Petition-Popularity-Prediction"><a href="#Petition-Popularity-Prediction" class="headerlink" title="Petition Popularity Prediction"></a>Petition Popularity Prediction</h2><p>  <img src="/images/DMDP11.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Summarization</tag>
        <tag>Discourse</tag>
        <tag>RST</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】linux批量杀死进程</title>
    <url>/2019/12/21/%E3%80%90shell%E3%80%91linux%E6%89%B9%E9%87%8F%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<p>批量杀死包含关键字“keyword1”但不包含“keyword2”的进程。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef|grep keyword1|grep -v keyword2|cut -c 9-15|xargs kill -9</span><br></pre></td></tr></table></figure><ul>
<li><strong>“ps -ef”</strong> ——查看所有进程</li>
<li><strong>“grep keyword1”</strong> ——列出所有含有关键字”keyword1”的进程</li>
<li><strong>“grep -v keyword2”</strong> ——在列出的进程中去除含有关键字”keyword2”的进程</li>
<li><strong>“cut -c 9-15″</strong> ——截取输入行的第9个字符到第15个字符，而这正好是进程号PID</li>
<li><strong>“xargs kill -9″</strong> ——xargs 命令是用来把前面命令的输出结果(PID)作为”kill -9″命令的参数，并执行该命令。”kill -9″会强行杀掉指定进程。</li>
</ul>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>批量</tag>
        <tag>杀死进程</tag>
      </tags>
  </entry>
  <entry>
    <title>好的研究想法从哪里来</title>
    <url>/2019/12/03/%E5%A5%BD%E7%9A%84%E7%A0%94%E7%A9%B6%E6%83%B3%E6%B3%95%E4%BB%8E%E5%93%AA%E9%87%8C%E6%9D%A5/</url>
    <content><![CDATA[<p>转载自：知乎专栏 NLP日知录 <a href="https://zhuanlan.zhihu.com/p/93765082" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/93765082</a></p><p>作者：刘知远 </p><p>背景说明：临近ACL 2020投稿截止时间，跟同学密集讨论，争论哪些研究想法适合投到ACL有机会命中。从自己十多年研究经历来看，如何判断一个研究想法好不好，以及这些研究想法从哪里来，对于初学者而言的确是个难题。所以，简单攒了这篇小短文，分享一些经验和想法，希望对刚进入NLP领域的新同学有用。多有舛误请指正。</p><a id="more"></a>


<p>王家卫的电影《一代宗师》中有段经典的比武桥段，宫会长对叶问说“今天我们不比武术，比想法”。其实，好的点子或者想法（idea），也是一篇优秀研究成果的灵魂。而计算机领域流行着一句话“IDEA is cheap, show me the code”，也说明对于重视实践的计算机学科而言，想法的好坏还取决于它的实际效能。这里就来谈下好的研究想法从哪里来。</p>
<h1 id="什么算是好的想法"><a href="#什么算是好的想法" class="headerlink" title="什么算是好的想法"></a>什么算是好的想法</h1><p>2015年，我在微博上写过一个调侃的小段子：</p>
<blockquote>
<p>ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。</p>
</blockquote>
<p>到了2018年，我又续了一小段：</p>
<blockquote>
<p>不期数年，北方DL神教异军突起，内修表示学习，外练神经网络，心法众多，曰门，曰注意，曰记忆，曰对抗，曰增强。经ImageNet一役威震武林，豢Alpha犬一匹无人可近。一时家家筑丹炉，人人炼丹忙，门徒云集，依附者众，有一统江湖之势。有童谣为证：左手大数据，右手英伟达，每逢顶会炼丹忙。</p>
</blockquote>
<p>这里面提到的图模型加圈、神经网络加层、优化目标加正则，神经网络中的门、注意、记忆等，都是一些改进模型性能的创新思路，被各大NLP任务广泛使用并发表论文，也许就是因为被不同NLP任务的重复使用和发表，多少有些审美疲劳而缺少更深的创新思想，被有些网友和学者诟为“灌水”，似乎都不算好的想法。</p>
<p>那么什么才是好的想法呢？我理解这个”好“字，至少有两个层面的意义。</p>
<h2 id="学科发展角度的“好”"><a href="#学科发展角度的“好”" class="headerlink" title="学科发展角度的“好”"></a>学科发展角度的“好”</h2><p>学术研究本质是对未知领域的探索，是对开放问题的答案的追寻。所以从推动学科发展的角度，评判什么是好的研究想法的标准，首先就在一个<strong>“新”</strong>字。</p>
<p>过去有个说法，人工智能学科有个魔咒，凡是人工智能被解决（或者有解决方案）的部分，就不再被认为代表“人类智能”。计算机视觉、自然语言处理、机器学习、机器人之所以还被列为人工智能主要方向，也许正是因为它们尚未被解决，尚能代表“人类智能”的尊严。而我们要开展创新研究，就是要提出新的想法解决这些问题。这其中的”新“字，可以体现在提出新的问题和任务，探索新的解决思路，提出新的算法技术，实现新的工具系统等。</p>
<p>在保证”新“的基础上，研究想法好不好，那就看它<strong>对推动学科发展的助力有多大</strong>。深度学习之所以拥有如此显赫的影响力，就在于它对于人工智能自然语言处理、语音识别、计算机视觉等各重要方向都产生了革命性的影响，彻底改变了对无结构信号（语音、图像、文本）的语义表示的技术路线。</p>
<h2 id="研究实践角度的”好“"><a href="#研究实践角度的”好“" class="headerlink" title="研究实践角度的”好“"></a>研究实践角度的”好“</h2><p>那是不是想法只要够”新“就好呢？是不是越新越好呢？我认为应该还不是。因为，只有<strong>能做得出来的想法</strong>才有资格被分析好不好。所以，从研究实践角度，还需要考虑研究想法的<strong>可实现性</strong>和<strong>可验证性</strong>。</p>
<p>可实现性，体现在该想法是否有足够的数学或机器学习工具支持实现。可验证性，体现在该想法是否有合适的数据集合和广泛接受的评价标准。很多民间科学家的想法之所以得不到学术界的认同，就是因为这些想法往往缺乏可实现性和可验证性，只停留在天马行空的纸面，只是些虚无缥缈的理念。</p>
<h1 id="好的研究想法从哪里来"><a href="#好的研究想法从哪里来" class="headerlink" title="好的研究想法从哪里来"></a>好的研究想法从哪里来</h1><p>想法好还是不好，并不是非黑即白的二分问题，而是像光谱一样呈连续分布，因时而异，因人而宜。计算机科技领域的发展既有积累的过程，也有跃迁的奇点，积累量变才会产生质变，吃第三个馒头饱了，也是因为前面两个馒头打底。</p>
<p>现在的学术研究已经成为高度专业化的职业，有庞大的研究者群体。”Publish or Perish“，是从事学术职业（如教授、研究员、研究生）的人必须做好平衡的事情，不能要求研究者的每份工作都是“诺贝尔奖”或“图灵奖”级的才值得发表。只要对研究领域的发展有所助力，就值得发表出来，帮助同行前进。鲁迅说：天才并不是自生自长在深林荒野里的怪物，是由可以使天才生长的民众产生，长育出来的，所以没有这种民众，就没有天才。这个庞大研究者群体正是天才成长的群众基础。同时，学术新人也是在开展创新研究训练中，不断磨砺寻找好想法能力，鲁迅也说：即使天才，在生下来的时候的第一声啼哭，也和平常的儿童的一样，决不会就是一首好诗。</p>
<p>那么，好的研究想法从哪里来呢？我总结，首先要有区分研究想法好与不好的能力，这需要<strong>深入全面了解所在研究方向的历史与现状</strong>，具体就是对学科文献的全面掌握。人是最善于学习的动物，完全可以将既有文献中不同时期研究工作的想法作为学习对象，通过了解它们提出后对学科发展的影响——具体体现在论文引用、学术评价情况等各方面——建立对研究想法好与不好的评价模型。我们很难条分缕析完美地列出区分好与不好想法的所有特征向量，但人脑强大的学习能力，只要给予足够的输入数据，就可以在神经网络中自动学习建立判别的模型，鉴古知今，见微知著，这也许就是常说的学术洞察力。</p>
<p>做过一些研究的同学会有感受，仅阅读自己研究方向的文献，新想法还是不会特别多。这是因为，读到的都是该研究问题已经完成时的想法，它们本身无法启发新的想法。如何产生新的想法呢？我总结有三种可行的基本途径：</p>
<p><strong>实践法</strong>。即在研究任务上实现已有最好的算法，通过分析实验结果，例如发现这些算法计算复杂度特别高、训练收敛特别慢，或者发现该算法的错误样例呈现明显的规律，都可以启发你改进已有算法的思路。现在很多自然语言处理任务的Leaderboard上的最新算法，就是通过分析错误样例来有针对性改进算法的 [1]。</p>
<p><strong>类比法</strong>。即将研究问题与其他任务建立类比联系，调研其他相似任务上最新的有效思想、算法或工具，通过合理的转换迁移，运用到当前的研究问题上来。例如，当初注意力机制在神经网络机器翻译中大获成功，当时主要是在词级别建立注意力，后来我们课题组的林衍凯和沈世奇提出建立句子级别的注意力解决关系抽取的远程监督训练数据的标注噪音问题 [2]，这就是一种类比的做法。</p>
<p><strong>组合法</strong>。即将新的研究问题分解为若干已被较好解决的子问题，通过有机地组合这些子问题上的最好做法，建立对新的研究问题的解决方案。例如，我们提出的融合知识图谱的预训练语言模型，就是将BERT和TransE等已有算法融合起来建立的新模型 [3]。</p>
<p>正如武侠中的最高境界是无招胜有招，好的研究想法并不拘泥于以上的路径，很多时候是在研究者对研究问题深刻认知的基础上，综合丰富的研究阅历和聪明才智产生”顿悟“的结果。这对初学者而言恐怕还很难一窥门径，需要从基本功做起，经过大量科研实践训练后，才能有登堂入室之感。</p>
<p>在科研实践过程中，除了通过大量文献阅读了解历史，通过深入思考总结产生洞察力外，还有一项必不可少的工作，那就是主动开放的学术交流和合作意识。不同研究领域思想和成果交流碰撞，既为创新思想提供了新的来源，也为”类比“和”顿悟“提供了机会。了解一下历史就可以知晓，人工智能的提出，就是数学、计算机科学、控制论、信息论、脑科学等学科交叉融合的产物。而当红的深度学习的起源，1980年代的Parallel Distributed Processing （PDP），也是计算机科学、脑认知科学、心理学、生物学等领域研究者通力合作的产物。下面是1986年出版的名著《Parallel Distributed Processing: Explorations in the Microstructure of Cognition》第一卷的封面。</p>
<p><img src="https://pic2.zhimg.com/80/v2-a8d3f6e553f9f279cdafea5a3e218701_hd.jpg" alt></p>
<p>作者在前言中是这么讲他们的合作过程的，在最初长达六个月的时间里，它们每周见面交流两次讨论研究进展。</p>
<blockquote>
<p>We expected the project to take about six months. We began in January 1982 by bringing a number of our colleagues together to form a discussion group on these topics. During the first six months we met twice weekly and laid the foundation for most of the work presented in these volumes.</p>
</blockquote>
<p>而书中提供的PDP研究组的成员名单，40年后的今天仍让我惊叹其高度的跨机构、跨学科的交叉特点。所以，特别建议同学们在科研训练中，在专注研究问题的前提下，保持主动的学术交流意识，无论是听讲座报告，参加学术会议，还是选修课程，都有意识地扩宽学术交流的广度，不仅与小同行打成一片，更有看似八竿子打不着的研究领域的学术伙伴。随着研究经历的丰富，会越来越强烈地感受到，越是大跨度交叉的学术报告，越让你受到更大的启发，产生更多让自己兴奋的研究想法。</p>
<p><img src="https://pic2.zhimg.com/80/v2-404a752001300a69baabd40fb3d78b99_hd.jpg" alt></p>
<h1 id="初学者应该怎么做"><a href="#初学者应该怎么做" class="headerlink" title="初学者应该怎么做"></a>初学者应该怎么做</h1><p>与阅读论文、撰写论文、设计实验等环节相比，如何产生好的研究想法，是一个不太有章可循的环节，很难总结出固定的范式可供遵循。像小马过河，需要通过大量训练实践，来积累自己的研究经验。不过，对于初学者而言，仍然有几个简单可行的原则可以参考。</p>
<p><strong>一篇论文的可发表价值，取决于它与已有最直接相关工作间的Delta</strong>。我们大部分研究工作都是站在前人工作的基础上推进的。牛顿说：如果说我看得比别人更远些，那是因为我站在巨人的肩膀上。在我看来，评判一篇论文研究想法的价值，就是看它站在了哪个或哪些巨人的肩膀上，以及在此基础上又向上走了多远。反过来，在准备开始一项研究工作之前，在形成研究想法的时候，也许要首先明确准备站在哪个巨人的肩膀上，以及计划通过什么方式走得更远。与已有最直接相关工作之间的Delta，决定了这个研究想法的价值有多大。</p>
<p><strong>兼顾摘果子和啃骨头</strong>。人们一般把比较容易想到的研究想法，叫做Low Hanging Fruit（低垂果实）。低垂果实容易摘，但同时摘的人也多，选择摘果子就容易受到想法撞车的困扰。例如，2018年以BERT为首的预训练语言模型取得重大突破，2019年中就出现大量改进工作，其中以跨模态预训练模型为例，短短几个月里<a href="http://arxiv.org上挂出了超过六个来自不同团队的图像与文本融合的预训练模型" target="_blank" rel="noopener">http://arxiv.org上挂出了超过六个来自不同团队的图像与文本融合的预训练模型</a> [4]。设身处地去想，进行跨模态预训练模型研究，就是一个比较容易想到的方向，你一定需要有预判能力，知道世界上肯定会有很多团队也同时开展这方面研究，这时你如果选择入场，就一定要做得更深入更有特色，有自己独特的贡献才行。相对而言，那些困难的问题，愿意碰的人就少，潜下心来啃硬骨头，也是不错的选择，当然同时就会面临做不出来的风险，或者做出来也得不到太多关注的风险。同学需要根据自身特点、经验和需求，兼顾摘果子和啃骨头两种类型的研究想法。</p>
<p><img src="https://pic1.zhimg.com/80/v2-d71aaf2b86116e3ea1e891bf9230a2c4_hd.jpg" alt></p>
<p><strong>注意多项研究工作的主题连贯性</strong>。同学的研究训练往往持续数年，需要注意前后多项研究工作的主题连贯性，保证内在逻辑统一。需要考虑，在个人简历上，在出国申请Personal Statement中，或者在各类评奖展示中，能够将这些研究成果汇总在一起，讲出自己开展这些研究工作的总目标、总设想。客观上讲，人工智能领域研究节奏很快，技术更新换代快，所以成果发表也倾向于小型化、短平快。我有商学院、社科的朋友，他们一项研究工作往往需要持续一年甚至数年以上；高性能计算、计算机网络方向的研究周期也相对较长。人工智能这种小步快跑的特点，决定了很多同学即使本科毕业时，也会有多篇论文发表，更不用说硕士生、博士生。在这种情况下，就格外需要在研究选题时，注意前后工作的连贯性和照应关系。几项研究工作放在一起，到底是互相割裂说不上话，还是在为一个统一的大目标而努力，格外反映研究的大局意识和布局能力。例如，下图是我们课题组涂存超博士2018年毕业时博士论文《面向社会计算的网络表示学习》的章节设置，整体来看就比《社会计算的若干重要问题研究》等没有内在关联的写法要更让人信服一些。当然，对于初学者而言，一开始就想清楚五年的研究计划，根本不可能。但想，还是不去想，结果还是不同的。</p>
<p><img src="https://pic1.zhimg.com/80/v2-9fbee2d16f9c05fa1cb1ec86a27d265c_hd.jpg" alt></p>
<p><strong>注意总结和把握研究动态和趋势，因时而动</strong>。2019年在知乎上有这样一个问题：”2019年在NLP领域，资源有限的个人/团队能做哪些有价值有希望的工作？“ 我当时的回答如下：</p>
<blockquote>
<p>我感觉，产业界开始集团化搞的问题，说明其中主要的开放性难题已经被解决得差不多了，如语言识别、人脸识别等，在过去20年里面都陆续被广泛商业应用。看最近的BERT、GPT-2，我理解更多的是将深度学习对大规模数据拟合的能力发挥到极致，在深度学习技术路线基本成熟的前提下，大公司有强大计算能力支持，自然可以数据用得更多，模型做得更大，效果拟合更好。<br>成熟高新技术进入商用竞争，就大致会符合摩尔定律的发展规律。现在BERT等训练看似遥不可及，但随着计算能力等因素的发展普及，说不定再过几年，人人都能轻易训练BERT和GPT-2，大家又会在同一个起跑线上，把目光转移到下一个挑战性难题上。<br>所以不如提前考虑，哪些问题是纯数据驱动技术无法解决的。NLP和AI中的困难任务，如常识和知识推理，复杂语境和跨模态理解，可解释智能，都还没有可行的解决方案，我个人也不看好数据驱动方法能够彻底解决。更高层次的联想、创造、顿悟等认知能力，更是连边还没碰到。这些正是有远见的研究者们应该开始关注的方向。</p>
</blockquote>
<p>需要看到，不同时期的研究动态和趋势不同。把握这些动态和趋势，就能够做出研究社区感兴趣的成果。不然的话，即使研究成果没有变化，只是简单早几年或晚几年投稿，结果也会大不相同。例如，2013年word2vec发表，在2014-2016年之间开展词表示学习研究，就相对比较容易得到ACL、EMNLP等会议的录用；但到了2017-2018年，ACL等会议上的词表示学习的相关工作就比较少见了。</p>
<h1 id="最后的补充"><a href="#最后的补充" class="headerlink" title="最后的补充"></a>最后的补充</h1><p>这篇短文，主要是希望面向初学者，介绍一些求新过程中的经验和注意事项，希望大家少走一些弯路。但阅读文献，深入思考，接收拒稿不断改进的苦，该吃的还是要吃。学术研究和论文发表，对个人而言也许意味着高薪资和奖学金，但其最终的目的还是真正的推动学科的发展。所以，要做经得起考验的学术研究，关键就在”真“与”新“，需要我们始终恪守和孜孜以求。</p>
<p>著名历史学家、清华校友何炳棣先生曾在自传《读史阅世六十年》中提及著名数学家林家翘的一句嘱咐：“要紧的是不管搞哪一行，千万不要做第二等的题目。” 具体到每个领域，什么是一等的题目本身见仁见智，其实更指向内心“求真”的态度。</p>
<p>参考文献<br>[1] <a href="https://paperswithcode.com/" target="_blank" rel="noopener">https://paperswithcode.com/</a> &amp; <a href="http://nlpprogress.com/" target="_blank" rel="noopener">http://nlpprogress.com/</a></p>
<p>[2] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, Maosong Sun. Neural Relation Extraction with Selective Attention over Instances. The 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016).</p>
<p>[3] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu. ERNIE: Enhanced Language Representation with Informative Entities. The 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019).</p>
<p>[4] <a href="https://github.com/thunlp/PLMpapers" target="_blank" rel="noopener">https://github.com/thunlp/PLMpapers</a></p>
]]></content>
  </entry>
  <entry>
    <title>Keyphrase Generation任务综述</title>
    <url>/2019/12/02/Keyphrase-Generation%E4%BB%BB%E5%8A%A1%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="任务简介"><a href="#任务简介" class="headerlink" title="任务简介"></a>任务简介</h1><p>A <strong>keyphrase</strong> or keyword is a piece of short, summative content that expresses the main semantic meaning of a longer text. The typical use of a keyphrase or keyword is in scientific publications to provide the core information of a paper. </p><a id="more"></a>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><ul>
<li>F1@5</li>
<li>F1@10</li>
</ul>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><ul>
<li>KP20K</li>
<li>SemEval</li>
<li>NUS</li>
<li>Krapivin</li>
<li>Inspec</li>
</ul>
<h1 id="SOTA"><a href="#SOTA" class="headerlink" title="SOTA"></a>SOTA</h1><ul>
<li>2019-06-13 <strong>Title-Guided Encoding for Keyphrase Generation</strong></li>
</ul>
<table>
<thead>
<tr>
<th align="center">Dataset</th>
<th align="center">F1@5</th>
<th align="center">F1@10</th>
</tr>
</thead>
<tbody><tr>
<td align="center">KP20K</td>
<td align="center">0.372</td>
<td align="center">0.315</td>
</tr>
</tbody></table>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><ul>
<li><p><strong>Deep Keyphrase Generation</strong>. Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, Yu Chi. ACL 2017. <a href="https://aclweb.org/anthology/papers/P/P17/P17-1054/" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/memray/seq2seq-keyphrase" target="_blank" rel="noopener">[code]</a><br>Keyphrase Generation的第一篇paper，主要框架是 seq2seq + copy.</p>
</li>
<li><p><strong>Semi-Supervised Learning for Neural Keyphrase Generation</strong>. Hai Ye, Lu Wang. EMNLP 2018. <a href="https://aclweb.org/anthology/papers/D/D18/D18-1447/" target="_blank" rel="noopener">[pdf]</a><br>解决资源不足问题，提出两个策略：<br>（1）微调，通过keyphrase extraction方式人为构造大量数据预训练模型，再通过已有数据微调；<br>（2）多任务框架，生成keyphrase的同时生成title。</p>
</li>
<li><p><strong>Title-Guided Encoding for Keyphrase Generation</strong>. Wang Chen, Yifan Gao, Jiani Zhang, Irwin King, Michael R. Lyu1. AAAI 2019. <a href="https://arxiv.org/abs/1808.08575" target="_blank" rel="noopener">[pdf]</a><br>本文认为标题包含了文章的主要信息，通过标题来引导摘要的建模已提升模型性能。</p>
</li>
<li><p><strong>Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</strong>. Hou Pong Chan, Wang Chen, Lu Wang, Irwin King. ACL 2019. <a href="https://arxiv.org/abs/1906.04106" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/kenchan0226/keyphrase-generation-rl" target="_blank" rel="noopener">[code]</a></p>
</li>
</ul>
<ul>
<li><strong>Topic-Aware Neural Keyphrase Generation for Social Media Language. Yue Wang</strong>. Jing Li, Hou Pong Chan, Irwin King, Michael R. Lyu, Shuming Shi. ACL 2019. <a href="https://arxiv.org/abs/1906.03889" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/yuewang-cuhk/TAKG" target="_blank" rel="noopener">[code]</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>【shell】截断字符串</title>
    <url>/2019/11/29/%E3%80%90shell%E3%80%91%E6%88%AA%E6%96%AD%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/fengbohello/p/5954895.html" target="_blank" rel="noopener">https://www.cnblogs.com/fengbohello/p/5954895.html</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">line='abcd&lt;SEG&gt;efg'</span><br><span class="line">newline=$&#123;line#*&lt;SEG&gt;&#125;</span><br><span class="line">echo $newline # efg</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】判断字符串是否包含子串</title>
    <url>/2019/11/29/%E3%80%90shell%E3%80%91%E5%88%A4%E6%96%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%98%AF%E5%90%A6%E5%8C%85%E5%90%AB%E5%AD%90%E4%B8%B2/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/iamlihongwei/article/details/59484029" target="_blank" rel="noopener">https://blog.csdn.net/iamlihongwei/article/details/59484029</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [[ $line =~ "&lt;BEGIN&gt;" ]] </span><br><span class="line">then</span><br><span class="line">	echo "包含&lt;BEGIN&gt;"</span><br><span class="line">elif [[ $line =~ "&lt;SEG&gt;" ]]</span><br><span class="line">then</span><br><span class="line">	echo "包含&lt;SEG&gt;"</span><br><span class="line">else</span><br><span class="line">	echo "都不包含"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>使用python发送免费短信</title>
    <url>/2019/11/27/%E4%BD%BF%E7%94%A8python%E5%8F%91%E9%80%81%E5%85%8D%E8%B4%B9%E7%9F%AD%E4%BF%A1/</url>
    <content><![CDATA[<p>首先在 <a href="https://www.twilio.com/" target="_blank" rel="noopener">twilio</a> 上注册帐号，并申请一个 twilio 手机号，并认证自己的手机号，twilio只能给认证过的手机号发送短信。</p><p>使用 python 发送短信</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twilio.rest <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line">account_sid = &lt;your account sid&gt;</span><br><span class="line">auth_token = &lt;your auth token&gt;</span><br><span class="line">client = Client(account_sid, auth_token)</span><br><span class="line"></span><br><span class="line">message=client.messages.create(</span><br><span class="line">	from_=&lt;your twilio phone num&gt;,</span><br><span class="line">	body=&lt;your message&gt;,</span><br><span class="line">	to=&lt;your phone num&gt;</span><br><span class="line">)</span><br><span class="line">print(message.sid)</span><br></pre></td></tr></table></figure><a id="more"></a>



<p>注：试用版免费次数有限。</p>
]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
  </entry>
  <entry>
    <title>【论文笔记】Context-Aware Learning for Neural Machine Translation</title>
    <url>/2019/11/22/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Context-Aware-Learning-for-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Context-Aware Learning for Neural Machine Translation</strong>. Sébastien Jean, Kyunghyun Cho. ArXiv 1903. <a href="https://arxiv.org/pdf/1903.04715.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文提出一个正则化项，鼓励模型利用上下文信息，从而提高篇章翻译结果。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>人们认为使用上下文可以提高篇章翻译，也就是使用上下文信息后译文翻译概率会更高。</p><a id="more"></a>


<p><img src="/images/context-aware1.png" alt></p>
<p>这个不等式在token、sentence、data三个层次上都成立</p>
<p><img src="/images/context-aware2.png" alt></p>
<p>本文在损失函数中加入一个正则化项，正则化项由三个max margin组成。</p>
<p><img src="/images/context-aware3.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><img src="/images/context-aware4.png" alt></p>
<p>（a）句子级别翻译，不利用上下信息</p>
<p>（b）利用随机上下文，随机上下文的期望跟不利用上下文的期望一样，所以使用上下文没有提升</p>
<p>（c）利用前文上下文，对比随机上下文有提升，并且跟没有利用上下文相差0.4</p>
<p>（d）鼓励利用前文上下文，跟没有利用上下文相差3.74，并且比（c）有提升</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
        <tag>Context</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Pretrained Language Models for Document-Level Neural Machine Translation</title>
    <url>/2019/11/21/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Pretrained-Language-Models-for-Document-Level-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Pretrained Language Models for Document-Level Neural Machine Translation</strong>. Liangyou Li, Xin Jiang, Qun Liu. ArXiv. <a href="https://arxiv.org/pdf/1911.03110.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>现有篇章翻译工作大都只能有限的上下文（前面3句话），当利用更长上下文时，由于训练不稳定模型效果反而下降。理论上来说更长的上下文可以提供更多信息，更有助于翻译。本文就是希望能够在篇章翻译中利用更长的上下文。</p><a id="more"></a>

<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="BERT初始化"><a href="#BERT初始化" class="headerlink" title="BERT初始化"></a>BERT初始化</h2><p>现有有些篇章翻译工作先利用大量平行句对预训练（有些只有利用篇章语料的平行句对预训练），然后再利用平行篇章语料微调。本文不再使用平行句对预训练，而是使用别人训练好的BERT来初始化模型参数。（BERT是在大量单语篇章语料上训练得到的）</p>
<h2 id="利用上下文"><a href="#利用上下文" class="headerlink" title="利用上下文"></a>利用上下文</h2><p>本文使用的上下文为前面512个词。将上下文和当前句子拼接起来，中间有个分隔符，但是如果直接使用Encoder对拼接后的句子进行编码，生成的译文反而更差（训练不稳定）。</p>
<p><img src="/images/mlmdoc.png" alt></p>
<p>本文提出了三个改进方法：</p>
<ul>
<li><p><strong>Segment Embeddings</strong>:<br>用来标记每个词是属于当前要翻译句子，还是属于上下文。</p>
</li>
<li><p><strong>Reverse Position Embeddings</strong>:<br>先对当前要翻译句子进行编号，再对上下文进行编号。</p>
</li>
<li><p><strong>Context Masks</strong>:<br>经过编码器后，当前句子已经包含了上下文信息，对上下文的隐状态加 mask，使得解码器更加关注当前句子。（不加mask，训练不稳定）</p>
</li>
</ul>
<h2 id="多任务"><a href="#多任务" class="headerlink" title="多任务"></a>多任务</h2><p>本文引入了Mask Language Model</p>
<p><img src="/images/mlmdoc1.png" alt></p>
<p>X: 源端当前句子</p>
<p>C: 源端上下文</p>
<p>Y: 目标端当前句子</p>
<p>S: X 和 C 的拼接</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><img src="/images/mlmdoc2.png" alt></p>
<p>可以训12层encoder还是比较 NB 的。<br><font color="#FF0000">我认为作者可以补充一个只加BERT的实验结果。</font></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
      </tags>
  </entry>
  <entry>
    <title>【深度学习基础】Dropout</title>
    <url>/2019/11/15/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91Dropout/</url>
    <content><![CDATA[<p>** Improving neural networks by preventing co-adaptation of feature detectors**. Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov. arXiv 1207.0580. <a href="https://arxiv.org/abs/1207.0580" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="一些博客"><a href="#一些博客" class="headerlink" title="一些博客"></a>一些博客</h1><ul>
<li><a href="https://www.zhihu.com/question/61751133" target="_blank" rel="noopener">神经网络Dropout层中为什么dropout后还需要进行rescale？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/66337970" target="_blank" rel="noopener">Dropout的前世与今生</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>【git 使用】分支合并</title>
    <url>/2019/11/12/%E3%80%90git-%E4%BD%BF%E7%94%A8%E3%80%91%E5%88%86%E6%94%AF%E5%90%88%E5%B9%B6/</url>
    <content><![CDATA[<ul>
<li>master发生改变，同步到feature branch</li>
</ul><p>merge master into feature branch</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git checkout &lt;feature branch&gt;</span><br><span class="line">git merge master</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>ArXiv 论文 2019/11/2-2019/11/8</title>
    <url>/2019/11/10/ArXiv-%E8%AE%BA%E6%96%87-2019-11-2-2019-11-8/</url>
    <content><![CDATA[<ul>
<li><a href="https://arxiv.org/abs/1911.00492" target="_blank" rel="noopener">Reasoning Over Paths via Knowledge Base Completion</a></li>
<li><a href="https://arxiv.org/abs/1911.00069" target="_blank" rel="noopener">Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping</a></li>
<li><a href="https://arxiv.org/abs/1911.00133" target="_blank" rel="noopener">Dreaddit: A Reddit Dataset for Stress Analysis in Social Media</a></li>
<li><a href="https://arxiv.org/abs/1911.00176" target="_blank" rel="noopener"><strong>Sequence Modeling with Unconstrained Generation Order</strong></a></li>
<li><a href="https://arxiv.org/abs/1911.00317" target="_blank" rel="noopener">On the Linguistic Representational Power of Neural Machine Translation Models</a></li>
<li><a href="https://arxiv.org/abs/1911.00473" target="_blank" rel="noopener">BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding</a><a id="more"></a></li>
<li><a href="https://arxiv.org/abs/1911.00484" target="_blank" rel="noopener">Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents</a></li>
<li><a href="https://arxiv.org/abs/1911.00225" target="_blank" rel="noopener">When Choosing Plausible Alternatives, Clever Hans can be Clever</a></li>
<li><a href="https://arxiv.org/abs/1911.00269" target="_blank" rel="noopener">A Robust Data-Driven Approach for Dialogue State Tracking of Unseen Slot Values</a></li>
<li><a href="https://arxiv.org/abs/1911.00274" target="_blank" rel="noopener">Kernelized Bayesian Softmax for Text Generation</a></li>
<li><a href="https://arxiv.org/abs/1911.00359" target="_blank" rel="noopener">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data</a></li>
</ul>
]]></content>
      <categories>
        <category>arxiv</category>
      </categories>
      <tags>
        <tag>ArXiv</tag>
      </tags>
  </entry>
  <entry>
    <title>【git 使用】clone、branch、add、commit、push</title>
    <url>/2019/11/08/%E3%80%90git-%E4%BD%BF%E7%94%A8%E3%80%91clone%E3%80%81branch%E3%80%81add%E3%80%81commit%E3%80%81push/</url>
    <content><![CDATA[<h1 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h1><ul>
<li>普通</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@xxx.git</span><br></pre></td></tr></table></figure><ul>
<li>指定branch</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@xxx.git -b &lt;branchname&gt;</span><br></pre></td></tr></table></figure><h1 id="提交修改"><a href="#提交修改" class="headerlink" title="提交修改"></a>提交修改</h1><ul>
<li>提交某个文件</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add &lt;path to file&gt;</span><br><span class="line">git commit -m '&lt;message&gt;'</span><br><span class="line">git push origin &lt;branchname&gt;</span><br></pre></td></tr></table></figure><a id="more"></a>








<ul>
<li>提交多个文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add --all</span><br><span class="line">git commit -m '&lt;message&gt;'</span><br><span class="line">git push origin &lt;branchname&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】alias设置指令别名</title>
    <url>/2019/11/08/%E3%80%90shell%E3%80%91alias%E8%AE%BE%E7%BD%AE%E6%8C%87%E4%BB%A4%E5%88%AB%E5%90%8D/</url>
    <content><![CDATA[<p>alias 可以用来将一些较长的指令进行简化，使用alias时，用户必须使用单引号’’将原来的命令引起来，防止特殊字符导致错误。</p><h1 id="alias基本使用"><a href="#alias基本使用" class="headerlink" title="alias基本使用"></a>alias基本使用</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias 新指令=‘原指令 -选项/参数’</span><br></pre></td></tr></table></figure><p>如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias myscp='scp admin@192.168.72.77'</span><br></pre></td></tr></table></figure><h1 id="查看永久已设置别名"><a href="#查看永久已设置别名" class="headerlink" title="查看永久已设置别名"></a>查看永久已设置别名</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias -p</span><br></pre></td></tr></table></figure><a id="more"></a>







<h1 id="设置永久别名"><a href="#设置永久别名" class="headerlink" title="设置永久别名"></a>设置永久别名</h1><p>修改<code>~/.bashrc</code>文件</p>
<p>参考：<a href="https://man.linuxde.net/alias" target="_blank" rel="noopener">https://man.linuxde.net/alias</a></p>
]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>alias</tag>
      </tags>
  </entry>
  <entry>
    <title>ArXiv 论文 2019/10/28-2019/11/1</title>
    <url>/2019/11/02/ArXiv-%E8%AE%BA%E6%96%87-2019-10-28-2019-11-1/</url>
    <content><![CDATA[<ul>
<li><a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
<li><a href="https://arxiv.org/abs/1910.14659" target="_blank" rel="noopener">Pseudolikelihood Reranking with Masked Language Models</a></li>
<li><a href="https://arxiv.org/abs/1910.14549" target="_blank" rel="noopener">Positional Attention-based Frame Identification with BERT: A Deep Learning Approach to Target Disambiguation and Semantic Frame Selection</a></li>
<li><a href="https://arxiv.org/abs/1910.14192" target="_blank" rel="noopener">Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning</a></li>
<li><a href="https://arxiv.org/abs/1910.14176" target="_blank" rel="noopener">Predicting Discourse Structure using Distant Supervision from Sentiment</a><a id="more"></a></li>
<li><a href="https://arxiv.org/abs/1910.14142" target="_blank" rel="noopener">Discourse-Aware Neural Extractive Model for Text Summarization</a></li>
<li><a href="https://arxiv.org/abs/1910.14075" target="_blank" rel="noopener">Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.14613" target="_blank" rel="noopener">Neural Assistant: Joint Action Prediction, Response Generation, and Latent Knowledge Reasoning</a></li>
<li><a href="https://arxiv.org/abs/1910.14208" target="_blank" rel="noopener">Hidden State Guidance: Improving Image Captioning using An Image Conditioned Autoencoder</a></li>
<li><a href="https://arxiv.org/abs/1910.13890" target="_blank" rel="noopener">A Latent Morphology Model for Open-Vocabulary Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.13794" target="_blank" rel="noopener">Let Me Know What to Ask: Interrogative-Word-Aware Question Generation</a></li>
<li><a href="https://arxiv.org/abs/1910.13466" target="_blank" rel="noopener">Ordered Memory</a></li>
<li><a href="https://arxiv.org/abs/1910.13106" target="_blank" rel="noopener">Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots</a></li>
<li><a href="https://arxiv.org/abs/1910.13267" target="_blank" rel="noopener">BPE-Dropout: Simple and Effective Subword Regularization</a></li>
<li><a href="https://arxiv.org/abs/1910.13294" target="_blank" rel="noopener">Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control</a></li>
<li><a href="https://arxiv.org/abs/1910.13437" target="_blank" rel="noopener">An Empirical Study of Generation Order for Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.12708" target="_blank" rel="noopener">Evaluating Lottery Tickets Under Distributional Shifts</a></li>
<li><a href="https://arxiv.org/abs/1910.12702" target="_blank" rel="noopener">Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling</a></li>
<li><a href="https://arxiv.org/abs/1910.12698" target="_blank" rel="noopener">Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis</a></li>
<li><a href="https://arxiv.org/abs/1910.12527" target="_blank" rel="noopener">RPM-Oriented Query Rewriting Framework for E-commerce Keyword-Based Sponsored Search</a></li>
<li><a href="https://arxiv.org/abs/1910.12391" target="_blank" rel="noopener">What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?</a></li>
<li><a href="https://arxiv.org/abs/1910.12197" target="_blank" rel="noopener">Look-up and Adapt: A One-shot Semantic Parser</a></li>
<li><a href="https://arxiv.org/abs/1910.12196" target="_blank" rel="noopener">Open the Boxes of Words: Incorporating Sememes into Textual Adversarial Attack</a></li>
<li><a href="https://arxiv.org/abs/1910.11966" target="_blank" rel="noopener">Yall should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts</a></li>
<li><a href="https://arxiv.org/abs/1910.12038" target="_blank" rel="noopener">Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention</a></li>
<li><a href="https://arxiv.org/abs/1910.11959" target="_blank" rel="noopener">FineText: Text Classification via Attention-based Language Model Fine-tuning</a></li>
<li><a href="https://arxiv.org/abs/1910.12094" target="_blank" rel="noopener">Meta Learning for End-to-End Low-Resource Speech Recognition</a></li>
<li><a href="https://arxiv.org/abs/1910.11491" target="_blank" rel="noopener">Attention Optimization for Abstractive Document Summarization</a></li>
<li><a href="https://arxiv.org/abs/1910.11471" target="_blank" rel="noopener">Machine Translation from Natural Language to Code using Long-Short Term Memory</a></li>
<li><a href="https://arxiv.org/abs/1910.11470" target="_blank" rel="noopener">A Survey on Recent Advances in Named Entity Recognition from Deep Learning models</a></li>
<li><a href="https://arxiv.org/abs/1910.11411" target="_blank" rel="noopener">Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations</a></li>
<li><a href="https://arxiv.org/abs/1910.11399" target="_blank" rel="noopener">Comparison of Quality Indicators in User-generated Content Using Social Media and Scholarly Text</a></li>
<li><a href="https://arxiv.org/abs/1910.11494" target="_blank" rel="noopener">Fast and Accurate Knowledge-Aware Document Representation Enhancement for News Recommendations</a></li>
<li><a href="https://arxiv.org/abs/1910.11455" target="_blank" rel="noopener">Recognizing long-form speech using streaming end-to-end models</a></li>
</ul>
]]></content>
      <categories>
        <category>arxiv</category>
      </categories>
      <tags>
        <tag>ArXiv</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Discourse-Aware Neural Extractive Model for Text Summarization</title>
    <url>/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Discourse-Aware-Neural-Extractive-Model-for-Text-Summarization/</url>
    <content><![CDATA[<p><strong>Discourse-Aware Neural Extractive Model for Text Summarization</strong>. Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu. ArXiv 1910.14142.<a href="https://arxiv.org/pdf/1910.14142.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>作者分析认为现有抽取式文档摘要存在以下两个不足：</p><a id="more"></a>

<ul>
<li>抽取式文档摘要都是以句子级别进行抽取，导致结果包含冗余或者没有用的信息。</li>
<li>BERT常被SOTA文档摘要模型用在文档编码器，但是BERT是再句对上预训练的，不能很好捕捉长距离的句间依赖关系。</li>
</ul>
<p>针对以上两个不足，作者提出了两个解决方法：</p>
<ul>
<li>按EDU进行抽取 （EDU是RST中的基本单元，具体可以去了解discourse parsing）</li>
<li>构造RST Graph和Coreference Graph建模长距离句间依赖关系。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>Discourse Segmentation: sequence to EDU</p>
<p>Discourse Parsing: EDU to RST tree</p>
<h2 id="RST-Graph"><a href="#RST-Graph" class="headerlink" title="RST Graph"></a>RST Graph</h2><p>通过篇章分析，可以在篇章上构造得到一棵树，树的叶子节点是EDU，树上的边代表的是对应子节点的重要性程度，N代表主要，S代表次要，可以认为S是N的补充。相邻两个子节点可以有三种关系，N-N,N-S,S-N。</p>
<p>作者提出假设：S依赖N,所以存在一条路径从S指向N；如果两个节点都是N，就认为是右N依赖做N。</p>
<p>根据这个假设，可以将RST discourse tree转成成RST dependence graph。</p>
<p><img src="/images/discbert1.jpg" alt></p>
<p>注：论文原图中没有标N和S，为了好理解我标了N和S。</p>
<p>如果存在一条从第i个EDU指向第j个EDU的路径，则设GR[i][j]=1，否则为0,这样就可以将RST Graph转化成GR矩阵。</p>
<p><img src="/images/discbert2.jpg" alt></p>
<h2 id="Coreference-Graph"><a href="#Coreference-Graph" class="headerlink" title="Coreference Graph"></a>Coreference Graph</h2><p>通过斯坦福的CoreNLP工具，可以得到多个共指簇（coreference clusters），每个簇中的EDU都指向同一个实体。指向同一个实体的EDU存在联系，所以同一个簇中的所有EDU之间（包括自己跟自己）存在一条边。基于这个原则，作者设计一个构造coreference graph的算法，遍历所有簇，簇中每个EDU之间存在一个边。也就得到了共指矩阵GC。</p>
<p><img src="/images/discbert3.jpg" alt></p>
<h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p><img src="/images/discbert4.jpg" alt></p>
<p>首先使用BERT编码整个篇章，使用BERT得到的隐状态表示，每个EDU内部做self-attention得到EDU的表示，由得到的EDU表示和两个矩阵表示GR和GC，做GCN得到EDU新的表示，通过MLP预测EDU是否被抽取出来做EDU（0-1序列标注）。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在两个数据集上进行验证，并得到了SOTA结果。</p>
<p><img src="/images/discbert5.jpg" alt></p>
<p><img src="/images/discbert6.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Discourse Structure</tag>
        <tag>Extractive</tag>
        <tag>Summarization</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Document-level Neural Machine Translation with Inter-Sentence Attention</title>
    <url>/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Document-level-Neural-Machine-Translation-with-Inter-Sentence-Attention/</url>
    <content><![CDATA[<p><strong>Document-level Neural Machine Translation with Inter-Sentence Attention</strong>. Shu Jiang, Rui Wang, Zuchao Li, Masao Utiyama, Kehai Chen, Eiichiro Sumita, Hai Zhao, Bao-liang Lu. ArXiv 1910.14528. <a href="https://arxiv.org/pdf/1910.14528.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文认为大部分篇章翻译只是引入大体的篇章上下文信息，但不是所有的上下文信息都对当前句子翻译有效，本文希望对上下文信息进行筛选。于是本文提出一个associated memory network（AMN）考虑句间关系，建模更加相关的上下文。(<em>其实 SAN 和 QCN 都有对上下文进行筛选</em>)</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="/images/camn.jpg" alt></p>
<p>（1）使用RNN对previous sentences（cj）进行编码，得到每个词的隐状态表示<font color="#FF0000">(<em>不是很懂为什么要用RNN，不直接使用transformer，并且当前句子x也不用像c一样使用RNN编码</em>)</font></p>
<p>（2）MultiHead Self-Attention更新每个句子的表示<br><img src="/images/camn1.jpg" alt><br><img src="/images/camn2.jpg" alt></p>
<p>（3）当前句子x的每个词和前面每个句子cj中的每个词算一个相似度分数<br><img src="/images/camn3.jpg" alt></p>
<p>（4）对相似性分数按行做softmax作为最终的相似性分数<br><img src="/images/camn4.jpg" alt></p>
<p>（5）得到句子级别上下文表示<br><img src="/images/camn5.jpg" alt></p>
<p>（6）建模每个句子的权重<br><img src="/images/camn6.jpg" alt></p>
<p>（7）得到篇章级别上下文<br><img src="/images/camn7.jpg" alt></p>
<p>（8）在transformer encoder中融入篇章级别上下文信息<br><img src="/images/camn8.jpg" alt><br><img src="/images/camn9.jpg" alt></p>
<p><font color="#FF0000">整体上来说，这种方法略显粗暴。</font></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在TED Talks, Subtitles, News三个数据集上验证了自己的模型有效性。</p>
<p><img src="/images/camn10.jpg" alt></p>
<font color="#FF0000">
  我认为实验还是存在一些不足：（1）没有跟SAN、QCN等工作进行对比（2）按照HAN公开代码，HAN是没有做BPE的，但是本文有做BPE，而本文中报的结果是HAN中报的没有做BPE的结果。
</font>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
        <tag>Document NMT</tag>
        <tag>Inter-Sentence</tag>
      </tags>
  </entry>
  <entry>
    <title>Accepted Papers List</title>
    <url>/2019/11/01/Accepted-Papers-List/</url>
    <content><![CDATA[<h1 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h1><ul>
<li><a href="https://aaai.org/Conferences/AAAI-20/wp-content/uploads/2020/01/AAAI-20-Accepted-Paper-List.pdf" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://openreview.net/group?id=ICLR.cc/2020/Conference" target="_blank" rel="noopener">ICLR</a></li>
</ul>
<h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><ul>
<li><a href="https://dblp.org/db/conf/aaai/aaai2019" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://aclweb.org/anthology/events/acl-2019/" target="_blank" rel="noopener">ACL</a></li>
<li><a href="http://openaccess.thecvf.com/CVPR2019.py" target="_blank" rel="noopener">CVPR</a></li>
<li><a href="https://www.aclweb.org/anthology/events/emnlp-2019/" target="_blank" rel="noopener">EMNLP</a></li>
<li><a href="https://openreview.net/group?id=ICLR.cc/2019/Conference" target="_blank" rel="noopener">ICLR</a></li>
<li><a href="https://icml.cc/Conferences/2019/Schedule?type=Poster" target="_blank" rel="noopener">ICML</a></li>
<li><a href="https://www.ijcai19.org/accepted-papers.html" target="_blank" rel="noopener">IJCAI</a></li>
<li><a href="https://aclweb.org/anthology/events/naacl-2019/" target="_blank" rel="noopener">NAACL</a></li>
<li><a href="https://nips.cc/Conferences/2019/Schedule?type=Poster" target="_blank" rel="noopener">NeurIPS</a></li>
</ul>
<a id="more"></a>

<h1 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h1><ul>
<li><a href="https://dblp.org/db/conf/aaai/aaai2018" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://aclweb.org/anthology/events/acl-2018/" target="_blank" rel="noopener">ACL</a></li>
<li><a href="http://openaccess.thecvf.com/CVPR2018.py" target="_blank" rel="noopener">CVPR</a></li>
<li><a href="https://aclweb.org/anthology/events/emnlp-2018/" target="_blank" rel="noopener">EMNLP</a></li>
<li><a href="https://iclr.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">ICLR</a></li>
<li><a href="https://icml.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">ICML</a></li>
<li><a href="https://www.ijcai-18.org/accepted-papers/index.html" target="_blank" rel="noopener">IJCAI</a></li>
<li><a href="https://aclweb.org/anthology/events/naacl-2018/" target="_blank" rel="noopener">NAACL</a></li>
<li><a href="https://nips.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">NeurIPS</a></li>
</ul>
]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
      <tags>
        <tag>AAAI</tag>
        <tag>Accepted Papers</tag>
        <tag>ACL</tag>
        <tag>CVPR</tag>
        <tag>EMNLP</tag>
        <tag>ICLR</tag>
        <tag>ICML</tag>
        <tag>IJCAI</tag>
        <tag>NAACL</tag>
        <tag>NIPS</tag>
      </tags>
  </entry>
  <entry>
    <title>公开课推荐</title>
    <url>/2019/10/31/%E5%85%AC%E5%BC%80%E8%AF%BE%E6%8E%A8%E8%8D%90/</url>
    <content><![CDATA[<h1 id="机器学习（斯坦福大学）"><a href="#机器学习（斯坦福大学）" class="headerlink" title="机器学习（斯坦福大学）"></a>机器学习（斯坦福大学）</h1><p>机器学习是一门研究在非特定编程条件下让计算机采取行动的学科。最近二十年，机器学习为我们带来了自动驾驶汽车、实用的语音识别、高效的网络搜索，让我们对人类基因的解读能力大大提高。当今机器学习技术已经非常普遍，您很可能在毫无察觉情况下每天使用几十次。许多研究者还认为机器学习是人工智能（AI）取得进展的最有效途径。</p><a id="more"></a>
<p>本课程将广泛介绍机器学习、数据挖掘和统计模式识别。相关主题包括：(i) 监督式学习（参数和非参数算法、支持向量机、核函数和神经网络）。(ii) 无监督学习（集群、降维、推荐系统和深度学习）。(iii) 机器学习实例（偏见/方差理论；机器学习和AI领域的创新）。课程将引用很多案例和应用，您还需要学习如何在不同领域应用学习算法，例如智能机器人（感知和控制）、文本理解（网络搜索和垃圾邮件过滤）、计算机视觉、医学信息学、音频、数据库挖掘等领域。</p>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a></li>
<li><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="noopener">网易公开课</a></li>
</ul>
<hr>
<h1 id="CS231n（斯坦福大学）"><a href="#CS231n（斯坦福大学）" class="headerlink" title="CS231n（斯坦福大学）"></a>CS231n（斯坦福大学）</h1><p>计算机视觉已经在我们的社会中无处不在，在搜索，图像理解，应用程序，测绘，医药，无人驾驶飞机和自动驾驶汽车中的应用。许多这些应用程序的核心是视觉识别任务，如图像分类，定位和检测。神经网络（又名“深度学习”）方法的最新发展极大地提高了这些最先进的视觉识别系统的性能。本课程深入探讨深度学习架构的细节，重点是学习这些任务的端到端模型，尤其是图像分类。在为期10周的课程中，学生将学习实施，训练和调试自己的神经网络，并获得对计算机视觉尖端研究的详细了解。最后的任务将涉及培训一个数百万参数卷积神经网络，并将其应用于最大的图像分类数据集（ImageNet）。我们将着重教授如何设置图像识别问题，学习算法（例如反向传播），用于训练和微调网络的实际工程技巧，并引导学生通过实践任务和最终课程项目。本课程的大部分背景和材料都将从ImageNet挑战中吸取。</p>
<h2 id="链接-1"><a href="#链接-1" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="noopener">YouTube</a></li>
</ul>
<hr>
<h1 id="CS20SI（斯坦福大学）"><a href="#CS20SI（斯坦福大学）" class="headerlink" title="CS20SI（斯坦福大学）"></a>CS20SI（斯坦福大学）</h1><p>Tensorflow是Google Brain研究人员开发的一个功能强大的机器学习开源软件库。它具有许多预建功能，可以简化构建不同神经网络的任务。 Tensorflow允许在不同计算机之间分配计算，以及在一台机器中分配多个CPU和GPU。 TensorFlow提供了一个Python API，以及一个较少记录的C ++ API。对于本课程，我们将使用Python。</p>
<p>本课程将涵盖深入学习研究的Tensorflow图书馆的基本原理和当代用法。帮助学生理解Tensorflow的图形计算模型，探索其提供的功能，并学习如何构建和构建最适合深度学习项目的模型。通过本课程，学生将使用Tensorflow建立不同复杂度的模型，从简单的线性/逻辑回归到卷积神经网络和带有LSTM的递归神经网络，以解决词嵌入，翻译，光学字符识别等任务。学生还将学习最佳实践来构建模型并管理研究实验。</p>
<h2 id="链接-2"><a href="#链接-2" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://web.stanford.edu/class/cs20si/" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/watch?v=g-EvyKpZjmQ&list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="noopener">YouTube</a></li>
</ul>
<hr>
<h1 id="CS224d（斯坦福大学）"><a href="#CS224d（斯坦福大学）" class="headerlink" title="CS224d（斯坦福大学）"></a>CS224d（斯坦福大学）</h1><p>自然语言处理（NLP）是信息时代最重要的技术之一。理解复杂的语言也是人工智能的重要组成部分。 NLP的应用无处不在，因为人们用语言沟通大多数事物：网络搜索，广告，电子邮件，客户服务，语言翻译，放射学报告等等。NLP应用背后有大量的基础任务和机器学习模型。最近，深度学习方法在许多不同的NLP任务中获得了非常高的性能。这些模型通常可以通过单一的端到端模型进行培训，而且不需要传统的，特定于任务的特征工程。在这个冬季的季度课程中，学生将学习实施，培训，调试，可视化和创造自己的神经网络模型。本课程深入介绍了深入学习NLP的前沿研究。在模型方面，我们将介绍词向量表示，基于窗口的神经网络，递归神经网络，长期 - 短期记忆模型，递归神经网络，卷积神经网络以及一些涉及存储器组件的最新模型。通过讲座和编程作业，学生将学习使神经网络适应实际问题的必要工程技巧。</p>
<h2 id="链接-3"><a href="#链接-3" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://www.youtube.com/watch?v=g-EvyKpZjmQ&list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6" target="_blank" rel="noopener">YouTube</a></li>
</ul>
]]></content>
      <categories>
        <category>公开课</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>CS231n</tag>
        <tag>CS20SI</tag>
        <tag>CS224d</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Hierarchical-Modeling-of-Global-Context-for-Document-Level-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</strong>. Xin Tan, Longyin Zhang, Deyi Xiong, Guodong Zhou. EMNLP 2019. <a href="https://www.aclweb.org/anthology/D19-1168.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文觉得现有篇章翻译工作基于pre-context的方法存在两个不足：</p>
<p>（1）只利用一边（one-sidedness）的上下文可能还不够</p>
<p>（2）不正确的pre-context（translation bias propagation caused by improper pre-context）可能会导致翻译错误，所以本文想要利用整个篇章建模全局上下文（global context）来提升篇章翻译。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="使用层次结构建模全局上下文"><a href="#使用层次结构建模全局上下文" class="headerlink" title="使用层次结构建模全局上下文"></a>使用层次结构建模全局上下文</h2><p><img src="/images/HM-GDC.png" alt></p>
<p>A. Sentence Encoder</p>
<p>首先对句子进行编码得到每个词的隐状态表示，</p>
<p><img src="/images/h1.png" alt></p>
<p>求和得到整个句子的表示，</p>
<p><img src="/images/h21.png" alt></p>
<p>B. Document Encoder</p>
<p>对篇章所有句子进行编码，得到拥有篇章信息的句子表示（sentence-level global context）</p>
<p><img src="/images/h22.png" alt></p>
<p>C. Backpropagation of global context</p>
<p>由sentence-level global context得到word-level global context</p>
<p><img src="/images/h3.png" alt></p>
<h2 id="将全局上下文结合到NMT中"><a href="#将全局上下文结合到NMT中" class="headerlink" title="将全局上下文结合到NMT中"></a>将全局上下文结合到NMT中</h2><p>像其他工作一样，这个global context既结合在编码阶段，也可以结合在解码阶段。</p>
<p>A. 结合在编码阶段</p>
<p>使用word-level global context更新每个词的表示，P表示残差dropout，这里为0.1。</p>
<p><img src="/images/h5.png" alt></p>
<p>B. 结合在解码阶段</p>
<p><img src="/images/h4.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>本文实验在中英和德英两个数据集上进行。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>A. 中英</p>
<p>句子级别数据（用于预训练）：2.8M news corpora (LDC 2003E14, LDC2004T07, LDC2005T06, LDC2005T10, LDC2004T08)</p>
<p>篇章级别数据: IWSLT 2017 TED (1906个文档，226K个句对 )</p>
<p>B. 德英</p>
<p>(不进行预训练，没有句子级别数据)</p>
<p>篇章级别数据：IWSLT 2014 TED (1361个文档，172个句对)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>EMNLP</tag>
        <tag>NMT</tag>
        <tag>Context</tag>
        <tag>Document NMT</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Cross-Lingual-BERT-Transformation-for-Zero-Shot-Dependency-Parsing/</url>
    <content><![CDATA[<p><strong>Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</strong>. Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, Ting Liu. EMNLP 2019 <a href="https://arxiv.org/abs/1909.06775" target="_blank" rel="noopener">[PDF]</a>（短文）</p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本篇论文主要解决目前大部分cross-lingual word embedding技术存在的问题：</p><a id="more"></a>

<p>（1）依赖大量跨语言数据</p>
<p>（2）需要大量计算资源和训练时间</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>本文提出一种简单快捷的离线cross-lingual BERT线性映射方法：</p>
<p>（1）通过无监督词对齐方法获得上下文对齐次对（context-level，非词典）</p>
<p>（2）通过预训练好的BERT模型得到上下文对齐次对（x,y）中x,y的上下文表示</p>
<p>（3）通过SVD(奇异值分解)、GD(梯度下降)的方式求得两个表示的线性映射</p>
<p><img src="https://img-blog.csdnimg.cn/2019100320530798.png" alt></p>
<p>作者将获得的跨语言上下文词向量应用到zero-shot依存分析任务上，并获得了目前最好结果。并与XLM(利用跨语言数据重新训练BERT的方法)进行了对比，实验表明该方法在取得与XLM相近结果的情况下，需要的计算资源更少，训练速度也更快。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>EMNLP</tag>
        <tag>Cross Lingual</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Neural-Keyphrase-Generation-via-Reinforcement-Learning-with-Adaptive-Rewards/</url>
    <content><![CDATA[<p><strong>Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</strong>. Hou Pong Chan, Wang Chen, Lu Wang, Irwin King. ACL 2019. <a href="https://arxiv.org/abs/1906.04106" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本篇论文主要解决目前keyphrase generation任务中存在的两个不足：</p><a id="more"></a>

<p>（1）模型生成的keyphrase比真实的keyphrase个数少</p>
<p>（2）已有评价标准依赖词的完成匹配（不完全匹配就算错，如真实keyphrase为SVM，模型生成的keyphrase为support vector machine也算错）</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>keyphrase根据是否在原文中是否出现分present和absent，这里将一个document的所有keyphrase拼接成一个序列，present在前absent在后，并通过利用seq2seq编码document来生成所有的keyphrase。<br><img src="https://img-blog.csdnimg.cn/20190620092517159.png" width="55%" height="55%"></p>
<p>针对第一个不足，作者使用了reinforcement learning，</p>
<p>sample policy：<br><img src="https://img-blog.csdnimg.cn/20190620092800754.png" alt></p>
<p>reward function: RF1<br><img src="https://img-blog.csdnimg.cn/20190620091404860.png" alt></p>
<p>N为目前已生成的keyphrase个数，G为真实keyphrase个数。作者认为当生成keyphrase的个数还少于真实keyphrase个数时，应该鼓励模型去生成更多的keyphrase，所以用recall作为reward；当个数足够时，除了要求个数也要要求正确性，所以用的F1。看到这里可能也有人会有疑问，为什么前面只重视个数而忽视正确性呢？为什么不改变个数和正确性的权重呢（可以认为是F1的变形）？在这里我个人认为作者可能是实验驱动，只用recall就有效果了；如果没有效果作者可能会去设计吧。。。</p>
<p>presen keyphrase 和 absent keyphrase分别计算reward:<br><img src="https://img-blog.csdnimg.cn/20190620093050519.png" width="55%" height="55%"></p>
<p>针对第二个不足，思路也很容易理解，就是找keyphrase的各种形式，作者这里主要有三个方法</p>
<p>（1）Acronyms in the ground-truths</p>
<p>（2）Wikipedia entity titles</p>
<p>（3）Wikipedia disambiguation pages</p>
<p>然后作者在四个baseline基础上分别验证了方法的有效性，并且对生成的keyphrase的个数、RF1进行了分析。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>ACL</tag>
        <tag>Keyphrase Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 教程</title>
    <url>/2019/10/31/Hexo-%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<ul>
<li><a href="https://hexo-guide.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">hexo指南</a></li>
<li><a href="https://ahh666.com/posts/blog_gitalk_about.html" target="_blank" rel="noopener">添加gitalk评论</a></li>
<li><a href="https://blog.yleao.com/2018/0901/hexo-next%E4%B8%BB%E9%A2%98%E4%B8%8B%E7%9A%84%E7%BE%8E%E5%8C%96.html" target="_blank" rel="noopener">hexo-next主题下的美化</a></li>
<li><a href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/zh-CN/MATH.md" target="_blank" rel="noopener">hexo-next使用公式</a></li>
<li><a href="https://io-oi.me/tech/hexo-next-optimization/" target="_blank" rel="noopener">打造个性超赞博客 Hexo + NexT + GitHub Pages 的超深度优化</a></li>
<li><a href="https://muse.theme-next.org/" target="_blank" rel="noopener">Next官方文档</a></li>
<li><a href="https://fontawesome.com/v4.7.0/icons/" target="_blank" rel="noopener">图标</a></li>
</ul>]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Unsupervised%20Neural%20Single-Document%20Summarization%20of%20Reviews%20via/</url>
    <content><![CDATA[<p><strong>Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</strong>. Masaru Isonuma, Junichiro Mori, Ichiro Sakata. ACL 2019. <a href="https://arxiv.org/pdf/1906.05691.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文认为，评论（review）可以当作一个棵篇章树，树的根节点是其摘要，表示该评论的整体意思; 树的其他节点是对其父节点的细化。 也就是说这棵篇章树由摘要（根节点）与评论中所有句子（非根节点，每个非根节点代表一个句子）组成。于是本文通过学习构造这个隐式篇章树来建模得到评论摘要，并提出一种排序（rank）算法选择对生成摘要更加重要的句子。</p>
<p><img src="/images/strsum.png" alt></p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="模型整体方法"><a href="#模型整体方法" class="headerlink" title="模型整体方法"></a>模型整体方法</h2><p>（1）双向GRU+maxpooling 建模得到每个句子表示</p>
<p>（2）建模 父节点-子节点 对应关系权重（权重代表了树的关系）</p>
<p>（3）加权求和所有子节点表示，生成父节点（本文假设：子节点能够还原父节点，因为子节点包含了比父节点更多的信息。）</p>
<p>目标函数就是所有父节点生成概率最大。</p>
<p><img src="/images/strsum2.png" alt></p>
<h2 id="父节点-子节点-对应关系权重建模方法"><a href="#父节点-子节点-对应关系权重建模方法" class="headerlink" title="父节点-子节点 对应关系权重建模方法"></a>父节点-子节点 对应关系权重建模方法</h2><p>初始建模：边界概率（Marginal Probability of Dependency）</p>
<p><img src="/images/strsum3.png" alt></p>
<p>归一化（公式推导不是很懂）</p>
<p><img src="/images/strsum4.png" alt></p>
<p>调整：篇章排序（DiscourseRank）</p>
<p>受PageRank算法启发，更重要的句子有更多后代，迭代更新权重矩阵。<br><img src="/images/strsum5.png" alt></p>
<p><img src="/images/strsum6.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>ACL</tag>
        <tag>Discourse Structure</tag>
        <tag>Discourse Ranking</tag>
      </tags>
  </entry>
</search>

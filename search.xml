<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ArXiv 论文 2019/10/28-2019/11/1</title>
    <url>/2019/11/02/ArXiv-%E8%AE%BA%E6%96%87-2019-10-28-2019-11-1/</url>
    <content><![CDATA[<ul>
<li><a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
<li><a href="https://arxiv.org/abs/1910.14659" target="_blank" rel="noopener">Pseudolikelihood Reranking with Masked Language Models</a></li>
<li><a href="https://arxiv.org/abs/1910.14549" target="_blank" rel="noopener">Positional Attention-based Frame Identification with BERT: A Deep Learning Approach to Target Disambiguation and Semantic Frame Selection</a></li>
<li><a href="https://arxiv.org/abs/1910.14192" target="_blank" rel="noopener">Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning</a></li>
<li><a href="https://arxiv.org/abs/1910.14176" target="_blank" rel="noopener">Predicting Discourse Structure using Distant Supervision from Sentiment</a><a id="more"></a></li>
<li><a href="https://arxiv.org/abs/1910.14142" target="_blank" rel="noopener">Discourse-Aware Neural Extractive Model for Text Summarization</a></li>
<li><a href="https://arxiv.org/abs/1910.14075" target="_blank" rel="noopener">Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.14613" target="_blank" rel="noopener">Neural Assistant: Joint Action Prediction, Response Generation, and Latent Knowledge Reasoning</a></li>
<li><a href="https://arxiv.org/abs/1910.14208" target="_blank" rel="noopener">Hidden State Guidance: Improving Image Captioning using An Image Conditioned Autoencoder</a></li>
<li><a href="https://arxiv.org/abs/1910.13890" target="_blank" rel="noopener">A Latent Morphology Model for Open-Vocabulary Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.13794" target="_blank" rel="noopener">Let Me Know What to Ask: Interrogative-Word-Aware Question Generation</a></li>
<li><a href="https://arxiv.org/abs/1910.13466" target="_blank" rel="noopener">Ordered Memory</a></li>
<li><a href="https://arxiv.org/abs/1910.13106" target="_blank" rel="noopener">Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots</a></li>
<li><a href="https://arxiv.org/abs/1910.13267" target="_blank" rel="noopener">BPE-Dropout: Simple and Effective Subword Regularization</a></li>
<li><a href="https://arxiv.org/abs/1910.13294" target="_blank" rel="noopener">Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control</a></li>
<li><a href="https://arxiv.org/abs/1910.13437" target="_blank" rel="noopener">An Empirical Study of Generation Order for Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.12708" target="_blank" rel="noopener">Evaluating Lottery Tickets Under Distributional Shifts</a></li>
<li><a href="https://arxiv.org/abs/1910.12702" target="_blank" rel="noopener">Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling</a></li>
<li><a href="https://arxiv.org/abs/1910.12698" target="_blank" rel="noopener">Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis</a></li>
<li><a href="https://arxiv.org/abs/1910.12527" target="_blank" rel="noopener">RPM-Oriented Query Rewriting Framework for E-commerce Keyword-Based Sponsored Search</a></li>
<li><a href="https://arxiv.org/abs/1910.12391" target="_blank" rel="noopener">What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?</a></li>
<li><a href="https://arxiv.org/abs/1910.12197" target="_blank" rel="noopener">Look-up and Adapt: A One-shot Semantic Parser</a></li>
<li><a href="https://arxiv.org/abs/1910.12196" target="_blank" rel="noopener">Open the Boxes of Words: Incorporating Sememes into Textual Adversarial Attack</a></li>
<li><a href="https://arxiv.org/abs/1910.11966" target="_blank" rel="noopener">Yall should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts</a></li>
<li><a href="https://arxiv.org/abs/1910.12038" target="_blank" rel="noopener">Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention</a></li>
<li><a href="https://arxiv.org/abs/1910.11959" target="_blank" rel="noopener">FineText: Text Classification via Attention-based Language Model Fine-tuning</a></li>
<li><a href="https://arxiv.org/abs/1910.12094" target="_blank" rel="noopener">Meta Learning for End-to-End Low-Resource Speech Recognition</a></li>
<li><a href="https://arxiv.org/abs/1910.11491" target="_blank" rel="noopener">Attention Optimization for Abstractive Document Summarization</a></li>
<li><a href="https://arxiv.org/abs/1910.11471" target="_blank" rel="noopener">Machine Translation from Natural Language to Code using Long-Short Term Memory</a></li>
<li><a href="https://arxiv.org/abs/1910.11470" target="_blank" rel="noopener">A Survey on Recent Advances in Named Entity Recognition from Deep Learning models</a></li>
<li><a href="https://arxiv.org/abs/1910.11411" target="_blank" rel="noopener">Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations</a></li>
<li><a href="https://arxiv.org/abs/1910.11399" target="_blank" rel="noopener">Comparison of Quality Indicators in User-generated Content Using Social Media and Scholarly Text</a></li>
<li><a href="https://arxiv.org/abs/1910.11494" target="_blank" rel="noopener">Fast and Accurate Knowledge-Aware Document Representation Enhancement for News Recommendations</a></li>
<li><a href="https://arxiv.org/abs/1910.11455" target="_blank" rel="noopener">Recognizing long-form speech using streaming end-to-end models</a></li>
</ul>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>ArXiv</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Discourse-Aware Neural Extractive Model for Text Summarization</title>
    <url>/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Discourse-Aware-Neural-Extractive-Model-for-Text-Summarization/</url>
    <content><![CDATA[<p><strong>Discourse-Aware Neural Extractive Model for Text Summarization</strong>. Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu. ArXiv 1910.14142.<a href="https://arxiv.org/pdf/1910.14142.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>作者分析认为现有抽取式文档摘要存在以下两个不足：</p><a id="more"></a>

<ul>
<li>抽取式文档摘要都是以句子级别进行抽取，导致结果包含冗余或者没有用的信息。</li>
<li>BERT常被SOTA文档摘要模型用在文档编码器，但是BERT是再句对上预训练的，不能很好捕捉长距离的句间依赖关系。</li>
</ul>
<p>针对以上两个不足，作者提出了两个解决方法：</p>
<ul>
<li>按EDU进行抽取 （EDU是RST中的基本单元，具体可以去了解discourse parsing）</li>
<li>构造RST Graph和Coreference Graph建模长距离句间依赖关系。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>Discourse Segmentation: sequence to EDU</p>
<p>Discourse Parsing: EDU to RST tree</p>
<h2 id="RST-Graph"><a href="#RST-Graph" class="headerlink" title="RST Graph"></a>RST Graph</h2><p>通过篇章分析，可以在篇章上构造得到一棵树，树的叶子节点是EDU，树上的边代表的是对应子节点的重要性程度，N代表主要，S代表次要，可以认为S是N的补充。相邻两个子节点可以有三种关系，N-N,N-S,S-N。</p>
<p>作者提出假设：S依赖N,所以存在一条路径从S指向N；如果两个节点都是N，就认为是右N依赖做N。</p>
<p>根据这个假设，可以将RST discourse tree转成成RST dependence graph。</p>
<p><img src="/images/discbert1.jpg" alt></p>
<p>注：论文原图中没有标N和S，为了好理解我表了N和S。</p>
<p>如果存在一条从第i个EDU指向第j个EDU的路径，则设GR[i][j]=1，否则为0,这样就可以将RST Graph转化成GR矩阵。</p>
<p><img src="/images/discbert2.jpg" alt></p>
<h2 id="Coreference-Graph"><a href="#Coreference-Graph" class="headerlink" title="Coreference Graph"></a>Coreference Graph</h2><p>通过斯坦福的CoreNLP工具，可以得到多个共指簇（coreference clusters），每个簇中的EDU都指向同一个实体。指向同一个实体的EDU存在联系，所以同一个簇中的所有EDU之间（包括自己跟自己）存在一条边。基于这个原则，作者设计一个构造coreference graph的算法，遍历所有簇，簇中每个EDU之间存在一个边。也就得到了共指矩阵GC。</p>
<p><img src="/images/discbert3.jpg" alt></p>
<h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p><img src="/images/discbert4.jpg" alt></p>
<p>首先使用BERT编码整个篇章，使用BERT得到的隐状态表示，每个EDU内部做self-attention得到EDU的表示，由得到的EDU表示和两个矩阵表示GR和GC，做GCN得到EDU新的表示，通过MLP预测EDU是否被抽取出来做EDU（0-1序列标注）。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在两个数据集上进行验证，并得到了SOTA结果。</p>
<p><img src="/images/discbert5.jpg" alt></p>
<p><img src="/images/discbert6.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Discourse Structure</tag>
        <tag>Extractive</tag>
        <tag>Summarization</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Document-level Neural Machine Translation with Inter-Sentence Attention</title>
    <url>/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Document-level-Neural-Machine-Translation-with-Inter-Sentence-Attention/</url>
    <content><![CDATA[<p><strong>Document-level Neural Machine Translation with Inter-Sentence Attention</strong>. Shu Jiang, Rui Wang, Zuchao Li, Masao Utiyama, Kehai Chen, Eiichiro Sumita, Hai Zhao, Bao-liang Lu. ArXiv 1910.14528. <a href="https://arxiv.org/pdf/1910.14528.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><font color="#FF0000">红色：HAN没有做BPE,本文有做BPE，用BPE结果对比没有BPE结果，这种对比不太公平。</font></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
        <tag>Document NMT</tag>
        <tag>Inter-Sentence</tag>
      </tags>
  </entry>
  <entry>
    <title>Accepted Papers List</title>
    <url>/2019/11/01/Accepted-Papers-List/</url>
    <content><![CDATA[<h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><ul>
<li><a href="https://aaai.org/Conferences/AAAI-19/wp-content/uploads/2018/11/AAAI-19_Accepted_Papers.pdf" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://aclweb.org/anthology/events/acl-2019/" target="_blank" rel="noopener">ACL</a></li>
<li><a href="http://openaccess.thecvf.com/CVPR2019.py" target="_blank" rel="noopener">CVPR</a></li>
<li><a href="https://www.emnlp-ijcnlp2019.org/program/accepted/" target="_blank" rel="noopener">EMNLP</a></li>
<li><a href="https://openreview.net/group?id=ICLR.cc/2019/Conference" target="_blank" rel="noopener">ICLR</a></li>
<li><a href="https://icml.cc/Conferences/2019/Schedule?type=Poster" target="_blank" rel="noopener">ICML</a></li>
<li><a href="https://www.ijcai19.org/accepted-papers.html" target="_blank" rel="noopener">IJCAI</a></li>
<li><a href="https://aclweb.org/anthology/events/naacl-2019/" target="_blank" rel="noopener">NAACL</a></li>
<li><a href="https://nips.cc/Conferences/2019/Schedule?type=Poster" target="_blank" rel="noopener">NeurIPS</a></li>
</ul>
<a id="more"></a>

<h1 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h1><ul>
<li><a href="https://dblp.org/db/conf/aaai/aaai2018" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://aclweb.org/anthology/events/acl-2018/" target="_blank" rel="noopener">ACL</a></li>
<li><a href="http://openaccess.thecvf.com/CVPR2018.py" target="_blank" rel="noopener">CVPR</a></li>
<li><a href="https://aclweb.org/anthology/events/emnlp-2018/" target="_blank" rel="noopener">EMNLP</a></li>
<li><a href="https://iclr.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">ICLR</a></li>
<li><a href="https://icml.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">ICML</a></li>
<li><a href="https://www.ijcai-18.org/accepted-papers/index.html" target="_blank" rel="noopener">IJCAI</a></li>
<li><a href="https://aclweb.org/anthology/events/naacl-2018/" target="_blank" rel="noopener">NAACL</a></li>
<li><a href="https://nips.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">NeurIPS</a></li>
</ul>
]]></content>
      <categories>
        <category>Accepted Papers</category>
      </categories>
      <tags>
        <tag>Accepted Papers</tag>
        <tag>AAAI</tag>
        <tag>ACL</tag>
        <tag>CVPR</tag>
        <tag>EMNLP</tag>
        <tag>ICLR</tag>
        <tag>ICML</tag>
        <tag>IJCAI</tag>
        <tag>NAACL</tag>
        <tag>NIPS</tag>
      </tags>
  </entry>
  <entry>
    <title>公开课推荐</title>
    <url>/2019/10/31/%E5%85%AC%E5%BC%80%E8%AF%BE%E6%8E%A8%E8%8D%90/</url>
    <content><![CDATA[<h1 id="机器学习（斯坦福大学）"><a href="#机器学习（斯坦福大学）" class="headerlink" title="机器学习（斯坦福大学）"></a>机器学习（斯坦福大学）</h1><p>机器学习是一门研究在非特定编程条件下让计算机采取行动的学科。最近二十年，机器学习为我们带来了自动驾驶汽车、实用的语音识别、高效的网络搜索，让我们对人类基因的解读能力大大提高。当今机器学习技术已经非常普遍，您很可能在毫无察觉情况下每天使用几十次。许多研究者还认为机器学习是人工智能（AI）取得进展的最有效途径。</p><a id="more"></a>
<p>本课程将广泛介绍机器学习、数据挖掘和统计模式识别。相关主题包括：(i) 监督式学习（参数和非参数算法、支持向量机、核函数和神经网络）。(ii) 无监督学习（集群、降维、推荐系统和深度学习）。(iii) 机器学习实例（偏见/方差理论；机器学习和AI领域的创新）。课程将引用很多案例和应用，您还需要学习如何在不同领域应用学习算法，例如智能机器人（感知和控制）、文本理解（网络搜索和垃圾邮件过滤）、计算机视觉、医学信息学、音频、数据库挖掘等领域。</p>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a></li>
<li><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="noopener">网易公开课</a></li>
</ul>
<hr>
<h1 id="CS231n（斯坦福大学）"><a href="#CS231n（斯坦福大学）" class="headerlink" title="CS231n（斯坦福大学）"></a>CS231n（斯坦福大学）</h1><p>计算机视觉已经在我们的社会中无处不在，在搜索，图像理解，应用程序，测绘，医药，无人驾驶飞机和自动驾驶汽车中的应用。许多这些应用程序的核心是视觉识别任务，如图像分类，定位和检测。神经网络（又名“深度学习”）方法的最新发展极大地提高了这些最先进的视觉识别系统的性能。本课程深入探讨深度学习架构的细节，重点是学习这些任务的端到端模型，尤其是图像分类。在为期10周的课程中，学生将学习实施，训练和调试自己的神经网络，并获得对计算机视觉尖端研究的详细了解。最后的任务将涉及培训一个数百万参数卷积神经网络，并将其应用于最大的图像分类数据集（ImageNet）。我们将着重教授如何设置图像识别问题，学习算法（例如反向传播），用于训练和微调网络的实际工程技巧，并引导学生通过实践任务和最终课程项目。本课程的大部分背景和材料都将从ImageNet挑战中吸取。</p>
<h2 id="链接-1"><a href="#链接-1" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="noopener">YouTube</a></li>
</ul>
<hr>
<h1 id="CS20SI（斯坦福大学）"><a href="#CS20SI（斯坦福大学）" class="headerlink" title="CS20SI（斯坦福大学）"></a>CS20SI（斯坦福大学）</h1><p>Tensorflow是Google Brain研究人员开发的一个功能强大的机器学习开源软件库。它具有许多预建功能，可以简化构建不同神经网络的任务。 Tensorflow允许在不同计算机之间分配计算，以及在一台机器中分配多个CPU和GPU。 TensorFlow提供了一个Python API，以及一个较少记录的C ++ API。对于本课程，我们将使用Python。</p>
<p>本课程将涵盖深入学习研究的Tensorflow图书馆的基本原理和当代用法。帮助学生理解Tensorflow的图形计算模型，探索其提供的功能，并学习如何构建和构建最适合深度学习项目的模型。通过本课程，学生将使用Tensorflow建立不同复杂度的模型，从简单的线性/逻辑回归到卷积神经网络和带有LSTM的递归神经网络，以解决词嵌入，翻译，光学字符识别等任务。学生还将学习最佳实践来构建模型并管理研究实验。</p>
<h2 id="链接-2"><a href="#链接-2" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://web.stanford.edu/class/cs20si/" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/watch?v=g-EvyKpZjmQ&list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="noopener">YouTube</a></li>
</ul>
<hr>
<h1 id="CS224d（斯坦福大学）"><a href="#CS224d（斯坦福大学）" class="headerlink" title="CS224d（斯坦福大学）"></a>CS224d（斯坦福大学）</h1><p>自然语言处理（NLP）是信息时代最重要的技术之一。理解复杂的语言也是人工智能的重要组成部分。 NLP的应用无处不在，因为人们用语言沟通大多数事物：网络搜索，广告，电子邮件，客户服务，语言翻译，放射学报告等等。NLP应用背后有大量的基础任务和机器学习模型。最近，深度学习方法在许多不同的NLP任务中获得了非常高的性能。这些模型通常可以通过单一的端到端模型进行培训，而且不需要传统的，特定于任务的特征工程。在这个冬季的季度课程中，学生将学习实施，培训，调试，可视化和创造自己的神经网络模型。本课程深入介绍了深入学习NLP的前沿研究。在模型方面，我们将介绍词向量表示，基于窗口的神经网络，递归神经网络，长期 - 短期记忆模型，递归神经网络，卷积神经网络以及一些涉及存储器组件的最新模型。通过讲座和编程作业，学生将学习使神经网络适应实际问题的必要工程技巧。</p>
<h2 id="链接-3"><a href="#链接-3" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://www.youtube.com/watch?v=g-EvyKpZjmQ&list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6" target="_blank" rel="noopener">YouTube</a></li>
</ul>
]]></content>
      <categories>
        <category>公开课</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>CS231n</tag>
        <tag>CS20SI</tag>
        <tag>CS224d</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Hierarchical-Modeling-of-Global-Context-for-Document-Level-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</strong>. Xin Tan, Longyin Zhang, Deyi Xiong, Guodong Zhou. EMNLP 2019. <a href="https://www.aclweb.org/anthology/D19-1168.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文觉得现有篇章翻译工作基于pre-context的方法存在两个不足：</p>
<p>（1）只利用一边（one-sidedness）的上下文可能还不够</p>
<p>（2）不正确的pre-context（translation bias propagation caused by improper pre-context）可能会导致翻译错误，所以本文想要利用整个篇章建模全局上下文（global context）来提升篇章翻译。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="使用层次结构建模全局上下文"><a href="#使用层次结构建模全局上下文" class="headerlink" title="使用层次结构建模全局上下文"></a>使用层次结构建模全局上下文</h2><p><img src="/images/HM-GDC.png" alt></p>
<p>A. Sentence Encoder</p>
<p>首先对句子进行编码得到每个词的隐状态表示，</p>
<p><img src="/images/h1.png" alt></p>
<p>求和得到整个句子的表示，</p>
<p><img src="/images/h21.png" alt></p>
<p>B. Document Encoder</p>
<p>对篇章所有句子进行编码，得到拥有篇章信息的句子表示（sentence-level global context）</p>
<p><img src="/images/h22.png" alt></p>
<p>C. Backpropagation of global context</p>
<p>由sentence-level global context得到word-level global context</p>
<p><img src="/images/h3.png" alt></p>
<h2 id="将全局上下文结合到NMT中"><a href="#将全局上下文结合到NMT中" class="headerlink" title="将全局上下文结合到NMT中"></a>将全局上下文结合到NMT中</h2><p>像其他工作一样，这个global context既结合在编码阶段，也可以结合在解码阶段。</p>
<p>A. 结合在编码阶段</p>
<p>使用word-level global context更新每个词的表示，P表示残差dropout，这里为0.1。</p>
<p><img src="/images/h5.png" alt></p>
<p>B. 结合在解码阶段</p>
<p><img src="/images/h4.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>本文实验在中英和德英两个数据集上进行。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>A. 中英</p>
<p>句子级别数据（用于预训练）：2.8M news corpora (LDC 2003E14, LDC2004T07, LDC2005T06, LDC2005T10, LDC2004T08)</p>
<p>篇章级别数据: IWSLT 2017 TED (1906个文档，226K个句对 )</p>
<p>B. 德英</p>
<p>(不进行预训练，没有句子级别数据)</p>
<p>篇章级别数据：IWSLT 2014 TED (1361个文档，172个句对)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>EMNLP</tag>
        <tag>NMT</tag>
        <tag>Document NMT</tag>
        <tag>Context</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Cross-Lingual-BERT-Transformation-for-Zero-Shot-Dependency-Parsing/</url>
    <content><![CDATA[<p><strong>Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</strong>. Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, Ting Liu. EMNLP 2019 <a href="https://arxiv.org/abs/1909.06775" target="_blank" rel="noopener">[PDF]</a>（短文）</p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本篇论文主要解决目前大部分cross-lingual word embedding技术存在的问题：</p><a id="more"></a>

<p>（1）依赖大量跨语言数据</p>
<p>（2）需要大量计算资源和训练时间</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>本文提出一种简单快捷的离线cross-lingual BERT线性映射方法：</p>
<p>（1）通过无监督词对齐方法获得上下文对齐次对（context-level，非词典）</p>
<p>（2）通过预训练好的BERT模型得到上下文对齐次对（x,y）中x,y的上下文表示</p>
<p>（3）通过SVD(奇异值分解)、GD(梯度下降)的方式求得两个表示的线性映射</p>
<p><img src="https://img-blog.csdnimg.cn/2019100320530798.png" alt></p>
<p>作者将获得的跨语言上下文词向量应用到zero-shot依存分析任务上，并获得了目前最好结果。并与XLM(利用跨语言数据重新训练BERT的方法)进行了对比，实验表明该方法在取得与XLM相近结果的情况下，需要的计算资源更少，训练速度也更快。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>EMNLP</tag>
        <tag>Cross Lingual</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Neural-Keyphrase-Generation-via-Reinforcement-Learning-with-Adaptive-Rewards/</url>
    <content><![CDATA[<p><strong>Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</strong>. Hou Pong Chan, Wang Chen, Lu Wang, Irwin King. ACL 2019. <a href="https://arxiv.org/abs/1906.04106" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本篇论文主要解决目前keyphrase generation任务中存在的两个不足：</p><a id="more"></a>

<p>（1）模型生成的keyphrase比真实的keyphrase个数少</p>
<p>（2）已有评价标准依赖词的完成匹配（不完全匹配就算错，如真实keyphrase为SVM，模型生成的keyphrase为support vector machine也算错）</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>keyphrase根据是否在原文中是否出现分present和absent，这里将一个document的所有keyphrase拼接成一个序列，present在前absent在后，并通过利用seq2seq编码document来生成所有的keyphrase。<br><img src="https://img-blog.csdnimg.cn/20190620092517159.png" width="55%" height="55%"></p>
<p>针对第一个不足，作者使用了reinforcement learning，</p>
<p>sample policy：<br><img src="https://img-blog.csdnimg.cn/20190620092800754.png" alt></p>
<p>reward function: RF1<br><img src="https://img-blog.csdnimg.cn/20190620091404860.png" alt></p>
<p>N为目前已生成的keyphrase个数，G为真实keyphrase个数。作者认为当生成keyphrase的个数还少于真实keyphrase个数时，应该鼓励模型去生成更多的keyphrase，所以用recall作为reward；当个数足够时，除了要求个数也要要求正确性，所以用的F1。看到这里可能也有人会有疑问，为什么前面只重视个数而忽视正确性呢？为什么不改变个数和正确性的权重呢（可以认为是F1的变形）？在这里我个人认为作者可能是实验驱动，只用recall就有效果了；如果没有效果作者可能会去设计吧。。。</p>
<p>presen keyphrase 和 absent keyphrase分别计算reward:<br><img src="https://img-blog.csdnimg.cn/20190620093050519.png" width="55%" height="55%"></p>
<p>针对第二个不足，思路也很容易理解，就是找keyphrase的各种形式，作者这里主要有三个方法</p>
<p>（1）Acronyms in the ground-truths</p>
<p>（2）Wikipedia entity titles</p>
<p>（3）Wikipedia disambiguation pages</p>
<p>然后作者在四个baseline基础上分别验证了方法的有效性，并且对生成的keyphrase的个数、RF1进行了分析。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>ACL</tag>
        <tag>Keyphrase Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 教程</title>
    <url>/2019/10/31/Hexo-%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<ul>
<li><a href="https://hexo-guide.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">hexo指南</a></li>
</ul>]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>博客完善</title>
    <url>/2019/10/31/%E5%8D%9A%E5%AE%A2%E5%AE%8C%E5%96%84/</url>
    <content><![CDATA[<ul>
<li>hexo-admin 后台编辑功能完善</li>
<li><strong>多级分类</strong></li>
<li>博客阅读统计</li>
<li>标签云模块</li>
<li><strong>多级分类模块</strong></li>
<li>最新博客模块,热榜</li>
<li>评论模块</li>
<li>留言模块</li>
<li>hexo markdown 编辑语法</li>
<li>首页博客分栏</li>
<li>整理CSDN博客论文笔记 [完成]</li>
<li>google Adsense <a href>代码已经修改完成，需要等谷歌审核</a></li>
<li>部署到github page上</li>
<li>增加一个新栏目： 会议截稿</li>
<li>标签图案修改，并挪到开头位置</li>
<li><a href="https://hoxis.github.io/" target="_blank" rel="noopener">可借鉴博客</a></li>
<li>RSS订阅</li>
<li>置顶设置，Accepted Paper List</li>
<li>增加资源模块</li>
<li>arxiv内容每日自动发布</li>
</ul>]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Unsupervised%20Neural%20Single-Document%20Summarization%20of%20Reviews%20via/</url>
    <content><![CDATA[<p><strong>Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</strong>. Masaru Isonuma, Junichiro Mori, Ichiro Sakata. ACL 2019. <a href="https://arxiv.org/pdf/1906.05691.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文认为，评论（review）可以当作一个棵篇章树，树的根节点是其摘要，表示该评论的整体意思; 树的其他节点是对其父节点的细化。 也就是说这棵篇章树由摘要（根节点）与评论中所有句子（非根节点，每个非根节点代表一个句子）组成。于是本文通过学习构造这个隐式篇章树来建模得到评论摘要，并提出一种排序（rank）算法选择对生成摘要更加重要的句子。</p>
<p><img src="/images/strsum.png" alt></p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="模型整体方法"><a href="#模型整体方法" class="headerlink" title="模型整体方法"></a>模型整体方法</h2><p>（1）双向GRU+maxpooling 建模得到每个句子表示</p>
<p>（2）建模 父节点-子节点 对应关系权重（权重代表了树的关系）</p>
<p>（3）加权求和所有子节点表示，生成父节点（本文假设：子节点能够还原父节点，因为子节点包含了比父节点更多的信息。）</p>
<p>目标函数就是所有父节点生成概率最大。</p>
<p><img src="/images/strsum2.png" alt></p>
<h2 id="父节点-子节点-对应关系权重建模方法"><a href="#父节点-子节点-对应关系权重建模方法" class="headerlink" title="父节点-子节点 对应关系权重建模方法"></a>父节点-子节点 对应关系权重建模方法</h2><p>初始建模：边界概率（Marginal Probability of Dependency）</p>
<p><img src="/images/strsum3.png" alt></p>
<p>归一化（公式推导不是很懂）</p>
<p><img src="/images/strsum4.png" alt></p>
<p>调整：篇章排序（DiscourseRank）</p>
<p>受PageRank算法启发，更重要的句子有更多后代，迭代更新权重矩阵。<br><img src="/images/strsum5.png" alt></p>
<p><img src="/images/strsum6.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>ACL</tag>
        <tag>Discourse Structure</tag>
        <tag>Discourse Ranking</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/10/30/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hello</tag>
      </tags>
  </entry>
</search>

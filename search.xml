<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>python gtts 文本转语音</title>
    <url>/2020/01/19/python-gtts-%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3/</url>
    <content><![CDATA[<h1 id="安装gtts"><a href="#安装gtts" class="headerlink" title="安装gtts"></a>安装gtts</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install gTTS</span><br></pre></td></tr></table></figure><h1 id="文本转语音"><a href="#文本转语音" class="headerlink" title="文本转语音"></a>文本转语音</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gtts <span class="keyword">import</span> gTTS</span><br><span class="line">tts = gTTS(text=<span class="string">"Hello World"</span>, lang=<span class="string">'en'</span>)</span><br><span class="line">tts.save(<span class="string">"helloworld.mp3"</span>)</span><br></pre></td></tr></table></figure><h1 id="播放语音"><a href="#播放语音" class="headerlink" title="播放语音"></a>播放语音</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.system(<span class="string">"start helloworld.mp3"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>tts</tag>
      </tags>
  </entry>
  <entry>
    <title>python 调用谷歌翻译接口</title>
    <url>/2020/01/19/python-%E8%B0%83%E7%94%A8%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91%E6%8E%A5%E5%8F%A3/</url>
    <content><![CDATA[<p>googletrans 是一个封装了谷歌翻译接口的python代码库，可以通过googletrans实现免费、无限制调用谷歌翻译接口。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install googletrans</span><br></pre></td></tr></table></figure><h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trans=translator.translate(<span class="string">'Hello World'</span>, src=<span class="string">'en'</span>, dest=<span class="string">'zh-cn'</span>)</span><br><span class="line"><span class="comment"># 原文</span></span><br><span class="line">print(trans.origin)</span><br><span class="line"><span class="comment"># 译文</span></span><br><span class="line">print(trans.text)</span><br></pre></td></tr></table></figure><a id="more"></a>




<h1 id="语种识别"><a href="#语种识别" class="headerlink" title="语种识别"></a>语种识别</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">detection=translator.detect(<span class="string">'All with Love'</span>)</span><br><span class="line">print(detection.lang)</span><br></pre></td></tr></table></figure>

<h1 id="语种缩略表示"><a href="#语种缩略表示" class="headerlink" title="语种缩略表示"></a>语种缩略表示</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">LANGUAGES = &#123;</span><br><span class="line">    <span class="string">'af'</span>: <span class="string">'afrikaans'</span>,</span><br><span class="line">    <span class="string">'sq'</span>: <span class="string">'albanian'</span>,</span><br><span class="line">    <span class="string">'am'</span>: <span class="string">'amharic'</span>,</span><br><span class="line">    <span class="string">'ar'</span>: <span class="string">'arabic'</span>,</span><br><span class="line">    <span class="string">'hy'</span>: <span class="string">'armenian'</span>,</span><br><span class="line">    <span class="string">'az'</span>: <span class="string">'azerbaijani'</span>,</span><br><span class="line">    <span class="string">'eu'</span>: <span class="string">'basque'</span>,</span><br><span class="line">    <span class="string">'be'</span>: <span class="string">'belarusian'</span>,</span><br><span class="line">    <span class="string">'bn'</span>: <span class="string">'bengali'</span>,</span><br><span class="line">    <span class="string">'bs'</span>: <span class="string">'bosnian'</span>,</span><br><span class="line">    <span class="string">'bg'</span>: <span class="string">'bulgarian'</span>,</span><br><span class="line">    <span class="string">'ca'</span>: <span class="string">'catalan'</span>,</span><br><span class="line">    <span class="string">'ceb'</span>: <span class="string">'cebuano'</span>,</span><br><span class="line">    <span class="string">'ny'</span>: <span class="string">'chichewa'</span>,</span><br><span class="line">    <span class="string">'zh-cn'</span>: <span class="string">'chinese (simplified)'</span>,</span><br><span class="line">    <span class="string">'zh-tw'</span>: <span class="string">'chinese (traditional)'</span>,</span><br><span class="line">    <span class="string">'co'</span>: <span class="string">'corsican'</span>,</span><br><span class="line">    <span class="string">'hr'</span>: <span class="string">'croatian'</span>,</span><br><span class="line">    <span class="string">'cs'</span>: <span class="string">'czech'</span>,</span><br><span class="line">    <span class="string">'da'</span>: <span class="string">'danish'</span>,</span><br><span class="line">    <span class="string">'nl'</span>: <span class="string">'dutch'</span>,</span><br><span class="line">    <span class="string">'en'</span>: <span class="string">'english'</span>,</span><br><span class="line">    <span class="string">'eo'</span>: <span class="string">'esperanto'</span>,</span><br><span class="line">    <span class="string">'et'</span>: <span class="string">'estonian'</span>,</span><br><span class="line">    <span class="string">'tl'</span>: <span class="string">'filipino'</span>,</span><br><span class="line">    <span class="string">'fi'</span>: <span class="string">'finnish'</span>,</span><br><span class="line">    <span class="string">'fr'</span>: <span class="string">'french'</span>,</span><br><span class="line">    <span class="string">'fy'</span>: <span class="string">'frisian'</span>,</span><br><span class="line">    <span class="string">'gl'</span>: <span class="string">'galician'</span>,</span><br><span class="line">    <span class="string">'ka'</span>: <span class="string">'georgian'</span>,</span><br><span class="line">    <span class="string">'de'</span>: <span class="string">'german'</span>,</span><br><span class="line">    <span class="string">'el'</span>: <span class="string">'greek'</span>,</span><br><span class="line">    <span class="string">'gu'</span>: <span class="string">'gujarati'</span>,</span><br><span class="line">    <span class="string">'ht'</span>: <span class="string">'haitian creole'</span>,</span><br><span class="line">    <span class="string">'ha'</span>: <span class="string">'hausa'</span>,</span><br><span class="line">    <span class="string">'haw'</span>: <span class="string">'hawaiian'</span>,</span><br><span class="line">    <span class="string">'iw'</span>: <span class="string">'hebrew'</span>,</span><br><span class="line">    <span class="string">'hi'</span>: <span class="string">'hindi'</span>,</span><br><span class="line">    <span class="string">'hmn'</span>: <span class="string">'hmong'</span>,</span><br><span class="line">    <span class="string">'hu'</span>: <span class="string">'hungarian'</span>,</span><br><span class="line">    <span class="string">'is'</span>: <span class="string">'icelandic'</span>,</span><br><span class="line">    <span class="string">'ig'</span>: <span class="string">'igbo'</span>,</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'indonesian'</span>,</span><br><span class="line">    <span class="string">'ga'</span>: <span class="string">'irish'</span>,</span><br><span class="line">    <span class="string">'it'</span>: <span class="string">'italian'</span>,</span><br><span class="line">    <span class="string">'ja'</span>: <span class="string">'japanese'</span>,</span><br><span class="line">    <span class="string">'jw'</span>: <span class="string">'javanese'</span>,</span><br><span class="line">    <span class="string">'kn'</span>: <span class="string">'kannada'</span>,</span><br><span class="line">    <span class="string">'kk'</span>: <span class="string">'kazakh'</span>,</span><br><span class="line">    <span class="string">'km'</span>: <span class="string">'khmer'</span>,</span><br><span class="line">    <span class="string">'ko'</span>: <span class="string">'korean'</span>,</span><br><span class="line">    <span class="string">'ku'</span>: <span class="string">'kurdish (kurmanji)'</span>,</span><br><span class="line">    <span class="string">'ky'</span>: <span class="string">'kyrgyz'</span>,</span><br><span class="line">    <span class="string">'lo'</span>: <span class="string">'lao'</span>,</span><br><span class="line">    <span class="string">'la'</span>: <span class="string">'latin'</span>,</span><br><span class="line">    <span class="string">'lv'</span>: <span class="string">'latvian'</span>,</span><br><span class="line">    <span class="string">'lt'</span>: <span class="string">'lithuanian'</span>,</span><br><span class="line">    <span class="string">'lb'</span>: <span class="string">'luxembourgish'</span>,</span><br><span class="line">    <span class="string">'mk'</span>: <span class="string">'macedonian'</span>,</span><br><span class="line">    <span class="string">'mg'</span>: <span class="string">'malagasy'</span>,</span><br><span class="line">    <span class="string">'ms'</span>: <span class="string">'malay'</span>,</span><br><span class="line">    <span class="string">'ml'</span>: <span class="string">'malayalam'</span>,</span><br><span class="line">    <span class="string">'mt'</span>: <span class="string">'maltese'</span>,</span><br><span class="line">    <span class="string">'mi'</span>: <span class="string">'maori'</span>,</span><br><span class="line">    <span class="string">'mr'</span>: <span class="string">'marathi'</span>,</span><br><span class="line">    <span class="string">'mn'</span>: <span class="string">'mongolian'</span>,</span><br><span class="line">    <span class="string">'my'</span>: <span class="string">'myanmar (burmese)'</span>,</span><br><span class="line">    <span class="string">'ne'</span>: <span class="string">'nepali'</span>,</span><br><span class="line">    <span class="string">'no'</span>: <span class="string">'norwegian'</span>,</span><br><span class="line">    <span class="string">'ps'</span>: <span class="string">'pashto'</span>,</span><br><span class="line">    <span class="string">'fa'</span>: <span class="string">'persian'</span>,</span><br><span class="line">    <span class="string">'pl'</span>: <span class="string">'polish'</span>,</span><br><span class="line">    <span class="string">'pt'</span>: <span class="string">'portuguese'</span>,</span><br><span class="line">    <span class="string">'pa'</span>: <span class="string">'punjabi'</span>,</span><br><span class="line">    <span class="string">'ro'</span>: <span class="string">'romanian'</span>,</span><br><span class="line">    <span class="string">'ru'</span>: <span class="string">'russian'</span>,</span><br><span class="line">    <span class="string">'sm'</span>: <span class="string">'samoan'</span>,</span><br><span class="line">    <span class="string">'gd'</span>: <span class="string">'scots gaelic'</span>,</span><br><span class="line">    <span class="string">'sr'</span>: <span class="string">'serbian'</span>,</span><br><span class="line">    <span class="string">'st'</span>: <span class="string">'sesotho'</span>,</span><br><span class="line">    <span class="string">'sn'</span>: <span class="string">'shona'</span>,</span><br><span class="line">    <span class="string">'sd'</span>: <span class="string">'sindhi'</span>,</span><br><span class="line">    <span class="string">'si'</span>: <span class="string">'sinhala'</span>,</span><br><span class="line">    <span class="string">'sk'</span>: <span class="string">'slovak'</span>,</span><br><span class="line">    <span class="string">'sl'</span>: <span class="string">'slovenian'</span>,</span><br><span class="line">    <span class="string">'so'</span>: <span class="string">'somali'</span>,</span><br><span class="line">    <span class="string">'es'</span>: <span class="string">'spanish'</span>,</span><br><span class="line">    <span class="string">'su'</span>: <span class="string">'sundanese'</span>,</span><br><span class="line">    <span class="string">'sw'</span>: <span class="string">'swahili'</span>,</span><br><span class="line">    <span class="string">'sv'</span>: <span class="string">'swedish'</span>,</span><br><span class="line">    <span class="string">'tg'</span>: <span class="string">'tajik'</span>,</span><br><span class="line">    <span class="string">'ta'</span>: <span class="string">'tamil'</span>,</span><br><span class="line">    <span class="string">'te'</span>: <span class="string">'telugu'</span>,</span><br><span class="line">    <span class="string">'th'</span>: <span class="string">'thai'</span>,</span><br><span class="line">    <span class="string">'tr'</span>: <span class="string">'turkish'</span>,</span><br><span class="line">    <span class="string">'uk'</span>: <span class="string">'ukrainian'</span>,</span><br><span class="line">    <span class="string">'ur'</span>: <span class="string">'urdu'</span>,</span><br><span class="line">    <span class="string">'uz'</span>: <span class="string">'uzbek'</span>,</span><br><span class="line">    <span class="string">'vi'</span>: <span class="string">'vietnamese'</span>,</span><br><span class="line">    <span class="string">'cy'</span>: <span class="string">'welsh'</span>,</span><br><span class="line">    <span class="string">'xh'</span>: <span class="string">'xhosa'</span>,</span><br><span class="line">    <span class="string">'yi'</span>: <span class="string">'yiddish'</span>,</span><br><span class="line">    <span class="string">'yo'</span>: <span class="string">'yoruba'</span>,</span><br><span class="line">    <span class="string">'zu'</span>: <span class="string">'zulu'</span>,</span><br><span class="line">    <span class="string">'fil'</span>: <span class="string">'Filipino'</span>,</span><br><span class="line">    <span class="string">'he'</span>: <span class="string">'Hebrew'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>官方手册: <a href="https://py-googletrans.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://py-googletrans.readthedocs.io/en/latest/</a></p>
]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>谷歌翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>一些公司及高校在线翻译系统</title>
    <url>/2020/01/17/%E4%B8%80%E4%BA%9B%E5%85%AC%E5%8F%B8%E5%8F%8A%E9%AB%98%E6%A0%A1%E5%9C%A8%E7%BA%BF%E7%BF%BB%E8%AF%91%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h1 id="公司在线翻译系统"><a href="#公司在线翻译系统" class="headerlink" title="公司在线翻译系统"></a>公司在线翻译系统</h1><ul>
<li><a href="https://fanyi.baidu.com/" target="_blank" rel="noopener">百度翻译</a></li>
<li><a href="https://translate.google.cn/" target="_blank" rel="noopener">谷歌翻译</a></li>
<li><a href="http://fanyi.youdao.com/" target="_blank" rel="noopener">有道翻译</a></li>
<li><a href="https://fanyi.qq.com/" target="_blank" rel="noopener">腾讯翻译君</a></li>
<li><a href="https://fanyi.sogou.com/" target="_blank" rel="noopener">搜狗翻译</a></li>
<li><a href="https://niutrans.vip/trans" target="_blank" rel="noopener">小牛翻译</a></li>
<li><a href="https://cloudtranslation.com/online/" target="_blank" rel="noopener">云译</a></li>
</ul><h1 id="高校在线翻译系统"><a href="#高校在线翻译系统" class="headerlink" title="高校在线翻译系统"></a>高校在线翻译系统</h1><ul>
<li><a href="http://nmt.xmu.edu.cn/" target="_blank" rel="noopener">厦门大学</a></li>
</ul>]]></content>
  </entry>
  <entry>
    <title>【论文笔记】Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context</title>
    <url>/2020/01/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Task-Oriented-Dialog-Systems-that-Consider-Multiple-Appropriate-Responses-under-the-Same-Context/</url>
    <content><![CDATA[<p><strong>Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context</strong>. Yichi Zhang, Zhijian Ou, Zhou Yu. <a href="https://arxiv.org/abs/1911.10484" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p><img src="/images/DADL1.jpg" alt></p><p><img src="/images/DADL2.jpg" alt></p><p>在对话中，对于同一句话，可以有多种回复。但是，现有模型往往趋于生成出现概率最高的回复，而忽视了概率较低的回复。本文通过数据增强的方法，使得模型具备生成多样化回复的能力。</p><a id="more"></a>



<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p><img src="/images/DADL4.jpg" alt><br>在数据预处理阶段，在整个数据集中，找出所有的dialogue state相同的system actions，作为ground truth的补充增强。</p>
<h2 id="整体方法"><a href="#整体方法" class="headerlink" title="整体方法"></a>整体方法</h2><p><img src="/images/DADL3.jpg" alt><br>训练过程中，所有可能的回复概率都要最大，而不只需要ground truth概率最大。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>1 encoder + 3 decoder<br><img src="/images/DADL5.jpg" alt></p>
<p>作者认为通过这样训练，模型就具备了生成多样性回复的能力，在测试的时候可以通过multi beam search、top-k等方式生成多样性回复。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者本次实验主要在数据集MultiWoZ进行。<br><img src="/images/DADL7.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Dialog System</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Integrating Relation Constraints with Neural Relation Extractors</title>
    <url>/2020/01/08/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Integrating-Relation-Constraints-with-Neural-Relation-Extractors/</url>
    <content><![CDATA[<p><strong>Integrating Relation Constraints with Neural Relation Extractors</strong>. Yuan Ye, Yansong Feng, Bingfeng Luo, Yuxuan Lai, Dongyan Zhao. AAAI 2020. <a href="https://arxiv.org/abs/1911.11493" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>在关系抽取任务中，<strong>某个关系的所有subject或者object属于同一种类型</strong>（如：在“母校”的所有subject都属于“人”），或者<strong>多个关系之间往往存在依赖关系</strong>（如“城市”和“地区”的subject都是地名），但是现有模型都没有考虑这个约束，只是单独考虑每一个关系。本文工作利用这种约束以提升关系抽取任务的效果。</p><a id="more"></a>

<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>本文从Coherent和Semantic两个角度出发，提出两种方法。</p>
<h2 id="Coherent"><a href="#Coherent" class="headerlink" title="Coherent"></a>Coherent</h2><p>一致性：满足约束的两个关系，预测概率要同时高。</p>
<p><img src="/images/IER1.jpg" alt></p>
<p>矩阵v表示关系约束C,如果关系i和关系j满足约束，则v_ij=1。</p>
<h2 id="Semantic"><a href="#Semantic" class="headerlink" title="Semantic"></a>Semantic</h2><p>语义性：符合约束中某个规则的两个实例，至少有一个实例满足规则中的某个关系。</p>
<p><img src="/images/IER2.jpg" alt></p>
<p>矩阵u表示约束C,如果关系j和关系k满足约束i，则v_ij=1,v_ik=1.</p>
<p>最后loss由两部分构成，Lo为原始的loss，Lc为约束loss。<br><img src="/images/IER3.jpg" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在ACNN和APCNN两个模型上进行验证，均获得了提升。<br><img src="/images/IER4.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Neural Relation Extraction</tag>
        <tag>Relation Constraints</tag>
      </tags>
  </entry>
  <entry>
    <title>智源社区2019年大会PPT分享 </title>
    <url>/2020/01/08/%E6%99%BA%E6%BA%90%E7%A4%BE%E5%8C%BA2019%E5%B9%B4%E5%A4%A7%E4%BC%9APPT%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p>获取方式 <a href="https://mp.weixin.qq.com/s/zqqQVwr16EhqA2zxYUQSWQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/zqqQVwr16EhqA2zxYUQSWQ</a></p>
<a id="more"></a>

<p><img src="/images/zhiyuan2019-1.jpg" alt=""></p>
<p><img src="/images/zhiyuan2019-2.jpg" alt=""></p>
]]></content>
  </entry>
  <entry>
    <title>【shell】批量删除除了某个文件外的其他所有文件</title>
    <url>/2020/01/08/%E3%80%90shell%E3%80%91%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E9%99%A4%E4%BA%86%E6%9F%90%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%96%E7%9A%84%E5%85%B6%E4%BB%96%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -f !(no_delete_file1|no_delete_file2)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls |grep -v no_delete_file |xargs rm -f</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>批量删除</tag>
      </tags>
  </entry>
  <entry>
    <title>AAAI2020 预讲会</title>
    <url>/2019/12/22/AAAI2020-%E9%A2%84%E8%AE%B2%E4%BC%9A/</url>
    <content><![CDATA[<p>AAAI2020 预讲会翻译对话与文本生成、文本分析与内容挖掘两个Session比较有意思的论文。</p><ul>
<li><p>Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation <a href="https://arxiv.org/abs/1911.09320" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N2-%E9%82%B5%E6%99%A8%E6%B3%BD-%E4%B8%AD%E7%A7%91%E9%99%A2%E8%AE%A1%E7%AE%97%E6%89%80.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Modeling Fluency and Faithful ness for Diverse Neural Machine Translation <a href="https://arxiv.org/abs/1912.00178" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N1-%E8%B0%A2%E5%A9%89%E8%8E%B9-%E4%B8%AD%E7%A7%91%E9%99%A2%E8%AE%A1%E7%AE%97%E6%89%80.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Task-Oriented Dialog Systems that Consider Multiple Appropriate Response under the Same Context <a href="https://arxiv.org/abs/1911.10484" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N3-%E5%BC%A0%E4%BA%A6%E5%BC%9B-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Neural Machine Translation with Joint Representation <a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N8-%E6%9D%8E%E7%82%8E%E6%B4%8B-%E4%B8%9C%E5%8C%97%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Multi-Scale Self-Attention for Text Classification <a href="https://arxiv.org/abs/1912.00544" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S2N1-%E9%83%AD%E7%90%A6%E9%B9%8F-%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Intergrating Relation Constraints with Neural Relation Extractors <a href="https://arxiv.org/abs/1911.11493" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S2N5-%E5%8F%B6%E5%85%83-%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Cross-Lingual Natural Language Generation via Pre-Training <a href="https://arxiv.org/abs/1909.10481" target="_blank" rel="noopener">[PDF]</a></p>
</li>
</ul>]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
      <tags>
        <tag>AAAI</tag>
      </tags>
  </entry>
  <entry>
    <title>国内一些NLP实验室及老师主页</title>
    <url>/2019/12/21/%E5%9B%BD%E5%86%85%E4%B8%80%E4%BA%9BNLP%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%8F%8A%E8%80%81%E5%B8%88%E4%B8%BB%E9%A1%B5/</url>
    <content><![CDATA[<p>以下列表只是我个人整理，随意排序，欢迎大家补充与指正。</p>
<ul>
<li><p><a href="http://nlp.csai.tsinghua.edu.cn/site2/" target="_blank" rel="noopener">清华大学自然语言处理与社会人文计算实验室</a></p>
<p><a href="http://www.cs.tsinghua.edu.cn/publish/cs/4616/2013/20130424103737386785027/20130424103737386785027_.html" target="_blank" rel="noopener">孙茂松</a> <a href="http://nlp.csai.tsinghua.edu.cn/~ly/index_cn.html" target="_blank" rel="noopener">刘洋</a> <a href="http://nlp.csai.tsinghua.edu.cn/~lzy/" target="_blank" rel="noopener">刘知远</a></p>
</li>
<li><p><a href="http://www.wict.pku.edu.cn/" target="_blank" rel="noopener">北京大学王选计算机研究所</a></p>
<p><a href="http://www.icst.pku.edu.cn/xztd/xztd_01/1222625.htm" target="_blank" rel="noopener">万小军</a> <a href="http://www.icst.pku.edu.cn/xztd/xztd_01/1222614.htm" target="_blank" rel="noopener">严睿</a> <a href="http://www.wict.pku.edu.cn/zhaodongyan/" target="_blank" rel="noopener">赵东岩</a> <a href="https://sites.google.com/site/ysfeng/home" target="_blank" rel="noopener">冯岩松</a> <a href="https://www.cs.uic.edu/~liub/" target="_blank" rel="noopener">刘兵</a></p>
</li>
<li><p><a href="http://www.cs.tsinghua.edu.cn/publish/cs/index.html" target="_blank" rel="noopener">清华大学计算机科学与技术系</a></p>
<p><a href="http://coai.cs.tsinghua.edu.cn/hml/" target="_blank" rel="noopener">黄民烈</a></p>
</li>
</ul>
<a id="more"></a>

<ul>
<li><p><a href="http://www.icip.org.cn/zh/homepage/" target="_blank" rel="noopener">中国科学院软件研究所中文信息处理实验室</a></p>
<p><a href="http://www.icip.org.cn/team/sunle/" target="_blank" rel="noopener">孙乐</a> <a href="http://www.icip.org.cn/team/hanxianpei/" target="_blank" rel="noopener">韩先培</a></p>
</li>
<li><p><a href="http://iip.ict.ac.cn/" target="_blank" rel="noopener">中国科学院计算技术研究所智能信息处理重点实验室</a></p>
<p><a href="http://sourcedb.ict.cas.cn/cn/jssrck/201709/t20170910_4857722.html" target="_blank" rel="noopener">冯洋</a></p>
</li>
<li><p><a href="http://www.nlplab.com/niuplan/niutrans.ch.html" target="_blank" rel="noopener">东北大学自然语言处理实验室</a></p>
<p><a href="http://www.nlplab.com/members/zhujingbo.html" target="_blank" rel="noopener">朱靖波</a> <a href="http://www.nlplab.com/members/xiaotong.html" target="_blank" rel="noopener">肖桐</a></p>
</li>
<li><p><a href="http://www.cs.fudan.edu.cn/" target="_blank" rel="noopener">复旦大学计算机科学技术学院</a></p>
<p><a href="https://xpqiu.github.io/" target="_blank" rel="noopener">邱锡鹏</a></p>
</li>
<li><p><a href="http://cs.tju.edu.cn/csweb/" target="_blank" rel="noopener">天津大学计算机科学与技术学院</a></p>
<p><a href="https://zhangmeishan.github.io/chn.html" target="_blank" rel="noopener">张梅山</a> <a href="http://cs.tju.edu.cn/csweb/admin_teacher/view?id=232" target="_blank" rel="noopener">熊德意</a></p>
</li>
<li><p><a href="http://nlp.xmu.edu.cn/index.html" target="_blank" rel="noopener">厦门大学自然语言处理实验室</a></p>
<p><a href="http://121.192.180.171:8080/" target="_blank" rel="noopener">史晓东</a></p>
</li>
<li><p><a href="https://cdmc.xmu.edu.cn/index.htm" target="_blank" rel="noopener">厦门大学数字媒体计算研究中心</a></p>
<p><a href="https://cdmc.xmu.edu.cn/info/1010/1054.htm" target="_blank" rel="noopener">苏劲松</a></p>
</li>
<li><p><a href="http://bcmi.sjtu.edu.cn/index.cn.html" target="_blank" rel="noopener">上海交通大学BCMI实验室</a></p>
<p><a href="http://bcmi.sjtu.edu.cn/~zhaohai/" target="_blank" rel="noopener">赵海</a></p>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>写作助手</title>
    <url>/2019/12/21/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/</url>
    <content><![CDATA[<ul>
<li><p><a href="https://www.overleaf.com/" target="_blank" rel="noopener">Overleaf</a><br>在线编辑Latex。</p>
</li>
<li><p><a href="https://www.grammarly.com/" target="_blank" rel="noopener">Grammarly</a><br>自动检测语法。</p>
</li>
<li><p><a href="http://www.esoda.org/" target="_blank" rel="noopener">易搜搭</a><br>词语搭配。</p>
</li>
</ul>
<a id="more"></a>]]></content>
      <tags>
        <tag>写作助手</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Improved Document Modelling with a Neural Discourse Parser</title>
    <url>/2019/12/21/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Improved-Document-Modelling-with-a-Neural-Discourse-Parser/</url>
    <content><![CDATA[<p><strong>Improved Document Modelling with a Neural Discourse Parser</strong>.Fajri Koto, Jey Han Lau, Timothy Baldwin. ArXiv 1911.<a href="https://arxiv.org/abs/1911.06919" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>使用篇章结构信息提高篇章建模。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>文章的关键有两点，篇章结构是什么？如何利用篇章结构？</p><a id="more"></a>


<h2 id="篇章结构是什么？"><a href="#篇章结构是什么？" class="headerlink" title="篇章结构是什么？"></a>篇章结构是什么？</h2><p><img src="/images/DMDP1.jpg" alt></p>
<p>本篇文章的篇章结构由RST分析得到，首先将篇章切分成EDU，然后再EDU基础上建立篇章分析树，树上的叶子结点为EDU，非叶子结点为其两个子节点的篇章关系，树上的边为对应子节点在该关系中的重要性。（具体可以去了解一下RST官网介绍和相关论文）</p>
<h2 id="如何利用篇章结构？"><a href="#如何利用篇章结构？" class="headerlink" title="如何利用篇章结构？"></a>如何利用篇章结构？</h2><p>如何利用篇章结构，首先是如何编码篇章结构，也就是如何抽取篇章分析树的特征。针对每个树根节点到每个叶子结点的路径，作者设计两类特征：Shallow Discourse Features 和 Latent Discourse Features。</p>
<h3 id="Shallow-Discourse-Features"><a href="#Shallow-Discourse-Features" class="headerlink" title="Shallow Discourse Features"></a>Shallow Discourse Features</h3><ul>
<li>叶子结点重要性分数</li>
</ul>
<p><img src="/images/DMDP2.jpg" alt></p>
<p>统计路径上Nucleus的比例，h(root)为根节点高度。</p>
<ul>
<li>关系重要性分数</li>
</ul>
<p><img src="/images/DMDP3.jpg" alt></p>
<p>统计路径上每个关系的加权比例，h(x)为节点x的高度。</p>
<ul>
<li><p>结点类别</p>
<p>  Nucleus or satellite</p>
<ul>
<li>兄弟结点</li>
</ul>
<h3 id="Latent-Discourse-Features"><a href="#Latent-Discourse-Features" class="headerlink" title="Latent Discourse Features"></a>Latent Discourse Features</h3></li>
</ul>
<p><img src="/images/DMDP6.jpg" alt></p>
<p>使用两个Bi-LSTM分别编码词序列和句法特征序列，avg-pool，然后拼接。</p>
<p><img src="/images/DMDP4.jpg" alt></p>
<p>拼接后的序列再过一个Bi-LSTM得到最终特征表示。</p>
<p><img src="/images/DMDP5.jpg" alt></p>
<h3 id="如何利用篇章特征"><a href="#如何利用篇章特征" class="headerlink" title="如何利用篇章特征"></a>如何利用篇章特征</h3><p> 得到两类特征后，要如何利用呢？本文提出了三种方法。</p>
<ul>
<li><p>拼接word embedding</p>
<p><img src="/images/DMDP7.jpg" alt></p>
</li>
<li><p>加一层Bi-LSTM</p>
<p><img src="/images/DMDP8.jpg" alt></p>
</li>
<li><p>作为解码attention的一个额外输入</p>
<p><img src="/images/DMDP9.jpg" alt></p>
</li>
</ul>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="Document-Summarizatoin"><a href="#Document-Summarizatoin" class="headerlink" title="Document Summarizatoin"></a>Document Summarizatoin</h2><p>  <img src="/images/DMDP10.jpg" alt></p>
<p>  第一种和第二种方法较好。</p>
<h2 id="Petition-Popularity-Prediction"><a href="#Petition-Popularity-Prediction" class="headerlink" title="Petition Popularity Prediction"></a>Petition Popularity Prediction</h2><p>  <img src="/images/DMDP11.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Summarization</tag>
        <tag>Discourse</tag>
        <tag>RST</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】linux批量杀死进程</title>
    <url>/2019/12/21/%E3%80%90shell%E3%80%91linux%E6%89%B9%E9%87%8F%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<p>批量杀死包含关键字“keyword1”但不包含“keyword2”的进程。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef|grep keyword1|grep -v keyword2|cut -c 9-15|xargs kill -9</span><br></pre></td></tr></table></figure><ul>
<li><strong>“ps -ef”</strong> ——查看所有进程</li>
<li><strong>“grep keyword1”</strong> ——列出所有含有关键字”keyword1”的进程</li>
<li><strong>“grep -v keyword2”</strong> ——在列出的进程中去除含有关键字”keyword2”的进程</li>
<li><strong>“cut -c 9-15″</strong> ——截取输入行的第9个字符到第15个字符，而这正好是进程号PID</li>
<li><strong>“xargs kill -9″</strong> ——xargs 命令是用来把前面命令的输出结果(PID)作为”kill -9″命令的参数，并执行该命令。”kill -9″会强行杀掉指定进程。</li>
</ul>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>批量</tag>
        <tag>杀死进程</tag>
      </tags>
  </entry>
  <entry>
    <title>好的研究想法从哪里来</title>
    <url>/2019/12/03/%E5%A5%BD%E7%9A%84%E7%A0%94%E7%A9%B6%E6%83%B3%E6%B3%95%E4%BB%8E%E5%93%AA%E9%87%8C%E6%9D%A5/</url>
    <content><![CDATA[<p>转载自：知乎专栏 NLP日知录 <a href="https://zhuanlan.zhihu.com/p/93765082" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/93765082</a></p><p>作者：刘知远 </p><p>背景说明：临近ACL 2020投稿截止时间，跟同学密集讨论，争论哪些研究想法适合投到ACL有机会命中。从自己十多年研究经历来看，如何判断一个研究想法好不好，以及这些研究想法从哪里来，对于初学者而言的确是个难题。所以，简单攒了这篇小短文，分享一些经验和想法，希望对刚进入NLP领域的新同学有用。多有舛误请指正。</p><a id="more"></a>


<p>王家卫的电影《一代宗师》中有段经典的比武桥段，宫会长对叶问说“今天我们不比武术，比想法”。其实，好的点子或者想法（idea），也是一篇优秀研究成果的灵魂。而计算机领域流行着一句话“IDEA is cheap, show me the code”，也说明对于重视实践的计算机学科而言，想法的好坏还取决于它的实际效能。这里就来谈下好的研究想法从哪里来。</p>
<h1 id="什么算是好的想法"><a href="#什么算是好的想法" class="headerlink" title="什么算是好的想法"></a>什么算是好的想法</h1><p>2015年，我在微博上写过一个调侃的小段子：</p>
<blockquote>
<p>ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。</p>
</blockquote>
<p>到了2018年，我又续了一小段：</p>
<blockquote>
<p>不期数年，北方DL神教异军突起，内修表示学习，外练神经网络，心法众多，曰门，曰注意，曰记忆，曰对抗，曰增强。经ImageNet一役威震武林，豢Alpha犬一匹无人可近。一时家家筑丹炉，人人炼丹忙，门徒云集，依附者众，有一统江湖之势。有童谣为证：左手大数据，右手英伟达，每逢顶会炼丹忙。</p>
</blockquote>
<p>这里面提到的图模型加圈、神经网络加层、优化目标加正则，神经网络中的门、注意、记忆等，都是一些改进模型性能的创新思路，被各大NLP任务广泛使用并发表论文，也许就是因为被不同NLP任务的重复使用和发表，多少有些审美疲劳而缺少更深的创新思想，被有些网友和学者诟为“灌水”，似乎都不算好的想法。</p>
<p>那么什么才是好的想法呢？我理解这个”好“字，至少有两个层面的意义。</p>
<h2 id="学科发展角度的“好”"><a href="#学科发展角度的“好”" class="headerlink" title="学科发展角度的“好”"></a>学科发展角度的“好”</h2><p>学术研究本质是对未知领域的探索，是对开放问题的答案的追寻。所以从推动学科发展的角度，评判什么是好的研究想法的标准，首先就在一个<strong>“新”</strong>字。</p>
<p>过去有个说法，人工智能学科有个魔咒，凡是人工智能被解决（或者有解决方案）的部分，就不再被认为代表“人类智能”。计算机视觉、自然语言处理、机器学习、机器人之所以还被列为人工智能主要方向，也许正是因为它们尚未被解决，尚能代表“人类智能”的尊严。而我们要开展创新研究，就是要提出新的想法解决这些问题。这其中的”新“字，可以体现在提出新的问题和任务，探索新的解决思路，提出新的算法技术，实现新的工具系统等。</p>
<p>在保证”新“的基础上，研究想法好不好，那就看它<strong>对推动学科发展的助力有多大</strong>。深度学习之所以拥有如此显赫的影响力，就在于它对于人工智能自然语言处理、语音识别、计算机视觉等各重要方向都产生了革命性的影响，彻底改变了对无结构信号（语音、图像、文本）的语义表示的技术路线。</p>
<h2 id="研究实践角度的”好“"><a href="#研究实践角度的”好“" class="headerlink" title="研究实践角度的”好“"></a>研究实践角度的”好“</h2><p>那是不是想法只要够”新“就好呢？是不是越新越好呢？我认为应该还不是。因为，只有<strong>能做得出来的想法</strong>才有资格被分析好不好。所以，从研究实践角度，还需要考虑研究想法的<strong>可实现性</strong>和<strong>可验证性</strong>。</p>
<p>可实现性，体现在该想法是否有足够的数学或机器学习工具支持实现。可验证性，体现在该想法是否有合适的数据集合和广泛接受的评价标准。很多民间科学家的想法之所以得不到学术界的认同，就是因为这些想法往往缺乏可实现性和可验证性，只停留在天马行空的纸面，只是些虚无缥缈的理念。</p>
<h1 id="好的研究想法从哪里来"><a href="#好的研究想法从哪里来" class="headerlink" title="好的研究想法从哪里来"></a>好的研究想法从哪里来</h1><p>想法好还是不好，并不是非黑即白的二分问题，而是像光谱一样呈连续分布，因时而异，因人而宜。计算机科技领域的发展既有积累的过程，也有跃迁的奇点，积累量变才会产生质变，吃第三个馒头饱了，也是因为前面两个馒头打底。</p>
<p>现在的学术研究已经成为高度专业化的职业，有庞大的研究者群体。”Publish or Perish“，是从事学术职业（如教授、研究员、研究生）的人必须做好平衡的事情，不能要求研究者的每份工作都是“诺贝尔奖”或“图灵奖”级的才值得发表。只要对研究领域的发展有所助力，就值得发表出来，帮助同行前进。鲁迅说：天才并不是自生自长在深林荒野里的怪物，是由可以使天才生长的民众产生，长育出来的，所以没有这种民众，就没有天才。这个庞大研究者群体正是天才成长的群众基础。同时，学术新人也是在开展创新研究训练中，不断磨砺寻找好想法能力，鲁迅也说：即使天才，在生下来的时候的第一声啼哭，也和平常的儿童的一样，决不会就是一首好诗。</p>
<p>那么，好的研究想法从哪里来呢？我总结，首先要有区分研究想法好与不好的能力，这需要<strong>深入全面了解所在研究方向的历史与现状</strong>，具体就是对学科文献的全面掌握。人是最善于学习的动物，完全可以将既有文献中不同时期研究工作的想法作为学习对象，通过了解它们提出后对学科发展的影响——具体体现在论文引用、学术评价情况等各方面——建立对研究想法好与不好的评价模型。我们很难条分缕析完美地列出区分好与不好想法的所有特征向量，但人脑强大的学习能力，只要给予足够的输入数据，就可以在神经网络中自动学习建立判别的模型，鉴古知今，见微知著，这也许就是常说的学术洞察力。</p>
<p>做过一些研究的同学会有感受，仅阅读自己研究方向的文献，新想法还是不会特别多。这是因为，读到的都是该研究问题已经完成时的想法，它们本身无法启发新的想法。如何产生新的想法呢？我总结有三种可行的基本途径：</p>
<p><strong>实践法</strong>。即在研究任务上实现已有最好的算法，通过分析实验结果，例如发现这些算法计算复杂度特别高、训练收敛特别慢，或者发现该算法的错误样例呈现明显的规律，都可以启发你改进已有算法的思路。现在很多自然语言处理任务的Leaderboard上的最新算法，就是通过分析错误样例来有针对性改进算法的 [1]。</p>
<p><strong>类比法</strong>。即将研究问题与其他任务建立类比联系，调研其他相似任务上最新的有效思想、算法或工具，通过合理的转换迁移，运用到当前的研究问题上来。例如，当初注意力机制在神经网络机器翻译中大获成功，当时主要是在词级别建立注意力，后来我们课题组的林衍凯和沈世奇提出建立句子级别的注意力解决关系抽取的远程监督训练数据的标注噪音问题 [2]，这就是一种类比的做法。</p>
<p><strong>组合法</strong>。即将新的研究问题分解为若干已被较好解决的子问题，通过有机地组合这些子问题上的最好做法，建立对新的研究问题的解决方案。例如，我们提出的融合知识图谱的预训练语言模型，就是将BERT和TransE等已有算法融合起来建立的新模型 [3]。</p>
<p>正如武侠中的最高境界是无招胜有招，好的研究想法并不拘泥于以上的路径，很多时候是在研究者对研究问题深刻认知的基础上，综合丰富的研究阅历和聪明才智产生”顿悟“的结果。这对初学者而言恐怕还很难一窥门径，需要从基本功做起，经过大量科研实践训练后，才能有登堂入室之感。</p>
<p>在科研实践过程中，除了通过大量文献阅读了解历史，通过深入思考总结产生洞察力外，还有一项必不可少的工作，那就是主动开放的学术交流和合作意识。不同研究领域思想和成果交流碰撞，既为创新思想提供了新的来源，也为”类比“和”顿悟“提供了机会。了解一下历史就可以知晓，人工智能的提出，就是数学、计算机科学、控制论、信息论、脑科学等学科交叉融合的产物。而当红的深度学习的起源，1980年代的Parallel Distributed Processing （PDP），也是计算机科学、脑认知科学、心理学、生物学等领域研究者通力合作的产物。下面是1986年出版的名著《Parallel Distributed Processing: Explorations in the Microstructure of Cognition》第一卷的封面。</p>
<p><img src="https://pic2.zhimg.com/80/v2-a8d3f6e553f9f279cdafea5a3e218701_hd.jpg" alt></p>
<p>作者在前言中是这么讲他们的合作过程的，在最初长达六个月的时间里，它们每周见面交流两次讨论研究进展。</p>
<blockquote>
<p>We expected the project to take about six months. We began in January 1982 by bringing a number of our colleagues together to form a discussion group on these topics. During the first six months we met twice weekly and laid the foundation for most of the work presented in these volumes.</p>
</blockquote>
<p>而书中提供的PDP研究组的成员名单，40年后的今天仍让我惊叹其高度的跨机构、跨学科的交叉特点。所以，特别建议同学们在科研训练中，在专注研究问题的前提下，保持主动的学术交流意识，无论是听讲座报告，参加学术会议，还是选修课程，都有意识地扩宽学术交流的广度，不仅与小同行打成一片，更有看似八竿子打不着的研究领域的学术伙伴。随着研究经历的丰富，会越来越强烈地感受到，越是大跨度交叉的学术报告，越让你受到更大的启发，产生更多让自己兴奋的研究想法。</p>
<p><img src="https://pic2.zhimg.com/80/v2-404a752001300a69baabd40fb3d78b99_hd.jpg" alt></p>
<h1 id="初学者应该怎么做"><a href="#初学者应该怎么做" class="headerlink" title="初学者应该怎么做"></a>初学者应该怎么做</h1><p>与阅读论文、撰写论文、设计实验等环节相比，如何产生好的研究想法，是一个不太有章可循的环节，很难总结出固定的范式可供遵循。像小马过河，需要通过大量训练实践，来积累自己的研究经验。不过，对于初学者而言，仍然有几个简单可行的原则可以参考。</p>
<p><strong>一篇论文的可发表价值，取决于它与已有最直接相关工作间的Delta</strong>。我们大部分研究工作都是站在前人工作的基础上推进的。牛顿说：如果说我看得比别人更远些，那是因为我站在巨人的肩膀上。在我看来，评判一篇论文研究想法的价值，就是看它站在了哪个或哪些巨人的肩膀上，以及在此基础上又向上走了多远。反过来，在准备开始一项研究工作之前，在形成研究想法的时候，也许要首先明确准备站在哪个巨人的肩膀上，以及计划通过什么方式走得更远。与已有最直接相关工作之间的Delta，决定了这个研究想法的价值有多大。</p>
<p><strong>兼顾摘果子和啃骨头</strong>。人们一般把比较容易想到的研究想法，叫做Low Hanging Fruit（低垂果实）。低垂果实容易摘，但同时摘的人也多，选择摘果子就容易受到想法撞车的困扰。例如，2018年以BERT为首的预训练语言模型取得重大突破，2019年中就出现大量改进工作，其中以跨模态预训练模型为例，短短几个月里<a href="http://arxiv.org上挂出了超过六个来自不同团队的图像与文本融合的预训练模型" target="_blank" rel="noopener">http://arxiv.org上挂出了超过六个来自不同团队的图像与文本融合的预训练模型</a> [4]。设身处地去想，进行跨模态预训练模型研究，就是一个比较容易想到的方向，你一定需要有预判能力，知道世界上肯定会有很多团队也同时开展这方面研究，这时你如果选择入场，就一定要做得更深入更有特色，有自己独特的贡献才行。相对而言，那些困难的问题，愿意碰的人就少，潜下心来啃硬骨头，也是不错的选择，当然同时就会面临做不出来的风险，或者做出来也得不到太多关注的风险。同学需要根据自身特点、经验和需求，兼顾摘果子和啃骨头两种类型的研究想法。</p>
<p><img src="https://pic1.zhimg.com/80/v2-d71aaf2b86116e3ea1e891bf9230a2c4_hd.jpg" alt></p>
<p><strong>注意多项研究工作的主题连贯性</strong>。同学的研究训练往往持续数年，需要注意前后多项研究工作的主题连贯性，保证内在逻辑统一。需要考虑，在个人简历上，在出国申请Personal Statement中，或者在各类评奖展示中，能够将这些研究成果汇总在一起，讲出自己开展这些研究工作的总目标、总设想。客观上讲，人工智能领域研究节奏很快，技术更新换代快，所以成果发表也倾向于小型化、短平快。我有商学院、社科的朋友，他们一项研究工作往往需要持续一年甚至数年以上；高性能计算、计算机网络方向的研究周期也相对较长。人工智能这种小步快跑的特点，决定了很多同学即使本科毕业时，也会有多篇论文发表，更不用说硕士生、博士生。在这种情况下，就格外需要在研究选题时，注意前后工作的连贯性和照应关系。几项研究工作放在一起，到底是互相割裂说不上话，还是在为一个统一的大目标而努力，格外反映研究的大局意识和布局能力。例如，下图是我们课题组涂存超博士2018年毕业时博士论文《面向社会计算的网络表示学习》的章节设置，整体来看就比《社会计算的若干重要问题研究》等没有内在关联的写法要更让人信服一些。当然，对于初学者而言，一开始就想清楚五年的研究计划，根本不可能。但想，还是不去想，结果还是不同的。</p>
<p><img src="https://pic1.zhimg.com/80/v2-9fbee2d16f9c05fa1cb1ec86a27d265c_hd.jpg" alt></p>
<p><strong>注意总结和把握研究动态和趋势，因时而动</strong>。2019年在知乎上有这样一个问题：”2019年在NLP领域，资源有限的个人/团队能做哪些有价值有希望的工作？“ 我当时的回答如下：</p>
<blockquote>
<p>我感觉，产业界开始集团化搞的问题，说明其中主要的开放性难题已经被解决得差不多了，如语言识别、人脸识别等，在过去20年里面都陆续被广泛商业应用。看最近的BERT、GPT-2，我理解更多的是将深度学习对大规模数据拟合的能力发挥到极致，在深度学习技术路线基本成熟的前提下，大公司有强大计算能力支持，自然可以数据用得更多，模型做得更大，效果拟合更好。<br>成熟高新技术进入商用竞争，就大致会符合摩尔定律的发展规律。现在BERT等训练看似遥不可及，但随着计算能力等因素的发展普及，说不定再过几年，人人都能轻易训练BERT和GPT-2，大家又会在同一个起跑线上，把目光转移到下一个挑战性难题上。<br>所以不如提前考虑，哪些问题是纯数据驱动技术无法解决的。NLP和AI中的困难任务，如常识和知识推理，复杂语境和跨模态理解，可解释智能，都还没有可行的解决方案，我个人也不看好数据驱动方法能够彻底解决。更高层次的联想、创造、顿悟等认知能力，更是连边还没碰到。这些正是有远见的研究者们应该开始关注的方向。</p>
</blockquote>
<p>需要看到，不同时期的研究动态和趋势不同。把握这些动态和趋势，就能够做出研究社区感兴趣的成果。不然的话，即使研究成果没有变化，只是简单早几年或晚几年投稿，结果也会大不相同。例如，2013年word2vec发表，在2014-2016年之间开展词表示学习研究，就相对比较容易得到ACL、EMNLP等会议的录用；但到了2017-2018年，ACL等会议上的词表示学习的相关工作就比较少见了。</p>
<h1 id="最后的补充"><a href="#最后的补充" class="headerlink" title="最后的补充"></a>最后的补充</h1><p>这篇短文，主要是希望面向初学者，介绍一些求新过程中的经验和注意事项，希望大家少走一些弯路。但阅读文献，深入思考，接收拒稿不断改进的苦，该吃的还是要吃。学术研究和论文发表，对个人而言也许意味着高薪资和奖学金，但其最终的目的还是真正的推动学科的发展。所以，要做经得起考验的学术研究，关键就在”真“与”新“，需要我们始终恪守和孜孜以求。</p>
<p>著名历史学家、清华校友何炳棣先生曾在自传《读史阅世六十年》中提及著名数学家林家翘的一句嘱咐：“要紧的是不管搞哪一行，千万不要做第二等的题目。” 具体到每个领域，什么是一等的题目本身见仁见智，其实更指向内心“求真”的态度。</p>
<p>参考文献<br>[1] <a href="https://paperswithcode.com/" target="_blank" rel="noopener">https://paperswithcode.com/</a> &amp; <a href="http://nlpprogress.com/" target="_blank" rel="noopener">http://nlpprogress.com/</a></p>
<p>[2] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, Maosong Sun. Neural Relation Extraction with Selective Attention over Instances. The 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016).</p>
<p>[3] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu. ERNIE: Enhanced Language Representation with Informative Entities. The 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019).</p>
<p>[4] <a href="https://github.com/thunlp/PLMpapers" target="_blank" rel="noopener">https://github.com/thunlp/PLMpapers</a></p>
]]></content>
  </entry>
  <entry>
    <title>Keyphrase Generation任务综述</title>
    <url>/2019/12/02/Keyphrase-Generation%E4%BB%BB%E5%8A%A1%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="任务简介"><a href="#任务简介" class="headerlink" title="任务简介"></a>任务简介</h1><p>A <strong>keyphrase</strong> or keyword is a piece of short, summative content that expresses the main semantic meaning of a longer text. The typical use of a keyphrase or keyword is in scientific publications to provide the core information of a paper. </p><a id="more"></a>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><ul>
<li>F1@5</li>
<li>F1@10</li>
</ul>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><ul>
<li>KP20K</li>
<li>SemEval</li>
<li>NUS</li>
<li>Krapivin</li>
<li>Inspec</li>
</ul>
<h1 id="SOTA"><a href="#SOTA" class="headerlink" title="SOTA"></a>SOTA</h1><ul>
<li>2019-06-13 <strong>Title-Guided Encoding for Keyphrase Generation</strong></li>
</ul>
<table>
<thead>
<tr>
<th align="center">Dataset</th>
<th align="center">F1@5</th>
<th align="center">F1@10</th>
</tr>
</thead>
<tbody><tr>
<td align="center">KP20K</td>
<td align="center">0.372</td>
<td align="center">0.315</td>
</tr>
</tbody></table>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><ul>
<li><p><strong>Deep Keyphrase Generation</strong>. Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, Yu Chi. ACL 2017. <a href="https://aclweb.org/anthology/papers/P/P17/P17-1054/" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/memray/seq2seq-keyphrase" target="_blank" rel="noopener">[code]</a><br>Keyphrase Generation的第一篇paper，主要框架是 seq2seq + copy.</p>
</li>
<li><p><strong>Semi-Supervised Learning for Neural Keyphrase Generation</strong>. Hai Ye, Lu Wang. EMNLP 2018. <a href="https://aclweb.org/anthology/papers/D/D18/D18-1447/" target="_blank" rel="noopener">[pdf]</a><br>解决资源不足问题，提出两个策略：<br>（1）微调，通过keyphrase extraction方式人为构造大量数据预训练模型，再通过已有数据微调；<br>（2）多任务框架，生成keyphrase的同时生成title。</p>
</li>
<li><p><strong>Title-Guided Encoding for Keyphrase Generation</strong>. Wang Chen, Yifan Gao, Jiani Zhang, Irwin King, Michael R. Lyu1. AAAI 2019. <a href="https://arxiv.org/abs/1808.08575" target="_blank" rel="noopener">[pdf]</a><br>本文认为标题包含了文章的主要信息，通过标题来引导摘要的建模已提升模型性能。</p>
</li>
<li><p><strong>Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</strong>. Hou Pong Chan, Wang Chen, Lu Wang, Irwin King. ACL 2019. <a href="https://arxiv.org/abs/1906.04106" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/kenchan0226/keyphrase-generation-rl" target="_blank" rel="noopener">[code]</a></p>
</li>
</ul>
<ul>
<li><strong>Topic-Aware Neural Keyphrase Generation for Social Media Language. Yue Wang</strong>. Jing Li, Hou Pong Chan, Irwin King, Michael R. Lyu, Shuming Shi. ACL 2019. <a href="https://arxiv.org/abs/1906.03889" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/yuewang-cuhk/TAKG" target="_blank" rel="noopener">[code]</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>【shell】截断字符串</title>
    <url>/2019/11/29/%E3%80%90shell%E3%80%91%E6%88%AA%E6%96%AD%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/fengbohello/p/5954895.html" target="_blank" rel="noopener">https://www.cnblogs.com/fengbohello/p/5954895.html</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">line='abcd&lt;SEG&gt;efg'</span><br><span class="line">newline=$&#123;line#*&lt;SEG&gt;&#125;</span><br><span class="line">echo $newline # efg</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】判断字符串是否包含子串</title>
    <url>/2019/11/29/%E3%80%90shell%E3%80%91%E5%88%A4%E6%96%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%98%AF%E5%90%A6%E5%8C%85%E5%90%AB%E5%AD%90%E4%B8%B2/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/iamlihongwei/article/details/59484029" target="_blank" rel="noopener">https://blog.csdn.net/iamlihongwei/article/details/59484029</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [[ $line =~ "&lt;BEGIN&gt;" ]] </span><br><span class="line">then</span><br><span class="line">	echo "包含&lt;BEGIN&gt;"</span><br><span class="line">elif [[ $line =~ "&lt;SEG&gt;" ]]</span><br><span class="line">then</span><br><span class="line">	echo "包含&lt;SEG&gt;"</span><br><span class="line">else</span><br><span class="line">	echo "都不包含"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>使用python发送免费短信</title>
    <url>/2019/11/27/%E4%BD%BF%E7%94%A8python%E5%8F%91%E9%80%81%E5%85%8D%E8%B4%B9%E7%9F%AD%E4%BF%A1/</url>
    <content><![CDATA[<p>首先在 <a href="https://www.twilio.com/" target="_blank" rel="noopener">twilio</a> 上注册帐号，并申请一个 twilio 手机号，并认证自己的手机号，twilio只能给认证过的手机号发送短信。</p><p>使用 python 发送短信</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twilio.rest <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line">account_sid = &lt;your account sid&gt;</span><br><span class="line">auth_token = &lt;your auth token&gt;</span><br><span class="line">client = Client(account_sid, auth_token)</span><br><span class="line"></span><br><span class="line">message=client.messages.create(</span><br><span class="line">	from_=&lt;your twilio phone num&gt;,</span><br><span class="line">	body=&lt;your message&gt;,</span><br><span class="line">	to=&lt;your phone num&gt;</span><br><span class="line">)</span><br><span class="line">print(message.sid)</span><br></pre></td></tr></table></figure><a id="more"></a>



<p>注：试用版免费次数有限。</p>
]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
  </entry>
  <entry>
    <title>【论文笔记】Context-Aware Learning for Neural Machine Translation</title>
    <url>/2019/11/22/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Context-Aware-Learning-for-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Context-Aware Learning for Neural Machine Translation</strong>. Sébastien Jean, Kyunghyun Cho. ArXiv 1903. <a href="https://arxiv.org/pdf/1903.04715.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文提出一个正则化项，鼓励模型利用上下文信息，从而提高篇章翻译结果。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>人们认为使用上下文可以提高篇章翻译，也就是使用上下文信息后译文翻译概率会更高。</p><a id="more"></a>


<p><img src="/images/context-aware1.png" alt></p>
<p>这个不等式在token、sentence、data三个层次上都成立</p>
<p><img src="/images/context-aware2.png" alt></p>
<p>本文在损失函数中加入一个正则化项，正则化项由三个max margin组成。</p>
<p><img src="/images/context-aware3.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><img src="/images/context-aware4.png" alt></p>
<p>（a）句子级别翻译，不利用上下信息</p>
<p>（b）利用随机上下文，随机上下文的期望跟不利用上下文的期望一样，所以使用上下文没有提升</p>
<p>（c）利用前文上下文，对比随机上下文有提升，并且跟没有利用上下文相差0.4</p>
<p>（d）鼓励利用前文上下文，跟没有利用上下文相差3.74，并且比（c）有提升</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
        <tag>Context</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Pretrained Language Models for Document-Level Neural Machine Translation</title>
    <url>/2019/11/21/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Pretrained-Language-Models-for-Document-Level-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Pretrained Language Models for Document-Level Neural Machine Translation</strong>. Liangyou Li, Xin Jiang, Qun Liu. ArXiv. <a href="https://arxiv.org/pdf/1911.03110.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>现有篇章翻译工作大都只能有限的上下文（前面3句话），当利用更长上下文时，由于训练不稳定模型效果反而下降。理论上来说更长的上下文可以提供更多信息，更有助于翻译。本文就是希望能够在篇章翻译中利用更长的上下文。</p><a id="more"></a>

<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="BERT初始化"><a href="#BERT初始化" class="headerlink" title="BERT初始化"></a>BERT初始化</h2><p>现有有些篇章翻译工作先利用大量平行句对预训练（有些只有利用篇章语料的平行句对预训练），然后再利用平行篇章语料微调。本文不再使用平行句对预训练，而是使用别人训练好的BERT来初始化模型参数。（BERT是在大量单语篇章语料上训练得到的）</p>
<h2 id="利用上下文"><a href="#利用上下文" class="headerlink" title="利用上下文"></a>利用上下文</h2><p>本文使用的上下文为前面512个词。将上下文和当前句子拼接起来，中间有个分隔符，但是如果直接使用Encoder对拼接后的句子进行编码，生成的译文反而更差（训练不稳定）。</p>
<p><img src="/images/mlmdoc.png" alt></p>
<p>本文提出了三个改进方法：</p>
<ul>
<li><p><strong>Segment Embeddings</strong>:<br>用来标记每个词是属于当前要翻译句子，还是属于上下文。</p>
</li>
<li><p><strong>Reverse Position Embeddings</strong>:<br>先对当前要翻译句子进行编号，再对上下文进行编号。</p>
</li>
<li><p><strong>Context Masks</strong>:<br>经过编码器后，当前句子已经包含了上下文信息，对上下文的隐状态加 mask，使得解码器更加关注当前句子。（不加mask，训练不稳定）</p>
</li>
</ul>
<h2 id="多任务"><a href="#多任务" class="headerlink" title="多任务"></a>多任务</h2><p>本文引入了Mask Language Model</p>
<p><img src="/images/mlmdoc1.png" alt></p>
<p>X: 源端当前句子</p>
<p>C: 源端上下文</p>
<p>Y: 目标端当前句子</p>
<p>S: X 和 C 的拼接</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><img src="/images/mlmdoc2.png" alt></p>
<p>可以训12层encoder还是比较 NB 的。<br><font color="#FF0000">我认为作者可以补充一个只加BERT的实验结果。</font></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
      </tags>
  </entry>
  <entry>
    <title>【深度学习基础】Dropout</title>
    <url>/2019/11/15/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91Dropout/</url>
    <content><![CDATA[<p>** Improving neural networks by preventing co-adaptation of feature detectors**. Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov. arXiv 1207.0580. <a href="https://arxiv.org/abs/1207.0580" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="一些博客"><a href="#一些博客" class="headerlink" title="一些博客"></a>一些博客</h1><ul>
<li><a href="https://www.zhihu.com/question/61751133" target="_blank" rel="noopener">神经网络Dropout层中为什么dropout后还需要进行rescale？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/66337970" target="_blank" rel="noopener">Dropout的前世与今生</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>【git 使用】分支合并</title>
    <url>/2019/11/12/%E3%80%90git-%E4%BD%BF%E7%94%A8%E3%80%91%E5%88%86%E6%94%AF%E5%90%88%E5%B9%B6/</url>
    <content><![CDATA[<ul>
<li>master发生改变，同步到feature branch</li>
</ul><p>merge master into feature branch</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git checkout &lt;feature branch&gt;</span><br><span class="line">git merge master</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>ArXiv 论文 2019/11/2-2019/11/8</title>
    <url>/2019/11/10/ArXiv-%E8%AE%BA%E6%96%87-2019-11-2-2019-11-8/</url>
    <content><![CDATA[<ul>
<li><a href="https://arxiv.org/abs/1911.00492" target="_blank" rel="noopener">Reasoning Over Paths via Knowledge Base Completion</a></li>
<li><a href="https://arxiv.org/abs/1911.00069" target="_blank" rel="noopener">Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping</a></li>
<li><a href="https://arxiv.org/abs/1911.00133" target="_blank" rel="noopener">Dreaddit: A Reddit Dataset for Stress Analysis in Social Media</a></li>
<li><a href="https://arxiv.org/abs/1911.00176" target="_blank" rel="noopener"><strong>Sequence Modeling with Unconstrained Generation Order</strong></a></li>
<li><a href="https://arxiv.org/abs/1911.00317" target="_blank" rel="noopener">On the Linguistic Representational Power of Neural Machine Translation Models</a></li>
<li><a href="https://arxiv.org/abs/1911.00473" target="_blank" rel="noopener">BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding</a><a id="more"></a></li>
<li><a href="https://arxiv.org/abs/1911.00484" target="_blank" rel="noopener">Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents</a></li>
<li><a href="https://arxiv.org/abs/1911.00225" target="_blank" rel="noopener">When Choosing Plausible Alternatives, Clever Hans can be Clever</a></li>
<li><a href="https://arxiv.org/abs/1911.00269" target="_blank" rel="noopener">A Robust Data-Driven Approach for Dialogue State Tracking of Unseen Slot Values</a></li>
<li><a href="https://arxiv.org/abs/1911.00274" target="_blank" rel="noopener">Kernelized Bayesian Softmax for Text Generation</a></li>
<li><a href="https://arxiv.org/abs/1911.00359" target="_blank" rel="noopener">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data</a></li>
</ul>
]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
      <tags>
        <tag>ArXiv</tag>
      </tags>
  </entry>
  <entry>
    <title>【git 使用】clone、branch、add、commit、push</title>
    <url>/2019/11/08/%E3%80%90git-%E4%BD%BF%E7%94%A8%E3%80%91clone%E3%80%81branch%E3%80%81add%E3%80%81commit%E3%80%81push/</url>
    <content><![CDATA[<h1 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h1><ul>
<li>普通</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@xxx.git</span><br></pre></td></tr></table></figure><ul>
<li>指定branch</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@xxx.git -b &lt;branchname&gt;</span><br></pre></td></tr></table></figure><h1 id="提交修改"><a href="#提交修改" class="headerlink" title="提交修改"></a>提交修改</h1><ul>
<li>提交某个文件</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add &lt;path to file&gt;</span><br><span class="line">git commit -m '&lt;message&gt;'</span><br><span class="line">git push origin &lt;branchname&gt;</span><br></pre></td></tr></table></figure><a id="more"></a>








<ul>
<li>提交多个文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add --all</span><br><span class="line">git commit -m '&lt;message&gt;'</span><br><span class="line">git push origin &lt;branchname&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】alias设置指令别名</title>
    <url>/2019/11/08/%E3%80%90shell%E3%80%91alias%E8%AE%BE%E7%BD%AE%E6%8C%87%E4%BB%A4%E5%88%AB%E5%90%8D/</url>
    <content><![CDATA[<p>alias 可以用来将一些较长的指令进行简化，使用alias时，用户必须使用单引号’’将原来的命令引起来，防止特殊字符导致错误。</p><h1 id="alias基本使用"><a href="#alias基本使用" class="headerlink" title="alias基本使用"></a>alias基本使用</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias 新指令=‘原指令 -选项/参数’</span><br></pre></td></tr></table></figure><p>如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias myscp='scp admin@192.168.72.77'</span><br></pre></td></tr></table></figure><h1 id="查看永久已设置别名"><a href="#查看永久已设置别名" class="headerlink" title="查看永久已设置别名"></a>查看永久已设置别名</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias -p</span><br></pre></td></tr></table></figure><a id="more"></a>







<h1 id="设置永久别名"><a href="#设置永久别名" class="headerlink" title="设置永久别名"></a>设置永久别名</h1><p>修改<code>~/.bashrc</code>文件</p>
<p>参考：<a href="https://man.linuxde.net/alias" target="_blank" rel="noopener">https://man.linuxde.net/alias</a></p>
]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>alias</tag>
      </tags>
  </entry>
  <entry>
    <title>ArXiv 论文 2019/10/28-2019/11/1</title>
    <url>/2019/11/02/ArXiv-%E8%AE%BA%E6%96%87-2019-10-28-2019-11-1/</url>
    <content><![CDATA[<ul>
<li><a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
<li><a href="https://arxiv.org/abs/1910.14659" target="_blank" rel="noopener">Pseudolikelihood Reranking with Masked Language Models</a></li>
<li><a href="https://arxiv.org/abs/1910.14549" target="_blank" rel="noopener">Positional Attention-based Frame Identification with BERT: A Deep Learning Approach to Target Disambiguation and Semantic Frame Selection</a></li>
<li><a href="https://arxiv.org/abs/1910.14192" target="_blank" rel="noopener">Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning</a></li>
<li><a href="https://arxiv.org/abs/1910.14176" target="_blank" rel="noopener">Predicting Discourse Structure using Distant Supervision from Sentiment</a><a id="more"></a></li>
<li><a href="https://arxiv.org/abs/1910.14142" target="_blank" rel="noopener">Discourse-Aware Neural Extractive Model for Text Summarization</a></li>
<li><a href="https://arxiv.org/abs/1910.14075" target="_blank" rel="noopener">Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.14613" target="_blank" rel="noopener">Neural Assistant: Joint Action Prediction, Response Generation, and Latent Knowledge Reasoning</a></li>
<li><a href="https://arxiv.org/abs/1910.14208" target="_blank" rel="noopener">Hidden State Guidance: Improving Image Captioning using An Image Conditioned Autoencoder</a></li>
<li><a href="https://arxiv.org/abs/1910.13890" target="_blank" rel="noopener">A Latent Morphology Model for Open-Vocabulary Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.13794" target="_blank" rel="noopener">Let Me Know What to Ask: Interrogative-Word-Aware Question Generation</a></li>
<li><a href="https://arxiv.org/abs/1910.13466" target="_blank" rel="noopener">Ordered Memory</a></li>
<li><a href="https://arxiv.org/abs/1910.13106" target="_blank" rel="noopener">Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots</a></li>
<li><a href="https://arxiv.org/abs/1910.13267" target="_blank" rel="noopener">BPE-Dropout: Simple and Effective Subword Regularization</a></li>
<li><a href="https://arxiv.org/abs/1910.13294" target="_blank" rel="noopener">Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control</a></li>
<li><a href="https://arxiv.org/abs/1910.13437" target="_blank" rel="noopener">An Empirical Study of Generation Order for Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.12708" target="_blank" rel="noopener">Evaluating Lottery Tickets Under Distributional Shifts</a></li>
<li><a href="https://arxiv.org/abs/1910.12702" target="_blank" rel="noopener">Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling</a></li>
<li><a href="https://arxiv.org/abs/1910.12698" target="_blank" rel="noopener">Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis</a></li>
<li><a href="https://arxiv.org/abs/1910.12527" target="_blank" rel="noopener">RPM-Oriented Query Rewriting Framework for E-commerce Keyword-Based Sponsored Search</a></li>
<li><a href="https://arxiv.org/abs/1910.12391" target="_blank" rel="noopener">What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?</a></li>
<li><a href="https://arxiv.org/abs/1910.12197" target="_blank" rel="noopener">Look-up and Adapt: A One-shot Semantic Parser</a></li>
<li><a href="https://arxiv.org/abs/1910.12196" target="_blank" rel="noopener">Open the Boxes of Words: Incorporating Sememes into Textual Adversarial Attack</a></li>
<li><a href="https://arxiv.org/abs/1910.11966" target="_blank" rel="noopener">Yall should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts</a></li>
<li><a href="https://arxiv.org/abs/1910.12038" target="_blank" rel="noopener">Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention</a></li>
<li><a href="https://arxiv.org/abs/1910.11959" target="_blank" rel="noopener">FineText: Text Classification via Attention-based Language Model Fine-tuning</a></li>
<li><a href="https://arxiv.org/abs/1910.12094" target="_blank" rel="noopener">Meta Learning for End-to-End Low-Resource Speech Recognition</a></li>
<li><a href="https://arxiv.org/abs/1910.11491" target="_blank" rel="noopener">Attention Optimization for Abstractive Document Summarization</a></li>
<li><a href="https://arxiv.org/abs/1910.11471" target="_blank" rel="noopener">Machine Translation from Natural Language to Code using Long-Short Term Memory</a></li>
<li><a href="https://arxiv.org/abs/1910.11470" target="_blank" rel="noopener">A Survey on Recent Advances in Named Entity Recognition from Deep Learning models</a></li>
<li><a href="https://arxiv.org/abs/1910.11411" target="_blank" rel="noopener">Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations</a></li>
<li><a href="https://arxiv.org/abs/1910.11399" target="_blank" rel="noopener">Comparison of Quality Indicators in User-generated Content Using Social Media and Scholarly Text</a></li>
<li><a href="https://arxiv.org/abs/1910.11494" target="_blank" rel="noopener">Fast and Accurate Knowledge-Aware Document Representation Enhancement for News Recommendations</a></li>
<li><a href="https://arxiv.org/abs/1910.11455" target="_blank" rel="noopener">Recognizing long-form speech using streaming end-to-end models</a></li>
</ul>
]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
      <tags>
        <tag>ArXiv</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Discourse-Aware Neural Extractive Model for Text Summarization</title>
    <url>/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Discourse-Aware-Neural-Extractive-Model-for-Text-Summarization/</url>
    <content><![CDATA[<p><strong>Discourse-Aware Neural Extractive Model for Text Summarization</strong>. Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu. ArXiv 1910.14142.<a href="https://arxiv.org/pdf/1910.14142.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>作者分析认为现有抽取式文档摘要存在以下两个不足：</p><a id="more"></a>

<ul>
<li>抽取式文档摘要都是以句子级别进行抽取，导致结果包含冗余或者没有用的信息。</li>
<li>BERT常被SOTA文档摘要模型用在文档编码器，但是BERT是再句对上预训练的，不能很好捕捉长距离的句间依赖关系。</li>
</ul>
<p>针对以上两个不足，作者提出了两个解决方法：</p>
<ul>
<li>按EDU进行抽取 （EDU是RST中的基本单元，具体可以去了解discourse parsing）</li>
<li>构造RST Graph和Coreference Graph建模长距离句间依赖关系。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>Discourse Segmentation: sequence to EDU</p>
<p>Discourse Parsing: EDU to RST tree</p>
<h2 id="RST-Graph"><a href="#RST-Graph" class="headerlink" title="RST Graph"></a>RST Graph</h2><p>通过篇章分析，可以在篇章上构造得到一棵树，树的叶子节点是EDU，树上的边代表的是对应子节点的重要性程度，N代表主要，S代表次要，可以认为S是N的补充。相邻两个子节点可以有三种关系，N-N,N-S,S-N。</p>
<p>作者提出假设：S依赖N,所以存在一条路径从S指向N；如果两个节点都是N，就认为是右N依赖做N。</p>
<p>根据这个假设，可以将RST discourse tree转成成RST dependence graph。</p>
<p><img src="/images/discbert1.jpg" alt></p>
<p>注：论文原图中没有标N和S，为了好理解我标了N和S。</p>
<p>如果存在一条从第i个EDU指向第j个EDU的路径，则设GR[i][j]=1，否则为0,这样就可以将RST Graph转化成GR矩阵。</p>
<p><img src="/images/discbert2.jpg" alt></p>
<h2 id="Coreference-Graph"><a href="#Coreference-Graph" class="headerlink" title="Coreference Graph"></a>Coreference Graph</h2><p>通过斯坦福的CoreNLP工具，可以得到多个共指簇（coreference clusters），每个簇中的EDU都指向同一个实体。指向同一个实体的EDU存在联系，所以同一个簇中的所有EDU之间（包括自己跟自己）存在一条边。基于这个原则，作者设计一个构造coreference graph的算法，遍历所有簇，簇中每个EDU之间存在一个边。也就得到了共指矩阵GC。</p>
<p><img src="/images/discbert3.jpg" alt></p>
<h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p><img src="/images/discbert4.jpg" alt></p>
<p>首先使用BERT编码整个篇章，使用BERT得到的隐状态表示，每个EDU内部做self-attention得到EDU的表示，由得到的EDU表示和两个矩阵表示GR和GC，做GCN得到EDU新的表示，通过MLP预测EDU是否被抽取出来做EDU（0-1序列标注）。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在两个数据集上进行验证，并得到了SOTA结果。</p>
<p><img src="/images/discbert5.jpg" alt></p>
<p><img src="/images/discbert6.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Discourse Structure</tag>
        <tag>Extractive</tag>
        <tag>Summarization</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Document-level Neural Machine Translation with Inter-Sentence Attention</title>
    <url>/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Document-level-Neural-Machine-Translation-with-Inter-Sentence-Attention/</url>
    <content><![CDATA[<p><strong>Document-level Neural Machine Translation with Inter-Sentence Attention</strong>. Shu Jiang, Rui Wang, Zuchao Li, Masao Utiyama, Kehai Chen, Eiichiro Sumita, Hai Zhao, Bao-liang Lu. ArXiv 1910.14528. <a href="https://arxiv.org/pdf/1910.14528.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文认为大部分篇章翻译只是引入大体的篇章上下文信息，但不是所有的上下文信息都对当前句子翻译有效，本文希望对上下文信息进行筛选。于是本文提出一个associated memory network（AMN）考虑句间关系，建模更加相关的上下文。(<em>其实 SAN 和 QCN 都有对上下文进行筛选</em>)</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="/images/camn.jpg" alt></p>
<p>（1）使用RNN对previous sentences（cj）进行编码，得到每个词的隐状态表示<font color="#FF0000">(<em>不是很懂为什么要用RNN，不直接使用transformer，并且当前句子x也不用像c一样使用RNN编码</em>)</font></p>
<p>（2）MultiHead Self-Attention更新每个句子的表示<br><img src="/images/camn1.jpg" alt><br><img src="/images/camn2.jpg" alt></p>
<p>（3）当前句子x的每个词和前面每个句子cj中的每个词算一个相似度分数<br><img src="/images/camn3.jpg" alt></p>
<p>（4）对相似性分数按行做softmax作为最终的相似性分数<br><img src="/images/camn4.jpg" alt></p>
<p>（5）得到句子级别上下文表示<br><img src="/images/camn5.jpg" alt></p>
<p>（6）建模每个句子的权重<br><img src="/images/camn6.jpg" alt></p>
<p>（7）得到篇章级别上下文<br><img src="/images/camn7.jpg" alt></p>
<p>（8）在transformer encoder中融入篇章级别上下文信息<br><img src="/images/camn8.jpg" alt><br><img src="/images/camn9.jpg" alt></p>
<p><font color="#FF0000">整体上来说，这种方法略显粗暴。</font></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在TED Talks, Subtitles, News三个数据集上验证了自己的模型有效性。</p>
<p><img src="/images/camn10.jpg" alt></p>
<font color="#FF0000">
  我认为实验还是存在一些不足：（1）没有跟SAN、QCN等工作进行对比（2）按照HAN公开代码，HAN是没有做BPE的，但是本文有做BPE，而本文中报的结果是HAN中报的没有做BPE的结果。
</font>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
        <tag>Document NMT</tag>
        <tag>Inter-Sentence</tag>
      </tags>
  </entry>
  <entry>
    <title>Accepted Papers List</title>
    <url>/2019/11/01/Accepted-Papers-List/</url>
    <content><![CDATA[<h1 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h1><ul>
<li><a href="">AAAI</a></li>
<li><a href="https://openreview.net/group?id=ICLR.cc/2020/Conference" target="_blank" rel="noopener">ICLR</a></li>
</ul>
<h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><ul>
<li><a href="https://dblp.org/db/conf/aaai/aaai2019" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://aclweb.org/anthology/events/acl-2019/" target="_blank" rel="noopener">ACL</a></li>
<li><a href="http://openaccess.thecvf.com/CVPR2019.py" target="_blank" rel="noopener">CVPR</a></li>
<li><a href="https://www.aclweb.org/anthology/events/emnlp-2019/" target="_blank" rel="noopener">EMNLP</a></li>
<li><a href="https://openreview.net/group?id=ICLR.cc/2019/Conference" target="_blank" rel="noopener">ICLR</a></li>
<li><a href="https://icml.cc/Conferences/2019/Schedule?type=Poster" target="_blank" rel="noopener">ICML</a></li>
<li><a href="https://www.ijcai19.org/accepted-papers.html" target="_blank" rel="noopener">IJCAI</a></li>
<li><a href="https://aclweb.org/anthology/events/naacl-2019/" target="_blank" rel="noopener">NAACL</a></li>
<li><a href="https://nips.cc/Conferences/2019/Schedule?type=Poster" target="_blank" rel="noopener">NeurIPS</a></li>
</ul>
<a id="more"></a>

<h1 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h1><ul>
<li><a href="https://dblp.org/db/conf/aaai/aaai2018" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://aclweb.org/anthology/events/acl-2018/" target="_blank" rel="noopener">ACL</a></li>
<li><a href="http://openaccess.thecvf.com/CVPR2018.py" target="_blank" rel="noopener">CVPR</a></li>
<li><a href="https://aclweb.org/anthology/events/emnlp-2018/" target="_blank" rel="noopener">EMNLP</a></li>
<li><a href="https://iclr.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">ICLR</a></li>
<li><a href="https://icml.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">ICML</a></li>
<li><a href="https://www.ijcai-18.org/accepted-papers/index.html" target="_blank" rel="noopener">IJCAI</a></li>
<li><a href="https://aclweb.org/anthology/events/naacl-2018/" target="_blank" rel="noopener">NAACL</a></li>
<li><a href="https://nips.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">NeurIPS</a></li>
</ul>
]]></content>
      <categories>
        <category>论文列表</category>
        <category>Accepted Papers</category>
      </categories>
      <tags>
        <tag>AAAI</tag>
        <tag>Accepted Papers</tag>
        <tag>ACL</tag>
        <tag>CVPR</tag>
        <tag>EMNLP</tag>
        <tag>ICLR</tag>
        <tag>ICML</tag>
        <tag>IJCAI</tag>
        <tag>NAACL</tag>
        <tag>NIPS</tag>
      </tags>
  </entry>
  <entry>
    <title>公开课推荐</title>
    <url>/2019/10/31/%E5%85%AC%E5%BC%80%E8%AF%BE%E6%8E%A8%E8%8D%90/</url>
    <content><![CDATA[<h1 id="机器学习（斯坦福大学）"><a href="#机器学习（斯坦福大学）" class="headerlink" title="机器学习（斯坦福大学）"></a>机器学习（斯坦福大学）</h1><p>机器学习是一门研究在非特定编程条件下让计算机采取行动的学科。最近二十年，机器学习为我们带来了自动驾驶汽车、实用的语音识别、高效的网络搜索，让我们对人类基因的解读能力大大提高。当今机器学习技术已经非常普遍，您很可能在毫无察觉情况下每天使用几十次。许多研究者还认为机器学习是人工智能（AI）取得进展的最有效途径。</p><a id="more"></a>
<p>本课程将广泛介绍机器学习、数据挖掘和统计模式识别。相关主题包括：(i) 监督式学习（参数和非参数算法、支持向量机、核函数和神经网络）。(ii) 无监督学习（集群、降维、推荐系统和深度学习）。(iii) 机器学习实例（偏见/方差理论；机器学习和AI领域的创新）。课程将引用很多案例和应用，您还需要学习如何在不同领域应用学习算法，例如智能机器人（感知和控制）、文本理解（网络搜索和垃圾邮件过滤）、计算机视觉、医学信息学、音频、数据库挖掘等领域。</p>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a></li>
<li><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="noopener">网易公开课</a></li>
</ul>
<hr>
<h1 id="CS231n（斯坦福大学）"><a href="#CS231n（斯坦福大学）" class="headerlink" title="CS231n（斯坦福大学）"></a>CS231n（斯坦福大学）</h1><p>计算机视觉已经在我们的社会中无处不在，在搜索，图像理解，应用程序，测绘，医药，无人驾驶飞机和自动驾驶汽车中的应用。许多这些应用程序的核心是视觉识别任务，如图像分类，定位和检测。神经网络（又名“深度学习”）方法的最新发展极大地提高了这些最先进的视觉识别系统的性能。本课程深入探讨深度学习架构的细节，重点是学习这些任务的端到端模型，尤其是图像分类。在为期10周的课程中，学生将学习实施，训练和调试自己的神经网络，并获得对计算机视觉尖端研究的详细了解。最后的任务将涉及培训一个数百万参数卷积神经网络，并将其应用于最大的图像分类数据集（ImageNet）。我们将着重教授如何设置图像识别问题，学习算法（例如反向传播），用于训练和微调网络的实际工程技巧，并引导学生通过实践任务和最终课程项目。本课程的大部分背景和材料都将从ImageNet挑战中吸取。</p>
<h2 id="链接-1"><a href="#链接-1" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="noopener">YouTube</a></li>
</ul>
<hr>
<h1 id="CS20SI（斯坦福大学）"><a href="#CS20SI（斯坦福大学）" class="headerlink" title="CS20SI（斯坦福大学）"></a>CS20SI（斯坦福大学）</h1><p>Tensorflow是Google Brain研究人员开发的一个功能强大的机器学习开源软件库。它具有许多预建功能，可以简化构建不同神经网络的任务。 Tensorflow允许在不同计算机之间分配计算，以及在一台机器中分配多个CPU和GPU。 TensorFlow提供了一个Python API，以及一个较少记录的C ++ API。对于本课程，我们将使用Python。</p>
<p>本课程将涵盖深入学习研究的Tensorflow图书馆的基本原理和当代用法。帮助学生理解Tensorflow的图形计算模型，探索其提供的功能，并学习如何构建和构建最适合深度学习项目的模型。通过本课程，学生将使用Tensorflow建立不同复杂度的模型，从简单的线性/逻辑回归到卷积神经网络和带有LSTM的递归神经网络，以解决词嵌入，翻译，光学字符识别等任务。学生还将学习最佳实践来构建模型并管理研究实验。</p>
<h2 id="链接-2"><a href="#链接-2" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://web.stanford.edu/class/cs20si/" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/watch?v=g-EvyKpZjmQ&list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="noopener">YouTube</a></li>
</ul>
<hr>
<h1 id="CS224d（斯坦福大学）"><a href="#CS224d（斯坦福大学）" class="headerlink" title="CS224d（斯坦福大学）"></a>CS224d（斯坦福大学）</h1><p>自然语言处理（NLP）是信息时代最重要的技术之一。理解复杂的语言也是人工智能的重要组成部分。 NLP的应用无处不在，因为人们用语言沟通大多数事物：网络搜索，广告，电子邮件，客户服务，语言翻译，放射学报告等等。NLP应用背后有大量的基础任务和机器学习模型。最近，深度学习方法在许多不同的NLP任务中获得了非常高的性能。这些模型通常可以通过单一的端到端模型进行培训，而且不需要传统的，特定于任务的特征工程。在这个冬季的季度课程中，学生将学习实施，培训，调试，可视化和创造自己的神经网络模型。本课程深入介绍了深入学习NLP的前沿研究。在模型方面，我们将介绍词向量表示，基于窗口的神经网络，递归神经网络，长期 - 短期记忆模型，递归神经网络，卷积神经网络以及一些涉及存储器组件的最新模型。通过讲座和编程作业，学生将学习使神经网络适应实际问题的必要工程技巧。</p>
<h2 id="链接-3"><a href="#链接-3" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://www.youtube.com/watch?v=g-EvyKpZjmQ&list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6" target="_blank" rel="noopener">YouTube</a></li>
</ul>
]]></content>
      <categories>
        <category>公开课</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>CS231n</tag>
        <tag>CS20SI</tag>
        <tag>CS224d</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Hierarchical-Modeling-of-Global-Context-for-Document-Level-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</strong>. Xin Tan, Longyin Zhang, Deyi Xiong, Guodong Zhou. EMNLP 2019. <a href="https://www.aclweb.org/anthology/D19-1168.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文觉得现有篇章翻译工作基于pre-context的方法存在两个不足：</p>
<p>（1）只利用一边（one-sidedness）的上下文可能还不够</p>
<p>（2）不正确的pre-context（translation bias propagation caused by improper pre-context）可能会导致翻译错误，所以本文想要利用整个篇章建模全局上下文（global context）来提升篇章翻译。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="使用层次结构建模全局上下文"><a href="#使用层次结构建模全局上下文" class="headerlink" title="使用层次结构建模全局上下文"></a>使用层次结构建模全局上下文</h2><p><img src="/images/HM-GDC.png" alt></p>
<p>A. Sentence Encoder</p>
<p>首先对句子进行编码得到每个词的隐状态表示，</p>
<p><img src="/images/h1.png" alt></p>
<p>求和得到整个句子的表示，</p>
<p><img src="/images/h21.png" alt></p>
<p>B. Document Encoder</p>
<p>对篇章所有句子进行编码，得到拥有篇章信息的句子表示（sentence-level global context）</p>
<p><img src="/images/h22.png" alt></p>
<p>C. Backpropagation of global context</p>
<p>由sentence-level global context得到word-level global context</p>
<p><img src="/images/h3.png" alt></p>
<h2 id="将全局上下文结合到NMT中"><a href="#将全局上下文结合到NMT中" class="headerlink" title="将全局上下文结合到NMT中"></a>将全局上下文结合到NMT中</h2><p>像其他工作一样，这个global context既结合在编码阶段，也可以结合在解码阶段。</p>
<p>A. 结合在编码阶段</p>
<p>使用word-level global context更新每个词的表示，P表示残差dropout，这里为0.1。</p>
<p><img src="/images/h5.png" alt></p>
<p>B. 结合在解码阶段</p>
<p><img src="/images/h4.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>本文实验在中英和德英两个数据集上进行。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>A. 中英</p>
<p>句子级别数据（用于预训练）：2.8M news corpora (LDC 2003E14, LDC2004T07, LDC2005T06, LDC2005T10, LDC2004T08)</p>
<p>篇章级别数据: IWSLT 2017 TED (1906个文档，226K个句对 )</p>
<p>B. 德英</p>
<p>(不进行预训练，没有句子级别数据)</p>
<p>篇章级别数据：IWSLT 2014 TED (1361个文档，172个句对)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>EMNLP</tag>
        <tag>NMT</tag>
        <tag>Context</tag>
        <tag>Document NMT</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Cross-Lingual-BERT-Transformation-for-Zero-Shot-Dependency-Parsing/</url>
    <content><![CDATA[<p><strong>Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</strong>. Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, Ting Liu. EMNLP 2019 <a href="https://arxiv.org/abs/1909.06775" target="_blank" rel="noopener">[PDF]</a>（短文）</p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本篇论文主要解决目前大部分cross-lingual word embedding技术存在的问题：</p><a id="more"></a>

<p>（1）依赖大量跨语言数据</p>
<p>（2）需要大量计算资源和训练时间</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>本文提出一种简单快捷的离线cross-lingual BERT线性映射方法：</p>
<p>（1）通过无监督词对齐方法获得上下文对齐次对（context-level，非词典）</p>
<p>（2）通过预训练好的BERT模型得到上下文对齐次对（x,y）中x,y的上下文表示</p>
<p>（3）通过SVD(奇异值分解)、GD(梯度下降)的方式求得两个表示的线性映射</p>
<p><img src="https://img-blog.csdnimg.cn/2019100320530798.png" alt></p>
<p>作者将获得的跨语言上下文词向量应用到zero-shot依存分析任务上，并获得了目前最好结果。并与XLM(利用跨语言数据重新训练BERT的方法)进行了对比，实验表明该方法在取得与XLM相近结果的情况下，需要的计算资源更少，训练速度也更快。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>EMNLP</tag>
        <tag>Cross Lingual</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Neural-Keyphrase-Generation-via-Reinforcement-Learning-with-Adaptive-Rewards/</url>
    <content><![CDATA[<p><strong>Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</strong>. Hou Pong Chan, Wang Chen, Lu Wang, Irwin King. ACL 2019. <a href="https://arxiv.org/abs/1906.04106" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本篇论文主要解决目前keyphrase generation任务中存在的两个不足：</p><a id="more"></a>

<p>（1）模型生成的keyphrase比真实的keyphrase个数少</p>
<p>（2）已有评价标准依赖词的完成匹配（不完全匹配就算错，如真实keyphrase为SVM，模型生成的keyphrase为support vector machine也算错）</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>keyphrase根据是否在原文中是否出现分present和absent，这里将一个document的所有keyphrase拼接成一个序列，present在前absent在后，并通过利用seq2seq编码document来生成所有的keyphrase。<br><img src="https://img-blog.csdnimg.cn/20190620092517159.png" width="55%" height="55%"></p>
<p>针对第一个不足，作者使用了reinforcement learning，</p>
<p>sample policy：<br><img src="https://img-blog.csdnimg.cn/20190620092800754.png" alt></p>
<p>reward function: RF1<br><img src="https://img-blog.csdnimg.cn/20190620091404860.png" alt></p>
<p>N为目前已生成的keyphrase个数，G为真实keyphrase个数。作者认为当生成keyphrase的个数还少于真实keyphrase个数时，应该鼓励模型去生成更多的keyphrase，所以用recall作为reward；当个数足够时，除了要求个数也要要求正确性，所以用的F1。看到这里可能也有人会有疑问，为什么前面只重视个数而忽视正确性呢？为什么不改变个数和正确性的权重呢（可以认为是F1的变形）？在这里我个人认为作者可能是实验驱动，只用recall就有效果了；如果没有效果作者可能会去设计吧。。。</p>
<p>presen keyphrase 和 absent keyphrase分别计算reward:<br><img src="https://img-blog.csdnimg.cn/20190620093050519.png" width="55%" height="55%"></p>
<p>针对第二个不足，思路也很容易理解，就是找keyphrase的各种形式，作者这里主要有三个方法</p>
<p>（1）Acronyms in the ground-truths</p>
<p>（2）Wikipedia entity titles</p>
<p>（3）Wikipedia disambiguation pages</p>
<p>然后作者在四个baseline基础上分别验证了方法的有效性，并且对生成的keyphrase的个数、RF1进行了分析。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>ACL</tag>
        <tag>Keyphrase Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 教程</title>
    <url>/2019/10/31/Hexo-%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<ul>
<li><a href="https://hexo-guide.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">hexo指南</a></li>
<li><a href="https://ahh666.com/posts/blog_gitalk_about.html" target="_blank" rel="noopener">添加gitalk评论</a></li>
<li><a href="https://blog.yleao.com/2018/0901/hexo-next%E4%B8%BB%E9%A2%98%E4%B8%8B%E7%9A%84%E7%BE%8E%E5%8C%96.html" target="_blank" rel="noopener">hexo-next主题下的美化</a></li>
<li><a href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/zh-CN/MATH.md" target="_blank" rel="noopener">hexo-next使用公式</a></li>
<li><a href="https://io-oi.me/tech/hexo-next-optimization/" target="_blank" rel="noopener">打造个性超赞博客 Hexo + NexT + GitHub Pages 的超深度优化</a></li>
</ul>]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Unsupervised%20Neural%20Single-Document%20Summarization%20of%20Reviews%20via/</url>
    <content><![CDATA[<p><strong>Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</strong>. Masaru Isonuma, Junichiro Mori, Ichiro Sakata. ACL 2019. <a href="https://arxiv.org/pdf/1906.05691.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文认为，评论（review）可以当作一个棵篇章树，树的根节点是其摘要，表示该评论的整体意思; 树的其他节点是对其父节点的细化。 也就是说这棵篇章树由摘要（根节点）与评论中所有句子（非根节点，每个非根节点代表一个句子）组成。于是本文通过学习构造这个隐式篇章树来建模得到评论摘要，并提出一种排序（rank）算法选择对生成摘要更加重要的句子。</p>
<p><img src="/images/strsum.png" alt></p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="模型整体方法"><a href="#模型整体方法" class="headerlink" title="模型整体方法"></a>模型整体方法</h2><p>（1）双向GRU+maxpooling 建模得到每个句子表示</p>
<p>（2）建模 父节点-子节点 对应关系权重（权重代表了树的关系）</p>
<p>（3）加权求和所有子节点表示，生成父节点（本文假设：子节点能够还原父节点，因为子节点包含了比父节点更多的信息。）</p>
<p>目标函数就是所有父节点生成概率最大。</p>
<p><img src="/images/strsum2.png" alt></p>
<h2 id="父节点-子节点-对应关系权重建模方法"><a href="#父节点-子节点-对应关系权重建模方法" class="headerlink" title="父节点-子节点 对应关系权重建模方法"></a>父节点-子节点 对应关系权重建模方法</h2><p>初始建模：边界概率（Marginal Probability of Dependency）</p>
<p><img src="/images/strsum3.png" alt></p>
<p>归一化（公式推导不是很懂）</p>
<p><img src="/images/strsum4.png" alt></p>
<p>调整：篇章排序（DiscourseRank）</p>
<p>受PageRank算法启发，更重要的句子有更多后代，迭代更新权重矩阵。<br><img src="/images/strsum5.png" alt></p>
<p><img src="/images/strsum6.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>ACL</tag>
        <tag>Discourse Structure</tag>
        <tag>Discourse Ranking</tag>
      </tags>
  </entry>
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【算法】冒泡排序和选择排序有什么不同？</title>
    <url>/2020/03/06/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F%E5%92%8C%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C%EF%BC%9F/</url>
    <content><![CDATA[<p>前面整理了冒泡排序和选择排序</p><p>1.<a href="https://procjx.github.io/2020/03/06/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F-Bubble-Sort">冒泡排序</a><br>2.<a href="https://procjx.github.io/2020/03/06/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F-Select-Sort/">选择排序</a></p><p>相信很多人在学习这两个算法的时候像我一样，冒泡排序和选择排序傻傻分不清楚，在写选择排序的时候一不留神就写成了冒泡排序。那么冒泡排序和选择排序究竟有什么不同呢？<br>1.什么时候交换数据，这也是他们最大的不同<br> 冒泡排序：每一次比较数据都有可能交换数据；<br> 选择排序：每轮所有比较结束后，才有可能交换一次数据。<br>2.是否需要辅助标记<br>只要记住这一点，就能够区分开冒泡排序和选择排序了。</p>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>冒泡排序</tag>
        <tag>选择排序</tag>
        <tag>冒泡</tag>
        <tag>选择</tag>
      </tags>
  </entry>
  <entry>
    <title>【算法】选择排序 Select Sort</title>
    <url>/2020/03/06/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F-Select-Sort/</url>
    <content><![CDATA[<h1 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h1><p>选择算法，以升序排序为例，每次从未排序的数据中找出（选出）最小数，然后将这个数放在数据未排序部分的最前面。</p><h1 id="视频演示"><a href="#视频演示" class="headerlink" title="视频演示"></a>视频演示</h1><iframe src="//player.bilibili.com/player.html?aid=18176082" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen style="position:absolute; width:100%;height:100%;left:0;top:0"></iframe><p>视频地址：<a href="https://www.bilibili.com/video/av18176082" target="_blank" rel="noopener">https://www.bilibili.com/video/av18176082</a></p><h1 id="python代码"><a href="#python代码" class="headerlink" title="python代码"></a>python代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_sort</span><span class="params">(data)</span>:</span></span><br><span class="line">    n = len(data)</span><br><span class="line">    <span class="comment"># 外循环，i 代表扫描轮数，一共扫描 n-1 轮</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        min_idx = i</span><br><span class="line">        <span class="comment"># 内循环，从未排序数据中找出最小数</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> data[min_idx]&gt;data[j]:</span><br><span class="line">                min_idx=j</span><br><span class="line">        <span class="keyword">if</span> min_idx!=i:</span><br><span class="line">            data[min_idx], data[i] = data[i], data[min_idx]</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    data = [<span class="number">64</span>, <span class="number">25</span>, <span class="number">12</span>, <span class="number">22</span>, <span class="number">11</span>]</span><br><span class="line">    sorted_data = select_sort(data.copy())</span><br><span class="line">    print(<span class="string">"排序前："</span>, data)</span><br><span class="line">    print(<span class="string">"排序后："</span>, sorted_data)</span><br></pre></td></tr></table></figure><a id="more"></a>





<h1 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h1><p>无论真实数据如何，比较次数固定为：n*(n-1)/2。</p>
<p>最好情况下，数据刚好是升序的，不需要交换数据；<br>最坏情况下，数据刚好是降序的，每轮选择都需要交换数据，即交换数据次数为：n-1</p>
<p>冒泡排序时间复杂度为：<strong>O(n^2)</strong></p>
]]></content>
      <categories>
        <category>算法</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序</tag>
        <tag>算法</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title>【算法】冒泡排序 Bubble Sort</title>
    <url>/2020/03/06/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%91%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F-Bubble-Sort/</url>
    <content><![CDATA[<h1 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h1><p>冒泡算法源于水中气泡变化，以升序排序为例，从第一个元素开始，依次比较相邻两个元素的大小，将大的数放在后面；扫描一遍后，最大的数就被放在了最后面；然后进行第二轮扫描，将第二大的数放在倒数第二的位置上；一直重复扫描 <em>n-1</em> 遍。</p><h1 id="视频演示"><a href="#视频演示" class="headerlink" title="视频演示"></a>视频演示</h1><iframe src="//player.bilibili.com/player.html?aid=18176281" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen style="position:absolute; width:100%;height:100%;left:0;top:0"></iframe><p>视频地址：<a href="https://www.bilibili.com/video/av18176281" target="_blank" rel="noopener">https://www.bilibili.com/video/av18176281</a></p><a id="more"></a>



<h1 id="python代码"><a href="#python代码" class="headerlink" title="python代码"></a>python代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 升序</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(data)</span>:</span></span><br><span class="line">    n = len(data)</span><br><span class="line">    <span class="comment"># 外循环， i 代表扫描轮数，一共扫描 n-1 轮</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        <span class="comment"># 内循环，第 i 轮，扫描前 n-i 个数</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, n-i):</span><br><span class="line">            <span class="keyword">if</span> data[j]&gt;data[j<span class="number">-1</span>]:</span><br><span class="line">                data[j], data[j<span class="number">-1</span>] = data[j<span class="number">-1</span>], data[j]</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    data = [<span class="number">5</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">    sorted_data = bubble_sort(data.copy())</span><br><span class="line">    print(<span class="string">"排序前："</span>, data)</span><br><span class="line">    print(<span class="string">"排序后："</span>, sorted_data)</span><br></pre></td></tr></table></figure>

<h1 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h1><p>无论真实数据如何，比较次数固定为：n*(n-1)/2。</p>
<p>最好情况下，数据刚好是升序的，不需要交换数据；<br>最坏情况下，数据刚好是降序的，每次比较都需要交换数据，即交换数据次数为：n*(n-1)/2。</p>
<p>冒泡排序时间复杂度为：<strong>O(n^2)</strong></p>
]]></content>
      <categories>
        <category>算法</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>冒泡排序</tag>
        <tag>排序</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-06</title>
    <url>/2020/03/06/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-06/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Action Segmentation with Joint Self-Supervised Temporal Domain  Adaptation <a href="https://arxiv.org/pdf/2003.02824" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Feature Extraction for Hyperspectral Imagery: The Evolution from Shallow  to Deep <a href="https://arxiv.org/pdf/2003.02822" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Multi-object Tracking via End-to-end Tracklet Searching and Ranking <a href="https://arxiv.org/pdf/2003.02795" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Combating noisy labels by agreement: A joint training method with  co-regularization <a href="https://arxiv.org/pdf/2003.02752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Search Space of Adversarial Perturbations against Image Filters <a href="https://arxiv.org/pdf/2003.02750" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Self-Supervised Spatio-Temporal Representation Learning Using Variable  Playback Speed Prediction <a href="https://arxiv.org/pdf/2003.02692" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Image Generation from Freehand Scene Sketches <a href="https://arxiv.org/pdf/2003.02683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> AI outperformed every dermatologist: Improved dermoscopic melanoma  diagnosis through customizing batch logic and loss function in an optimized  Deep CNN architecture <a href="https://arxiv.org/pdf/2003.02597" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> MarginDistillation: distillation for margin-based softmax <a href="https://arxiv.org/pdf/2003.02586" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> GANwriting: Content-Conditioned Generation of Styled Handwritten Word  Images <a href="https://arxiv.org/pdf/2003.02567" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Embedding Expansion: Augmentation in Embedding Space for Deep Metric  Learning <a href="https://arxiv.org/pdf/2003.02546" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation <a href="https://arxiv.org/pdf/2003.02541" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Detecting Attended Visual Targets in Video <a href="https://arxiv.org/pdf/2003.02501" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Adversarial Vertex Mixup: Toward Better Adversarially Robust  Generalization <a href="https://arxiv.org/pdf/2003.02484" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Fake Generated Painting Detection via Frequency Analysis <a href="https://arxiv.org/pdf/2003.02467" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Cluster Pruning: An Efficient Filter Pruning Method for Edge AI Vision  Applications <a href="https://arxiv.org/pdf/2003.02449" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> End-to-End Trainable One-Stage Parking Slot Detection Integrating Global  and Local Information <a href="https://arxiv.org/pdf/2003.02445" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Drone Based RGBT Vehicle Detection and Counting: A Challenge <a href="https://arxiv.org/pdf/2003.02437" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Who Make Drivers Stop? Towards Driver-centric Risk Assessment: Risk  Object Identification via Causal Inference <a href="https://arxiv.org/pdf/2003.02425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI <a href="https://arxiv.org/pdf/2003.02371" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Towards Fair Cross-Domain Adaptation via Generative Learning <a href="https://arxiv.org/pdf/2003.02366" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Creating High Resolution Images with a Latent Adversarial Generator <a href="https://arxiv.org/pdf/2003.02365" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Learning View and Target Invariant Visual Servoing for Navigation <a href="https://arxiv.org/pdf/2003.02327" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> The Impact of Hole Geometry on Relative Robustness of In-Painting  Networks: An Empirical Study <a href="https://arxiv.org/pdf/2003.02314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Exploring Partial Intrinsic and Extrinsic Symmetry in 3D Medical Imaging <a href="https://arxiv.org/pdf/2003.02294" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Event-Based Angular Velocity Regression with Spiking Networks <a href="https://arxiv.org/pdf/2003.02790" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Dimensionality Reduction and Motion Clustering during Activities of  Daily Living: 3, 4, and 7 Degree-of-Freedom Arm Movements <a href="https://arxiv.org/pdf/2003.02641" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Learning the sense of touch in simulation: a sim-to-real strategy for  vision-based tactile sensing <a href="https://arxiv.org/pdf/2003.02640" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Demographic Bias in Biometrics: A Survey on an Emerging Challenge <a href="https://arxiv.org/pdf/2003.02488" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Cumulant-free closed-form formulas for some common (dis)similarities  between densities of an exponential family <a href="https://arxiv.org/pdf/2003.02469" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging <a href="https://arxiv.org/pdf/2003.02438" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Team O2AS at the World Robot Summit 2018: An Approach to Robotic Kitting  and Assembly Tasks using General Purpose Grippers and Tools <a href="https://arxiv.org/pdf/2003.02427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Action Segmentation with Joint Self-Supervised Temporal Domain  Adaptation</b>  <a href="https://arxiv.org/pdf/2003.02824" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Min-Hung Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baopu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingze Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=AlRegib%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Ghassan AlRegib</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kira%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zsolt Kira</a><br>
<font size="3">
Abstract: Despite the recent progress of fully-supervised action segmentation techniques, the performance is still not fully satisfactory. One main challenge is the problem of spatiotemporal variations (e.g. different people may perform the same activity in various ways). Therefore, we exploit unlabeled videos to address this problem by reformulating the action segmentation task as a cross-domain problem with domain discrepancy caused by spatio-temporal variations. To reduce the discrepancy, we propose Self-Supervised Temporal Domain Adaptation (SSTDA), which contains two self-supervised auxiliary tasks (binary and sequential domain prediction) to jointly align cross-domain feature spaces embedded with local and global temporal dynamics, achieving better performance than other Domain Adaptation (DA) approaches. On three challenging benchmark datasets (GTEA, 50Salads, and Breakfast), SSTDA outperforms the current state-of-the-art method by large margins (e.g. for the F1@25 score, from 59.6% to 69.1% on Breakfast, from 73.4% to 81.5% on 50Salads, and from 83.6% to 89.1% on GTEA), and requires only 65% of the labeled training data for comparable performance, demonstrating the usefulness of adapting to unlabeled target videos across variations. The source code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管最近的充分监督的作用分割技术的进步，其性能仍不能完全令人满意。其中一个主要的挑战是时空变化的问题（例如，不同的人可以进行各种方式相同的活动）。因此，我们利用未标记的视频通过重整动作分割任务作为跨域问题引起的时空变化域差异来解决这个问题。为了减少这种差异，我们提出自我监督的时空领域适应性（SSTDA），它包含两个自我监督的辅助任务（二进制和连续域预测），嵌入式与本地和全球的时空动态共同对准跨域特征空间，实现更好性能上比其他领域适应性（DA）方法。在三个挑战基准数据集（GTEA，50Salads，和早餐），SSTDA优于由大余量的当前状态的最先进的方法（例如，对于F1 @ 25得分，从59.6％到早餐69.1％，从73.4％对50Salads 81.5％，和从83.6％到89.1 GTEA％），并且只需要65％的相当的性能的标记的训练数据的，表明跨越变化适应未标记的目标视频的有用性。源代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper2"> <b>2. Feature Extraction for Hyperspectral Imagery: The Evolution from Shallow  to Deep</b>  <a href="https://arxiv.org/pdf/2003.02822" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rasti%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Behnood Rasti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danfeng Hong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Renlong Hang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghamisi%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pedram Ghamisi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xudong Kang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chanussot%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jocelyn Chanussot</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Benediktsson%2C+J+A" target="_blank" rel="noopener" style="color:#0000EE;">Jon Atli Benediktsson</a><br>
<font size="3">
Abstract: Hyperspectral images provide detailed spectral information through hundreds of (narrow) spectral channels (also known as dimensionality or bands) with continuous spectral information that can accurately classify diverse materials of interest. The increased dimensionality of such data makes it possible to significantly improve data information content but provides a challenge to the conventional techniques (the so-called curse of dimensionality) for accurate analysis of hyperspectral images. Feature extraction, as a vibrant field of research in the hyperspectral community, evolved through decades of research to address this issue and extract informative features suitable for data representation and classification. The advances in feature extraction have been inspired by two fields of research, including the popularization of image and signal processing as well as machine (deep) learning, leading to two types of feature extraction approaches named shallow and deep techniques. This article outlines the advances in feature extraction approaches for hyperspectral imagery by providing a technical overview of the state-of-the-art techniques, providing useful entry points for researchers at different levels, including students, researchers, and senior researchers, willing to explore novel investigations on this challenging topic. % by supplying a rich amount of detail and references. In more detail, this paper provides a bird's eye view over shallow (both supervised and unsupervised) and deep feature extraction approaches specifically dedicated to the topic of hyperspectral feature extraction and its application on hyperspectral image classification. Additionally, this paper compares 15 advanced techniques with an emphasis on their methodological foundations in terms of classification accuracies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：高光谱图像提供通过数百个（窄）光谱信道（也称为维数或条带）具有连续的光谱信息的兴趣，可以准确地分类多样材料的详细的光谱信息。这样的数据的增加的维数，能够提高显著数据信息内容，但提供了一种用于高光谱图像的准确分析的常规技术挑战（维数的所谓的诅咒）。特征提取，如在高光谱社区研究一个充满活力的领域，通过几十年的研究发展到解决这个问题，适用于数据表示和分类提取信息量大的特点。在特征提取的进步已经由两个研究领域，包括图像和信号处理的普及以及机（深）学习启发，从而导致两种类型的特征提取办法命名浅层和深层技术。本文概述了在特征提取通过提供先进设备，最先进的技术的技术概述，研究人员在不同层面，包括学生，研究人员和高级研究人员提供有用的切入点超光谱成像方法的进步，乐于探究在这个富有挑战性的课题新颖的调查。 ％通过提供丰富的细节和引用的数量。更具体地说，本文提供了一个浅浅的鸟瞰图（包括监督和无监督）和深层特征提取办法专门用于高光谱特征提取的主题，其对高光谱影像分类中的应用。此外，本文在分类精度方面对他们的方法论基础的重点15项先进的技术进行比较。</font>
</div>


<hr>
<div id="paper3"> <b>3. Multi-object Tracking via End-to-end Tracklet Searching and Ranking</b>  <a href="https://arxiv.org/pdf/2003.02795" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lichao Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Shen</a><br>
<font size="3">
Abstract: Recent works in multiple object tracking use sequence model to calculate the similarity score between the detections and the previous tracklets. However, the forced exposure to ground-truth in the training stage leads to the training-inference discrepancy problem, i.e., exposure bias, where association error could accumulate in the inference and make the trajectories drift. In this paper, we propose a novel method for optimizing tracklet consistency, which directly takes the prediction errors into account by introducing an online, end-to-end tracklet search training process. Notably, our methods directly optimize the whole tracklet score instead of pairwise affinity. With sequence model as appearance encoders of tracklet, our tracker achieves remarkable performance gain from conventional tracklet association baseline. Our methods have also achieved state-of-the-art in MOT15~17 challenge benchmarks using public detection and online settings. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多目标跟踪使用序列模型最近的作品来计算检测和以前tracklets之间的相似性得分。然而，被迫暴露在地面实况在训练阶段，导致训练推论的差异问题，即，曝光补偿，其中关联错误可能会在推理积聚，使轨道漂移。在本文中，我们提出了用于优化tracklet一致性，这通过引入在线，端至端tracklet搜索训练过程直接将预测误差在内的新方法。值得注意的是，我们的方法直接优化整个tracklet得分，而不是成对的亲和力。随着序列模型tracklet的外观编码器，我们的跟踪器实现从传统tracklet协会基线显着的性能增益。我们的方法也取得了国家的最先进的使用公共检测和在线设置MOT15〜17挑战基准。</font>
</div>


<hr>
<div id="paper4"> <b>4. Combating noisy labels by agreement: A joint training method with  co-regularization</b>  <a href="https://arxiv.org/pdf/2003.02752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongxin Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=An%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo An</a><br>
<font size="3">
Abstract: Deep Learning with noisy labels is a practically challenging problem in weakly-supervised learning. The state-of-the-art approaches "Decoupling" and "Co-teaching+" claim that the "disagreement" strategy is crucial for alleviating the problem of learning with noisy labels. In this paper, we start from a different perspective and propose a robust learning paradigm called JoCoR, which aims to reduce the diversity of two networks during training. Specifically, we first use two networks to make predictions on the same mini-batch data and calculate a joint loss with Co-Regularization for each training example. Then we select small-loss examples to update the parameters of both two networks simultaneously. Trained by the joint loss, these two networks would be more and more similar due to the effect of Co-Regularization. Extensive experimental results on corrupted data from benchmark datasets including MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is superior to many state-of-the-art approaches for learning with noisy labels. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习与嘈杂的标签是弱监督学习实用具有挑战性的问题。国家的最先进的方法“脱钩”和“合作教学+”要求的“分歧”战略是缓解与嘈杂的标签学习的问题至关重要的。在本文中，我们从不同的角度入手，提出了一种强大的学习范例，称之为JoCoR，其目的是减少训练期间，两个网络的多样性。具体而言，我们首先使用两个网络，使在相同的小批量数据的预测和计算与合作正则联合损失每个训练例子。然后，我们选择小损失的例子来同时更新两个两个网络的参数。由联合损失的训练，这两个网络会越来越相似，由于联合正规化的效果。来自基准数据集包括MNIST，CIFAR-10，CIFAR-100和Clothing1M损坏的数据的广泛实验结果表明，JoCoR优于许多国家的最先进的方法用于与嘈杂标签学习。</font>
</div>


<hr>
<div id="paper5"> <b>5. Search Space of Adversarial Perturbations against Image Filters</b>  <a href="https://arxiv.org/pdf/2003.02750" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Thang%2C+D+D" target="_blank" rel="noopener" style="color:#0000EE;">Dang Duy Thang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Matsui%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Toshihiro Matsui</a><br>
<font size="3">
Abstract: The superiority of deep learning performance is threatened by safety issues for itself. Recent findings have shown that deep learning systems are very weak to adversarial examples, an attack form that was altered by the attacker's intent to deceive the deep learning system. There are many proposed defensive methods to protect deep learning systems against adversarial examples. However, there is still a lack of principal strategies to deceive those defensive methods. Any time a particular countermeasure is proposed, a new powerful adversarial attack will be invented to deceive that countermeasure. In this study, we focus on investigating the ability to create adversarial patterns in search space against defensive methods that use image filters. Experimental results conducted on the ImageNet dataset with image classification tasks showed the correlation between the search space of adversarial perturbation and filters. These findings open a new direction for building stronger offensive methods towards deep learning systems. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习性能的优越性是由安全问题为自己的威胁。最近的研究结果表明，深度学习系统是非常弱对抗的例子，这是由攻击者改变攻击形式是有意欺骗的深度学习系统。有许多建议防御方法来保护深度学习系统免受敌对的例子。然而，仍然缺乏主要策略，以欺骗那些防御方法。特定的对策建议任何时候，一个新的强大的对抗攻击将被发明欺骗该对策。在这项研究中，我们重点研究建立在对使用图像过滤器的防御方法搜索空间对抗模式的能力。在ImageNet数据集图像分类任务进行的实验结果表明，对抗扰动和过滤器的搜索空间之间的相关性。这些发现开辟了建设向深度学习系统强大的进攻方法的一个新方向。</font>
</div>


<hr>
<div id="paper6"> <b>6. Self-Supervised Spatio-Temporal Representation Learning Using Variable  Playback Speed Prediction</b>  <a href="https://arxiv.org/pdf/2003.02692" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyeon Cho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taehoon Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+H+J" target="_blank" rel="noopener" style="color:#0000EE;">Hyung Jin Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wonjun Hwang</a><br>
<font size="3">
Abstract: We propose a self-supervised learning method by predicting the variable playback speeds of a video. Without semantic labels, we learn the spatio-temporal representation of the video by leveraging the variations in the visual appearance according to different playback speeds under the assumption of temporal coherence. To learn the spatio-temporal variations in the entire video, we have not only predicted a single playback speed but also generated clips of various playback speeds with randomized starting points. We then train a 3D convolutional network by solving the formulation that sorts the shuffled clips by their playback speed. In this case, the playback speed includes both forward and reverse directions; hence the visual representation can be successfully learned from the directional dynamics of the video. We also propose a novel layer-dependable temporal group normalization method that can be applied to 3D convolutional networks to improve the representation learning performance where we divide the temporal features into several groups and normalize each one using the different corresponding parameters. We validate the effectiveness of the proposed method by fine-tuning it to the action recognition task. The experimental results show that the proposed method outperforms state-of-the-art self-supervised learning methods in action recognition. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出通过预测视频的可变播放速度自我监督学习方法。没有语义标签，我们通过根据时间相干性的假设下不同的回放速度利用在视觉外观变化学习视频的时空表示。要了解在整个视频的时空变化，我们不仅预测一个单一的播放速度，而且产生的各种播放速度的剪辑随机起点。然后，我们解决了通过播放速度排序洗牌剪辑制定培养3D卷积网络。在这种情况下，播放速度包括正向和反向方向上;因此视觉表示可以从视频的定向动力学成功地获知。我们还建议，可以应用到三维卷积网络，其中我们把时间特征分成若干组，并归一化每一个使用不同的相应的参数，以改善表示学习性能的新型层可靠颞组归一化方法。我们通过它微调动作识别任务验证了该方法的有效性。实验结果表明，在动作识别，该方法优于国家的最先进的自我监督学习方法。</font>
</div>


<hr>
<div id="paper7"> <b>7. Image Generation from Freehand Scene Sketches</b>  <a href="https://arxiv.org/pdf/2003.02683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengying Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianzhuang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Limin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changqing Zou</a><br>
<font size="3">
Abstract: We introduce the first method for automatic image generation from scene-level freehand sketches. Our model allows for controllable image generation by specifying the synthesis goal via freehand sketches. The key contribution is an attribute vector bridged generative adversarial network called edgeGAN which supports high visual-quality image content generation without using freehand sketches as training data. We build a large-scale composite dataset called SketchyCOCO to comprehensively evaluate our solution. We validate our approach on the task of both objectlevel and scene-level image generation on SketchyCOCO. We demonstrate the method's capacity to generate realistic complex scene-level images from a variety of freehand sketches by quantitative, qualitative results, and ablation studies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍从现场级的手绘草图自动图像生成的第一个方法。我们的模型允许可控​​的图像生成由通过手绘草图指定合成目标。关键贡献是属性向量桥接生成称为edgeGAN对抗性网络支持高视觉质量的图像内容生成，而无需使用手绘草图作为训练数据。我们建立所谓SketchyCOCO全面评估我们的解决方案的大型复合数据集。我们确认我们的两个objectlevel和场景级图像生成上SketchyCOCO的任务的方法。我们证明该方法的产生来自各种通过定量，定性结果，并切除研究手绘草图的现实复杂场景级图像的能力。</font>
</div>


<hr>
<div id="paper8"> <b>8. AI outperformed every dermatologist: Improved dermoscopic melanoma  diagnosis through customizing batch logic and loss function in an optimized  Deep CNN architecture</b>  <a href="https://arxiv.org/pdf/2003.02597" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+C+T" target="_blank" rel="noopener" style="color:#0000EE;">Cong Tri Pham</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luong%2C+M+C" target="_blank" rel="noopener" style="color:#0000EE;">Mai Chi Luong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Hoang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dung Van Hoang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Doucet%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antoine Doucet</a><br>
<font size="3">
Abstract: Melanoma, one of most dangerous types of skin cancer, re-sults in a very high mortality rate. Early detection and resection are two key points for a successful cure. Recent research has used artificial intelligence to classify melanoma and nevus and to compare the assessment of these algorithms to that of dermatologists. However, an imbalance of sensitivity and specificity measures affected the performance of existing models. This study proposes a method using deep convolutional neural networks aiming to detect melanoma as a binary classification problem. It involves 3 key features, namely customized batch logic, customized loss function and reformed fully connected layers. The training dataset is kept up to date including 17,302 images of melanoma and nevus; this is the largest dataset by far. The model performance is compared to that of 157 dermatologists from 12 university hospitals in Germany based on MClass-D dataset. The model outperformed all 157 dermatologists and achieved state-of-the-art performance with AUC at 94.4% with sensitivity of 85.0% and specificity of 95.0% using a prediction threshold of 0.5 on the MClass-D dataset of 100 dermoscopic images. Moreover, a threshold of 0.40858 showed the most balanced measure compared to other researches, and is promisingly application to medical diagnosis, with sensitivity of 90.0% and specificity of 93.8%. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：黑色素瘤，在一个非常高的死亡率最危险类型的皮肤癌，重新sults之一。早期发现和切除术是成功治愈的两个关键点。最近的研究中使用的人工智能分类黑色素瘤和痣和这些算法的评估比较的是皮肤科医生。然而，敏感性和特异性措施失衡的影响现有车型的性能。本研究提出采用深卷积神经网络，旨在检测黑素瘤的二元分类问题的方法。它涉及到3个的关键特征，即定制批量逻辑，定制的损失函数和重整完全连接层。训练数据集保持最新，包括黑色素瘤和痣的17302个图像;这是最大的数据集远远。该模型的性能相比，则是根据MClass-d数据集在德国12个大学附属医院皮肤科医生157。该模型表现优于所有157名皮肤科医生和使用上的100个皮肤镜图像MClass-d数据集0.5预测阈值与在95.0％的85.0％的灵敏度和特异性94.4％实现状态的最先进的性能与AUC。此外，0.40858阈值显示最平衡的措施相比其他的研究，并且是很有希望应用在医学诊断，以93.8％的90.0％的灵敏度和特异性。</font>
</div>


<hr>
<div id="paper9"> <b>9. MarginDistillation: distillation for margin-based softmax</b>  <a href="https://arxiv.org/pdf/2003.02586" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Svitov%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Svitov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alyamkin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergey Alyamkin</a><br>
<font size="3">
Abstract: The usage of convolutional neural networks (CNNs) in conjunction with a margin-based softmax approach demonstrates a state-of-the-art performance for the face recognition problem. Recently, lightweight neural network models trained with the margin-based softmax have been introduced for the face identification task for edge devices. In this paper, we propose a novel distillation method for lightweight neural network architectures that outperforms other known methods for the face recognition task on LFW, AgeDB-30 and Megaface datasets. The idea of the proposed method is to use class centers from the teacher network for the student network. Then the student network is trained to get the same angles between the class centers and the face embeddings, predicted by the teacher network. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在与基于容限SOFTMAX方法结合卷积神经网络（细胞神经网络）的使用表明国家的最先进的表现为面部识别的问题。近年来，随着基于保证金SOFTMAX训练的轻量级神经网络模型已经推出了针对边缘设备的面部识别任务。在本文中，我们提出了其性能优于对LFW，AgeDB-30和活性剂Megaface数据集面部识别任务的其他已知的方法轻质神经网络结构的新型蒸馏法。该方法的想法是使用类中心从教师网络为学生网络。那么学生网络进行训练，以获取类中心和面嵌入物，由教师网络预测之间相同的角度。</font>
</div>


<hr>
<div id="paper10"> <b>10. GANwriting: Content-Conditioned Generation of Styled Handwritten Word  Images</b>  <a href="https://arxiv.org/pdf/2003.02567" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Kang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Riba%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pau Riba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaxing Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rusi%C3%B1ol%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marçal Rusiñol</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Forn%C3%A9s%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alicia Fornés</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Villegas%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mauricio Villegas</a><br>
<font size="3">
Abstract: Although current image generation methods have reached impressive quality levels, they are still unable to produce plausible yet diverse images of handwritten words. On the contrary, when writing by hand, a great variability is observed across different writers, and even when analyzing words scribbled by the same individual, involuntary variations are conspicuous. In this work, we take a step closer to producing realistic and varied artificially rendered handwritten words. We propose a novel method that is able to produce credible handwritten word images by conditioning the generative process with both calligraphic style features and textual content. Our generator is guided by three complementary learning objectives: to produce realistic images, to imitate a certain handwriting style and to convey a specific textual content. Our model is unconstrained to any predefined vocabulary, being able to render whatever input word. Given a sample writer, it is also able to mimic its calligraphic features in a few-shot setup. We significantly advance over prior art and demonstrate with qualitative, quantitative and human-based evaluations the realistic aspect of our synthetically produced images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管目前的图像生成方法已经达到了令人印象深刻的质量水平，但仍无法生产的手写字似是而非而又多种多样的图像。相反，手写的时候，一个很大的可变性跨不同的作家观察和分析同一个人胡乱写着即使，不自主的变化是显着的。在这项工作中，我们采取了一步接近现实的生产和多样化人为渲染的手写字。我们建议，可以通过调节产生可信的手写字的图像生成过程既书法的风格特点和文本内容的新方法。我们的发电机由三个互补的学习目标导向：生产逼真的图像，模仿有一定的手写风格，并传达特定的文本内容。我们的模型是不受约束任何预定义的词汇，能够使任何输入的单词。给定样本的作家，也能在几炮设置模仿其书法特点。我们显著推进了现有技术，并与定性，定量和以人为本的评价我们的人工合成图像的真实感展示。</font>
</div>


<hr>
<div id="paper11"> <b>11. Embedding Expansion: Augmentation in Embedding Space for Deep Metric  Learning</b>  <a href="https://arxiv.org/pdf/2003.02546" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Byungsoo Ko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geonmo Gu</a><br>
<font size="3">
Abstract: Learning the distance metric between pairs of samples has been studied for image retrieval and clustering. With the remarkable success of pair-based metric learning losses, recent works have proposed the use of generated synthetic points on metric learning losses for augmentation and generalization. However, these methods require additional generative networks along with the main network, which can lead to a larger model size, slower training speed, and harder optimization. Meanwhile, post-processing techniques, such as query expansion and database augmentation, have proposed the combination of feature points to obtain additional semantic information. In this paper, inspired by query expansion and database augmentation, we propose an augmentation method in an embedding space for pair-based metric learning losses, called embedding expansion. The proposed method generates synthetic points containing augmented information by a combination of feature points and performs hard negative pair mining to learn with the most informative feature representations. Because of its simplicity and flexibility, it can be used for existing metric learning losses without affecting model size, training speed, or optimization difficulty. Finally, the combination of embedding expansion and representative metric learning losses outperforms the state-of-the-art losses and previous sample generation methods in both image retrieval and clustering tasks. The implementation will be publicly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：学习样本对之间的距离度量已经研究了图像检索和集群。随着对基于度量学习的损失显着的成功，最近的作品提出了关于增强和推广度量学习的损失利用生成的合成分。然而，这些方法需要与主网，这可能会导致更大的模型大小，速度较慢的训练速度，更难以优化沿着额外生成的网络。同时，后处理技术，例如查询扩展和数据库增强，已经提出的特征点的组合，以获得附加的语义信息。在本文中，通过查询扩展和数据库增强的启发，我们提出在对基于度量学习损失的嵌入空间，称为嵌入扩展的增强方法。所提出的方法产生的合成点由硬负对采矿学会与信息量最大特征表示特征点，并且执行组合含有增强信息。由于它的简单性和灵活性，它可以用于现有的度量学习的损失，而不会影响模型的大小，训练速度，或优化的难度。最后，嵌入膨胀和代表度量学习损失的组合优于国家的最先进的损失和在这两个图像检索先前样本的生成方法和聚类任务。实施将是公开的。</font>
</div>


<hr>
<div id="paper12"> <b>12. A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2003.02541" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunbo Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dapeng Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ran He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiashi Feng</a><br>
<font size="3">
Abstract: This work addresses the unsupervised domain adaptation problem, especially for the partial scenario where the class labels in the target domain are only a subset of those in the source domain. Such a partial transfer setting sounds realistic but challenging while existing methods always suffer from two key problems, i.e., negative transfer and uncertainty propagation. In this paper, we build on domain adversarial learning and propose a novel domain adaptation method BA$^3$US with two new techniques termed Balanced Adversarial Alignment (BAA) and Adaptive Uncertainty Suppression (AUS), respectively. On one hand, negative transfer results in that target samples are misclassified to the classes only present in the source domain. To address this issue, BAA aims to pursue the balance between label distributions across domains in a quite simple manner. Specifically, it randomly leverages a few source samples to augment the smaller target domain during domain alignment so that classes in different domains are symmetric. On the other hand, a source sample is denoted as uncertain if there is an incorrect class that has a relatively high prediction score. Such uncertainty is easily propagated to the unlabeled target data around it during alignment, which severely deteriorates the adaptation performance. Thus, AUS emphasizes uncertain samples and exploits an adaptive weighted complement entropy objective to expect that incorrect classes have the uniform and low prediction scores. Experimental results on multiple benchmarks demonstrate that BA$^3$US surpasses state-of-the-arts for partial domain adaptation tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：这项工作解决了无人监管的领域适应性问题，尤其是对于部分场景在目标域中的类标签只有那些在源域的一个子集。这样的部分传送设定听起来现实但挑战而现有的方法总是从两个关键问题，即，负转移和不确定性传播受到影响。在本文中，我们建立域对抗性学习，提出了一种新的领域适应性方法BA $ ^ 3 $美国有两个新技术称为平衡对抗性对齐（BAA）和自适应不确定性抑制（AUS），分别。一方面，该目标样品中的负转印结果错误分类到只存在于所述源域的类。为了解决这个问题，BAA的目的是追求跨域标签分布之间的平衡在一个相当简单的方式。具体而言，利用随机数源样本，以增加结构域对准，使得在不同的域中的类是对称的过程中更小的目标域。在另一方面中，源样本被表示为不确定的，如果存在具有相对高的预测分数的不正确的类。这种不确定性是很容易传播到对准期间它周围的未标记的目标数据，这严重损害了适应性能。因此，AUS强调不确定样品并利用自适应加权补熵客观地预期不正确类具有均匀且低预测得分。在多个基准测试实验结果表明，BA $ ^ 3的国家的最艺术的一部分的区域适应任务$ US反超。</font>
</div>


<hr>
<div id="paper13"> <b>13. Detecting Attended Visual Targets in Video</b>  <a href="https://arxiv.org/pdf/2003.02501" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chong%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eunji Chong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongxin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruiz%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nataniel Ruiz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rehg%2C+J+M" target="_blank" rel="noopener" style="color:#0000EE;">James M. Rehg</a><br>
<font size="3">
Abstract: We address the problem of detecting attention targets in video. Specifically, our goal is to identify where each person in each frame of a video is looking, and correctly handle the out-of-frame case. Our novel architecture effectively models the dynamic interaction between the scene and head features in order to infer time-varying attention targets. We introduce a new dataset, VideoAttentionTarget, consisting of fully-annotated video clips containing complex and dynamic patterns of real-world gaze behavior. Experiments on this dataset show that our model can effectively infer attention in videos. To further demonstrate the utility of our approach, we apply our predicted attention maps to two social gaze behavior recognition tasks, and show that the resulting classifiers significantly outperform existing methods. We achieve state-of-the-art performance on three datasets: GazeFollow (static images), VideoAttentionTarget (videos), and VideoCoAtt (videos), and obtain the first results for automatically classifying clinically-relevant gaze behavior without wearable cameras or eye trackers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：解决视频检测关注目标的问题。具体来说，我们的目标是确定每个人在视频的每一帧寻找，正确处理了框外的情况。我们新颖的架构有效模式，以推断随时间变化的关注目标的场景和头部特征之间的动态交互。我们推出了新的数据集，VideoAttentionTarget，由含有真实世界的目光行为复杂和动态模式完全注释的视频剪辑。在此数据集上的实验，我们的模型能够有效地推断出影片的关注。为了进一步证明了该方法的实用性，我们应用我们的预测关注映射到两个社会的注视行为识别任务，并表明，所产生的分类显著优于现有的方法。我们对三个数据集中实现国家的最先进的性能：GazeFollow（静态图像），VideoAttentionTarget（视频）和VideoCoAtt（视频），并获得第一个结果，而不穿戴式摄像机或眼动仪自动分类临床相关的注视行为。</font>
</div>


<hr>
<div id="paper14"> <b>14. Adversarial Vertex Mixup: Toward Better Adversarially Robust  Generalization</b>  <a href="https://arxiv.org/pdf/2003.02484" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Saehyung Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyungyu Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yoon%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sungroh Yoon</a><br>
<font size="3">
Abstract: Adversarial examples cause neural networks to produce incorrect outputs with high confidence. Although adversarial training is one of the most effective forms of defense against adversarial examples, unfortunately, a large gap exists between test accuracy and training accuracy in adversarial training. In this paper, we identify Adversarial Feature Overfitting (AFO), which may cause poor adversarially robust generalization, and we show that adversarial training can overshoot the optimal point in terms of robust generalization, leading to AFO in our simple Gaussian model. Considering these theoretical results, we present soft labeling as a solution to the AFO problem. Furthermore, we propose Adversarial Vertex mixup (AVmixup), a soft-labeled data augmentation approach for improving adversarially robust generalization. We complement our theoretical analysis with experiments on CIFAR10, CIFAR100, SVHN, and Tiny ImageNet, and show that AVmixup significantly improves the robust generalization performance and that it reduces the trade-off between standard accuracy and adversarial robustness. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对抗性的例子引起神经网络，产生高可信度不正确的输出。虽然对抗训练是针对敌对例子最有效的形式防御的，不幸的是，测试的准确性和训练精度在对抗训练之间存在很大的差距。在本文中，我们确定对抗性功能过度拟合（AFO），这可能会导致较差adversarially强大的泛化，我们表明，对抗性训练可以超调的最佳点在强大的推广方面，导致AFO我们简单的高斯模型。考虑到这些理论成果，提出了软标签作为一个解决问题的AFO。此外，建议对抗性顶点的mixup（AVmixup），用于提高adversarially强大的推广软标签的数据增强方法。我们补充我们对CIFAR10，CIFAR100，SVHN和微型ImageNet实验的理论分析，并表明AVmixup显著提高了稳健的泛化性能，它降低了标准精度和对抗性鲁棒性之间进行权衡。</font>
</div>


<hr>
<div id="paper15"> <b>15. Fake Generated Painting Detection via Frequency Analysis</b>  <a href="https://arxiv.org/pdf/2003.02467" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanfang Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinjie Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Lu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunhong Wang</a><br>
<font size="3">
Abstract: With the development of deep neural networks, digital fake paintings can be generated by various style transfer this http URL detect the fake generated paintings, we analyze the fake generated and real paintings in Fourier frequency domain and observe statistical differences and artifacts. Based on our observations, we propose Fake Generated Painting Detection via Frequency Analysis (FGPD-FA) by extracting three types of features in frequency domain. Besides, we also propose a digital fake painting detection database for assessing the proposed method. Experimental results demonstrate the excellence of the proposed method in different testing conditions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着深层神经网络的发展，可以通过各种风格的转移而产生的数字假画这个HTTP URL检测假画产生，我们分析了傅立叶频域假产生和真正的绘画和观察统计差异和文物。根据我们的观察，我们提出假生成在频域提取三种类型的特性通过绘画频率分析（FGPD-FA）检测。此外，我们还提出了一个数字假画检测数据库来评估所提出的方法。实验结果表明，在不同的测试条件下所提出的方法的卓越。</font>
</div>


<hr>
<div id="paper16"> <b>16. Cluster Pruning: An Efficient Filter Pruning Method for Edge AI Vision  Applications</b>  <a href="https://arxiv.org/pdf/2003.02449" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gamanayake%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chinthaka Gamanayake</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jayasinghe%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lahiru Jayasinghe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benny Ng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuen%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chau Yuen</a><br>
<font size="3">
Abstract: Even though the Convolutional Neural Networks (CNN) has shown superior results in the field of computer vision, it is still a challenging task to implement computer vision algorithms in real-time at the edge, especially using a low-cost IoT device due to high memory consumption and computation complexities in a CNN. Network compression methodologies such as weight pruning, filter pruning, and quantization are used to overcome the above mentioned problem. Even though filter pruning methodology has shown better performances compared to other techniques, irregularity of the number of filters pruned across different layers of a CNN might not comply with majority of the neural computing hardware architectures. In this paper, a novel greedy approach called cluster pruning has been proposed, which provides a structured way of removing filters in a CNN by considering the importance of filters and the underlying hardware architecture. The proposed methodology is compared with the conventional filter pruning algorithm on Pascal-VOC open dataset, and Head-Counting dataset, which is our own dataset developed to detect and count people entering a room. We benchmark our proposed method on three hardware architectures, namely CPU, GPU, and Intel Movidius Neural Computer Stick (NCS) using the popular SSD-MobileNet and SSD-SqueezeNet neural network architectures used for edge-AI vision applications. Results demonstrate that our method outperforms the conventional filter pruning methodology, using both datasets on above mentioned hardware architectures. Furthermore, a low cost IoT hardware setup consisting of an Intel Movidius-NCS is proposed to deploy an edge-AI application using our proposed pruning methodology. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管卷积神经网络（CNN）已经显示在计算机视觉领域卓越的成绩，但仍处于边缘实现实时计算机视觉算法，尤其是在使用，由于低成本的物联网设备一项艰巨的任务在CNN的高内存消耗和计算复杂性。网络压缩方法，如重修剪，滤波器修剪，并且量化用于克服上述问题。即使过滤器修剪方法已与其他技术相比显示更好的性能，跨越一个CNN的不同层次修剪过滤器的数量的不规则性可能不符合广大神经计算的硬件架构。在本文中，一种新颖的贪婪方法称为群集修剪已经提出，其提供通过考虑滤波器的重要性和底层硬件架构除去在CNN滤波器的结构的方式。所提出的方法与帕斯卡-VOC打开的数据集以往的滤波修正算法进行比较，头计数数据集，这是我们自己开发的检测和计数进入房间的人数据集。我们的基准使用三个硬件架构，即CPU，GPU和Intel Movidius神经计算机棒（NCS），我们提出的方法流行的SSD-MobileNet和SSD-SqueezeNet用于边缘-AI视觉应用神经网络结构。结果表明，我们的方法优于传统的过滤器修剪方法，使用上文提到的硬件架构两个数据集。此外，由英特尔Movidius-NCS的低成本物联网硬件设置建议使用我们提出的修剪方法来部署边缘AI应用。</font>
</div>


<hr>
<div id="paper17"> <b>17. End-to-End Trainable One-Stage Parking Slot Detection Integrating Global  and Local Information</b>  <a href="https://arxiv.org/pdf/2003.02445" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Suhr%2C+J+K" target="_blank" rel="noopener" style="color:#0000EE;">Jae Kyu Suhr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+H+G" target="_blank" rel="noopener" style="color:#0000EE;">Ho Gi Jung</a><br>
<font size="3">
Abstract: This paper proposes an end-to-end trainable one-stage parking slot detection method for around view monitor (AVM) images. The proposed method simultaneously acquires global information (entrance, type, and occupancy of parking slot) and local information (location and orientation of junction) by using a convolutional neural network (CNN), and integrates them to detect parking slots with their properties. This method divides an AVM image into a grid and performs a CNN-based feature extraction. For each cell of the grid, the global and local information of the parking slot is obtained by applying convolution filters to the extracted feature map. Final detection results are produced by integrating the global and local information of the parking slot through non-maximum suppression (NMS). Since the proposed method obtains most of the information of the parking slot using a fully convolutional network without a region proposal stage, it is an end-to-end trainable one-stage detector. In experiments, this method was quantitatively evaluated using the public dataset and outperforms previous methods by showing both recall and precision of 99.77%, type classification accuracy of 100%, and occupancy classification accuracy of 99.31% while processing 60 frames per second. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出的端至端可训练一阶段停车间隙检测周边视监视器（AVM）图像的方法。该方法同时获取全球信息（入口，类型和停车位的占用），并通过使用卷积神经网络（CNN）的本地信息（位置和路口方向），并集成他们检测停车位与他们的财产。此方法共分AVM图像划分为网格，执行基于CNN-特征提取。为网格的每个单元中，通过施加卷积滤波器所提取的特征地图获得的停车位的全局和局部信息。最终检测结果通过通过非最大值抑制（NMS）积分停车时隙的全局和局部信息产生的。由于所提出的方法获得的大部分使用全卷积网络停车时隙的信息，而无需一个区域提案阶段，它是一个端至端可训练一阶段检测器。在实验中，该方法是使用公共数据集定量评价和优于通过示出在处理每秒60帧两者召回和99.77％的精度，100％类型分类准确性和99.31％占用分类精度以前的方法。</font>
</div>


<hr>
<div id="paper18"> <b>18. Drone Based RGBT Vehicle Detection and Counting: A Challenge</b>  <a href="https://arxiv.org/pdf/2003.02437" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiming Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Longyin Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghua Hu</a><br>
<font size="3">
Abstract: Camera-equipped drones can capture targets on the ground from a wider field of view than static cameras or moving sensors over the ground. In this paper we present a large-scale vehicle detection and counting benchmark, named DroneVehicle, aiming at advancing visual analysis tasks on the drone platform. The images in the benchmark were captured over various urban areas, which include different types of urban roads, residential areas, parking lots, highways, etc., from day to night. Specifically, DroneVehicle consists of 15,532 pairs of images, i.e., RGB images and infrared images with rich annotations, including oriented object bounding boxes, object categories, etc. With intensive amount of effort, our benchmark has 441,642 annotated instances in 31,064 images. As a large-scale dataset with both RGB and thermal infrared (RGBT) images, the benchmark enables extensive evaluation and investigation of visual analysis algorithms on the drone platform. In particular, we design two popular tasks with the benchmark, including object detection and object counting. All these tasks are extremely challenging in the proposed dataset due to factors such as illumination, occlusion, and scale variations. We hope the benchmark largely boost the research and development in visual analysis on drone platforms. The DroneVehicle dataset can be download from this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：使用带相机的无人驾驶飞机可以从视图比静态照相机或在地面上移动的传感器更宽的视场在地面上捕获的目标。在本文中，我们提出了一种大型车辆检测和计数基准，命名DroneVehicle，针对无人机平台上推进可视化分析的任务。在基准图像在不同的城市地区，其中包括不同类型的城市道路，居住区，停车场，高速公路等，从白天到夜晚被抓获。具体而言，DroneVehicle由15532双图像，即，RGB图像和红外图像的具有丰富的注解，包括面向对象的包围盒，对象类别等等随着努力密集量，我们的基准具有31064幅图像441642个注释实例。如RGB和热红外（RGBT）图像的大规模数据集，基准使广泛的评估和无人驾驶飞机平台上视觉分析算法调查。特别是，我们设计了一个标杆两个流行的任务，包括目标检测和计数的对象。所有这些任务都极其在所提出的数据集由于因素如照明，闭塞，和尺度变化挑战。我们希望基准很大程度上提振可视化分析的研究和开发无人机平台。该DroneVehicle数据集可以从这个HTTPS URL下载。</font>
</div>


<hr>
<div id="paper19"> <b>19. Who Make Drivers Stop? Towards Driver-centric Risk Assessment: Risk  Object Identification via Causal Inference</b>  <a href="https://arxiv.org/pdf/2003.02425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengxi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+S+H" target="_blank" rel="noopener" style="color:#0000EE;">Stanley H. Chan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi-Ting Chen</a><br>
<font size="3">
Abstract: We propose a framework based on causal inference for risk object identification, an essential task towards driver-centric risk assessment. In this work, risk objects are defined as objects influencing driver's goal-oriented behavior. There are two limitations of the existing approaches. First, they require strong supervisions such as risk object location or human gaze location. Second, there is no explicit reasoning stage for identifying risk object. To address these issues, the task of identifying causes of driver behavioral change is formalized in the language of functional causal models and interventions. Specifically, we iteratively simulate causal effect by removing an object using the proposed driving model. The risk object is determined as the one causing the most substantial causal effect. We evaluate the proposed framework on the Honda Research Institute Driving Dataset (HDD). The dataset provides the annotation for risk object localization to enable systematic benchmarking with existing approaches. Our framework demonstrates a substantial average performance boost over a strong baseline by 7.5%. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提出了一种基于因果推理风险对象标识，向驾驶员为中心的风险评估的一项重要任务的框架。在这项工作中，危险物品被定义为影响驾驶者的目标导向行为的对象。有现有的方法中的两个限制。首先，他们需要强有力的监督，如风险对象的位置或人的注视位置。其次，对于风险识别对象没有明确的论证阶段。为了解决这些问题，识别驾驶行为变化的原因的任务在功能因果模型和干预的语言形式化。具体而言，通过去除使用所提出的驱动模型中的对象，我们迭代模拟因果效应。风险对象被确定为一个引起的最实质性的因果关系。我们评估对本田研究所驾驶数据集（HDD）所提出的框架。该数据集提供了风险目标定位标注，以实现与现有方法系统化基准的。我们的框架展示了一个强有力的基线7.5％大幅平均性能提升。</font>
</div>


<hr>
<div id="paper20"> <b>20. A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI</b>  <a href="https://arxiv.org/pdf/2003.02371" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Behley%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jens Behley</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milioto%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andres Milioto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stachniss%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cyrill Stachniss</a><br>
<font size="3">
Abstract: Panoptic segmentation is the recently introduced task that tackles semantic segmentation and instance segmentation jointly. In this paper, we present an extension of SemanticKITTI, which is a large-scale dataset providing dense point-wise semantic labels for all sequences of the KITTI Odometry Benchmark, for training and evaluation of laser-based panoptic segmentation. We provide the data and discuss the processing steps needed to enrich a given semantic annotation with temporally consistent instance information, i.e., instance information that supplements the semantic labels and identifies the same instance over sequences of LiDAR point clouds. Additionally, we present two strong baselines that combine state-of-the-art LiDAR-based semantic segmentation approaches with a state-of-the-art detector enriching the segmentation with instance information and that allow other researchers to compare their approaches against. We hope that our extension of SemanticKITTI with strong baselines enables the creation of novel algorithms for LiDAR-based panoptic segmentation as much as it has for the original semantic segmentation and semantic scene completion tasks. Data, code, and an online evaluation using a hidden test set will be published on this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：全景分割是最近推出的任务铲球语义分割和实例分割共同所有。在本文中，我们提出SemanticKITTI，这是一个大型数据集为KITTI里程计基准的所有序列，用于基于激光的全景分割的训练和评估提供致密的逐点的语义标签的延伸。我们所提供的数据和讨论，以丰富给定的语义标注与时间一致的情况下的信息，即，例如信息所需要的处理步骤，以补充语义标签和识别在激光雷达点云的序列相同的实例。此外，我们提出结合国家的最先进的基于激光雷达的语义分割与国家的最先进的探测器以丰富的实例信息，并允许其他研究人员对他们的做法比较接近分割两个强基线。我们希望我们的SemanticKITTI的扩展具有很强的基准使基于激光雷达的全景分割创造新的算法，它更具有原始语义分割和语义现场完成的任务。数据，代码，并使用一个隐藏的测试集在线评估将在此HTTP URL发布。</font>
</div>


<hr>
<div id="paper21"> <b>21. Towards Fair Cross-Domain Adaptation via Generative Learning</b>  <a href="https://arxiv.org/pdf/2003.02366" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tongxin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengming Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Shao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haixu Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Huang</a><br>
<font size="3">
Abstract: Domain Adaptation (DA) targets at adapting a model trained over the well-labeled source domain to the unlabeled target domain lying in different distributions. Existing DA normally assumes the well-labeled source domain is class-wise balanced, which means the size per source class is relatively similar. However, in real-world applications, labeled samples for some categories in the source domain could be extremely few due to the difficulty of data collection and annotation, which leads to decreasing performance over target domain on those few-shot categories. To perform fair cross-domain adaptation and boost the performance on these minority categories, we develop a novel Generative Few-shot Cross-domain Adaptation (GFCA) algorithm for fair cross-domain classification. Specifically, generative feature augmentation is explored to synthesize effective training data for few-shot source classes, while effective cross-domain alignment aims to adapt knowledge from source to facilitate the target learning. Experimental results on two large cross-domain visual datasets demonstrate the effectiveness of our proposed method on improving both few-shot and overall classification accuracy comparing with the state-of-the-art DA approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在适应培训了良好标记源域到未标记的目标域躺在不同的分布模型领域适应性（DA）的目标。现有DA通常假定井标记的源域是类明智平衡，这意味着每个源类大小相对类似。某些类别的源域然而，在实际应用中，标记的样品可能是极少的，由于数据收集和注释，这将导致在那些极少数次类下降超过目标域性能的难度。为了进行公平的跨域适应和提高这些少数类别的表现，我们开发了一个新的剖成很少拍跨域适应（GFCA）算法公平跨域分类。具体来说，生成功能增强是探索以综合几拍源类有效的训练数据，而有效的跨域对准目标，从源头适应知识，以促进目标的学习。在两个大的跨域视觉数据集实验结果表明，我们所提出的改善都很少拍和总体分类精度与国家的最先进的方法DA比较方法的有效性。</font>
</div>


<hr>
<div id="paper22"> <b>22. Creating High Resolution Images with a Latent Adversarial Generator</b>  <a href="https://arxiv.org/pdf/2003.02365" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Berthelot%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Berthelot</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milanfar%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peyman Milanfar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goodfellow%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ian Goodfellow</a><br>
<font size="3">
Abstract: Generating realistic images is difficult, and many formulations for this task have been proposed recently. If we restrict the task to that of generating a particular class of images, however, the task becomes more tractable. That is to say, instead of generating an arbitrary image as a sample from the manifold of natural images, we propose to sample images from a particular "subspace" of natural images, directed by a low-resolution image from the same subspace. The problem we address, while close to the formulation of the single-image super-resolution problem, is in fact rather different. Single image super-resolution is the task of predicting the image closest to the ground truth from a relatively low resolution image. We propose to produce samples of high resolution images given extremely small inputs with a new method called Latent Adversarial Generator (LAG). In our generative sampling framework, we only use the input (possibly of very low-resolution) to direct what class of samples the network should produce. As such, the output of our algorithm is not a unique image that relates to the input, but rather a possible se} of related images sampled from the manifold of natural images. Our method learns exclusively in the latent space of the adversary using perceptual loss -- it does not have a pixel loss. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生成逼真的图像是困难的，这个任务很多配方近来已经提出。如果我们限制任务是产生一类特殊的图像，但是，任务变得更容易处理。也就是说，代替生成任意图像作为自然图像的歧管中的样品，我们建议的样本图像从自然图像的一个特定的“子空间”，从同一子空间由低分辨率图像定向。这个问题，我们的地址，而接近单张影像超分辨率问题的提法，实际上是相当不同的。单图像超分辨率图像从一个相对较低分辨率图像预测最接近地面真理的任务。我们建议，产生了一个名为潜在对抗性生成器（LAG）的新方法给出极小输入高分辨率图像的样本。在我们的生成抽样框，我们只用了输入（可能是非常低的分辨率），以指导网络应该生产什么类别的样本。因此，我们的算法的输出是不是从自然图像的歧管采样相关的图像的唯一图像，涉及到输入，而是可能本身}。我们的方法可以学习专门在利用感知损失对手的潜在空间 - 它没有一个像素的损失。</font>
</div>


<hr>
<div id="paper23"> <b>23. Learning View and Target Invariant Visual Servoing for Navigation</b>  <a href="https://arxiv.org/pdf/2003.02327" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yimeng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kosecka%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jana Kosecka</a><br>
<font size="3">
Abstract: The advances in deep reinforcement learning recently revived interest in data-driven learning based approaches to navigation. In this paper we propose to learn viewpoint invariant and target invariant visual servoing for local mobile robot navigation; given an initial view and the goal view or an image of a target, we train deep convolutional network controller to reach the desired goal. We present a new architecture for this task which rests on the ability of establishing correspondences between the initial and goal view and novel reward structure motivated by the traditional feedback control error. The advantage of the proposed model is that it does not require calibration and depth information and achieves robust visual servoing in a variety of environments and targets without any parameter fine tuning. We present comprehensive evaluation of the approach and comparison with other deep learning architectures as well as classical visual servoing methods in visually realistic simulation environment. The presented model overcomes the brittleness of classical visual servoing based methods and achieves significantly higher generalization capability compared to the previous learning approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深强化学习基于数据驱动的学习方法，以导航最近重新兴趣进展。在本文中，我们提出了学习的观点不变，并针对本地移动机器人导航不变的视觉伺服;给定的初始视图和目标视图或目标的图像，我们培养深卷积网络控制器以达到期望的目标。我们提出了一个新的架构完成这个任务，它靠在建立由传统的反馈控制误差激励初始和目标视图和新的奖励结构之间的对应关系的能力。该模型的优点是，它不需要校准和深度信息和在各种环境和目标的实现强劲的视觉伺服不带任何参数的微调。我们目前的做法，并与其他深度学习架构以及在逼真的模拟环境，经典视觉伺服方法相比，综合评价。所提出的模型克服相比之前的学习方法的经典基于视觉伺服的方法和实现显著较高泛化能力的脆性。</font>
</div>


<hr>
<div id="paper24"> <b>24. The Impact of Hole Geometry on Relative Robustness of In-Painting  Networks: An Empirical Study</b>  <a href="https://arxiv.org/pdf/2003.02314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mortazavi%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Masood S. Mortazavi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ning Yan</a><br>
<font size="3">
Abstract: In-painting networks use existing pixels to generate appropriate pixels to fill "holes" placed on parts of an image. A 2-D in-painting network's input usually consists of (1) a three-channel 2-D image, and (2) an additional channel for the "holes" to be in-painted in that image. In this paper, we study the robustness of a given in-painting neural network against variations in hole geometry distributions. We observe that the robustness of an in-painting network is dependent on the probability distribution function (PDF) of the hole geometry presented to it during its training even if the underlying image dataset used (in training and testing) does not alter. We develop an experimental methodology for testing and evaluating relative robustness of in-painting networks against four different kinds of hole geometry PDFs. We examine a number of hypothesis regarding (1) the natural bias of in-painting networks to the hole distribution used for their training, (2) the underlying dataset's ability to differentiate relative robustness as hole distributions vary in a train-test (cross-comparison) grid, and (3) the impact of the directional distribution of edges in the holes and in the image dataset. We present results for L1, PSNR and SSIM quality metrics and develop a specific measure of relative in-painting robustness to be used in cross-comparison grids based on these quality metrics. (One can incorporate other quality metrics in this relative measure.) The empirical work reported here is an initial step in a broader and deeper investigation of "filling the blank" neural networks' sensitivity, robustness and regularization with respect to hole "geometry" PDFs, and it suggests further research in this domain. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：绘画网络使用现有像素以生成适当像素以填充放置在图像的部分的“洞”。在绘画网络的输入A 2-d通常包括（1）三通道2-d的图像，以及（2）对于“孔”的附加信道是在涂该映像中。在本文中，我们研究了在绘画针对孔几何分布变化的神经网络给出的鲁棒性。我们观察到的在绘画网络的健壮性是其训练过程中依赖于提交给它的孔几何形状的概率分布函数（PDF），即使使用的底层图像数据集（在训练和测试）不会改变。我们开发测试和评估的绘画对四种不同的孔几何PDF文件的网络相对稳健性的实验方法。我们研究关于数假设（1）的自然偏压在绘画网络（2）用于其训练的孔的分布，潜在的数据集的区分相对鲁棒性孔分布在列车试验而变化的能力（交比较）的网格，和（3）的在孔和图像数据组中的边缘的方向分布的影响。为L1，PSNR和SSIM质量度量，我们目前的研究结果，开发的相对在绘画的鲁棒性在交叉比较网格基于这些质量度量被使用的特定量度。 （可以以此相对量度掺入其他质量度量。）的经验性工作这里报告处于“填充空白”神经网络的灵敏度，鲁棒性和正规化相对于孔‘几何’的PDF更广泛和更深入的调查的初始步骤和它表明在这一领域的进一步研究。</font>
</div>


<hr>
<div id="paper25"> <b>25. Exploring Partial Intrinsic and Extrinsic Symmetry in 3D Medical Imaging</b>  <a href="https://arxiv.org/pdf/2003.02294" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fotouhi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Javad Fotouhi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Taylor%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giacomo Taylor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Unberath%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mathias Unberath</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alex Johnson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S+C" target="_blank" rel="noopener" style="color:#0000EE;">Sing Chun Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Osgood%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Greg Osgood</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Armand%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehran Armand</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Navab%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nassir Navab</a><br>
<font size="3">
Abstract: We present a novel methodology to detect imperfect bilateral symmetry in CT of human anatomy. In this paper, the structurally symmetric nature of the pelvic bone is explored and is used to provide interventional image augmentation for treatment of unilateral fractures in patients with traumatic injuries. The mathematical basis of our solution is on the incorporation of attributes and characteristics that satisfy the properties of intrinsic and extrinsic symmetry and are robust to outliers. In the first step, feature points that satisfy intrinsic symmetry are automatically detected in the Möbius space defined on the CT data. These features are then pruned via a two-stage RANSAC to attain correspondences that satisfy also the extrinsic symmetry. Then, a disparity function based on Tukey's biweight robust estimator is introduced and minimized to identify a symmetry plane parametrization that yields maximum contralateral similarity. Finally, a novel regularization term is introduced to enhance similarity between bone density histograms across the partial symmetry plane, relying on the important biological observation that, even if injured, the dislocated bone segments remain within the body. Our extensive evaluations on various cases of common fracture types demonstrate the validity of the novel concepts and the robustness and accuracy of the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种新的方法来检测不完善的两侧对称的人体解剖结构的CT。在本文中，骨盆骨的结构上对称的性质探索和用于提供用于治疗患者的创伤性损伤单方面骨折的介入图像增强。我们的解决方案的数学基础是满足内在和外在的对称特性，是稳健的异常值属性和特征的结合。在第一步骤中，满足固有对称性的特征点中的CT数据定义的莫比乌斯空间被自动检测。这些特征随后经由两级RANSAC修剪实现这一也满足外在对称性对应关系。然后，根据杜克的biweight强大的估计悬殊功能介绍和最小化的识别对称平面参数化能产生最大对侧相似。最后，一种新颖的正则化项被引入，以提高骨密度直方图之间的相似性在整个局部对称平面，依靠重要生物观测的是，即使受伤，脱臼骨段保留在体内。我们的共同骨折类型的各种情况下广泛评价显示的新颖概念所提出的方法的有效性和鲁棒性和准确性。</font>
</div>


<hr>
<div id="paper26"> <b>26. Event-Based Angular Velocity Regression with Spiking Networks</b>  <a href="https://arxiv.org/pdf/2003.02790" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gehrig%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mathias Gehrig</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shrestha%2C+S+B" target="_blank" rel="noopener" style="color:#0000EE;">Sumit Bam Shrestha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mouritzen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Mouritzen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scaramuzza%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Scaramuzza</a><br>
<font size="3">
Abstract: Spiking Neural Networks (SNNs) are bio-inspired networks that process information conveyed as temporal spikes rather than numeric values. A spiking neuron of an SNN only produces a spike whenever a significant number of spikes occur within a short period of time. Due to their spike-based computational model, SNNs can process output from event-based, asynchronous sensors without any pre-processing at extremely lower power unlike standard artificial neural networks. This is possible due to specialized neuromorphic hardware that implements the highly-parallelizable concept of SNNs in silicon. Yet, SNNs have not enjoyed the same rise of popularity as artificial neural networks. This not only stems from the fact that their input format is rather unconventional but also due to the challenges in training spiking networks. Despite their temporal nature and recent algorithmic advances, they have been mostly evaluated on classification problems. We propose, for the first time, a temporal regression problem of numerical values given events from an event camera. We specifically investigate the prediction of the 3-DOF angular velocity of a rotating event camera with an SNN. The difficulty of this problem arises from the prediction of angular velocities continuously in time directly from irregular, asynchronous event-based input. Directly utilising the output of event cameras without any pre-processing ensures that we inherit all the benefits that they provide over conventional cameras. That is high-temporal resolution, high-dynamic range and no motion blur. To assess the performance of SNNs on this task, we introduce a synthetic event camera dataset generated from real-world panoramic images and show that we can successfully train an SNN to perform angular velocity regression. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：扣球神经网络（SNNS）是生物启发的网络处理信息的传达为颞区棘波，而不是数值。一个SNN的尖峰神经元仅产生每当发生的短时间内尖峰的显著数的峰值。由于它们基于尖峰计算模型，SNNS可以在不同于标准人工神经网络非常低功率过程中没有任何前处理从基于事件的，异步的传感器输出。这是可能的，因为专门的神经形态硬件实现硅SNNS的高度并行化的概念。然而，SNNS都没有享受过的人气一样崛起，人工神经网络。这不仅从一个事实，即他们的输入格式是相当不同寻常，但也由于在训练扣球网络的挑战茎。尽管他们的时间特性和最新算法的进步，他们已经大多的分类问题进行评估。我们建议，对于第一次，数值的时间回归问题给出从事件摄像机的事件。我们具体地研究旋转事件照相机的与SNN 3-DOF角速度的预测。这个问题的困难来自角速度的预测在时间上连续直接从不规则，异步基于事件的输入。直接利用的情况下相机的输出，无需任何预处理，确保我们继承他们提供比传统相机的所有好处。即高时间分辨率，高动态范围和没有运动模糊。为了评估此任务SNNS的性能，我们引入从现实世界的全景图像和显示，我们能够成功地训练的SNN执行角速度回归产生的合成事件相机数据集。</font>
</div>


<hr>
<div id="paper27"> <b>27. Dimensionality Reduction and Motion Clustering during Activities of  Daily Living: 3, 4, and 7 Degree-of-Freedom Arm Movements</b>  <a href="https://arxiv.org/pdf/2003.02641" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gloumakov%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Gloumakov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Spiers%2C+A+J" target="_blank" rel="noopener" style="color:#0000EE;">Adam J. Spiers</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dollar%2C+A+M" target="_blank" rel="noopener" style="color:#0000EE;">Aaron M. Dollar</a><br>
<font size="3">
Abstract: The wide variety of motions performed by the human arm during daily tasks makes it desirable to find representative subsets to reduce the dimensionality of these movements for a variety of applications, including the design and control of robotic and prosthetic devices. This paper presents a novel method and the results of an extensive human subjects study to obtain representative arm joint angle trajectories that span naturalistic motions during Activities of Daily Living (ADLs). In particular, we seek to identify sets of useful motion trajectories of the upper limb that are functions of a single variable, allowing, for instance, an entire prosthetic or robotic arm to be controlled with a single input from a user, along with a means to select between motions for different tasks. Data driven approaches are used to obtain clusters as well as representative motion averages for the full-arm 7 degree of freedom (DOF), elbow-wrist 4 DOF, and wrist-only 3 DOF motions. The proposed method makes use of well-known techniques such as dynamic time warping (DTW) to obtain a divergence measure between motion segments, DTW barycenter averaging (DBA) to obtain averages, Ward's distance criterion to build hierarchical trees, batch-DTW to simultaneously align multiple motion data, and functional principal component analysis (fPCA) to evaluate cluster variability. The clusters that emerge associate various recorded motions into primarily hand start and end location for the full-arm system, motion direction for the wrist-only system, and an intermediate between the two qualities for the elbow-wrist system. The proposed clustering methodology is justified by comparing results against alternative approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：各种各样期间的日常工作由人的手臂进行运动使得期望找到有代表性的子集，以减少这些运动的维度用于各种应用，包括设计和机器人和假肢装置的控制。本文提出了一种新颖的方法和广泛的人类受试者研究的结果，以获得跨越日常生活（ADL的）的活动期间自然运动代表臂关节角度的轨迹。特别是，我们寻求确定集，它们是单个变量的函数的上肢的有用的运动轨迹，使得，例如，一个完整的假体或机器人臂将与来自用户的单个输入用装置控制，沿运动对不同的任务之间进行选择。数据驱动方法被用于获得集群以及代表运动的平均值为（DOF），肘腕4 DOF，和腕只有3 DOF运动全臂7自由度。所提出的方法使用公知的技术，例如动态时间规整（DTW），以获得DTW重心平均（DBA）运动节段之间的偏差的措施，以获得平均值，Ward的距离标准来构建分层树，分批DTW同时对准多个运动数据，和功能性主成分分析（FPCA）评价簇的可变性。所出现的各种关联记录运动到聚类主要手为全臂系统，运动方向的唯一手腕系统，以及两种品质的肘腕系统之间的中间开始和结束位置。所提出的聚类方法是通过比较其他方法结果合理。</font>
</div>


<hr>
<div id="paper28"> <b>28. Learning the sense of touch in simulation: a sim-to-real strategy for  vision-based tactile sensing</b>  <a href="https://arxiv.org/pdf/2003.02640" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sferrazza%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carmelo Sferrazza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Bi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=D%27Andrea%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raffaello D'Andrea</a><br>
<font size="3">
Abstract: Data-driven approaches to tactile sensing aim to overcome the complexity of accurately modeling contact with soft materials. However, their widespread adoption is impaired by concerns about data efficiency and the capability to generalize when applied to various tasks. This paper focuses on both these aspects with regard to a vision-based tactile sensor, which aims to reconstruct the distribution of the three-dimensional contact forces applied on its soft surface. Accurate models for the soft materials and the camera projection, derived via state-of-the-art techniques in the respective domains, are employed to generate a dataset in simulation. A strategy is proposed to train a tailored deep neural network entirely from the simulation data. The resulting learning architecture is directly transferable across multiple tactile sensors without further training and yields accurate predictions on real data, while showing promising generalization capabilities to unseen contact conditions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据驱动方法触觉感测的目标是克服的准确建模有软材料接触的复杂性。然而，它们的广泛应用是通过有关数据的效率，当应用于各种任务来概括能力的担忧减弱。本文围绕这两方面关于基于视觉的触觉传感器，其目的是重建三维接触力的分布施加在其柔软的表面。为软质材料和相机投影精确的模型，通过在相应的域状态的最先进的技术获得的，被用来产生在模拟的数据集。策略是提出了从模拟数据完全训练量身定做的深层神经网络。将得到的学习架构是跨多个触觉传感器直接转让无需进一步培养和产生实际数据准确的预测，同时表现出有前途的泛化能力，以看不见的接触条件。</font>
</div>


<hr>
<div id="paper29"> <b>29. Demographic Bias in Biometrics: A Survey on an Emerging Challenge</b>  <a href="https://arxiv.org/pdf/2003.02488" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Drozdowski%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">P. Drozdowski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rathgeb%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">C. Rathgeb</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dantcheva%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">A. Dantcheva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Damer%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">N. Damer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Busch%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">C. Busch</a><br>
<font size="3">
Abstract: Systems incorporating biometric technologies have become ubiquitous in personal, commercial, and governmental identity management applications. Both cooperative (e.g. access control) and non-cooperative (e.g. surveillance and forensics) systems have benefited from biometrics. Such systems rely on the uniqueness of certain biological or behavioural characteristics of human beings, which enable for individuals to be reliably recognised using automated algorithms. Recently, however, there has been a wave of public and academic concerns regarding the existence of systemic bias in automated decision systems (including biometrics). Most prominently, face recognition algorithms have often been labelled as "racist" or "biased" by the media, non-governmental organisations, and researchers alike. The main contributions of this article are: (1) an overview of the topic of algorithmic bias in the context of biometrics, (2) a comprehensive survey of the existing literature on biometric bias estimation and mitigation, (3) a discussion of the pertinent technical and social matters, and (4) an outline of the remaining challenges and future work items, both from technological and social points of view. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结合生物识别技术系统已经在个人，商业和政府的身份管理应用程序变得无处不在。二者合作（例如访问控制）和非合作（例如监测和取证）系统已经从生物特征中获益。这样的系统依赖于的人类的某些生物学或行为特征，它启用对使用自动算法来可靠地识别个体唯一性。然而，最近出现了关于系统性偏差的自动决策系统的存在（包括生物）公共和学术方面的一浪。最突出的是，面部识别算法常常被贴上“种族主义”或通过媒体，非政府组织和研究人员都“有偏见”。本文的主要贡献是：（1）算法偏压的生物识别的上下文中的主题的概述，（2）对生物统计偏差估计和缓解现有文献的全面调查，（3）相关的讨论技术和社会问题，以及（4）其余的挑战和未来的工作项目，无论是从技术和社会百分点轮廓。</font>
</div>


<hr>
<div id="paper30"> <b>30. Cumulant-free closed-form formulas for some common (dis)similarities  between densities of an exponential family</b>  <a href="https://arxiv.org/pdf/2003.02469" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/math?searchtype=author&query=Nielsen%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Frank Nielsen</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Nock%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Nock</a><br>
<font size="3">
Abstract: It is well-known that the Bhattacharyya, Hellinger, Kullback-Leibler, $\alpha$-divergences, and Jeffreys' divergences between densities belonging to a same exponential family have generic closed-form formulas relying on the strictly convex and real-analytic cumulant function characterizing the exponential family. In this work, we report (dis)similarity formulas which bypass the explicit use of the cumulant function and highlight the role of quasi-arithmetic means and their multivariate mean operator extensions. In practice, these cumulant-free formulas are handy when implementing these (dis)similarities using legacy Application Programming Interfaces (APIs) since our method requires only to partially factorize the densities canonically of the considered exponential family. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：众所周知，巴氏，海林格，库勒巴克-莱布勒，$ \ $阿尔法-divergences和杰弗里斯属于同一个指数族的密度之间的分歧有通用的封闭形式的公式依赖于严格凸和现实解析累积功能特征的指数系列。在这项工作中，我们报告（DIS）的相似性公式，它们绕过明确使用累积功能和突出的准算术平均值和他们的多元平均算扩展的作用。在实践中，实现这些（DIS）在使用传统的应用程序编程接口（API），因为我们的方法只需要对部分比化密度标准地所考虑的指数家族的相似性，这些无累积量公式是得心应手。</font>
</div>


<hr>
<div id="paper31"> <b>31. Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging</b>  <a href="https://arxiv.org/pdf/2003.02438" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Lamba%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Lamba</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kranthi Kumar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mitra%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaushik Mitra</a><br>
<font size="3">
Abstract: Light Field (LF) offers unique advantages such as post-capture refocusing and depth estimation, but low-light conditions limit these capabilities. To restore low-light LFs we should harness the geometric cues present in different LF views, which is not possible using single-frame low-light enhancement techniques. We, therefore, propose a deep neural network for Low-Light Light Field (L3F) restoration, which we refer to as L3Fnet. The proposed L3Fnet not only performs the necessary visual enhancement of each LF view but also preserves the epipolar geometry across views. We achieve this by adopting a two-stage architecture for L3Fnet. Stage-I looks at all the LF views to encode the LF geometry. This encoded information is then used in Stage-II to reconstruct each LF view. To facilitate learning-based techniques for low-light LF imaging, we collected a comprehensive LF dataset of various scenes. For each scene, we captured four LFs, one with near-optimal exposure and ISO settings and the others at different levels of low-light conditions varying from low to extreme low-light settings. The effectiveness of the proposed L3Fnet is supported by both visual and numerical comparisons on this dataset. To further analyze the performance of low-light reconstruction methods, we also propose an L3F-wild dataset that contains LF captured late at night with almost zero lux values. No ground truth is available in this dataset. To perform well on the L3F-wild dataset, any method must adapt to the light level of the captured scene. To do this we propose a novel pre-processing block that makes L3Fnet robust to various degrees of low-light conditions. Lastly, we show that L3Fnet can also be used for low-light enhancement of single-frame images, despite it being engineered for LF data. We do so by converting the single-frame DSLR image into a form suitable to L3Fnet, which we call as pseudo-LF. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：光场（LF）优惠，例如捕获后重新聚焦和深度估计，但低光照条件下的独特优势限制了这些功能。要恢复的低光-LF类我们应该利用几何线索存在于不同LF视图，使用单帧低光增强技术，该技术是不可能的。因此，我们提出了弱光光场（L3F）恢复，这是我们称之为L3Fnet了深刻的神经网络。所提出的L3Fnet不仅执行每个LF鉴于必要的视觉增强，但还保留了跨观点对极几何。我们采用了L3Fnet两级架构实现这一目标。舞台我看所有的LF意见来编码LF几何。此经编码的信息，然后在阶段-II用于重建每个LF图。为了方便弱光成像LF基于学习的技术，我们收集了各种场景的综合LF数据集。对于每一个场景，我们掌握四个LF类，一个以接近最佳曝光和ISO设置，和其他人不同层次的低光照条件下，从不同的低到极低的照明设置。所提出的L3Fnet的有效性是通过在此数据集可视和数值比较的支持。为了进一步分析的低光重构方法的性能，我们还提出了一个包含LF后期拍摄，晚上几乎为零照度值的L3F野生的数据集。没有地面实况在此数据集是可用的。要在L3F野生集表现良好，任何方法都必须适应拍摄场景的光线水平。要做到这一点，我们提出了一种新颖的前处理块，使得L3Fnet坚固以不同程度的低光照条件下。最后，我们表明，L3Fnet也可用于单个帧图像的低光增强，尽管它是专为LF数据。我们通过单帧DSLR图像转换成适合于L3Fnet一种形式，我们称之为伪-LF这样做。</font>
</div>


<hr>
<div id="paper32"> <b>32. Team O2AS at the World Robot Summit 2018: An Approach to Robotic Kitting  and Assembly Tasks using General Purpose Grippers and Tools</b>  <a href="https://arxiv.org/pdf/2003.02427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Drigalski%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Felix von Drigalski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nakashima%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chisato Nakashima</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shibata%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoshiya Shibata</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Konishi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoshinori Konishi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Triyonoputro%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">Joshua C. Triyonoputro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaidi Nie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Petit%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Damien Petit</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ueshiba%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Toshio Ueshiba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Takase%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryuichi Takase</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Domae%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yukiyasu Domae</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yoshioka%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taku Yoshioka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ijiri%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoshihisa Ijiri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramirez-Alpizar%2C+I+G" target="_blank" rel="noopener" style="color:#0000EE;">Ixchel G. Ramirez-Alpizar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weiwei Wan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Harada%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kensuke Harada</a><br>
<font size="3">
Abstract: We propose a versatile robotic system for kitting and assembly tasks which uses no jigs or commercial tool changers. Instead of specialized end effectors, it uses its two-finger grippers to grasp and hold tools to perform subtasks such as screwing and suctioning. A third gripper is used as a precision picking and centering tool, and uses in-built passive compliance to compensate for small position errors and uncertainty. A novel grasp point detection for bin picking is described for the kitting task, using a single depth map. Using the proposed system we competed in the Assembly Challenge of the Industrial Robotics Category of the World Robot Challenge at the World Robot Summit 2018, obtaining 4th place and the SICE award for lean design and versatile tool use. We show the effectiveness of our approach through experiments performed during the competition. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了一个多功能机器人系统的配套和组装任务，它不使用夹具或商业换刀。而不是专门的末端执行器，它使用两个手指夹爪抓住不放工具来执行的子任务，诸如螺丝及抽吸。第三夹具被用作精密采摘和居中工具，内置被动地顺从的用途，以弥补小位置误差和不确定性。针对备料任务描述了一种新的把握点检测为仓采摘，使用单一的深度图。利用所提出的系统，我们在世界机器人挑战赛的工业机器人范畴大会挑战竞争在世界机器人峰会2018年，获得第4名和精益设计和灵活的工具使用SICE奖。我们证明我们的方法，通过在比赛中进行实验的有效性。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-03-06</title>
    <url>/2020/03/06/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-06/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> An Empirical Accuracy Law for Sequential Machine Translation: the Case  of Google Translate <a href="https://arxiv.org/pdf/2003.02817" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> HypoNLI: Exploring the Artificial Patterns of Hypothesis-only Bias in  Natural Language Inference <a href="https://arxiv.org/pdf/2003.02756" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Zero-Shot Cross-Lingual Transfer with Meta Learning <a href="https://arxiv.org/pdf/2003.02739" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Fact Check-Worthiness Detection as Positive Unlabelled Learning <a href="https://arxiv.org/pdf/2003.02736" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> SentenceMIM: A Latent Variable Language Model <a href="https://arxiv.org/pdf/2003.02645" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> RecipeGPT: Generative Pre-training Based Cooking Recipe Generation and  Evaluation System <a href="https://arxiv.org/pdf/2003.02498" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Kleister: A novel task for Information Extraction involving Long  Documents with Complex Layout <a href="https://arxiv.org/pdf/2003.02356" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> A Study on Efficiency, Accuracy and Document Structure for Answer  Sentence Selection <a href="https://arxiv.org/pdf/2003.02349" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward <a href="https://arxiv.org/pdf/2003.02738" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Phase transitions in a decentralized graph-based approach to human  language <a href="https://arxiv.org/pdf/2003.02639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> An Incremental Explanation of Inference in Hybrid Bayesian Networks for  Increasing Model Trustworthiness and Supporting Clinical Decision Making <a href="https://arxiv.org/pdf/2003.02599" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Real-time, Universal, and Robust Adversarial Attacks Against Speaker  Recognition Systems <a href="https://arxiv.org/pdf/2003.02301" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. An Empirical Accuracy Law for Sequential Machine Translation: the Case  of Google Translate</b>  <a href="https://arxiv.org/pdf/2003.02817" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sequeira%2C+L+N" target="_blank" rel="noopener" style="color:#0000EE;">Lucas Nunes Sequeira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moreschi%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bruno Moreschi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cozman%2C+F+G" target="_blank" rel="noopener" style="color:#0000EE;">Fabio Gagliardi Cozman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fontes%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernardo Fontes</a><br>
<font size="3">
Abstract: We have established, through empirical testing, a law that relates the number of translating hops to translation accuracy in sequential machine translation in Google Translate. Both accuracy and size decrease with the number of hops; the former displays a decrease closely following a power law. Such a law allows one to predict the behavior of translation chains that may be built as society increasingly depends on automated devices. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们已经建立，通过实证检验，这涉及谷歌翻译啤酒花翻译准确性顺序机器翻译翻译数量的法律。精确度和尺寸的减小与跳数;前者显示的下降密切关注功法。这样的法律允许预测可能被构建为社会越来越依赖于自动化设备转换链的行为。</font>
</div>


<hr>
<div id="paper2"> <b>2. HypoNLI: Exploring the Artificial Patterns of Hypothesis-only Bias in  Natural Language Inference</b>  <a href="https://arxiv.org/pdf/2003.02756" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianyu Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baobao Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhifang Sui</a><br>
<font size="3">
Abstract: Many recent studies have shown that for models trained on datasets for natural language inference (NLI), it is possible to make correct predictions by merely looking at the hypothesis while completely ignoring the premise. In this work, we manage to derive adversarial examples in terms of the hypothesis-only bias and explore eligible ways to mitigate such bias. Specifically, we extract various phrases from the hypotheses (artificial patterns) in the training sets, and show that they have been strong indicators to the specific labels. We then figure out `hard' and `easy' instances from the original test sets whose labels are opposite to or consistent with those indications. We also set up baselines including both pretrained models (BERT, RoBERTa, XLNet) and competitive non-pretrained models (InferSent, DAM, ESIM). Apart from the benchmark and baselines, we also investigate two debiasing approaches which exploit the artificial pattern modeling to mitigate such hypothesis-only bias: down-sampling and adversarial training. We believe those methods can be treated as competitive baselines in NLI debiasing tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近许多研究表明，经过训练数据集上的自然语言推理（NLI）模式，可以通过仅仅是看着假说，而完全忽视的前提下做出正确的预测。在这项工作中，我们成功地汲取对抗的例子中唯一的假设，偏见方面，探索符合条件的方式来减轻这种偏见。具体来说，我们提取的训练集的假设（人工模式）不同的短语，并表明他们一直坚挺指标的特定标签。然后，我们计算出从原来的测试集，其标签是相反的或与这些说明一致的'硬“和'方便”的情况。我们还建立了基准既包括预训练模型（BERT，罗伯塔，XLNet）和有竞争力的非预训练模型（InferSent，DAM，ESIM）。除了基准和基准，我们还调查其利用人工图案造型，以减轻这种只假设偏置2点消除直流偏压的方法：下采样和对抗性训练。我们相信，这些方法可以在NLI消除直流偏压任务有竞争力的基线处理。</font>
</div>


<hr>
<div id="paper3"> <b>3. Zero-Shot Cross-Lingual Transfer with Meta Learning</b>  <a href="https://arxiv.org/pdf/2003.02739" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nooralahzadeh%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Farhad Nooralahzadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bekoulis%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giannis Bekoulis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bjerva%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Johannes Bjerva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Isabelle Augenstein</a><br>
<font size="3">
Abstract: Learning what to share between tasks has been a topic of high importance recently, as strategic sharing of knowledge has been shown to improve the performance of downstream tasks. The same applies to sharing between languages, and is especially important when considering the fact that most languages in the world suffer from being under-resourced. In this paper, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning, where, in addition to training a source language model, another model learns to select which training instances are the most beneficial. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning, on a total 16 languages. We improve upon state-of-the-art on zero-shot and few-shot NLI and QA tasks on the XNLI and X-WikiRe datasets, respectively. We further conduct a comprehensive analysis which indicates that correlation of typological features between languages can further explain when parameter sharing learned via meta learning is beneficial. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：学习如何在任务间共享一直是高度重视的话题近来，随着知识的共享战略已经显示出改善的下游任务的性能。这同样适用于语言之间共享，并考虑到在世界上大多数语言从资源不足是受害事实时尤为重要。在本文中，我们考虑在同一时间，在很少或根本没有数据可用于英语以外的语言在多个不同的语言培训模式的设置。我们表明，这种具有挑战性的设置可以使用元学习，在那里，除了训练源语言模型，另一个模型学会选择哪些训练实例是最有利的接近。我们尝试使用标准的监督，零次跨语言，以及为不同的自然语言理解任务（自然语言推理，问题解答）为数不多的射门跨语言设置。我们广泛的实验装置演示元学习的一致有效性上共有16种语言。我们分别提高在国家的最先进的零次和几个次NLI和QA任务的XNLI和X-WikiRe数据集。我们进一步进行了全面的分析，这表明的类型学特征语言之间的相关性可以进一步解释，当参数共享通过学习荟萃学是有益的。</font>
</div>


<hr>
<div id="paper4"> <b>4. Fact Check-Worthiness Detection as Positive Unlabelled Learning</b>  <a href="https://arxiv.org/pdf/2003.02736" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wright%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dustin Wright</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Augenstein%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Isabelle Augenstein</a><br>
<font size="3">
Abstract: A critical component of automatically combating misinformation is the detection of fact check-worthiness, i.e. determining if a piece of information should be checked for veracity. There are multiple isolated lines of research which address this core issue: check-worthiness detection from political speeches and debates, rumour detection on Twitter, and citation needed detection from Wikipedia. What is still lacking is a structured comparison of these variants of check-worthiness, as well as a unified approach to them. We find that check-worthiness detection is a very challenging task in any domain, because it both hinges upon detecting how factual a sentence is, and how likely a sentence is to be believed without verification. As such, annotators often only mark those instances they judge to be clear-cut check-worthy. Our best-performing method automatically corrects for this, using a variant of positive unlabelled learning, which learns when an instance annotated as not check-worthy should in fact have been annotated as being check-worthy. In applying this, we outperform the state of the art in two of the three domains studied for check-worthiness detection in English. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动打击误传关键部件是检测事实检查适航的，即确定是否一条信息应真实性进行检查。有研究其解决这一核心问题，多个相互隔离的线路：从政治演讲和辩论，在Twitter上辟谣检测和维基百科引文需要识别检验适航检测。什么是仍然缺乏是检查适航这些变体，以及作为一个统一的方式对他们的结构比较。我们发现，检查适航检测是在任何领域一个非常具有挑战性的任务，因为它在检测两个铰链的句子怎么事实是，怎么可能是一个句子是不进行验证可以相信的。这样，注释者往往只标出那些他们判断是明确的检查值得实例。我们表现​​最好的方法自动纠正这一点，使用正未标记的学习，这获悉当实例标注为未入住值得其实应该被标注为检查值得的变体。在应用此，我们胜过两个研究了在英语检查适航检测三个领域的技术状态。</font>
</div>


<hr>
<div id="paper5"> <b>5. SentenceMIM: A Latent Variable Language Model</b>  <a href="https://arxiv.org/pdf/2003.02645" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Livne%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Micha Livne</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Swersky%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Swersky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fleet%2C+D+J" target="_blank" rel="noopener" style="color:#0000EE;">David J. Fleet</a><br>
<font size="3">
Abstract: We introduce sentenceMIM, a probabilistic auto-encoder for language modelling, trained with Mutual Information Machine (MIM) learning. Previous attempts to learn variational auto-encoders for language data? have had mixed success, with empirical performance well below state-of-the-art auto-regressive models, a key barrier being the? occurrence of posterior collapse with VAEs. The recently proposed MIM framework encourages high mutual information between observations and latent variables, and is more robust against posterior collapse. This paper formulates a MIM model for text data, along with a corresponding learning algorithm. We demonstrate excellent perplexity (PPL) results on several datasets, and show that the framework learns a rich latent space, allowing for interpolation between sentences of different lengths with a fixed-dimensional latent representation. We also demonstrate the versatility of sentenceMIM by utilizing a trained model for question-answering, a transfer learning task, without fine-tuning. To the best of our knowledge, this is the first latent variable model (LVM) for text modelling that achieves competitive performance with non-LVM models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍sentenceMIM，概率自动编码器来进行语言建模，用互信息机（MIM）学习培训。以前曾试图学习变自动编码语言数据？有成败参半，有经验的表现远远低于国家的最先进的自回归模型，一个主要障碍是对？与VAES后坍塌的发生。最近提出的MIM框架鼓励人们观察和潜在变量之间的高互信息，并针对后崩溃更稳健。本文制定的文本数据的MIM模型，具有相应的学习算法一起。我们证明在几个数据集优异的困惑（PPL）的结果，并显示该框架学习了丰富的潜在空间，允许具有固定维的潜在表示不同长度的句子之间的插值。我们还利用对问题回答，传递学习任务训练模型，无需微调证明sentenceMIM的多功能性。据我们所知，这是为实现与非LVM车型竞争力的性能文字造型的第一潜变量模型（LVM）。</font>
</div>


<hr>
<div id="paper6"> <b>6. RecipeGPT: Generative Pre-training Based Cooking Recipe Generation and  Evaluation System</b>  <a href="https://arxiv.org/pdf/2003.02498" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H+H" target="_blank" rel="noopener" style="color:#0000EE;">Helena H. Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Shu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Achananuparp%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Palakorn Achananuparp</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prasetyo%2C+P+K" target="_blank" rel="noopener" style="color:#0000EE;">Philips Kokoh Prasetyo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lim%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ee-Peng Lim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Varshney%2C+L+R" target="_blank" rel="noopener" style="color:#0000EE;">Lav R. Varshney</a><br>
<font size="3">
Abstract: Interests in the automatic generation of cooking recipes have been growing steadily over the past few years thanks to a large amount of online cooking recipes. We present RecipeGPT, a novel online recipe generation and evaluation system. The system provides two modes of text generations: (1) instruction generation from given recipe title and ingredients; and (2) ingredient generation from recipe title and cooking instructions. Its back-end text generation module comprises a generative pre-trained language model GPT-2 fine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation module allows the users to conveniently inspect the quality of the generated recipe contents and store the results for future reference. RecipeGPT can be accessed online at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：兴趣在自动生成烹饪食谱已经过去由于大量的在线烹饪食谱的几年以上稳定增长。我们提出RecipeGPT，一种新型的在线配方产生和评价体系。该系统提供文本代的两种模式：（1）从给定的配方标题和成分指令产生;和（2）成分生成从配方标题和烹饪的指令。其后端文本生成模块包括生成预训练语言模型在一个大的烹饪食谱数据集GPT-2微调。此外，配方评估模块允许用户方便地检查所产生的配方内容的质量和储存以备将来参考的结果。 RecipeGPT可在网上这个HTTPS URL访问。</font>
</div>


<hr>
<div id="paper7"> <b>7. Kleister: A novel task for Information Extraction involving Long  Documents with Complex Layout</b>  <a href="https://arxiv.org/pdf/2003.02356" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Grali%C5%84ski%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Filip Graliński</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stanis%C5%82awek%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomasz Stanisławek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wr%C3%B3blewska%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Wróblewska</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lipi%C5%84ski%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dawid Lipiński</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaliska%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Agnieszka Kaliska</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosalska%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paulina Rosalska</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Topolski%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bartosz Topolski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Biecek%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Przemysław Biecek</a><br>
<font size="3">
Abstract: State-of-the-art solutions for Natural Language Processing (NLP) are able to capture a broad range of contexts, like the sentence level context or document level context for short documents. But these solutions are still struggling when it comes to real-world longer documents with information encoded in the spatial structure of the document, in elements like tables, forms, headers, openings or footers, or the complex layout of pages or multiple pages. To encourage progress on deeper and more complex information extraction, we present a new task (named Kleister) with two new datasets. Based on textual and structural layout features, an NLP system must find the most important information, about various types of entities, in formal long documents. These entities are not only classes from standard named entity recognition (NER) systems (e.g. location, date, or amount) but also the roles of the entities in the whole documents (e.g. company town address, report date, income amount). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最先进的国家的最自然语言处理（NLP）解决方案能够捕捉到广泛的背景，如短文件句子层面语境或文档级别的上下文。但是，当谈到与文档的空间结构进行编码，像表格，表单，页眉，开口或页脚或页面或多个页面布局复杂元素的信息真实世界更长的文档这些解决方案仍在挣扎。为了鼓励更深入和更复杂的信息提取的进步，我们提出了两个新的数据集，一个新的任务（名为Kleister）。基于文本和结构布局特点，一个NLP系统必须找到最重要的信息，关于各类型的实体，在正式的长文档。这些实体是从标准的命名实体识别（NER）系统（例如，位置，日期或金额）不仅课而且在整个文件中的实体的角色（如公司地址镇，报告日期，收入金额）。</font>
</div>


<hr>
<div id="paper8"> <b>8. A Study on Efficiency, Accuracy and Document Structure for Answer  Sentence Selection</b>  <a href="https://arxiv.org/pdf/2003.02349" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bonadiman%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniele Bonadiman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moschitti%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessandro Moschitti</a><br>
<font size="3">
Abstract: An essential task of most Question Answering (QA) systems is to re-rank the set of answer candidates, i.e., Answer Sentence Selection (A2S). These candidates are typically sentences either extracted from one or more documents preserving their natural order or retrieved by a search engine. Most state-of-the-art approaches to the task use huge neural models, such as BERT, or complex attentive architectures. In this paper, we argue that by exploiting the intrinsic structure of the original rank together with an effective word-relatedness encoder, we can achieve competitive results with respect to the state of the art while retaining high efficiency. Our model takes 9.5 seconds to train on the WikiQA dataset, i.e., very fast in comparison with the $\sim 18$ minutes required by a standard BERT-base fine-tuning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数问题回答（QA）系统的一个重要任务是重新排名的一组答案考生，即答句精选（A2S）。这些候选人通常是从保持它们的自然顺序或由搜索引擎检索到的一个或多个文档或者提取句子。大多数国家的最先进的方法，以任务使用巨大的神经模型，如BERT，或复杂的周到架构。在本文中，我们认为，通过用有效字关联性编码器利用原始等级的本征结构一起，就可以实现相对于现有技术的状态的竞争结果，同时保持高效率。我们的模型需要9.5秒对WikiQA数据集训练，即很快与一个标准的BERT基微调所需的$ \卡$ 18分钟比较。</font>
</div>


<hr>
<div id="paper9"> <b>9. BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward</b>  <a href="https://arxiv.org/pdf/2003.02738" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schmidt%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Schmidt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hofmann%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Hofmann</a><br>
<font size="3">
Abstract: Measuring the quality of a generated sequence against a set of references is a central problem in many learning frameworks, be it to compute a score, to assign a reward, or to perform discrimination. Despite great advances in model architectures, metrics that scale independently of the number of references are still based on n-gram estimates. We show that the underlying operations, counting words and comparing counts, can be lifted to embedding words and comparing embeddings. An in-depth analysis of BERT embeddings shows empirically that contextual embeddings can be employed to capture the required dependencies while maintaining the necessary scalability through appropriate pruning and smoothing techniques. We cast unconditional generation as a reinforcement learning problem and show that our reward function indeed provides a more effective learning signal than n-gram reward in this challenging setting. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：测量与一组引用的生成序列的质量是许多学习框架的中心问题，是它以计算得分，分配奖励，或进行歧视。尽管在模型架构，即独立引用的数量规模指标的巨大进步仍是基于正克估计。我们表明，底层操作，字数统计和比较计数，可以解除嵌入文字和比较的嵌入。的BERT的嵌入示出了深入分析凭经验该上下文的嵌入可被用于捕获所需要的依赖，同时保持通过适当的修剪必要的可扩展性和平滑技术。我们投无条件一代不如强化学习问题，表明我们的奖励功能的确提供了比在这个充满挑战的设置n元的奖励更有效的学习信号。</font>
</div>


<hr>
<div id="paper10"> <b>10. Phase transitions in a decentralized graph-based approach to human  language</b>  <a href="https://arxiv.org/pdf/2003.02639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/physics?searchtype=author&query=Vera%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Javier Vera</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Urbina%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Felipe Urbina</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Palma%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenceslao Palma</a><br>
<font size="3">
Abstract: Zipf's law establishes a scaling behavior for word-frequencies in large text corpora. The appearance of Zipfian properties in human language has been previously explained as an optimization problem for the interests of speakers and hearers. On the other hand, human-like vocabularies can be viewed as bipartite graphs. The aim here is double: within a bipartite-graph approach to human vocabularies, to propose a decentralized language game model for the formation of Zipfian properties. To do this, we define a language game, in which a population of artificial agents is involved in idealized linguistic interactions. Numerical simulations show the appearance of a phase transition from an initially disordered state to three possible phases for language formation. Our results suggest that Zipfian properties in language seem to arise partly from decentralized linguistic interactions between agents endowed with bipartite word-meaning mappings. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：齐普夫定律建立了大型语料库字频率缩放行为。在人类语言Zipfian性质的出现为演讲者和听众的利益的最优化问题事先说明。在另一方面，类似人类的词汇可以被看作是二分图。本文的目的是双重：二分，图法对人的词汇中，提出了Zipfian特性的形成分散的语言游戏模式。要做到这一点，我们定义了一个语言游戏，其中人工坐席的群体参与理想化的语言互动。数值仿真表明的相变的从初始的无序状态的外观以三种可能的相对于语言的形成。我们的研究结果表明，在语言Zipfian性质似乎从赋有二分词意映射代理之间的分散化语言的交互部分出现。</font>
</div>


<hr>
<div id="paper11"> <b>11. An Incremental Explanation of Inference in Hybrid Bayesian Networks for  Increasing Model Trustworthiness and Supporting Clinical Decision Making</b>  <a href="https://arxiv.org/pdf/2003.02599" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kyrimi%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evangelia Kyrimi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mossadegh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Somayyeh Mossadegh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tai%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nigel Tai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marsh%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Marsh</a><br>
<font size="3">
Abstract: Various AI models are increasingly being considered as part of clinical decision-support tools. However, the trustworthiness of such models is rarely considered. Clinicians are more likely to use a model if they can understand and trust its predictions. Key to this is if its underlying reasoning can be explained. A Bayesian network (BN) model has the advantage that it is not a black-box and its reasoning can be explained. In this paper, we propose an incremental explanation of inference that can be applied to hybrid BNs, i.e. those that contain both discrete and continuous nodes. The key questions that we answer are: (1) which important evidence supports or contradicts the prediction, and (2) through which intermediate variables does the information flow. The explanation is illustrated using a real clinical case study. A small evaluation study is also conducted. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：各种型号的AI越来越多地被视为临床决策支持工具的一部分。然而，这种模式的可信度很少被考虑。临床医生更容易使用的模型，如果他们能理解并相信它的预测。关键是，如果它的根本理由可以解释。贝叶斯网络（BN）模型的优势在于它不是一个黑盒子和推理来解释。在本文中，我们提出了推断的增量解释，即可以适用于混合动力贝叶斯网络，即那些含有两个离散和连续节点。我们回答的关键问题是：（1）重要的证据支持或违背了预测;（2）通过中间变量确实的信息流。对此的解释是使用一个真正的临床病例研究说明。一个小的评价研究还进行。</font>
</div>


<hr>
<div id="paper12"> <b>12. Real-time, Universal, and Robust Adversarial Attacks Against Speaker  Recognition Systems</b>  <a href="https://arxiv.org/pdf/2003.02301" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Xie</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Shi%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cong Shi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuohang Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingying Chen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Yuan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Yuan</a><br>
<font size="3">
Abstract: As the popularity of voice user interface (VUI) exploded in recent years, speaker recognition system has emerged as an important medium of identifying a speaker in many security-required applications and services. In this paper, we propose the first real-time, universal, and robust adversarial attack against the state-of-the-art deep neural network (DNN) based speaker recognition system. Through adding an audio-agnostic universal perturbation on arbitrary enrolled speaker's voice input, the DNN-based speaker recognition system would identify the speaker as any target (i.e., adversary-desired) speaker label. In addition, we improve the robustness of our attack by modeling the sound distortions caused by the physical over-the-air propagation through estimating room impulse response (RIR). Experiment using a public dataset of $109$ English speakers demonstrates the effectiveness and robustness of our proposed attack with a high attack success rate of over 90%. The attack launching time also achieves a 100X speedup over contemporary non-universal attacks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着语音用户界面（VUI）的普及，近年来爆炸，说话人识别系统已经成为识别许多与安全所需的应用程序和服务的扬声器的重要媒介。在本文中，我们提出了对国家的最先进的深层神经网络（DNN）的说话人识别系统的第一个实时的，普遍的和强大的敌对攻击。通过添加音频无关的普遍的扰动上的任意登记的演讲人的语音输入，所述基于DNN-说话人识别系统将确定所述扬声器作为任何目标（即，攻击者期望的）扬声器的标签。此外，我们通过模拟通过估计房间脉冲响应（RIR）所造成的物理过度的空气传播的声音失真提高我们的攻击的鲁棒性。实验使用的$ $ 109英语为母语的公开数据集显示了我们提出的攻击有超过90％的高攻成功率的有效性和鲁棒性。攻击发起时间也实现了100倍的加速比当代非通用攻击。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-05</title>
    <url>/2020/03/05/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-05/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> Spatiotemporal-Aware Augmented Reality: Redefining HCI in Image-Guided  Therapy <a href="https://arxiv.org/pdf/2003.02260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Robust Perceptual Night Vision in Thermal Colorization <a href="https://arxiv.org/pdf/2003.02204" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> HintPose <a href="https://arxiv.org/pdf/2003.02170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> VESR-Net: The Winning Solution to Youku Video Enhancement and  Super-Resolution Challenge <a href="https://arxiv.org/pdf/2003.02115" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Unity Style Transfer for Person Re-Identification <a href="https://arxiv.org/pdf/2003.02068" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Mixup Regularization for Region Proposal based Object Detectors <a href="https://arxiv.org/pdf/2003.02065" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Vehicle-Human Interactive Behaviors in Emergency: Data Extraction from  Traffic Accident Videos <a href="https://arxiv.org/pdf/2003.02059" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Learning to Transfer Texture from Clothing Images to 3D Humans <a href="https://arxiv.org/pdf/2003.02050" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Annotation-free Learning of Deep Representations for Word Spotting using  Synthetic Data and Self Labeling <a href="https://arxiv.org/pdf/2003.01989" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Occlusion Aware Unsupervised Learning of Optical Flow From Video <a href="https://arxiv.org/pdf/2003.01960" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Automatic Signboard Detection from Natural Scene Image in Context of  Bangladesh Google Street View <a href="https://arxiv.org/pdf/2003.01936" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Reveal of Domain Effect: How Visual Restoration Contributes to Object  Detection in Aquatic Scenes <a href="https://arxiv.org/pdf/2003.01913" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Double Backpropagation for Training Autoencoders against Adversarial  Attack <a href="https://arxiv.org/pdf/2003.01895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> GarmentGAN: Photo-realistic Adversarial Fashion Transfer <a href="https://arxiv.org/pdf/2003.01894" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> MoVi: A Large Multipurpose Motion and Video Dataset <a href="https://arxiv.org/pdf/2003.01888" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> A Deep Learning Method for Complex Human Activity Recognition Using  Virtual Wearable Sensors <a href="https://arxiv.org/pdf/2003.01874" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Type I Attack for Generative Models <a href="https://arxiv.org/pdf/2003.01872" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Region adaptive graph fourier transform for 3d point clouds <a href="https://arxiv.org/pdf/2003.01866" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are  Failing to Reproduce Spectral Distributions <a href="https://arxiv.org/pdf/2003.01826" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Implicitly Defined Layers in Neural Networks <a href="https://arxiv.org/pdf/2003.01822" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> RODNet: Object Detection under Severe Conditions Using Vision-Radio  Cross-Modal Supervision <a href="https://arxiv.org/pdf/2003.01816" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> TimeConvNets: A Deep Time Windowed Convolution Neural Network Design for  Real-time Video Facial Expression Recognition <a href="https://arxiv.org/pdf/2003.01791" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> A Robust Imbalanced SAR Image Change Detection Approach Based on Deep  Difference Image and PCANet <a href="https://arxiv.org/pdf/2003.01768" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Blind Image Restoration without Prior Knowledge <a href="https://arxiv.org/pdf/2003.01764" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Image-based OoD-Detector Principles on Graph-based Input Data in Human  Action Recognition <a href="https://arxiv.org/pdf/2003.01719" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Voxel Map for Visual SLAM <a href="https://arxiv.org/pdf/2003.02247" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Colored Noise Injection for Training Adversarially Robust Neural  Networks <a href="https://arxiv.org/pdf/2003.02188" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Deep Joint Transmission-Recognition for Power-Constrained IoT Devices <a href="https://arxiv.org/pdf/2003.02027" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Redesigning SLAM for Arbitrary Multi-Camera Systems <a href="https://arxiv.org/pdf/2003.02014" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> G-VAE: A Continuously Variable Rate Deep Image Compression Framework <a href="https://arxiv.org/pdf/2003.02012" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> A Learning Strategy for Contrast-agnostic MRI Segmentation <a href="https://arxiv.org/pdf/2003.01995" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> The iCub multisensor datasets for robot and computer vision applications <a href="https://arxiv.org/pdf/2003.01994" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Metrics and methods for robustness evaluation of neural networks with  generative models <a href="https://arxiv.org/pdf/2003.01993" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Learning for Video Compression with Hierarchical Quality and Recurrent  Enhancement <a href="https://arxiv.org/pdf/2003.01966" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> ADRN: Attention-based Deep Residual Network for Hyperspectral Image  Denoising <a href="https://arxiv.org/pdf/2003.01947" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> \textit{Semixup}: In- and Out-of-Manifold Regularization for Deep  Semi-Supervised Knee Osteoarthritis Severity Grading from Plain Radiographs <a href="https://arxiv.org/pdf/2003.01944" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Gaussianization Flows <a href="https://arxiv.org/pdf/2003.01941" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> ETRI-Activity3D: A Large-Scale RGB-D Dataset for Robots to Recognize  Daily Activities of the Elderly <a href="https://arxiv.org/pdf/2003.01920" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Black-box Smoothing: A Provable Defense for Pretrained Classifiers <a href="https://arxiv.org/pdf/2003.01908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Localising Faster: Efficient and precise lidar-based robot localisation  in large-scale environments <a href="https://arxiv.org/pdf/2003.01875" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Semantic sensor fusion: from camera to sparse lidar information <a href="https://arxiv.org/pdf/2003.01871" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Learning Rope Manipulation Policies Using Dense Object Descriptors  Trained on Synthetic Depth Data <a href="https://arxiv.org/pdf/2003.01835" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> RMP-SNNs: Residual Membrane Potential Neuron for Enabling Deeper  High-Accuracy and Low-Latency Spiking Neural Networks <a href="https://arxiv.org/pdf/2003.01811" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Security of Deep Learning based Lane Keeping System under Physical-World  Adversarial Attack <a href="https://arxiv.org/pdf/2003.01782" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Spatiotemporal-Aware Augmented Reality: Redefining HCI in Image-Guided  Therapy</b>  <a href="https://arxiv.org/pdf/2003.02260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fotouhi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Javad Fotouhi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mehrfard%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arian Mehrfard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianyu Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alex Johnson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Osgood%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Greg Osgood</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Unberath%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mathias Unberath</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Armand%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehran Armand</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Navab%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nassir Navab</a><br>
<font size="3">
Abstract: Suboptimal interaction with patient data and challenges in mastering 3D anatomy based on ill-posed 2D interventional images are essential concerns in image-guided therapies. Augmented reality (AR) has been introduced in the operating rooms in the last decade; however, in image-guided interventions, it has often only been considered as a visualization device improving traditional workflows. As a consequence, the technology is gaining minimum maturity that it requires to redefine new procedures, user interfaces, and interactions. The main contribution of this paper is to reveal how exemplary workflows are redefined by taking full advantage of head-mounted displays when entirely co-registered with the imaging system at all times. The proposed AR landscape is enabled by co-localizing the users and the imaging devices via the operating room environment and exploiting all involved frustums to move spatial information between different bodies. The awareness of the system from the geometric and physical characteristics of X-ray imaging allows the redefinition of different human-machine interfaces. We demonstrate that this AR paradigm is generic, and can benefit a wide variety of procedures. Our system achieved an error of $4.76\pm2.91$ mm for placing K-wire in a fracture management procedure, and yielded errors of $1.57\pm1.16^\circ$ and $1.46\pm1.00^\circ$ in the abduction and anteversion angles, respectively, for total hip arthroplasty. We hope that our holistic approach towards improving the interface of surgery not only augments the surgeon's capabilities but also augments the surgical team's experience in carrying out an effective intervention with reduced complications and provide novel approaches of documenting procedures for training purposes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于病态的2D影像介入患者数据和掌握3D解剖挑战次优的交互在图像引导的治疗至关重要的担忧。增强现实（AR）已经在过去十年的手术室被引入;然而，在图像引导的介入，它经常仅被认为是一个可视化装置改进传统工作流程。因此，该技术得到了最低的成熟，它需要重新定义新的程序，用户界面和交互。本文的主要贡献是揭示如何工作流示例性的，通过利用充分利用重新定义头戴式显示器时完全与在所有时间成像系统共同配准。所提出的AR景观是由共定位通过手术室环境中的用户和成像设备和利用所有相关的截锥移动不同机构之间的空间信息启用。从X射线成像的几何和物理特性的系统的认识允许不同的人机界面的重新定义。我们表明，这种AR模式是通用的，可以享受各种各样的程序。我们的系统来实现的$ 4.76 \ pm2.91 $毫米的误差用于放置克氏针在骨折管理程序，并在绑架产生的$ 1.57 \ pm1.16 ^ \ CIRC $ $和1.46 \ pm1.00 ^ \ CIRC错误$和前倾角度，分别用于全髋关节置换。希望我们的努力改善手术不仅界面整体方法增强了外科医生的能力，同时也增强了手术小组在开展以减少并发症的有效干预措施的经验和记录提供用于训练目的的程序的新方法。</font>
</div>


<hr>
<div id="paper2"> <b>2. Robust Perceptual Night Vision in Thermal Colorization</b>  <a href="https://arxiv.org/pdf/2003.02204" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Almasri%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feras Almasri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Debeir%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Olivier Debeir</a><br>
<font size="3">
Abstract: Transforming a thermal infrared image into a robust perceptual colour Visible image is an ill-posed problem due to the differences in their spectral domains and in the objects' representations. Objects appear in one spectrum but not necessarily in the other, and the thermal signature of a single object may have different colours in its Visible representation. This makes a direct mapping from thermal to Visible images impossible and necessitates a solution that preserves texture captured in the thermal spectrum while predicting the possible colour for certain objects. In this work, a deep learning method to map the thermal signature from the thermal image's spectrum to a Visible representation in their low-frequency space is proposed. A pan-sharpening method is then used to merge the predicted low-frequency representation with the high-frequency representation extracted from the thermal image. The proposed model generates colour values consistent with the Visible ground truth when the object does not vary much in its appearance and generates averaged grey values in other cases. The proposed method shows robust perceptual night vision images in preserving the object's appearance and image context compared with the existing state-of-the-art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：转变热红外图像分割成一个强大感知颜色可见图像是病态问题由于在它们的频谱域的差异，并在对象的表示。对象显示在一个频谱，但不一定在其它，和一个单一的对象的热签名可以在其可见表示具有不同的颜色。这使得从热的直接映射到不可能可见图像，并且需要该蜜饯在热谱纹理捕获而预测所述可能的颜色为特定对象的溶液。在这项工作中，深刻的学习方法，从热图像的光谱热签名映射到它们的低频空间可见表示建议。然后，将全色锐化方法用于合并从所述热图像中提取的高频率表示所预测的低频表示。当对象并不在它的外观变化很大，在其他情况下产生的平均灰度值，该模型生成与可见地面实况一致的颜色值。所提出的方法示出了健壮感知夜视图像中与现有状态的最先进的相比保持对象的外观和形象上下文。</font>
</div>


<hr>
<div id="paper3"> <b>3. HintPose</b>  <a href="https://arxiv.org/pdf/2003.02170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanghoon Hong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hunchul Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonghyuk Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sukhyun Cho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heewoong Park</a><br>
<font size="3">
Abstract: Most of the top-down pose estimation models assume that there exists only one person in a bounding box. However, the assumption is not always correct. In this technical report, we introduce two ideas, instance cue and recurrent refinement, to an existing pose estimator so that the model is able to handle detection boxes with multiple persons properly. When we evaluated our model on the COCO17 keypoints dataset, it showed non-negligible improvement compared to its baseline model. Our model achieved 76.2 mAP as a single model and 77.3 mAP as an ensemble on the test-dev set without additional training data. After additional post-processing with a separate refinement network, our final predictions achieved 77.8 mAP on the COCO test-dev set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数自顶向下的姿势估计模型的假设只存在一个在边框中的人。但是，假设并不总是正确的。本技术报告中，我们引入两个概念，例如提示和反复提炼，以现有的姿势估计使模型能够处理检测箱多的人正确。当我们评估的关键点COCO17我们的数据集模型，它比其基线模型显示出不可忽视的改善。我们的模型实现76.2地图作为一个单一的模型和77.3地图作为合奏在测试-dev的组而无需额外的训练数据。一个单独的细化网络后处理的附加之后，我们最终的预测实现的测试COCO-dev的一套77.8地图。</font>
</div>


<hr>
<div id="paper4"> <b>4. VESR-Net: The Winning Solution to Youku Video Enhancement and  Super-Resolution Challenge</b>  <a href="https://arxiv.org/pdf/2003.02115" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiale Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xu Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaowei Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sen Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhibo Chen</a><br>
<font size="3">
Abstract: This paper introduces VESR-Net, a method for video enhancement and super-resolution (VESR). We design a separate non-local module to explore the relations among video frames and fuse video frames efficiently, and a channel attention residual block to capture the relations among feature maps for video frame reconstruction in VESR-Net. We conduct experiments to analyze the effectiveness of these designs in VESR-Net, which demonstrates the advantages of VESR-Net over previous state-of-the-art VESR methods. It is worth to mention that among more than thousands of participants for Youku video enhancement and super-resolution (Youku-VESR) challenge, our proposed VESR-Net beat other competitive methods and ranked the first place. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了VESR型网，视频增强和超分辨率（VESR）的方法。我们设计一个独立的非本地模块探索有效视频帧和保险丝的视频帧之间的关系，以及其中特征为视频帧重构在VESR-Net的映射的信道注意残余块以捕获的关系。我们进行实验来分析VESR-Net的，它比国家的最先进的前面VESR方法演示VESR-网的优势，这些设计的有效性。值得一提的是在多于参与者优酷视频增强和超分辨率十万（优酷，VESR）的挑战，我们提出的VESR，净击败其他竞争方法和排名第一的位置。</font>
</div>


<hr>
<div id="paper5"> <b>5. Unity Style Transfer for Person Re-Identification</b>  <a href="https://arxiv.org/pdf/2003.02068" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojun Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi-Dong Shen</a><br>
<font size="3">
Abstract: Style variation has been a major challenge for person re-identification, which aims to match the same pedestrians across different cameras. Existing works attempted to address this problem with camera-invariant descriptor subspace learning. However, there will be more image artifacts when the difference between the images taken by different cameras is larger. To solve this problem, we propose a UnityStyle adaption method, which can smooth the style disparities within the same camera and across different cameras. Specifically, we firstly create UnityGAN to learn the style changes between cameras, producing shape-stable style-unity images for each camera, which is called UnityStyle images. Meanwhile, we use UnityStyle images to eliminate style differences between different images, which makes a better match between query and gallery. Then, we apply the proposed method to Re-ID models, expecting to obtain more style-robust depth features for querying. We conduct extensive experiments on widely used benchmark datasets to evaluate the performance of the proposed framework, the results of which confirm the superiority of the proposed model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：风格变化一直是人重新鉴定的一个重大挑战，其目的是匹配在不同的相机相同的行人。现有的作品试图解决这个问题，摄像头，不变的描述符子空间学习。然而，将会有更多的图像伪影当由不同的照相机拍摄的图像之间的差异较大。为了解决这个问题，我们提出了一个UnityStyle适应方法，它可以在同一个镜头内和跨不同的相机流畅的风格差异。具体而言，我们首先创建UnityGAN学习摄像机之间的风格变化，对于每一个摄像头，这被称为UnityStyle图像制作形状稳定的风格统一的图像。同时，我们使用UnityStyle图像，以消除不同的图像，这使得查询和画廊之间更好的匹配之间的差异风格。然后，我们应用所提出的方法重新编号机型，希望获得更多的风格，强大的深度功能进行查询。我们进行了广泛使用的基准数据集了广泛的实验，以评估该框架的性能，其结果验证了该模式的优越性。</font>
</div>


<hr>
<div id="paper6"> <b>6. Mixup Regularization for Region Proposal based Object Detectors</b>  <a href="https://arxiv.org/pdf/2003.02065" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bouabid%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shahine Bouabid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Delaitre%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vincent Delaitre</a><br>
<font size="3">
Abstract: Mixup - a neural network regularization technique based on linear interpolation of labeled sample pairs - has stood out by its capacity to improve model's robustness and generalizability through a surprisingly simple formalism. However, its extension to the field of object detection remains unclear as the interpolation of bounding boxes cannot be naively defined. In this paper, we propose to leverage the inherent region mapping structure of anchors to introduce a mixup-driven training regularization for region proposal based object detectors. The proposed method is benchmarked on standard datasets with challenging detection settings. Our experiments show an enhanced robustness to image alterations along with an ability to decontextualize detections, resulting in an improved generalization power. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：的mixup  - 基于标记的样品对线性内插的神经网络正则化技术 - 已经站在出通过它的能力，通过一个非常简单的形式主义，以改善模型的稳健性和普遍性。然而，由于包围盒的内插不能天真地定义其延伸到对象检测领域仍不清楚。在本文中，我们提出利用锚的固有区域映射结构以引入用于基于区域提案对象检测器一个的mixup驱动训练正规化。该方法是在基准具有挑战性的检测设置标准数据集。我们的实验显示增强的鲁棒性的图像变化的情况与向decontextualize检测，从而得到改善的泛化功率的能力一起。</font>
</div>


<hr>
<div id="paper7"> <b>7. Vehicle-Human Interactive Behaviors in Emergency: Data Extraction from  Traffic Accident Videos</b>  <a href="https://arxiv.org/pdf/2003.02059" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wansong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danyang Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changxu Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghui Zheng</a><br>
<font size="3">
Abstract: Currently, studying the vehicle-human interactive behavior in the emergency needs a large amount of datasets in the actual emergent situations that are almost unavailable. Existing public data sources on autonomous vehicles (AVs) mainly focus either on the normal driving scenarios or on emergency situations without human involvement. To fill this gap and facilitate related research, this paper provides a new yet convenient way to extract the interactive behavior data (i.e., the trajectories of vehicles and humans) from actual accident videos that were captured by both the surveillance cameras and driving recorders. The main challenge for data extraction from real-time accident video lies in the fact that the recording cameras are un-calibrated and the angles of surveillance are unknown. The approach proposed in this paper employs image processing to obtain a new perspective which is different from the original video's perspective. Meanwhile, we manually detect and mark object feature points in each image frame. In order to acquire a gradient of reference ratios, a geometric model is implemented in the analysis of reference pixel value, and the feature points are then scaled to the object trajectory based on the gradient of ratios. The generated trajectories not only restore the object movements completely but also reflect changes in vehicle velocity and rotation based on the feature points distributions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目前，在紧急研究汽车人的互动行为需要在几乎不可用实际紧急情况的大量数据集。对自主车（AVS）现有的公共数据源主要要么集中在正常驾驶情况下或者在没有人为干预的紧急情况。为了填补这一空白，并促进相关的研究，本文提出了一种新的还没有方便的方法来提取交互行为数据（即，车辆和人的轨迹）从被监控摄像头和行车纪录器两种捕捉实际发生的事故视频。从实时视频事故在于一个事实，即记录相机未校准和监控的角度是未知的数据提取的主要挑战。在本文所提出的方法采用图像处理，以获得一个新的角度，其是从原始视频的角度不同。同时，我们手动检测和在每个图像帧标记对象的特征点。为了获取的参考比的梯度，几何模型是在参考像素值的分析来实现，然后将特征点被缩放到物体轨迹基于比率的梯度。所生成的轨迹不仅完全恢复的对象的运动，而且反映基于所述特征点分布在车辆速度和旋转的变化。</font>
</div>


<hr>
<div id="paper8"> <b>8. Learning to Transfer Texture from Clothing Images to 3D Humans</b>  <a href="https://arxiv.org/pdf/2003.02050" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mir%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aymen Mir</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alldieck%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thiemo Alldieck</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pons-Moll%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerard Pons-Moll</a><br>
<font size="3">
Abstract: In this paper, we present a simple yet effective method to automatically transfer textures of clothing images (front and back) to 3D garments worn on top SMPL, in real time. We first automatically compute training pairs of images with aligned 3D garments using a custom non-rigid 3D to 2D registration method, which is accurate but slow. Using these pairs, we learn a mapping from pixels to the 3D garment surface. Our idea is to learn dense correspondences from garment image silhouettes to a 2D-UV map of a 3D garment surface using shape information alone, completely ignoring texture, which allows us to generalize to the wide range of web images. Several experiments demonstrate that our model is more accurate than widely used baselines such as thin-plate-spline warping and image-to-image translation networks while being orders of magnitude faster. Our model opens the door for applications such as virtual-try on, and allows generation of 3D humans with varied textures which is necessary for learning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个简单而服装的图像（正面和背面）的纹理自动传输到佩戴在顶部SMPL 3D服装，实时有效的方法。我们的图像的第一自动计算训练对与使用自定义非刚性3D到2D配准方法，该方法是准确的，但慢对齐3D服装。利用这些对，我们学会从像素到三维服装表面的映射。我们的想法是单独使用的形状信息，完全无视质地，这使我们能够推广到宽范围的Web图像的学习从服装图像轮廓致密对应于3D衣服表面的2D-UV地图。一些实验表明，我们的模型比广泛使用的基线，如薄板样条变形和图像 - 图像平移网络更加准确，同时快几个数量级。我们的模型打开的应用，如虚拟试穿门，并允许代3D人类具有不同纹理所必需的学习。</font>
</div>


<hr>
<div id="paper9"> <b>9. Annotation-free Learning of Deep Representations for Word Spotting using  Synthetic Data and Self Labeling</b>  <a href="https://arxiv.org/pdf/2003.01989" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wolf%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fabian Wolf</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fink%2C+G+A" target="_blank" rel="noopener" style="color:#0000EE;">Gernot A. Fink</a><br>
<font size="3">
Abstract: Word spotting is a popular tool for supporting the first exploration of historic, handwritten document collections. Today, the best performing methods rely on machine learning techniques, which require a high amount of annotated training material. As training data is usually not available in the application scenario, annotation-free methods aim at solving the retrieval task without representative training samples. In this work, we present an annotation-free method that still employs machine learning techniques and therefore outperforms other learning-free approaches. The weakly supervised training scheme relies on a lexicon, that does not need to precisely fit the dataset. In combination with a confidence based selection of pseudo-labeled training samples, we achieve state-of-the-art query-by-example performances. Furthermore, our method allows to perform query-by-string, which is usually not the case for other annotation-free methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：单词辨别是支持的历史，手写文档集合的第一口勘探的流行工具。今天表现最好的方法依赖于机器学习技术，这需要注释的训练材料的高量。作为训练数据通常不是在应用方案中可用，无注释的方法旨在解决检索任务没有代表性的训练样本。在这项工作中，我们目前仍然采用机器学习技术的无注释的方法，因此优于其他免费的学习方法。弱指导训练方案依赖于一个词汇，这并不需要精确地适应数据集。在与伪标记的训练样本的信心为基础的选择组合，实现我们国家的最先进的查询通过例如表演。此外，我们的方法可以进行查询通过字符串，它通常是不为别的自由注释的方法的情况。</font>
</div>


<hr>
<div id="paper10"> <b>10. Occlusion Aware Unsupervised Learning of Optical Flow From Video</b>  <a href="https://arxiv.org/pdf/2003.01960" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junqiao Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tiantian Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Ye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lu Xiong</a><br>
<font size="3">
Abstract: In this paper, we proposed an unsupervised learning method for estimating the optical flow between video frames, especially to solve the occlusion problem. Occlusion is caused by the movement of an object or the movement of the camera, defined as when certain pixels are visible in one video frame but not in adjacent frames. Due to the lack of pixel correspondence between frames in the occluded area, incorrect photometric loss calculation can mislead the optical flow training process. In the video sequence, we found that the occlusion in the forward ($t\rightarrow t+1$) and backward ($t\rightarrow t-1$) frame pairs are usually complementary. That is, pixels that are occluded in subsequent frames are often not occluded in the previous frame and vice versa. Therefore, by using this complementarity, a new weighted loss is proposed to solve the occlusion problem. In addition, we calculate gradients in multiple directions to provide richer supervision information. Our method achieves competitive optical flow accuracy compared to the baseline and some supervised methods on KITTI 2012 and 2015 benchmarks. This source code has been released at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一种无监督学习方法用于估计视频帧之间的光流，尤其是解决了遮挡问题。闭塞由对象或相机的运动的运动造成的，定义为当某些像素处于相邻帧在一个视频帧可见的，但不是。由于缺乏在闭塞区域帧之间的像素对应的，不正确的光度损耗计算可能误导光流训练过程。在视频序列中，我们发现，在正向闭塞（$吨\ RIGHTARROW吨+ $ 1）和向后（$吨\ RIGHTARROW叔$ 1）帧对通常是互补的。即，将在随后的帧中遮挡的像素往往没有在先前帧中，并且反之亦然闭塞。因此，通过使用这种互补性，一种新的加权损失提出了解决遮挡问题。另外，我们计算在多个方向上的梯度，以提供更丰富的监管信息。我们的方法实现了比基线和KITTI 2012和2015点的基准一些监管方法具有竞争力的光流精确度。此源代码已经在这个HTTPS URL被释放。</font>
</div>


<hr>
<div id="paper11"> <b>11. Automatic Signboard Detection from Natural Scene Image in Context of  Bangladesh Google Street View</b>  <a href="https://arxiv.org/pdf/2003.01936" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Toaha%2C+M+S+I" target="_blank" rel="noopener" style="color:#0000EE;">Md. Sadrul Islam Toaha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+C+R" target="_blank" rel="noopener" style="color:#0000EE;">Chowdhury Rafeed Rahman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Asad%2C+S+B" target="_blank" rel="noopener" style="color:#0000EE;">Sakib Bin Asad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ahmed%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tashin Ahmed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Proma%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Mahfuz Ara Proma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haque%2C+S+M+S" target="_blank" rel="noopener" style="color:#0000EE;">S.M. Shahriar Haque</a><br>
<font size="3">
Abstract: Automatic signboard region detection is the first step of information extraction about establishments from an image, especially when there is a complex background and multiple signboard regions are present in the image. Automatic signboard detection in Bangladesh is a challenging task because of low quality street view image, presence of overlapping objects and presence of signboard like objects which are not actually signboards. In this research, we provide a novel dataset from the perspective of Bangladesh city streets with an aim of signboard detection, namely Bangladesh Street View Signboard Objects (BSVSO) image dataset. We introduce a novel approach to detect signboard accurately by applying smart image processing techniques and statistically determined hyperparameter based deep learning method, Faster R-CNN. Comparison of different variations of this segmentation based learning method have also been performed in this research. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动招牌区域检测为约从图像场所信息提取的第一步骤中，尤其是当有一个复杂的背景和多个招牌区均存在在图像中。自动招牌孟加拉国检测是由于低质量的街道视图图像的一个具有挑战性的任务，重叠的物体和等，其实际上不是招牌对象招牌的存在的存在。在这项研究中，我们提供了从孟加拉国城市街道与告示牌检测，即孟加拉国街景招牌对象（BSVSO）图像数据集的目标角度新颖的数据集。我们介绍一种新颖的方法来通过应用智能图像处理技术和统计确定的超参数基于深度学习方法，更快R-CNN招牌精确地检测。基于学习方法这个细分的不同变化的比较也已经在这项研究进行。</font>
</div>


<hr>
<div id="paper12"> <b>12. Reveal of Domain Effect: How Visual Restoration Contributes to Object  Detection in Aquatic Scenes</b>  <a href="https://arxiv.org/pdf/2003.01913" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xingyu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Lu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengxing Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junzhi Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Wen</a><br>
<font size="3">
Abstract: Underwater robotic perception usually requires visual restoration and object detection, both of which have been studied for many years. Meanwhile, data domain has a huge impact on modern data-driven leaning process. However, exactly indicating domain effect, the relation between restoration and detection remains unclear. In this paper, we generally investigate the relation of quality-diverse data domain to detection performance. In the meantime, we unveil how visual restoration contributes to object detection in real-world underwater scenes. According to our analysis, five key discoveries are reported: 1) Domain quality has an ignorable effect on within-domain convolutional representation and detection accuracy; 2) low-quality domain leads to higher generalization ability in cross-domain detection; 3) low-quality domain can hardly be well learned in a domain-mixed learning process; 4) degrading recall efficiency, restoration cannot improve within-domain detection accuracy; 5) visual restoration is beneficial to detection in the wild by reducing the domain shift between training data and real-world scenes. Finally, as an illustrative example, we successfully perform underwater object detection with an aquatic robot. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：水下机器人感知通常需要视觉恢复和检测物体，这两者已经研究了很多年。同时，数据域对现代数据驱动的学习过程产生巨大的影响。然而，恰恰说明域效果，恢复和检测之间的关系尚不清楚。在本文中，我们一般调查的质量，不同的数据域，以检测性能的关系。在此期间，我们推出怎样的视觉恢复有利于检测对象在现实世界的水下场景。根据我们的分析，五个关键的发现报道：1）域质量对内域卷积表示和检测精度可忽略的影响; 2）低质量域，因而在交叉域检测更高泛化能力; 3）低质量域几乎不能在一个域中混合学习过程很好地了解到; 4）降解召回效率，恢复不能改善内域的检测精度; 5）视觉恢复是通过减少训练数据和真实世界的场景之间的域转移到检测在野生有益的。最后，作为一个说明性的例子，我们成功地执行与水生机器人水下物体检测。</font>
</div>


<hr>
<div id="paper13"> <b>13. Double Backpropagation for Training Autoencoders against Adversarial  Attack</b>  <a href="https://arxiv.org/pdf/2003.01895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengjin Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sizhe Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Huang</a><br>
<font size="3">
Abstract: Deep learning, as widely known, is vulnerable to adversarial samples. This paper focuses on the adversarial attack on autoencoders. Safety of the autoencoders (AEs) is important because they are widely used as a compression scheme for data storage and transmission, however, the current autoencoders are easily attacked, i.e., one can slightly modify an input but has totally different codes. The vulnerability is rooted the sensitivity of the autoencoders and to enhance the robustness, we propose to adopt double backpropagation (DBP) to secure autoencoder such as VAE and DRAW. We restrict the gradient from the reconstruction image to the original one so that the autoencoder is not sensitive to trivial perturbation produced by the adversarial attack. After smoothing the gradient by DBP, we further smooth the label by Gaussian Mixture Model (GMM), aiming for accurate and robust classification. We demonstrate in MNIST, CelebA, SVHN that our method leads to a robust autoencoder resistant to attack and a robust classifier able for image transition and immune to adversarial attack if combined with GMM. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习，如广为人知的，很容易受到敌对样本。本文重点对自动编码对抗攻击。在自动编码（AES）的安全性是重要的，因为它们被广泛地用作数据存储和传输的压缩方案，但是，目前的自动编码易于受到攻击，即，一个可以稍微修改的输入，但具有完全不同的代码。该漏洞是根的自动编码的灵敏度和增强鲁棒性，我们建议采用双反向传播（DBP），以确保自动编码如VAE和借鉴。我们限制从重建图像与原件的一个梯度，使得自动编码器不是由对抗攻击产生微不足道的扰动敏感。平滑的DBP梯度后，我们进一步平滑由高斯混合模型（GMM）标签，旨在准确和稳健的分类。我们证明MNIST，CelebA，SVHN我们的方法导致一个强大的自动编码抗攻击并能够用于图像转换，不受敌对攻击一个强大的分级如果与GMM相结合。</font>
</div>


<hr>
<div id="paper14"> <b>14. GarmentGAN: Photo-realistic Adversarial Fashion Transfer</b>  <a href="https://arxiv.org/pdf/2003.01894" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Raffiee%2C+A+H" target="_blank" rel="noopener" style="color:#0000EE;">Amir Hossein Raffiee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sollami%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Sollami</a><br>
<font size="3">
Abstract: The garment transfer problem comprises two tasks: learning to separate a person's body (pose, shape, color) from their clothing (garment type, shape, style) and then generating new images of the wearer dressed in arbitrary garments. We present GarmentGAN, a new algorithm that performs image-based garment transfer through generative adversarial methods. The GarmentGAN framework allows users to virtually try-on items before purchase and generalizes to various apparel types. GarmentGAN requires as input only two images, namely, a picture of the target fashion item and an image containing the customer. The output is a synthetic image wherein the customer is wearing the target apparel. In order to make the generated image look photo-realistic, we employ the use of novel generative adversarial techniques. GarmentGAN improves on existing methods in the realism of generated imagery and solves various problems related to self-occlusions. Our proposed model incorporates additional information during training, utilizing both segmentation maps and body key-point information. We show qualitative and quantitative comparisons to several other networks to demonstrate the effectiveness of this technique. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：服装转移问题包括两个任务：学习一个人的身体（姿势，形状，颜色）从他们的服装（服装种类，形状，样式）分离，然后生成任意服装穿着佩戴者的新形象。我们提出GarmentGAN，一种新的算法，通过生成对抗性的方法为基础的图像，进行服装转移。该GarmentGAN框架允许用户几乎试穿购买之前，项目和推广到各种服装类型。 GarmentGAN需要作为输入仅两个图像，即，目标时尚项目的图片和包含客户图像。输出是其中所述客户佩戴目标服饰的合成图像。为了使生成的图像看起来逼真的，我们采用采用新颖的生成对抗技术。 GarmentGAN提高对生成的图像的真实感现有的方法和解决与自我闭塞的各种问题。我们提出的模型训练过程中包含更多的信息，同时利用分割地图和身体的关键点信息。我们展示的定性和定量比较其他几个网络，以证明这种技术的有效性。</font>
</div>


<hr>
<div id="paper15"> <b>15. MoVi: A Large Multipurpose Motion and Video Dataset</b>  <a href="https://arxiv.org/pdf/2003.01888" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghorbani%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Saeed Ghorbani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mahdaviani%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kimia Mahdaviani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thaler%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anne Thaler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kording%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Konrad Kording</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cook%2C+D+J" target="_blank" rel="noopener" style="color:#0000EE;">Douglas James Cook</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Blohm%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gunnar Blohm</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Troje%2C+N+F" target="_blank" rel="noopener" style="color:#0000EE;">Nikolaus F. Troje</a><br>
<font size="3">
Abstract: Human movements are both an area of intense study and the basis of many applications such as character animation. For many applications, it is crucial to identify movements from videos or analyze datasets of movements. Here we introduce a new human Motion and Video dataset MoVi, which we make available publicly. It contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement. In five capture rounds, the same actors and movements were recorded using different hardware systems, including an optical motion capture system, video cameras, and inertial measurement units (IMU). For some of the capture rounds, the actors were recorded when wearing natural clothing, for the other rounds they wore minimal clothing. In total, our dataset contains 9 hours of motion capture data, 17 hours of video data from 4 different points of view (including one hand-held camera), and 6.6 hours of IMU data. In this paper, we describe how the dataset was collected and post-processed; We present state-of-the-art estimates of skeletal motions and full-body shape deformations associated with skeletal motion. We discuss examples for potential studies this dataset could enable. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类活动都深入研究的领域和许多应用，如角色动画的基础。对于许多应用，关键的是，从视频识别运动或运动分析的数据集。这里我们介绍一种新的人类运动和视频数据集MOVI，我们对其进行公开提供。它包含了60个雌性和30执行预定义的20个日常活动和体育的运动，和一个自选动作的集合男演员。在5个捕捉回合中，相同的角色和动作使用不同的硬件系统，其中包括一个光学运动捕捉系统，摄像机和惯性测量单元（IMU）的记录。对于一些捕获轮，穿衣服自然当演员被记录下来，因为他们穿的服装最少其他回合。总之，我们的数据包含9个小时的运动捕捉数据的，从4个不同的视点17小时视频数据（包括一个手持相机），以及6.6小时IMU数据。在本文中，我们描述了数据集是如何收集和处理后的;我们目前的状态的最先进的估计与骨骼相关联的运动的运动骨骼和全身体形状变形。我们讨论潜在的研究数据集可以使例子。</font>
</div>


<hr>
<div id="paper16"> <b>16. A Deep Learning Method for Complex Human Activity Recognition Using  Virtual Wearable Sensors</b>  <a href="https://arxiv.org/pdf/2003.01874" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fanyi Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pei%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Ling Pei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Chu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danping Zou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenxian Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yifan Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Li</a><br>
<font size="3">
Abstract: Sensor-based human activity recognition (HAR) is now a research hotspot in multiple application areas. With the rise of smart wearable devices equipped with inertial measurement units (IMUs), researchers begin to utilize IMU data for HAR. By employing machine learning algorithms, early IMU-based research for HAR can achieve accurate classification results on traditional classical HAR datasets, containing only simple and repetitive daily activities. However, these datasets rarely display a rich diversity of information in real-scene. In this paper, we propose a novel method based on deep learning for complex HAR in the real-scene. Specially, in the off-line training stage, the AMASS dataset, containing abundant human poses and virtual IMU data, is innovatively adopted for enhancing the variety and diversity. Moreover, a deep convolutional neural network with an unsupervised penalty is proposed to automatically extract the features of AMASS and improve the robustness. In the on-line testing stage, by leveraging advantages of the transfer learning, we obtain the final result by fine-tuning the partial neural network (optimizing the parameters in the fully-connected layers) using the real IMU data. The experimental results show that the proposed method can surprisingly converge in a few iterations and achieve an accuracy of 91.15% on a real IMU dataset, demonstrating the efficiency and effectiveness of the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于传感器的人类活动识别（HAR）现在是在多个应用领域的一个研究热点。用装有惯性测量单元（IMU）的智能可穿戴设备的兴起，研究者开始利用用于HAR IMU数据。通过采用机器学习算法，早IMU为基础的研究为HAR可以实现对传统经典HAR数据集准确分类结果，只包含简单的和重复的日常活动。然而，这些数据集很少展示的实景信息丰富的多样性。在本文中，我们提出了基于深度学习在真实场景复杂HAR的新方法。特别地，在离线训练阶段，数据集AMASS，含有丰富的人力姿势和虚拟IMU数据，创新性采用用于增强各种和多样性。此外，与无监督惩罚了深刻的卷积神经网络提出了自动提取AMASS的特点，提高了耐用性。在上线的测试阶段，通过利用转印学习的优势，我们通过微调获得最终结果使用实IMU数据的部分的神经网络（优化在全连接层中的参数）。实验结果表明，所提出的方法可以在几次迭代出奇会聚并达到91.15％的准确度上的实际数据集IMU，证明所提出的方法的效率和效果。</font>
</div>


<hr>
<div id="paper17"> <b>17. Type I Attack for Generative Models</b>  <a href="https://arxiv.org/pdf/2003.01872" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengjin Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sizhe Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jia Cai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Huang</a><br>
<font size="3">
Abstract: Generative models are popular tools with a wide range of applications. Nevertheless, it is as vulnerable to adversarial samples as classifiers. The existing attack methods mainly focus on generating adversarial examples by adding imperceptible perturbations to input, which leads to wrong result. However, we focus on another aspect of attack, i.e., cheating models by significant changes. The former induces Type II error and the latter causes Type I error. In this paper, we propose Type I attack to generative models such as VAE and GAN. One example given in VAE is that we can change an original image significantly to a meaningless one but their reconstruction results are similar. To implement the Type I attack, we destroy the original one by increasing the distance in input space while keeping the output similar because different inputs may correspond to similar features for the property of deep neural network. Experimental results show that our attack method is effective to generate Type I adversarial examples for generative models on large-scale image datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生成模型是流行的工具具有广泛的应用前景。尽管如此，它是脆弱的对抗性样本作为分类。现有的攻击方法主要集中于通过将察觉不到扰动输入，这导致错误的结果，产生对抗的例子。然而，我们专注于进攻的另一个方面，即通过显著变化作弊模式。前者诱发Ⅱ型错误，而后者导致I型错误。在本文中，我们提出了I型攻击生成模型，如VAE和GaN。在VAE给出的一个例子是，我们可以显著改变原始图像的意义之一，但他们的重建结果是相似的。为了实现I型攻击，我们破坏了通过增加输入空间的距离，同时保持输出相似的，因为不同的输入可以对应于类似的功能深神经网络的属性的原始之一。实验结果表明，我们的攻击方法是有效的能够生成用于大规模数据集图像生成模型I型对抗性的例子。</font>
</div>


<hr>
<div id="paper18"> <b>18. Region adaptive graph fourier transform for 3d point clouds</b>  <a href="https://arxiv.org/pdf/2003.01866" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pavez%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eduardo Pavez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Girault%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin Girault</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ortega%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonio Ortega</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chou%2C+P+A" target="_blank" rel="noopener" style="color:#0000EE;">Philip A. Chou</a><br>
<font size="3">
Abstract: We introduce the Region Adaptive Graph Fourier Transform (RA-GFT) for compression of 3D point cloud attributes. We assume the points are organized by a family of nested partitions represented by a tree. The RA-GFT is a multiresolution transform, formed by combining spatially localized block transforms. At each resolution level, attributes are processed in clusters by a set of block transforms. Each block transform produces a single approximation (DC) coefficient, and various detail (AC) coefficients. The DC coefficients are promoted up the tree to the next (lower resolution) level, where the process can be repeated until reaching the root. Since clusters may have a different numbers of points, each block transform must incorporate the relative importance of each coefficient. For this, we introduce the $\mathbf{Q}$-normalized graph Laplacian, and propose using its eigenvectors as the block transform. The RA-GFT outperforms the Region Adaptive Haar Transform (RAHT) by up to 2.5 dB, with a small complexity overhead. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍了自适应区域图傅立叶变换（RA-GFT）的三维点云的属性压缩。我们假设点由一个家庭一棵树代表嵌套分区的组织。所述RA-GFT是多分辨率变换，通过组合空间局部块变换形成。在每个分辨率水平，属性在簇由一组块变换的处理。每个块的变换产生一个单一的近似（DC）系数和各种细节（AC）系数。 DC系数被促进了树到下一个（较低分辨率）级别，其中可以重复该过程，直到到达根。因为簇可具有点的不同数量，每个块变换必须结合各系数的相对重要性。对于这一点，我们引入$ \ mathbf {Q} $  - 归一化图的拉普拉斯，并使用其特征向量作为块变换建议。该RA-GFT优于地区自适应哈尔高达变换（RAHT）至2.5 dB时，用小的复杂性开销。</font>
</div>


<hr>
<div id="paper19"> <b>19. Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are  Failing to Reproduce Spectral Distributions</b>  <a href="https://arxiv.org/pdf/2003.01826" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Durall%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ricard Durall</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keuper%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Margret Keuper</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keuper%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Janis Keuper</a><br>
<font size="3">
Abstract: Generative convolutional deep neural networks, e.g. popular GAN architectures, are relying on convolution based up-sampling methods to produce non-scalar outputs like images or video sequences. In this paper, we show that common up-sampling methods, i.e. known as up-convolution or transposed convolution, are causing the inability of such models to reproduce spectral distributions of natural training data correctly. This effect is independent of the underlying architecture and we show that it can be used to easily detect generated data like deepfakes with up to 100% accuracy on public benchmarks. To overcome this drawback of current generative models, we propose to add a novel spectral regularization term to the training optimization objective. We show that this approach not only allows to train spectral consistent GANs that are avoiding high frequency errors. Also, we show that a correct approximation of the frequency spectrum has positive effects on the training stability and output quality of generative networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：剖成卷积深层神经网络，例如流行GAN架构中，都是靠卷积基于上采样方法来生产像图像或视频序列的非标量输出。在本文中，我们表明，常见的上采样方法，即，是造成这种模型的无法正确地再现自然的训练数据的光谱分布称为上行卷积或转置卷积。这种效果是独立于基础架构的和我们表明，它可被用于容易地检测像高达100％的准确度上的公共基准deepfakes生成的数据。为了克服当前生成模型的这个缺点，我们提出了一种新的频谱则项添加到训练优化目标。我们表明，这种方法不仅可以培养被避免高频率误差频谱一致甘斯。此外，我们证明了频谱的正确的近似对培训的稳定性和生成网络的输出质量的积极影响。</font>
</div>


<hr>
<div id="paper20"> <b>20. Implicitly Defined Layers in Neural Networks</b>  <a href="https://arxiv.org/pdf/2003.01822" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianggong Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanyang Gu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mateusz%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michalkiewicz Mateusz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baktashmotlagh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahsa Baktashmotlagh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Eriksson%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anders Eriksson</a><br>
<font size="3">
Abstract: In conventional formulations of multilayer feedforward neural networks, the individual layers are customarily defined by explicit functions. In this paper we demonstrate that defining individual layers in a neural network \emph{implicitly} provide much richer representations over the standard explicit one, consequently enabling a vastly broader class of end-to-end trainable architectures. We present a general framework of implicitly defined layers, where much of the theoretical analysis of such layers can be addressed through the implicit function theorem. We also show how implicitly defined layers can be seamlessly incorporated into existing machine learning libraries. In particular with respect to current automatic differentiation techniques for use in backpropagation based training. Finally, we demonstrate the versatility and relevance of our proposed approach on a number of diverse example problems with promising results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多层前馈神经网络的常规制剂，各个层通过惯用显函数定义的。在本文中，我们表明，限定在神经网络中各个层\ EMPH {隐含}提供更丰富的表示比标准明确的一个，从而使得能够极大地更广泛种类的端至端可训练架构。我们提出隐式定义的层，其中许多这样的层的理论分析可通过隐函数定理来解决的一般框架。我们还展示了如何隐含定义层可以无缝地整合到现有的机器学习库。特别是相对于当前的自动微分技术基于反传训练中使用。最后，我们展示了对一些有希望的结果不同的问题，例如我们所提出的方法的多样性和针对性。</font>
</div>


<hr>
<div id="paper21"> <b>21. RODNet: Object Detection under Severe Conditions Using Vision-Radio  Cross-Modal Supervision</b>  <a href="https://arxiv.org/pdf/2003.01816" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yizhou Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongyu Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyu Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jenq-Neng Hwang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guanbin Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Liu</a><br>
<font size="3">
Abstract: Radar is usually more robust than the camera in severe autonomous driving scenarios, e.g., weak/strong lighting and bad weather. However, the semantic information from the radio signals is difficult to extract. In this paper, we propose a radio object detection network (RODNet) to detect objects purely from the processed radar data in the format of range-azimuth frequency heatmaps (RAMaps). To train the RODNet, we introduce a cross-modal supervision framework, which utilizes the rich information extracted by a vision-based object 3D localization technique to teach object detection for the radar. In order to train and evaluate our method, we build a new dataset -- CRUW, containing synchronized video sequences and RAMaps in various scenarios. After intensive experiments, our RODNet shows favorable object detection performance without the presence of the camera. To the best of our knowledge, this is the first work that can achieve accurate multi-class object detection purely using radar data as the input. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：雷达通常比相机在恶劣自主驾驶的情况，例如，弱/强照明和恶劣天气更稳健。然而，来自无线电信号的语义信息是困难的提取物。在本文中，我们提出了一种无线电物体检测网络（RODNet），以从在距离 - 方位频率热图（RAMaps）的格式的处理雷达数据纯粹检测对象。训练RODNet，我们引入了一个跨模式的监督框架，它利用一个基于视觉的目标3D定位技术教对象检测雷达中提取的丰富信息。为了培养和评价我们的方法，我们建立了一个新的数据集 -  CRUW，含在各种情况下同步的视频序列和RAMaps。经过深入的实验，我们RODNet显示了没有摄像头的存在有利的物体检测性能。据我们所知，这是可以做到精确的多类目标检测纯粹用雷达数据作为输入的第一部作品。</font>
</div>


<hr>
<div id="paper22"> <b>22. TimeConvNets: A Deep Time Windowed Convolution Neural Network Design for  Real-time Video Facial Expression Recognition</b>  <a href="https://arxiv.org/pdf/2003.01791" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J+R+H" target="_blank" rel="noopener" style="color:#0000EE;">James Ren Hou Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Wong</a><br>
<font size="3">
Abstract: A core challenge faced by the majority of individuals with Autism Spectrum Disorder (ASD) is an impaired ability to infer other people's emotions based on their facial expressions. With significant recent advances in machine learning, one potential approach to leveraging technology to assist such individuals to better recognize facial expressions and reduce the risk of possible loneliness and depression due to social isolation is the design of computer vision-driven facial expression recognition systems. Motivated by this social need as well as the low latency requirement of such systems, this study explores a novel deep time windowed convolutional neural network design (TimeConvNets) for the purpose of real-time video facial expression recognition. More specifically, we explore an efficient convolutional deep neural network design for spatiotemporal encoding of time windowed video frame sub-sequences and study the respective balance between speed and accuracy. Furthermore, to evaluate the proposed TimeConvNet design, we introduce a more difficult dataset called BigFaceX, composed of a modified aggregation of the extended Cohn-Kanade (CK+), BAUM-1, and the eNTERFACE public datasets. Different variants of the proposed TimeConvNet design with different backbone network architectures were evaluated using BigFaceX alongside other network designs for capturing spatiotemporal information, and experimental results demonstrate that TimeConvNets can better capture the transient nuances of facial expressions and boost classification accuracy while maintaining a low inference time. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面对广大自闭症谱系障碍（ASD）的个人的核心挑战是基于他们的面部表情来推断他人的情绪的能力受损。近年来，随着机器学习显著的进步，以利用技术一个可能的方法来帮助这些人更好地识别面部表情和减少由于社会隔离可能孤独和抑郁症的风险是计算机视觉驱动的面部表情识别系统的设计。这个社会需要，以及这种系统的低延迟需求的推动下，这项研究探讨了新的深刻的时间窗卷积神经网络的设计（TimeConvNets）用于实时视频面部表情识别的目的。更具体地说，我们探索一种高效的卷积深神经网络设计的时间的时空编码窗口视频帧的子序列和学习速度和精度之间的相应的平衡。此外，为了评价所提出的设计TimeConvNet，我们引入一个更加困难的数据集称为BigFaceX，延伸的Cohn-奏（CK +），BAUM-1的改性聚合组成，并且eNTERFACE公共数据集。使用BigFaceX与其他网络设计用于捕捉时空信息所提出的TimeConvNet设计不同的骨干网络架构的不同变体进行了评估，结果与实验结果表明，TimeConvNets可以更好地捕捉面部表情和提升分类准确性的瞬态细微差别，同时保持较低的推理时间。</font>
</div>


<hr>
<div id="paper23"> <b>23. A Robust Imbalanced SAR Image Change Detection Approach Based on Deep  Difference Image and PCANet</b>  <a href="https://arxiv.org/pdf/2003.01768" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinzheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Su</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ce Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Atkinson%2C+P+M" target="_blank" rel="noopener" style="color:#0000EE;">Peter M. Atkinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoheng Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoping Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jian%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Jian</a><br>
<font size="3">
Abstract: In this research, a novel robust change detection approach is presented for imbalanced multi-temporal synthetic aperture radar (SAR) image based on deep learning. Our main contribution is to develop a novel method for generating difference image and a parallel fuzzy c-means (FCM) clustering method. The main steps of our proposed approach are as follows: 1) Inspired by convolution and pooling in deep learning, a deep difference image (DDI) is obtained based on parameterized pooling leading to better speckle suppression and feature enhancement than traditional difference images. 2) Two different parameter Sigmoid nonlinear mapping are applied to the DDI to get two mapped DDIs. Parallel FCM are utilized on these two mapped DDIs to obtain three types of pseudo-label pixels, namely, changed pixels, unchanged pixels, and intermediate pixels. 3) A PCANet with support vector machine (SVM) are trained to classify intermediate pixels to be changed or unchanged. Three imbalanced multi-temporal SAR image sets are used for change detection experiments. The experimental results demonstrate that the proposed approach is effective and robust for imbalanced SAR data, and achieve up to 99.52% change detection accuracy superior to most state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本研究中，一个新颖的鲁棒变化检测方法是基于深学习提出了不平衡多时合成孔径雷达（SAR）图像。我们的主要贡献是开发用于生成差分图像和平行模糊c均值（FCM）聚类方法的新方法。是我们提出的方法的主要步骤如下：1）卷积的启发和深度学习汇集，基于参数汇集从而获得更好的斑点抑制和增强功能，比传统的差分图像，获得了深刻的差分图像（DDI）。 2）有两种不同参数的Sigmoid非线性映射施加到DDI得到两映射的DDI。平行FCM被利用在这两个映射的DDI，得到三种类型的伪标记象素，即，改变像素，不变的像素，和中间像素。 3）一种PCANet用支持向量机（SVM）被训练来分类像素中间被改变或不变。三个不平衡用于改变检测实验多时相SAR图像集。实验结果表明，所提出的方法是不平衡SAR数据有效和健壮的，并实现了高达99.52％的变化的检测精度优于大多数国家的最先进的方法。</font>
</div>


<hr>
<div id="paper24"> <b>24. Blind Image Restoration without Prior Knowledge</b>  <a href="https://arxiv.org/pdf/2003.01764" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Elron%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Noam Elron</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuval%2C+S+S" target="_blank" rel="noopener" style="color:#0000EE;">Shahar S. Yuval</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rudoy%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dmitry Rudoy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Levy%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Noam Levy</a><br>
<font size="3">
Abstract: Many image restoration techniques are highly dependent on the degradation used during training, and their performance declines significantly when applied to slightly different input. Blind and universal techniques attempt to mitigate this by producing a trained model that can adapt to varying conditions. However, blind techniques to date require prior knowledge of the degradation process, and assumptions regarding its parameter-space. In this paper we present the Self-Normalization Side-Chain (SCNC), a novel approach to blind universal restoration in which no prior knowledge of the degradation is needed. This module can be added to any existing CNN topology, and is trained along with the rest of the network in an end-to-end manner. The imaging parameters relevant to the task, as well as their dynamics, are deduced from the variety in the training data. We apply our solution to several image restoration tasks, and demonstrate that the SNSC encodes the degradation-parameters, improving restoration performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：许多图像复原技术是高度依赖于训练期间使用的退化，并且当应用于略有不同输入他们的表现显著下降。盲人和通用技术试图通过产生训练模型，能够适应变化的条件，以减轻这一点。然而，盲目的技术迄今为止需要关于它的参数空间中的降解过程的先验知识和假设。在本文中，我们目前的自规范化侧链（SCNC），一种新的方法，以盲其中需要事先没有退化的知识普遍恢复。此模块可以被添加到任何现有的CNN的拓扑结构，并且与网络中的端至端的方式，其余训练沿。与任务相关的成像参数，以及它们的动态，从训练数据中推导出品种。我们应用我们的解决方案的几个图像恢复的任务，并证明SNSC编码降解参数，提高恢复性能。</font>
</div>


<hr>
<div id="paper25"> <b>25. Image-based OoD-Detector Principles on Graph-based Input Data in Human  Action Recognition</b>  <a href="https://arxiv.org/pdf/2003.01719" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bayer%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jens Bayer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=M%C3%BCnch%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Münch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arens%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Arens</a><br>
<font size="3">
Abstract: Living in a complex world like ours makes it unacceptable that a practical implementation of a machine learning system assumes a closed world. Therefore, it is necessary for such a learning-based system in a real world environment, to be aware of its own capabilities and limits and to be able to distinguish between confident and unconfident results of the inference, especially if the sample cannot be explained by the underlying distribution. This knowledge is particularly essential in safety-critical environments and tasks e.g. self-driving cars or medical applications. Towards this end, we transfer image-based Out-of-Distribution (OoD)-methods to graph-based data and show the applicability in action recognition. The contribution of this work is (i) the examination of the portability of recent image-based OoD-detectors for graph-based input data, (ii) a Metric Learning-based approach to detect OoD-samples, and (iii) the introduction of a novel semi-synthetic action recognition dataset. The evaluation shows that image-based OoD-methods can be applied to graph-based data. Additionally, there is a gap between the performance on intraclass and intradataset results. First methods as the examined baseline or ODIN provide reasonable results. More sophisticated network architectures - in contrast to their image-based application - were surpassed in the intradataset comparison and even lead to less classification accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生活在一个复杂的世界像我们这样使得它不能接受的机器学习系统的实际实现假定一个封闭的世界。因此，有必要对这种基于学习系统在实际环境中，要注意自身的能力和限制，并能推断的自信和不自信的结果之间的区别，特别是如果样品不能被解释底层分布。这方面的知识是在例如安全相关的环境和任务尤其重要自动驾驶汽车或医疗应用。为此目的，我们基于转换的图像外的分布（OOD） - 方法到基于图的数据，并显示在动作识别的适用性。这项工作的贡献为（i）最近的基于图像的OOD探测器用于基于图形的输入数据的便携性的检查，（ⅱ）一个学习型度量的方法来检测OOD的样品，和（iii）引入的一种新颖的半合成动作识别数据集。的评价结果​​显示，基于图像的OOD的方法可以应用到基于图的数据。此外，对组内和intradataset结果之间的性能差距。首先方法作为检测基准或ODIN提供合理的结果。更复杂的网络架构 - 相比于他们的基于图像的应用 - 在intradataset相比还有所超越，甚至会导致更少的分类精度。</font>
</div>


<hr>
<div id="paper26"> <b>26. Voxel Map for Visual SLAM</b>  <a href="https://arxiv.org/pdf/2003.02247" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Muglikar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manasi Muglikar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zichao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scaramuzza%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Scaramuzza</a><br>
<font size="3">
Abstract: In modern visual SLAM systems, it is a standard practice to retrieve potential candidate map points from overlapping keyframes for further feature matching or direct tracking. In this work, we argue that keyframes are not the optimal choice for this task, due to several inherent limitations, such as weak geometric reasoning and poor scalability. We propose a voxel-map representation to efficiently retrieve map points for visual SLAM. In particular, we organize the map points in a regular voxel grid. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner, which can be done in constant time using an efficient voxel hashing method. Compared with keyframes, the retrieved points using our method are geometrically guaranteed to fall in the camera field-of-view, and occluded points can be identified and removed to a certain extend. This method also naturally scales up to large scenes and complicated multicamera configurations. Experimental results show that our voxel map representation is as efficient as a keyframe map with 5 keyframes and provides significantly higher localization accuracy (average 46% improvement in RMSE) on the EuRoC dataset. The proposed voxel-map representation is a general approach to a fundamental functionality in visual SLAM and widely applicable. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在现代视觉SLAM系统，它是一个标准的做法，以重叠进一步特征匹配或直接跟踪关键帧检索潜在候选图上的点。在这项工作中，我们认为关键帧不是最佳选择了这个任务，由于一些固有的局限性，如弱几何推理和可扩展性差。我们提出了一个体素图的表示形式来有效地检索地图点视觉SLAM。特别是，我们在常规网格体素的组织图上的点。从相机姿态可见点由在光线投射方式，其可在恒定的时间使用高效的体素的散列方法来进行采样相机平截头体查询。使用关键帧相比，使用我们的方法所获取的点被几何地保证落入相机领域的视图，和闭塞点可以被识别和去除至一定地延伸。这种方法也自然地扩展到大场面和复杂的多机位的配置。实验结果表明，我们的体素映射表示是一样有效关键帧地图5点的关键帧和所述数据集EuRoC（在RMSE平均46％的改善）提供显著更高的定位精度。所提出的体素图的表示是在视觉SLAM和广泛应用的一个基本功能的通用方法。</font>
</div>


<hr>
<div id="paper27"> <b>27. Colored Noise Injection for Training Adversarially Robust Neural  Networks</b>  <a href="https://arxiv.org/pdf/2003.02188" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheltonozhskii%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evgenii Zheltonozhskii</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baskin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaim Baskin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nemcovsky%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaniv Nemcovsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chmiel%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brian Chmiel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mendelson%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avi Mendelson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bronstein%2C+A+M" target="_blank" rel="noopener" style="color:#0000EE;">Alex M. Bronstein</a><br>
<font size="3">
Abstract: Even though deep learning have shown unmatched performance on various tasks, neural networks has been shown to be vulnerable to small adversarial perturbation of the input which lead to significant performance degradation. In this work we extend the idea of adding independent Gaussian noise to weights and activation during adversarial training (PNI) to injection of colored noise for defense against common white-box and black-box attacks. We show that our approach outperforms PNI and various previous approaches in terms of adversarial accuracy on CIFAR-10 dataset. In addition, we provide an extensive ablation study of the proposed method justifying the chosen configurations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管深度学习已经表现出对各种任务无与伦比的性能，神经网络已经被证明是脆弱的输入而导致显著的性能下降的小对抗扰动。在这项工作中，我们扩展对抗性训练（PNI）过程中添加独立的高斯噪声权重和激活注入的想法对普通白盒和黑盒攻击防御噪音着色。我们证明了我们的方法优于PNI和各种以前的方法在CIFAR-10数据集对抗性的准确性方面。另外，我们提供所提出的方法证明所选择的配置的一个广泛的研究消融。</font>
</div>


<hr>
<div id="paper28"> <b>28. Deep Joint Transmission-Recognition for Power-Constrained IoT Devices</b>  <a href="https://arxiv.org/pdf/2003.02027" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jankowski%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mikolaj Jankowski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gunduz%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deniz Gunduz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mikolajczyk%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Krystian Mikolajczyk</a><br>
<font size="3">
Abstract: We propose a joint transmission-recognition scheme for efficient inference at the wireless network edge. Our scheme allows for reliable image recognition over wireless channels with significant computational load reduction at the sender side. We incorporate recently proposed deep joint source-channel coding (JSCC) scheme, and combine it with novel filter pruning strategies aimed at reducing the redundant complexity from neural networks. We evaluate our approach on a classification task, and show satisfactory results in both transmission reliability and workload reduction. This is the first work that combines deep JSCC with network pruning and applies it to images classification over wireless network. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们在无线网络边缘提出了有效的推理联合传输识别方案。我们的方案允许可靠的图像识别在与在发送机侧显著计算负荷减少无线信道。我们结合最近提出的深联合信源信道编码（JSCC）方案，以及旨在从神经网络降低了冗余复杂新颖的过滤器剪枝策略相结合。我们评估我们在分类任务的方法，并显示在这两个传输的可靠性和工作量减少了满意的效果。这是第一部作品，结合了网络修剪深JSCC，并将其应用于图像分类通过无线网络。</font>
</div>


<hr>
<div id="paper29"> <b>29. Redesigning SLAM for Arbitrary Multi-Camera Systems</b>  <a href="https://arxiv.org/pdf/2003.02014" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuo%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juichung Kuo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Muglikar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manasi Muglikar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zichao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scaramuzza%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Scaramuzza</a><br>
<font size="3">
Abstract: Adding more cameras to SLAM systems improves robustness and accuracy but complicates the design of the visual front-end significantly. Thus, most systems in the literature are tailored for specific camera configurations. In this work, we aim at an adaptive SLAM system that works for arbitrary multi-camera setups. To this end, we revisit several common building blocks in visual SLAM. In particular, we propose an adaptive initialization scheme, a sensor-agnostic, information-theoretic keyframe selection algorithm, and a scalable voxel-based map. These techniques make little assumption about the actual camera setups and prefer theoretically grounded methods over heuristics. We adapt a state-of-the-art visual-inertial odometry with these modifications, and experimental results show that the modified pipeline can adapt to a wide range of camera setups (e.g., 2 to 6 cameras in one experiment) without the need of sensor-specific modifications or tuning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：添加更多的摄像机SLAM系统提高了耐用性和准确性，但视觉前端的设计显著复杂。因此，在文献中大多数系统专门针对某些照相机的配置。在这项工作中，我们瞄准的自适应SLAM系统，任意多台摄像机的设置工作。为此，我们重新审视在视觉SLAM几种常见的构建模块。特别是，我们提出了一种自适应初始化方案，传感器无关，信息理论的关键帧选择算法，和一个可伸缩的基于体素的图。这些技术做出实际的相机设置小的假设，更喜欢启发式理论基础的方法。我们适应的状态的最先进的视觉惯性里程计具有这些修饰的，和实验结果表明，经修饰的管道可以适应宽范围的相机设定的（例如，2至6个相机在一个实验中），而不需要的传感器特定的修改或调整。</font>
</div>


<hr>
<div id="paper30"> <b>30. G-VAE: A Continuously Variable Rate Deep Image Compression Framework</b>  <a href="https://arxiv.org/pdf/2003.02012" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Cui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ze Cui</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Bai</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Guo%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tiansheng Guo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Feng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yihui Feng</a><br>
<font size="3">
Abstract: Rate adaption of deep image compression in a single model will become one of the decisive factors competing with the classical image compression codecs. However, until now, there is no perfect solution that neither increases the computation nor affects the compression performance. In this paper, we propose a novel image compression framework G-VAE (Gained Variational Autoencoder), which could achieve continuously variable rate in a single model. Unlike the previous solutions that encode progressively or change the internal unit of the network, G-VAE only adds a pair of gain units at the output of encoder and the input of decoder. It is so concise that G-VAE could be applied to almost all the image compression methods and achieve continuously variable rate with negligible additional parameters and computation. We also propose a new deep image compression framework, which outperforms all the published results on Kodak datasets in PSNR and MS-SSIM metrics. Experimental results show that adding a pair of gain units will not affect the performance of the basic models while endowing them with continuously variable rate. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在一个模型深的图像压缩比率配将成为经典的图像压缩编码的竞争关键因素之一。然而，直到现在，也没有完美的解决方案，无论是增加了计算，也不影响压缩性能。在本文中，我们提出了一个新的图像压缩框架G-VAE（获变自动编码器），其可以在一个单一的模型实现连续可变的速度。不同于编码逐步或改变网络的内部单元中的以前的解决方案中，G-VAE只增加在编码器的输出在一对增益单元和解码器的输入端。它是如此简洁使得G-VAE可以适用于几乎所有的图像压缩方法，实现连续可变速率具有可忽略的额外的参数和计算。我们还提出了一个新的深图像压缩框架，它优于在PSNR和MS-SSIM指标柯达数据集所有公布的结果。实验结果表明，虽然连续可变速率赋予他们加入了对增益单元不会影响基本模型的性能。</font>
</div>


<hr>
<div id="paper31"> <b>31. A Learning Strategy for Contrast-agnostic MRI Segmentation</b>  <a href="https://arxiv.org/pdf/2003.01995" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Billot%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin Billot</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Greve%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Douglas Greve</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Van+Leemput%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Koen Van Leemput</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fischl%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bruce Fischl</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Iglesias%2C+J+E" target="_blank" rel="noopener" style="color:#0000EE;">Juan Eugenio Iglesias</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dalca%2C+A+V" target="_blank" rel="noopener" style="color:#0000EE;">Adrian V. Dalca</a><br>
<font size="3">
Abstract: We present a deep learning strategy that enables, for the first time, contrast-agnostic semantic segmentation of completely unpreprocessed brain MRI scans, without requiring additional training or fine-tuning for new modalities. Classical Bayesian methods address this segmentation problem with unsupervised intensity models, but require significant computational resources. In contrast, learning-based methods can be fast at test time, but are sensitive to the data available at training. Our proposed learning method, SynthSeg, leverages a set of training segmentations (no intensity images required) to generate synthetic sample images of widely varying contrasts on the fly during training. These samples are produced using the generative model of the classical Bayesian segmentation framework, with randomly sampled parameters for appearance, deformation, noise, and bias field. Because each mini-batch has a different synthetic contrast, the final network is not biased towards any MRI contrast. We comprehensively evaluate our approach on four datasets comprising over 1,000 subjects and four types of MR contrast. The results show that our approach successfully segments every contrast in the data, performing slightly better than classical Bayesian segmentation, and three orders of magnitude faster. Moreover, even within the same type of MRI contrast, our strategy generalizes significantly better across datasets, compared to training using real images. Finally, we find that synthesizing a broad range of contrasts, even if unrealistic, increases the generalization of the neural network. Our code and model are open source at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了一个深刻的学习策略，使，是第一次，完全unpreprocessed脑部MRI扫描造影无关语义分割，而不需要新的方式额外的培训或微调。古典贝叶斯方法解决与监督的强度模型这个分割问题，但需要显著的计算资源。相反，基于学习的方法可以在测试时要快，但在训练敏感的数据可用。我们提出的学习方法，SynthSeg，利用一组训练分割的（不需要强度图像）训练期间产生上飞广泛变化的对比合成样品的图像。这些样品采用经典贝叶斯分割框架的生成模型产生，具有外观，变形，噪声和偏置场随机采样参数。因为每个小批量具有不同的合成相反，最终网络未对任何MRI造影偏置。我们全面评估我们的四个数据集的方法，包括超过1,000个主题和四种类型的MR对比。结果表明，我们的方法成功地在细分数据每对比度，比传统的贝叶斯分割稍好表演，和三个数量级的速度更快。此外，即使在同一类型的MRI造影，我们的策略概括显著更好的跨数据集，相比使用真实的图像训练。最后，我们发现，合成一系列的对比，即使是不现实的，提高了神经网络的泛化。我们的代码和型号在此HTTPS URL开源。</font>
</div>


<hr>
<div id="paper32"> <b>32. The iCub multisensor datasets for robot and computer vision applications</b>  <a href="https://arxiv.org/pdf/2003.01994" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kirtay%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Murat Kirtay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Albanese%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Ugo Albanese</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vannucci%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lorenzo Vannucci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schillaci%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guido Schillaci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Laschi%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cecilia Laschi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Falotico%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Egidio Falotico</a><br>
<font size="3">
Abstract: This document presents novel datasets, constructed by employing the iCub robot equipped with an additional depth sensor and color camera. We used the robot to acquire color and depth information for 210 objects in different acquisition scenarios. At this end, the results were large scale datasets for robot and computer vision applications: object representation, object recognition and classification, and action recognition. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文档呈现新颖的数据集，通过采用的iCub机器人配备有附加的深度传感器和彩色照相机构成。我们使用机器人来获取颜色和用于在不同的采集场景210级的对象的深度信息。在这一端，结果是机器人和计算机视觉应用的大规模数据集：对象表示，对象识别和分类，以及动作识别。</font>
</div>


<hr>
<div id="paper33"> <b>33. Metrics and methods for robustness evaluation of neural networks with  generative models</b>  <a href="https://arxiv.org/pdf/2003.01993" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Buzhinsky%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Igor Buzhinsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nerinovsky%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arseny Nerinovsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tripakis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stavros Tripakis</a><br>
<font size="3">
Abstract: Recent studies have shown that modern deep neural network classifiers are easy to fool, assuming that an adversary is able to slightly modify their inputs. Many papers have proposed adversarial attacks, defenses and methods to measure robustness to such adversarial perturbations. However, most commonly considered adversarial examples are based on $\ell_p$-bounded perturbations in the input space of the neural network, which are unlikely to arise naturally. Recently, especially in computer vision, researchers discovered "natural" or "semantic" perturbations, such as rotations, changes of brightness, or more high-level changes, but these perturbations have not yet been systematically utilized to measure the performance of classifiers. In this paper, we propose several metrics to measure robustness of classifiers to natural adversarial examples, and methods to evaluate them. These metrics, called latent space performance metrics, are based on the ability of generative models to capture probability distributions, and are defined in their latent spaces. On three image classification case studies, we evaluate the proposed metrics for several classifiers, including ones trained in conventional and robust ways. We find that the latent counterparts of adversarial robustness are associated with the accuracy of the classifier rather than its conventional adversarial robustness, but the latter is still reflected on the properties of found latent perturbations. In addition, our novel method of finding latent adversarial perturbations demonstrates that these perturbations are often perceptually small. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的研究表明，现代深层神经网络分类很容易糊弄，假设对手能够稍微修改他们的投入。许多文献提出对抗攻击，防御和方法来衡量稳健这种对抗性的扰动。然而，最常考虑的对抗性例子在神经网络的输入空间，这是不可能自然产生的基于$ \ $ ell_p扰动-bounded。最近，特别是在计算机视觉，研究人员发现，“天然”或“语义”扰动，如旋转，亮度变化，或更高层的变化，但这些扰动还没有被系统地用来测量分类器的性能。在本文中，我们提出了多种指标来分类自然对抗的例子的鲁棒性措施和方法来评价他们。这些指标，所谓的潜在空间性能指标，是基于生成模型来捕捉概率分布的能力，并在他们的潜在空间的定义。在三个图像分类的案例研究，我们评估了几个分类，包括那些在传统和强大的方式训练提出的指标。我们发现，对抗性稳健潜同行与分类，而不是它的传统对抗性稳健的精度有关，但后者仍体现在发现潜在扰动的特性。此外，我们发现潜在的对抗扰动的新方法表明，这些扰动往往感知小。</font>
</div>


<hr>
<div id="paper34"> <b>34. Learning for Video Compression with Hierarchical Quality and Recurrent  Enhancement</b>  <a href="https://arxiv.org/pdf/2003.01966" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ren Yang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mentzer%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fabian Mentzer</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Van+Gool%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luc Van Gool</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Timofte%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Radu Timofte</a><br>
<font size="3">
Abstract: The recent years have witnessed the great potential of deep learning for video compression. In this paper, we propose the Hierarchical Learned Video Compression (HLVC) approach with three hierarchical quality layers and recurrent enhancement. To be specific, the frames in the first layer are compressed by image compression method with the highest quality. Using them as references, we propose the Bi-Directional Deep Compression (BDDC) network to compress the second layer with relatively high quality. Then, the third layer frames are compressed with the lowest quality, by the proposed Single Motion Deep Compression (SMDC) network, which adopts a single motion map to estimate the motions of multiple frames, thus saving the bit-rate for motion information. In our deep decoder, we develop the Weighted Recurrent Quality Enhancement (WRQE) network with the inputs of both compressed frames and bit stream. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement. In our HLVC approach, the hierarchical quality benefits the coding efficiency, since the high quality information facilitates the compression and enhancement of low quality frames at encoder and decoder sides, respectively. Finally, the experiments validate that our HLVC approach advances the state-of-the-art deep video compression methods, and outperforms x265 low delay P very fast mode in terms of both PSNR and MS-SSIM. The project page is at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，中深学习视频压缩的巨大潜力。在本文中，我们提出了三个层次的质量层和复发性增强分层了解到视频压缩（HLVC）的方法。具体而言，在第一层中的帧由具有最高质量的图像压缩方法压缩。使用它们作为参考，我们提出了双向深度压缩（BDDC）网络压缩与质量比较高的第二层。然后，第三层帧具有最低品质的压缩，由所提出的单一运动深度压缩（SMDC）网络，它采用一个单一的运动图来估计多个帧的运动，从而节省了比特率的运动信息。在我们的深层解码器，我们开发出既压缩帧和比特流的输入的加权复发质量增强（WRQE）网络。在WRQE，通过质量进行加权存储器和更新信号的复发性细胞功能，以用于增强合理杠杆多帧信息。在我们的HLVC方法，分层质量效益的编码效率，因为高质量的信息分别提供低质量的帧的压缩和增强在编码器和解码器两侧。最后，实验验证了我们的HLVC方法推进国家的最先进的深视频压缩方法，性能胜过两个PSNR和MS-SSIM方面X265低延迟P非常快的模式。该项目页面在这个HTTPS URL。</font>
</div>


<hr>
<div id="paper35"> <b>35. ADRN: Attention-based Deep Residual Network for Hyperspectral Image  Denoising</b>  <a href="https://arxiv.org/pdf/2003.01947" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongsen Zhao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deming Zhai</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Jiang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junjun Jiang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xianming Liu</a><br>
<font size="3">
Abstract: Hyperspectral image (HSI) denoising is of crucial importance for many subsequent applications, such as HSI classification and interpretation. In this paper, we propose an attention-based deep residual network to directly learn a mapping from noisy HSI to the clean one. To jointly utilize the spatial-spectral information, the current band and its $K$ adjacent bands are simultaneously exploited as the input. Then, we adopt convolution layer with different filter sizes to fuse the multi-scale feature, and use shortcut connection to incorporate the multi-level information for better noise removal. In addition, the channel attention mechanism is employed to make the network concentrate on the most relevant auxiliary information and features that are beneficial to the denoising process best. To ease the training procedure, we reconstruct the output through a residual mode rather than a straightforward prediction. Experimental results demonstrate that our proposed ADRN scheme outperforms the state-of-the-art methods both in quantitative and visual evaluations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：高光谱图像（HSI）降噪是许多后续的应用，如恒指分类和解释至关重要。在本文中，我们提出了一种基于注意机制深剩余网络直接学习从嘈杂的恒指向一个干净的映射。共同利用的空间谱的信息，当前频带和其$ $ķ相邻频带被同时利用作为输入。然后，我们采用不同的过滤器尺寸卷积层融合的多尺度特征，并使用快捷方式连接到包括更好的噪声去除多层次的信息。另外，采用的通道注意机制，使最相关的辅助信息，并且是于去噪处理最好的有益功能的网络精矿。为了缓解训练过程中，我们通过重构的残留模式，而不是一个简单的预测的输出。实验结果表明，我们提出的方案ADRN都优于定量和视觉评估的国家的最先进的方法。</font>
</div>


<hr>
<div id="paper36"> <b>36. \textit{Semixup}: In- and Out-of-Manifold Regularization for Deep  Semi-Supervised Knee Osteoarthritis Severity Grading from Plain Radiographs</b>  <a href="https://arxiv.org/pdf/2003.01944" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Nguyen%2C+H+H" target="_blank" rel="noopener" style="color:#0000EE;">Huy Hoang Nguyen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Saarakkala%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simo Saarakkala</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Blaschko%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Blaschko</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tiulpin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aleksei Tiulpin</a><br>
<font size="3">
Abstract: Knee osteoarthritis (OA) is one of the highest disability factors in the world in humans. This musculoskeletal disorder is assessed from clinical symptoms, and typically confirmed via radiographic assessment. This visual assessment done by a radiologist requires experience, and suffers from high inter-observer variability. The recent development in the literature has shown that deep learning (DL) methods can reliably perform the OA severity assessment according to the gold standard Kellgren-Lawrence (KL) grading system. However, these methods require large amounts of labeled data, which are costly to obtain. In this study, we propose the \emph{Semixup} algorithm, a semi-supervised learning (SSL) approach to leverage unlabeled data. \emph{Semixup} relies on consistency regularization using in- and out-of-manifold samples, together with interpolated consistency. On an independent test set, our method significantly outperformed other state-of-the-art SSL methods in most cases, and even achieved a comparable performance to a well-tuned fully supervised learning (SL) model that required over 12 times more labeled data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：膝关节骨性关节炎（OA）是最高的残疾因素在世界上人类的一个。这种肌肉骨骼疾病，从临床症状评估，通常通过影像学评估确认。由放射科医生做这个视觉评估需要丰富的经验，并从高观察员变异受到影响。最近在文献中的发展已经表明深度学习（DL）方法可以根据分级系统的黄金标准Kellgren-劳伦斯（KL）可靠地进行OA严重性评估。然而，这些方法需要大量的标记的数据，这是昂贵的，以获得的。在这项研究中，我们提出了\ {EMPH} Semixup算法，半监督学习（SSL）的方法来利用未标记数据。 \ {EMPH} Semixup使用入点和出的歧管的样品，与经内插的一致性一起依靠一致性正规化。上独立的测试组，我们的方法显著优于在大多数情况下的状态的最先进的其它SSL方法，甚至实现了相当的性能，需要一个调整良好的完全监督学习（SL）模型超过12倍以上的标记的数据。</font>
</div>


<hr>
<div id="paper37"> <b>37. Gaussianization Flows</b>  <a href="https://arxiv.org/pdf/2003.01941" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenlin Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaming Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ermon%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefano Ermon</a><br>
<font size="3">
Abstract: Iterative Gaussianization is a fixed-point iteration procedure that can transform any continuous random vector into a Gaussian one. Based on iterative Gaussianization, we propose a new type of normalizing flow model that enables both efficient computation of likelihoods and efficient inversion for sample generation. We demonstrate that these models, named Gaussianization flows, are universal approximators for continuous probability distributions under some regularity conditions. Because of this guaranteed expressivity, they can capture multimodal target distributions without compromising the efficiency of sample generation. Experimentally, we show that Gaussianization flows achieve better or comparable performance on several tabular datasets compared to other efficiently invertible flow models such as Real NVP, Glow and FFJORD. In particular, Gaussianization flows are easier to initialize, demonstrate better robustness with respect to different transformations of the training data, and generalize better on small training sets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：迭代Gaussianization是一个固定点迭代过程，可以改变任何连续随机载体导入高斯之一。基于迭代Gaussianization，我们提出了一种新型的标准化流程模型，使似然既高效计算和生成样本有效反转。我们证明了这些模型，命名Gaussianization流，对于一些规律性的条件下连续概率分布万能逼近。由于这种保证的表现力，它们可以捕获多峰分布目标而不损害样品产生的效率。在实验中，我们表明，Gaussianization流动性好或相当的性能达到几个表格数据集相对于其他高效可逆的流模型如Real NVP，发光和FFJORD。特别是，Gaussianization流动更容易初始化，表现出更好的稳健性对于训练数据的不同转换，并推广了更好的小训练集。</font>
</div>


<hr>
<div id="paper38"> <b>38. ETRI-Activity3D: A Large-Scale RGB-D Dataset for Robots to Recognize  Daily Activities of the Elderly</b>  <a href="https://arxiv.org/pdf/2003.01920" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinhyeok Jang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dohyung Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheonshu Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minsu Jang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaeyeon Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaehong Kim</a><br>
<font size="3">
Abstract: Deep learning, based on which many modern algorithms operate, is well known to be data-hungry. In particular, the datasets appropriate for the intended application are difficult to obtain. To cope with this situation, we introduce a new dataset called ETRI-Activity3D, focusing on the daily activities of the elderly in robot-view. The major characteristics of the new dataset are as follows: 1) practical action categories that are selected from the close observation of the daily lives of the elderly; 2) realistic data collection, which reflects the robot's working environment and service situations; and 3) a large-scale dataset that overcomes the limitations of the current 3D activity analysis benchmark datasets. The proposed dataset contains 112,620 samples including RGB videos, depth maps, and skeleton sequences. During the data acquisition, 100 subjects were asked to perform 55 daily activities. Additionally, we propose a novel network called four-stream adaptive CNN (FSA-CNN). The proposed FSA-CNN has three main properties: robustness to spatio-temporal variations, input-adaptive activation function, and extension of the conventional two-stream approach. In the experiment section, we confirmed the superiority of the proposed FSA-CNN using NTU RGB+D and ETRI-Activity3D. Further, the domain difference between both groups of age was verified experimentally. Finally, the extension of FSA-CNN to deal with the multimodal data was investigated. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习的基础上，很多现代的算法操作，是众所周知的是大量数据的。特别地，数据集适合于预期应用是很难获得的。为了应对这种情况，我们引入了一个名为ETRI-Activity3D新的数据集，着眼于老年人的机器人视图中的日常活动。是新的数据集的主要特点如下：即从老年人的日常生活密切观察选择1）用实际行动类别; 2）现实的数据收集，这反映了机器人的工作环境和服务的情况;和3）的大规模数据集，其克服了当前的3D活动分析基准数据集的限制。所提出的数据集包含112620个样品，包括RGB视频，深度图，和骨架序列。在数据采集期间，100名受试者被要求执行55个日常活动。此外，我们提出了一种所谓的四流自适应CNN（FSA-CNN）的新网络。所提出的FSA-CNN有三个主要性能：鲁棒性时空变型中，输入自适应激活功能，和常规的两流方法的扩展。在实验部分，我们证实了提出FSA-CNN的使用NTU RGB + d和ETRI-Activity3D的优越性。此外，年龄这两个群体之间的差异域实验验证。最后，FSA-CNN的扩展来处理多模数据进行了研究。</font>
</div>


<hr>
<div id="paper39"> <b>39. Black-box Smoothing: A Provable Defense for Pretrained Classifiers</b>  <a href="https://arxiv.org/pdf/2003.01908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Salman%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hadi Salman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingjie Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Greg Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kapoor%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashish Kapoor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kolter%2C+J+Z" target="_blank" rel="noopener" style="color:#0000EE;">J. Zico Kolter</a><br>
<font size="3">
Abstract: We present a method for provably defending any pretrained image classifier against $\ell_p$ adversarial attacks. By prepending a custom-trained denoiser to any off-the-shelf image classifier and using randomized smoothing, we effectively create a new classifier that is guaranteed to be $\ell_p$-robust to adversarial examples, without modifying the pretrained classifier. The approach applies both to the case where we have full access to the pretrained classifier as well as the case where we only have query access. We refer to this defense as black-box smoothing, and we demonstrate its effectiveness through extensive experimentation on ImageNet and CIFAR-10. Finally, we use our method to provably defend the Azure, Google, AWS, and ClarifAI image classification APIs. Our code replicating all the experiments in the paper can be found at this https URL . </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种可证明抵御$ \ $ ell_p对抗性攻击任何预训练的图像分类。前面加上一个自定义的训练降噪任何现成的货架图像分类和使用随机平滑，我们有效地创建一个保证是$ \ $ ell_p到-robust对抗的例子，而无需修改预训练的分类器的新分类。该方法既适用于我们已经完全进入预训练的分类，以及在哪里，我们只有查询访问时的情况。我们将此作为防御黑盒子平滑，和我们证明通过关于ImageNet和CIFAR-10大量的试验其有效性。最后，我们用我们的方法可证明保卫天青，谷歌，AWS和ClarifAI图像分类的API。我们的代码复制所有实验中的文件可以在此HTTPS URL中找到。</font>
</div>


<hr>
<div id="paper40"> <b>40. Localising Faster: Efficient and precise lidar-based robot localisation  in large-scale environments</b>  <a href="https://arxiv.org/pdf/2003.01875" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Adolfsson%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Adolfsson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Magnusson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Magnusson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Andreasson%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Henrik Andreasson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Posner%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ingmar Posner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duckett%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Duckett</a><br>
<font size="3">
Abstract: This paper proposes a novel approach for global localisation of mobile robots in large-scale environments. Our method leverages learning-based localisation and filtering-based localisation, to localise the robot efficiently and precisely through seeding Monte Carlo Localisation (MCL) with a deep-learned distribution. In particular, a fast localisation system rapidly estimates the 6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with a deep kernel), then a precise recursive estimator refines the estimated robot pose according to the geometric alignment. More importantly, the Gaussian method (i.e. deep probabilistic localisation) and non-Gaussian method (i.e. MCL) can be integrated naturally via importance sampling. Consequently, the two systems can be integrated seamlessly and mutually benefit from each other. To verify the proposed framework, we provide a case study in large-scale localisation with a 3D lidar sensor. Our experiments on the Michigan NCLT long-term dataset show that the proposed method is able to localise the robot in 1.94 s on average (median of 0.8 s) with precision 0.75~m in a large-scale environment of approximately 0.5 km2. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种在大型环境下移动机器人的全球定位的新方法。我们的方法的杠杆学习基于定位和滤波系定位，通过与深了解到分布播种蒙特卡罗定位（MCL）有效且精确地定位机器人。特别地，快速定位系统迅速估计通过深概率模型（高斯过程具有深内核回归）的6-DOF姿态，然后精确递归估计细化根据几何对准估计机器人的姿势。更重要的是，高斯方法（即深概率定位）和非高斯方法（即MCL）可以自然地通过重要性采样一体化。因此，这两个系统可以无缝集成，并且彼此相互受益。为了验证所提出的框架下，我们提供了三维激光雷达传感器在大规模本土化的案例研究。我们对密歇根NCLT长期数据集显示，所提出的方法能够定位所述机器人在1.94 S于平均值（的0.8秒中值）用实验精度0.75〜米约0.5平方公里的大规模环境。</font>
</div>


<hr>
<div id="paper41"> <b>41. Semantic sensor fusion: from camera to sparse lidar information</b>  <a href="https://arxiv.org/pdf/2003.01871" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Berrio%2C+J+S" target="_blank" rel="noopener" style="color:#0000EE;">Julie Stephany Berrio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mao Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Worrall%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stewart Worrall</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ward%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Ward</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nebot%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eduardo Nebot</a><br>
<font size="3">
Abstract: To navigate through urban roads, an automated vehicle must be able to perceive and recognize objects in a three-dimensional environment. A high-level contextual understanding of the surroundings is necessary to plan and execute accurate driving maneuvers. This paper presents an approach to fuse different sensory information, Light Detection and Ranging (lidar) scans and camera images. The output of a convolutional neural network (CNN) is used as classifier to obtain the labels of the environment. The transference of semantic information between the labelled image and the lidar point cloud is performed in four steps: initially, we use heuristic methods to associate probabilities to all the semantic classes contained in the labelled images. Then, the lidar points are corrected to compensate for the vehicle's motion given the difference between the timestamps of each lidar scan and camera image. In a third step, we calculate the pixel coordinate for the corresponding camera image. In the last step we perform the transfer of semantic information from the heuristic probability images to the lidar frame, while removing the lidar information that is not visible to the camera. We tested our approach in the Usyd Dataset \cite{usyd_dataset}, obtaining qualitative and quantitative results that demonstrate the validity of our probabilistic sensory fusion approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通过城市道路导航，自动车辆必须能够感知并在三维环境中识别物体。周围的一个高层次的语境理解是必要的规划和执行正确的驾驶操作。本文提出了一种方法来保险丝不同的感觉信息，光探测和测距（LIDAR）扫描和摄像机图像。卷积神经网络（CNN）的输出被用作分类器，以获得环境的标签。的标签图像和激光雷达点云之间的语义信息的传递是在四个步骤进行：首先，我们使用启发式方法来准概率包含在标记图像的所有语义类别。然后，激光雷达点被校正，以补偿给定的每个激光雷达扫描和摄像机图像的时间戳之间的差的车辆的运动。在第三步骤中，我们可以计算出相应的照相机图像的像素坐标。在最后一步中，我们执行的语义信息从启发概率图像转移到激光雷达帧，同时除去激光雷达信息不是到相机可见的。我们测试了我们的Usyd数据集的方法\ {引用} usyd_dataset，获得展现我们的感官概率融合方法的有效性，定性和定量结果。</font>
</div>


<hr>
<div id="paper42"> <b>42. Learning Rope Manipulation Policies Using Dense Object Descriptors  Trained on Synthetic Depth Data</b>  <a href="https://arxiv.org/pdf/2003.01835" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sundaresan%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Priya Sundaresan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grannen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jennifer Grannen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thananjeyan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brijen Thananjeyan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Balakrishna%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashwin Balakrishna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Laskey%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Laskey</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stone%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Stone</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gonzalez%2C+J+E" target="_blank" rel="noopener" style="color:#0000EE;">Joseph E. Gonzalez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ken Goldberg</a><br>
<font size="3">
Abstract: Robotic manipulation of deformable 1D objects such as ropes, cables, and hoses is challenging due to the lack of high-fidelity analytic models and large configuration spaces. Furthermore, learning end-to-end manipulation policies directly from images and physical interaction requires significant time on a robot and can fail to generalize across tasks. We address these challenges using interpretable deep visual representations for rope, extending recent work on dense object descriptors for robot manipulation. This facilitates the design of interpretable and transferable geometric policies built on top of the learned representations, decoupling visual reasoning and control. We present an approach that learns point-pair correspondences between initial and goal rope configurations, which implicitly encodes geometric structure, entirely in simulation from synthetic depth images. We demonstrate that the learned representation -- dense depth object descriptors (DDODs) -- can be used to manipulate a real rope into a variety of different arrangements either by learning from demonstrations or using interpretable geometric policies. In 50 trials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66% knot-tying success rate from previously unseen configurations. See this https URL for supplementary material and videos. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一维变形的机器人操作对象，如绳索，电缆和胶管是由于缺乏高保真分析模型和大的配置空间的挑战。此外，直接从图像和物理互动中学习终端到终端的操作策略，需要在机器人显著时间，可以不善于任务一概而论。我们使用可解释深视觉表现为绳，延续近期对机器人操作密集的对象描述符的工作应对这些挑战。这有利于建立在了解到表示的顶部解释，可转让的几何政策，去耦视觉推理和控制的设计。我们提出了一种方法，初始和目标绳索配置，这隐含地编码几何结构，完全由合成深度图像模拟之间获悉点配对的对应关系。我们表明，学习表现 - 稠密深度对象描述符（DDODs） - 可以通过从示范学习或使用可解释的几何政策被用来操纵一个真正的绳子成各种不同的安排。与ABB机器人裕美一个打结任务的50次试验中，系统实现了从以前看不见的结构的66％的打结的成功率。看到这个HTTPS URL的辅助材料和视频。</font>
</div>


<hr>
<div id="paper43"> <b>43. RMP-SNNs: Residual Membrane Potential Neuron for Enabling Deeper  High-Accuracy and Low-Latency Spiking Neural Networks</b>  <a href="https://arxiv.org/pdf/2003.01811" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bing Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gopalakrishnan Srinivasan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roy%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaushik Roy</a><br>
<font size="3">
Abstract: Spiking Neural Networks (SNNs) have recently attracted significant research interest as the third generation of artificial neural networks that can enable low-power event-driven data analytics. The best performing SNNs for image recognition tasks are obtained by converting a trained Analog Neural Network (ANN), consisting of Rectified Linear Units (ReLU), to SNN composed of integrate-and-fire neurons with "proper" firing thresholds. The converted SNNs typically incur loss in accuracy compared to that provided by the original ANN and require sizable number of inference time-steps to achieve the best accuracy. We find that performance degradation in the converted SNN stems from using "hard reset" spiking neuron that is driven to fixed reset potential once its membrane potential exceeds the firing threshold, leading to information loss during SNN inference. We propose ANN-SNN conversion using "soft reset" spiking neuron model, referred to as Residual Membrane Potential (RMP) spiking neuron, which retains the "residual" membrane potential above threshold at the firing instants. We demonstrate near loss-less ANN-SNN conversion using RMP neurons for VGG-16, ResNet-20, and ResNet-34 SNNs on challenging datasets including CIFAR-10 (93.63% top-1), CIFAR-100 (70.928% top-1), and ImageNet (73.26% top-1 accuracy). Our results also show that RMP-SNN achieves comparable accuracy to that provided by the converted SNN with "hard reset" spiking neurons using 2-8 times fewer inference time-steps across datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：扣球神经网络（SNNS）最近引起显著的研究兴趣为第三代人工神经网络，可以使低功率事件驱动的数据分析。图像识别任务的最佳执行SNNS被转换受过训练模拟神经网络（ANN）获得，由整流线性单位（RELU）的，到与SNN“适当”击发阈值整合和火神经元组成。转换后的SNNS通常招致精度损失相比，由原始ANN提供和要求的推断时间步长相当数量，以达到最佳的精度。我们发现在转换SNN性能下降使用“硬重置”尖峰一旦其膜电位，超过放电阈值时，SNN推理过程中导致信息丢失被驱动到固定复位电位神经元造成的。我们使用“软复位”尖峰神经元模型中，被称为剩余膜电位（RMP）尖峰神经元，它保留高于阈值的“残余”膜电位在击发时刻提出ANN-SNN转换。我们证明近使用无损耗ANN-SNN转换RMP神经元VGG-16，RESNET-20，和RESNET-34 SNNS上具有挑战性的数据集包括CIFAR-10（93.63％顶部-1），CIFAR-100（70.928％顶1），和ImageNet（73.26％顶部-1精度）。我们的研究结果还表明，RMP-SNN达到媲美精度提供由使用整个数据集的2-8倍更少的推理时间步“硬重置”尖峰神经元转换SNN。</font>
</div>


<hr>
<div id="paper44"> <b>44. Security of Deep Learning based Lane Keeping System under Physical-World  Adversarial Attack</b>  <a href="https://arxiv.org/pdf/2003.01782" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sato%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takami Sato</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junjie Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ningfei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+Y+J" target="_blank" rel="noopener" style="color:#0000EE;">Yunhan Jack Jia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xue Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q+A" target="_blank" rel="noopener" style="color:#0000EE;">Qi Alfred Chen</a><br>
<font size="3">
Abstract: Lane-Keeping Assistance System (LKAS) is convenient and widely available today, but also extremely security and safety critical. In this work, we design and implement the first systematic approach to attack real-world DNN-based LKASes. We identify dirty road patches as a novel and domain-specific threat model for practicality and stealthiness. We formulate the attack as an optimization problem, and address the challenge from the inter-dependencies among attacks on consecutive camera frames. We evaluate our approach on a state-of-the-art LKAS and our preliminary results show that our attack can successfully cause it to drive off lane boundaries within as short as 1.3 seconds. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：车道保持辅助系统（LKAS）是当今便捷和广泛使用，而且非常安全和安全的关键。在这项工作中，我们设计并实现了第一个系统化的方法来攻击真实世界中基础DNN-LKASes。我们确定了脏道路补丁的实用性和隐蔽性的新颖和特定域威胁模型。我们制定攻击为最优化问题，并消除对连续帧的摄像头中的攻击从相互依赖性的挑战。我们评估对我们的做法一个国家的最先进的LKAS和我们的初步结果表明，该攻击能够成功导致其内驱赶车道标线短1.3秒。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-03-05</title>
    <url>/2020/03/05/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-05/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><p><font size="4"><div id="title1"><br><b>1.</b> jiant: A Software Toolkit for Research on General-Purpose Text  Understanding Models <a href="https://arxiv.org/pdf/2003.02249" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div></font></p><div id="title2">
<b>2.</b> Data Augmentation using Pre-trained Transformer Models <a href="https://arxiv.org/pdf/2003.02245" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div><a id="more"></a>

<div id="title3">
<b>3.</b> Unsupervised Adversarial Domain Adaptation for Implicit Discourse  Relation Classification <a href="https://arxiv.org/pdf/2003.02244" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Evaluating Low-Resource Machine Translation between Chinese and  Vietnamese with Back-Translation <a href="https://arxiv.org/pdf/2003.02197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Sequential Neural Networks for Noetic End-to-End Response Selection <a href="https://arxiv.org/pdf/2003.02126" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Posterior-GAN: Towards Informative and Coherent Response Generation with  Posterior Generative Adversarial Network <a href="https://arxiv.org/pdf/2003.02020" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Restoration of Fragmentary Babylonian Texts Using Recurrent Neural  Networks <a href="https://arxiv.org/pdf/2003.01912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> SeMemNN: A Semantic Matrix-Based Memory Neural Network for Text  Classification <a href="https://arxiv.org/pdf/2003.01857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks  with Hyperdimensional Computing enabled Embedding of n-gram Statistics <a href="https://arxiv.org/pdf/2003.01821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit  Alignment <a href="https://arxiv.org/pdf/2003.01950" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> GraphTTS: graph-to-sequence modelling in neural text-to-speech <a href="https://arxiv.org/pdf/2003.01924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> On Emergent Communication in Competitive Multi-Agent Teams <a href="https://arxiv.org/pdf/2003.01848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Discover Your Social Identity from What You Tweet: a Content Based  Approach <a href="https://arxiv.org/pdf/2003.01797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Untangling in Invariant Speech Recognition <a href="https://arxiv.org/pdf/2003.01787" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Phonetic Feedback for Speech Enhancement With and Without Parallel  Speech Data <a href="https://arxiv.org/pdf/2003.01769" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Towards Real-time Mispronunciation Detection in Kids' Speech <a href="https://arxiv.org/pdf/2003.01765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<font><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. jiant: A Software Toolkit for Research on General-Purpose Text  Understanding Models</b>  <a href="https://arxiv.org/pdf/2003.02249" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pruksachatkun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yada Pruksachatkun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yeres%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phil Yeres</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haokun Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Phang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Phang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Htut%2C+P+M" target="_blank" rel="noopener" style="color:#0000EE;">Phu Mon Htut</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alex Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tenney%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ian Tenney</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bowman%2C+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Samuel R. Bowman</a><br>
<font size="3">
Abstract: We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks. jiant enables modular and configuration-driven experimentation with state-of-the-art models and implements a broad set of tasks for probing, transfer learning, and multitask training experiments. jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published performance on a variety of tasks and models, including BERT and RoBERTa. jiant is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍jiant，一个开源工具包进行英语自然语言理解任务，多任务和迁移学习实验。 jiant使模块化和配置驱动的实验与国家的最先进的车型，并实施一系列针对探测任务，传递学习和多任务训练实验。 50个NLU任务jiant工具，包括所有的胶水，强力胶基准任务。我们证明在各种不同的任务和模型，包括BERT和罗伯塔公布业绩的是jiant再现。 jiant可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper2"> <b>2. Data Augmentation using Pre-trained Transformer Models</b>  <a href="https://arxiv.org/pdf/2003.02245" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Varun Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choudhary%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashutosh Choudhary</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eunah Cho</a><br>
<font size="3">
Abstract: Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of pre-trained transformer based models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. On three classification benchmarks, pre-trained Seq2Seq model outperforms other models. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言模型基于预先训练模式，如BERT提供了在不同的NLP任务显著的收益。在本文中，我们研究了不同类型的预训练的基于变压器的模型，比如有条件数据增强自回归模型（GPT-2），自动编码器模型（BERT），以及seq2seq模型（BART）。我们发现，在前面加上类标签文本序列提供了一个简单而有效的方法来调理增强数据预先训练的模式。三个分类基准，预先训练Seq2Seq模型优于其他车型。此外，我们探讨，如何条件不同的预先训练的基于模型的数据增强不同数据的多样性，以及如何以及这种方法保留类的标签信息。</font>
</div>


<hr>
<div id="paper3"> <b>3. Unsupervised Adversarial Domain Adaptation for Implicit Discourse  Relation Classification</b>  <a href="https://arxiv.org/pdf/2003.02244" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hsin-Ping Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J+J" target="_blank" rel="noopener" style="color:#0000EE;">Junyi Jessy Li</a><br>
<font size="3">
Abstract: Implicit discourse relations are not only more challenging to classify, but also to annotate, than their explicit counterparts. We tackle situations where training data for implicit relations are lacking, and exploit domain adaptation from explicit relations (Ji et al., 2015). We present an unsupervised adversarial domain adaptive network equipped with a reconstruction component. Our system outperforms prior works and other adversarial benchmarks for unsupervised domain adaptation. Additionally, we extend our system to take advantage of labeled data if some are available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：隐性语篇关系不仅更具挑战性进行分类，而且还注释，比他们的同行明确。我们解决了隐含关系训练数据匮乏的地方的情况，并利用从显性关系领域适应性（Ji等人，2015年）。我们提出装备有一重建成分的无监督对抗性域自适应网络。我们的系统优于之前的作品和无监督领域适应性等对抗性的基准。此外，我们扩展我们的系统采取标记数据的优势，如果一些可用。</font>
</div>


<hr>
<div id="paper4"> <b>4. Evaluating Low-Resource Machine Translation between Chinese and  Vietnamese with Back-Translation</b>  <a href="https://arxiv.org/pdf/2003.02197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongzheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heyan Huang</a><br>
<font size="3">
Abstract: Back translation (BT) has been widely used and become one of standard techniques for data augmentation in Neural Machine Translation (NMT), BT has proven to be helpful for improving the performance of translation effectively, especially for low-resource scenarios. While most works related to BT mainly focus on European languages, few of them study languages in other areas around the world. In this paper, we investigate the impacts of BT on Asia language translations between the extremely low-resource Chinese and Vietnamese language pair. We evaluate and compare the effects of different sizes of synthetic data on both NMT and Statistical Machine Translation (SMT) models for Chinese to Vietnamese and Vietnamese to Chinese, with character-based and word-based settings. Some conclusions from previous works are partially confirmed and we also draw some other interesting findings and conclusions, which are beneficial to understand BT further. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：回译（BT）已被广泛使用，成为在神经机器翻译（NMT）数据增强标准技术之一，BT已经被证明是有效地提高翻译的性能，特别是对于低资源场景很有帮助。虽然与BT大多数作品主要集中在欧洲语言，他们几个人的研究在世界各地的其他地区语言。在本文中，我们研究了BT对亚洲语言翻译极低的资源的中国和越南的语言对之间的影响。我们评价和比较不同的尺寸上都NMT和统计机器翻译（SMT）模型对中国越南和越南到中国的综合数据，与基于词的基于字符和设置的影响。从以前的作品有些结论部分证实，我们也借鉴了其他一些有趣的发现和结论，这有利于进一步了解BT。</font>
</div>


<hr>
<div id="paper5"> <b>5. Sequential Neural Networks for Noetic End-to-End Response Selection</b>  <a href="https://arxiv.org/pdf/2003.02126" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Wang</a><br>
<font size="3">
Abstract: The noetic end-to-end response selection challenge as one track in the 7th Dialog System Technology Challenges (DSTC7) aims to push the state of the art of utterance classification for real world goal-oriented dialog systems, for which participants need to select the correct next utterances from a set of candidates for the multi-turn context. This paper presents our systems that are ranked top 1 on both datasets under this challenge, one focused and small (Advising) and the other more diverse and large (Ubuntu). Previous state-of-the-art models use hierarchy-based (utterance-level and token-level) neural networks to explicitly model the interactions among different turns' utterances for context modeling. In this paper, we investigate a sequential matching model based only on chain sequence for multi-turn response selection. Our results demonstrate that the potentials of sequential matching approaches have not yet been fully exploited in the past for multi-turn response selection. In addition to ranking top 1 in the challenge, the proposed model outperforms all previous models, including state-of-the-art hierarchy-based models, on two large-scale public multi-turn response selection benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：智力的终端到终端的反应选择的挑战，因为在第7对话系统的技术挑战（DSTC7）目标之一的轨道，推动艺术的真实世界面向目标的对话系统，其参与者需要话语分类的状态从一组的多匝背景候选中选择正确的下话语。本文介绍了我们正在根据这一挑战排名榜首1在两个数据集中系统，一个专注和小（通知）和其他更加多样化和大型（Ubuntu的）。上一页国家的最先进的机型采用层次结构为基础的（话语级别和标记级别）神经网络，以明确的相互作用不同转弯话语的上下文建模中的模型。在本文中，我们调查仅仅基于多转响应选定链序列连续匹配模型。我们的研究结果表明，顺序匹配方法的潜力尚未完全过去，多转反应选择利用。除了挑战排名前1，该模型优于以前的所有型号，包括国家的最先进的基于层次的模型，两个大型公共多圈响应的选择标准数据集。</font>
</div>


<hr>
<div id="paper6"> <b>6. Posterior-GAN: Towards Informative and Coherent Response Generation with  Posterior Generative Adversarial Network</b>  <a href="https://arxiv.org/pdf/2003.02020" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shaoxiong Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongshen Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dawei Yin</a><br>
<font size="3">
Abstract: Neural conversational models learn to generate responses by taking into account the dialog history. These models are typically optimized over the query-response pairs with a maximum likelihood estimation objective. However, the query-response tuples are naturally loosely coupled, and there exist multiple responses that can respond to a given query, which leads the conversational model learning burdensome. Besides, the general dull response problem is even worsened when the model is confronted with meaningless response training instances. Intuitively, a high-quality response not only responds to the given query but also links up to the future conversations, in this paper, we leverage the query-response-future turn triples to induce the generated responses that consider both the given context and the future conversations. To facilitate the modeling of these triples, we further propose a novel encoder-decoder based generative adversarial learning framework, Posterior Generative Adversarial Network (Posterior-GAN), which consists of a forward and a backward generative discriminator to cooperatively encourage the generated response to be informative and coherent by two complementary assessment perspectives. Experimental results demonstrate that our method effectively boosts the informativeness and coherence of the generated response on both automatic and human evaluation, which verifies the advantages of considering two assessment perspectives. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经会话模型学会考虑对话历史产生响应。这些模型在用最大似然估计目标的查询和响应对优化典型。但是，查询响应元组自然松散耦合，并存在可以对给定的查询，从而导致会话模型学习负担响应多个响应。此外，当模型正面临着毫无意义的应对训练情况一般沉闷的响应问题甚至恶化。直观地说，一个高品质的响应，不仅响应给定的查询，但还链接到未来的对话，在本文中，我们利用查询响应，未来又将三元组诱导考虑给定的背景和双方产生的响应未来的对话。为了便于这些三元组的建模，我们进一步提出了一种基于生成对抗学习框架的新的编码器 - 解码器，后剖成对抗性网络（后路-GAN），其由前向和后向生成鉴别器的协同促进所产生的响应为内容丰富，由两个互补的评估观点一致。实验结果表明，该方法有效地提升在自动和人工评估，从而验证考虑两种评价视点的优势产生响应的信息量和连贯性。</font>
</div>


<hr>
<div id="paper7"> <b>7. Restoration of Fragmentary Babylonian Texts Using Recurrent Neural  Networks</b>  <a href="https://arxiv.org/pdf/2003.01912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fetaya%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ethan Fetaya</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lifshitz%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yonatan Lifshitz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Aaron%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elad Aaron</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gordin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shai Gordin</a><br>
<font size="3">
Abstract: The main source of information regarding ancient Mesopotamian history and culture are clay cuneiform tablets. Despite being an invaluable resource, many tablets are fragmented leading to missing information. Currently these missing parts are manually completed by experts. In this work we investigate the possibility of assisting scholars and even automatically completing the breaks in ancient Akkadian texts from Achaemenid period Babylonia by modelling the language using recurrent neural networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：信息关于古代美索不达米亚的历史和文化的主要来源是粘土楔形文字片。尽管是一个非常宝贵的资源，许多片支离破碎导致丢失的信息。目前，这些缺失的部分是由人工完成的专家。在这项工作中，我们协助调查的学者，甚至通过模拟使用递归神经网络的语言自动完成从阿契美尼德时期巴比伦古阿卡德文字，断裂的可能性。</font>
</div>


<hr>
<div id="paper8"> <b>8. SeMemNN: A Semantic Matrix-Based Memory Neural Network for Text  Classification</b>  <a href="https://arxiv.org/pdf/2003.01857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changzeng Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaoran Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ishi%2C+C+T" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Toshinori Ishi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yoshikawa%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuichiro Yoshikawa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ishiguro%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiroshi Ishiguro</a><br>
<font size="3">
Abstract: Text categorization is the task of assigning labels to documents written in a natural language, and it has numerous real-world applications including sentiment analysis as well as traditional topic assignment tasks. In this paper, we propose 5 different configurations for the semantic matrix-based memory neural network with end-to-end learning manner and evaluate our proposed method on two corpora of news articles (AG news, Sogou news). The best performance of our proposed method outperforms the baseline VDCNN models on the text classification task and gives a faster speed for learning semantics. Moreover, we also evaluate our model on small scale datasets. The results show that our proposed method can still achieve better results in comparison to VDCNN on the small scale dataset. This paper is to appear in the Proceedings of the 2020 IEEE 14th International Conference on Semantic Computing (ICSC 2020), San Diego, California, 2020. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文本分类是分配标签写在一个自然语言文档的任务，它有许多现实世界的应用，包括情感分析，以及传统的话题分配任务。在本文中，我们提出了与终端到终端的学习方式，基于语义矩阵存储神经网络5个不同的配置和评估我们提出了新闻报道的两个语料库（AG新闻，搜狗新闻）方法。我们提出的方法的最佳性能优于对文本分类的任务基线VDCNN模型，并给出了学习语义更快的速度。此外，我们还评估了小规模的数据集模型。结果表明，该方法仍然可以取得更好的成绩相比，VDCNN在小规模数据集。本文是出现在语义计算的2020年IEEE第14届国际大会（ICSC 2020）的诉讼，圣迭戈，加利福尼亚州，2020年</font>
</div>


<hr>
<div id="paper9"> <b>9. HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks  with Hyperdimensional Computing enabled Embedding of n-gram Statistics</b>  <a href="https://arxiv.org/pdf/2003.01821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Alonso%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Alonso</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shridhar%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kumar Shridhar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kleyko%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Denis Kleyko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Osipov%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evgeny Osipov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marcus Liwicki</a><br>
<font size="3">
Abstract: Recent advances in Deep Learning have led to a significant performance increase on several NLP tasks, however, the models become more and more computationally demanding. Therefore, this paper tackles the domain of computationally efficient algorithms for NLP tasks. In particular, it investigates distributed representations of n-gram statistics of texts. The representations are formed using hyperdimensional computing enabled embedding. These representations then serve as features, which are used as input to standard classifiers. We investigate the applicability of the embedding on one large and three small standard datasets for classification tasks using nine classifiers. The embedding achieved on par F1 scores while decreasing the time and memory requirements by several times compared to the conventional n-gram statistics, e.g., for one of the classifiers on a small dataset, the memory reduction was 6.18 times; while train and test speed-ups were 4.62 and 3.84 times, respectively. For many classifiers on the large dataset, the memory reduction was about 100 times and train and test speed-ups were over 100 times. More importantly, the usage of distributed representations formed via hyperdimensional computing allows dissecting the strict dependency between the dimensionality of the representation and the parameters of n-gram statistics, thus, opening a room for tradeoffs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学的最新进展已经导致了一些NLP任务显著的性能提升，但是，模型越来越多的计算能力的要求。因此，本文铲球对NLP任务的高效计算算法的域。特别是，调查分布文本的n-gram统计表示。这些表示使用超维度计算启用了嵌入形成。这些表述则充当功能，这被用作输入标准分类。我们研究了嵌入在一个大的和三个小数据集标准对采用九个分类分类任务的适用性。看齐F1分数实现同时降低相比于常规的n-gram统计数倍的时间和内存需求，嵌入例如，用于在一个小数据集的分类器中的一个，所述存储器减少为6.18倍;而火车和测试速度起坐分别为4.62和3.84倍。有关大数据集众多分类，存储量减少了约100倍，培养和测试速度起坐均超过100倍。更重要的是，经由超维度计算形成分布表示的使用允许解剖表示的维数和n-gram中的统计参数之间的严格相关性，因此，开启了折衷的余地。</font>
</div>


<hr>
<div id="paper10"> <b>10. AlignTTS: Efficient Feed-Forward Text-to-Speech System without Explicit  Alignment</b>  <a href="https://arxiv.org/pdf/2003.01950" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zeng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Zeng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianzong Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cheng%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ning Cheng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xia%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tian Xia</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xiao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Xiao</a><br>
<font size="3">
Abstract: Targeting at both high efficiency and performance, we propose AlignTTS to predict the mel-spectrum in parallel. AlignTTS is based on a Feed-Forward Transformer which generates mel-spectrum from a sequence of characters, and the duration of each character is determined by a duration predictor.Instead of adopting the attention mechanism in Transformer TTS to align text to mel-spectrum, the alignment loss is presented to consider all possible alignments in training by use of dynamic programming. Experiments on the LJSpeech dataset show that our model achieves not only state-of-the-art performance which outperforms Transformer TTS by 0.03 in mean option score (MOS), but also a high efficiency which is more than 50 times faster than real-time. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在两个高效率和性能定位，我们提出AlignTTS预测并行梅尔频谱。 AlignTTS是基于前馈变压器，其从字符序列生成梅尔频谱，并且确定每一个字符的持续时间被持续时间predictor.Instead采用在变压器TTS注意机制来对齐文本到梅尔谱，提出对准损失利用动态规划的考虑培训所有可能的路线。在LJSpeech数据集的实验表明我们的模型实现不仅是国家的最先进的，其平均选项得分（MOS）优于变压器TTS 0.03的表现，也是一种高效率的比实时更快的超过50倍。</font>
</div>


<hr>
<div id="paper11"> <b>11. GraphTTS: graph-to-sequence modelling in neural text-to-speech</b>  <a href="https://arxiv.org/pdf/2003.01924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Sun%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aolan Sun</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianzong Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cheng%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ning Cheng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Peng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huayi Peng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zeng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Zeng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xiao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Xiao</a><br>
<font size="3">
Abstract: This paper leverages the graph-to-sequence method in neural text-to-speech (GraphTTS), which maps the graph embedding of the input sequence to spectrograms. The graphical inputs consist of node and edge representations constructed from input texts. The encoding of these graphical inputs incorporates syntax information by a GNN encoder module. Besides, applying the encoder of GraphTTS as a graph auxiliary encoder (GAE) can analyse prosody information from the semantic structure of texts. This can remove the manual selection of reference audios process and makes prosody modelling an end-to-end procedure. Experimental analysis shows that GraphTTS outperforms the state-of-the-art sequence-to-sequence models by 0.24 in Mean Opinion Score (MOS). GAE can adjust the pause, ventilation and tones of synthesised audios automatically. This experimental conclusion may give some inspiration to researchers working on improving speech synthesis prosody. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：利用神经文本到语音（GraphTTS），该图形中嵌入输入序列谱图映射图对序列的方法。所述图形输入包括从输入的文本构成节点和边表示。这些图形输入的编码包含由GNN编码器模块的语法信息。此外，施加GraphTTS的编码器的图辅助编码器（GAE）可以分析从文本的语义结构韵律信息。这可以去除参考音频处理的手动选择和使韵律模型的端至端的过程。实验分析显示，GraphTTS 0.24在平均意见得分（MOS）优于状态的最先进的序列到序列的机型。 GAE可以自动调整合成音的暂停，通风和音调。这个实验的结论可能会提供一些灵感，在提高语音合成韵律工作的研究人员。</font>
</div>


<hr>
<div id="paper12"> <b>12. On Emergent Communication in Competitive Multi-Agent Teams</b>  <a href="https://arxiv.org/pdf/2003.01848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+P+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Pu Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeffrey Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruslan Salakhutdinov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morency%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Louis-Philippe Morency</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kottur%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Satwik Kottur</a><br>
<font size="3">
Abstract: Several recent works have found the emergence of grounded compositional language in the communication protocols developed by mostly cooperative multi-agent systems when learned end-to-end to maximize performance on a downstream task. However, human populations learn to solve complex tasks involving communicative behaviors not only in fully cooperative settings but also in scenarios where competition acts as an additional external pressure for improvement. In this work, we investigate whether competition for performance from an external, similar agent team could act as a social influence that encourages multi-agent populations to develop better communication protocols for improved performance, compositionality, and convergence speed. We start from Task & Talk, a previously proposed referential game between two cooperative agents as our testbed and extend it into Task, Talk & Compete, a game involving two competitive teams each consisting of two aforementioned cooperative agents. Using this new setting, we provide an empirical study demonstrating the impact of competitive influence on multi-agent teams. Our results show that an external competitive influence leads to improved accuracy and generalization, as well as faster emergence of communicative languages that are more informative and compositional. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的一些作品已经发现接地组成的语言中所学到的端至端时，上下游任务，最大限度地提高性能的主要合作多智能体系统开发的通信协议的出现。然而，人类群体学会解决不仅涉及在完全合作的设置，而且在场景中的竞争作为改善额外的外部压力的交际行为，复杂的任务。在这项工作中，我们研究了来自外部，类似的代理团队绩效的竞争是否能为鼓励多主体人群制定，以改善性能，组合性和收敛速度更好的通信协议的社会影响作用。我们从任务与交流，为我们的测试平台两个互惠代理之间的先前提出的指称游戏开始，并延伸到任务，对话和竞争，涉及到两个有竞争力的球队分别由上述两个合作代理的游戏。使用这个新的设置，我们提供了一个实证研究表明竞争影响多代理团队的影响。我们的研究结果表明，在外部竞争的影响导致提高精度和泛化，以及更具信息和组成交际语言的更快出现。</font>
</div>


<hr>
<div id="paper13"> <b>13. Discover Your Social Identity from What You Tweet: a Content Based  Approach</b>  <a href="https://arxiv.org/pdf/2003.01797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Binxuan Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carley%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">Kathleen M. Carley</a><br>
<font size="3">
Abstract: An identity denotes the role an individual or a group plays in highly differentiated contemporary societies. In this paper, our goal is to classify Twitter users based on their role identities. We first collect a coarse-grained public figure dataset automatically, then manually label a more fine-grained identity dataset. We propose a hierarchical self-attention neural network for Twitter user role identity classification. Our experiments demonstrate that the proposed model significantly outperforms multiple baselines. We further propose a transfer learning scheme that improves our model's performance by a large margin. Such transfer learning also greatly reduces the need for a large amount of human labeled data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：身份表示角色的个人或在高度分化当代社会的基团戏剧。在本文中，我们的目标是基于角色的身份Twitter用户进行分类。我们首先会自动收集粗粒度的公众人物数据集，然后手动标注更细粒度的身份数据集。我们提出了Twitter的用户角色身份分类分层的自我关注的神经网络。我们的实验表明，该模型显著优于多个基准。我们进一步建议，提高了一大截我们的模型的性能转移的学习方案。这种转移学习也大大减少了对大量的人力标记数据的需要。</font>
</div>


<hr>
<div id="paper14"> <b>14. Untangling in Invariant Speech Recognition</b>  <a href="https://arxiv.org/pdf/2003.01787" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Stephenson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cory Stephenson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feather%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jenelle Feather</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Padhy%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suchismita Padhy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Elibol%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oguz Elibol</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanlin Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McDermott%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Josh McDermott</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">SueYeon Chung</a><br>
<font size="3">
Abstract: Encouraged by the success of deep neural networks on a variety of visual tasks, much theoretical and experimental work has been aimed at understanding and interpreting how vision networks operate. Meanwhile, deep neural networks have also achieved impressive performance in audio processing applications, both as sub-components of larger systems and as complete end-to-end systems by themselves. Despite their empirical successes, comparatively little is understood about how these audio models accomplish these tasks. In this work, we employ a recently developed statistical mechanical theory that connects geometric properties of network representations and the separability of classes to probe how information is untangled within neural networks trained to recognize speech. We observe that speaker-specific nuisance variations are discarded by the network's hierarchy, whereas task-relevant properties such as words and phonemes are untangled in later layers. Higher level concepts such as parts-of-speech and context dependence also emerge in the later layers of the network. Finally, we find that the deep representations carry out significant temporal untangling by efficiently extracting task-relevant features at each time step of the computation. Taken together, these findings shed light on how deep auditory models process time dependent input signals to achieve invariant speech recognition, and show how different concepts emerge through the layers of the network. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通过深层神经网络对各种视觉任务成功的鼓舞，许多理论和实验工作已经瞄准理解和解释视觉网络是如何运作的。同时，深层神经网络也取得了骄人的业绩在音频处理应用中，无论是作为较大系统的子组件和完整自己的终端到终端系统。尽管他们的成功经验，相对较少了解关于这些音频模式如何完成这些任务。在这项工作中，我们采用连接网络表示的几何特性和类探头的可分性信息如何被训练识别语音神经网络内解开的一个新近开发的统计力学理论。我们观察到的说话者特定滋扰变化是由网络的层次结构丢弃，而与任务相关的属性，如单词和音素在以后层解开。更高层次的概念，例如部件的词性和上下文依赖性也出现在网络的后面的层。最后，我们发现，深表示通过在计算中的每一步高效提取与任务相关的功能进行显著时间解开。总之，这些研究结果揭示听觉模型有多深加工时间相关的输入信号，以实现不变的语音识别，并显示不同的概念如何通过网络的各层出现光。</font>
</div>


<hr>
<div id="paper15"> <b>15. Phonetic Feedback for Speech Enhancement With and Without Parallel  Speech Data</b>  <a href="https://arxiv.org/pdf/2003.01769" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Plantinga%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Plantinga</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bagchi%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deblin Bagchi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fosler-Lussier%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Fosler-Lussier</a><br>
<font size="3">
Abstract: While deep learning systems have gained significant ground in speech enhancement research, these systems have yet to make use of the full potential of deep learning systems to provide high-level feedback. In particular, phonetic feedback is rare in speech enhancement research even though it includes valuable top-down information. We use the technique of mimic loss to provide phonetic feedback to an off-the-shelf enhancement system, and find gains in objective intelligibility scores on CHiME-4 data. This technique takes a frozen acoustic model trained on clean speech to provide valuable feedback to the enhancement model, even in the case where no parallel speech data is available. Our work is one of the first to show intelligibility improvement for neural enhancement systems without parallel speech data, and we show phonetic feedback can improve a state-of-the-art neural enhancement system trained with parallel speech data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然深学习系统在语音增强的研究已经获得了显著地，这些系统还没有充分利用深度学习系统的全部潜能提供高层次的反馈。特别是，语音反馈是，即使它包含有价值的自上而下的信息，语音增强研究少见。我们使用模拟损失的技术来提供语音反馈关闭的，现成的增强系统，并找到磬-4数据的客观清晰度得分收益。这种技术需要训练有素的清洁讲话冻结的声学模型提供有价值的反馈，以增强模式，即使在没有并行语音数据是可用的情况下。我们的工作是第一次，以示对神经增强系统的清晰度提高无并行语音数据之一，我们将展示语音反馈可以改善与并行语音数据训练一个国家的最先进的神经增强系统。</font>
</div>


<hr>
<div id="paper16"> <b>16. Towards Real-time Mispronunciation Detection in Kids' Speech</b>  <a href="https://arxiv.org/pdf/2003.01765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Plantinga%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Plantinga</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fosler-Lussier%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Fosler-Lussier</a><br>
<font size="3">
Abstract: Modern mispronunciation detection and diagnosis systems have seen significant gains in accuracy due to the introduction of deep learning. However, these systems have not been evaluated for the ability to be run in real-time, an important factor in applications that provide rapid feedback. In particular, the state-of-the-art uses bi-directional recurrent networks, where a uni-directional network may be more appropriate. Teacher-student learning is a natural approach to use to improve a uni-directional model, but when using a CTC objective, this is limited by poor alignment of outputs to evidence. We address this limitation by trying two loss terms for improving the alignments of our models. One loss is an "alignment loss" term that encourages outputs only when features do not resemble silence. The other loss term uses a uni-directional model as teacher model to align the bi-directional model. Our proposed model uses these aligned bi-directional models as teacher models. Experiments on the CSLU kids' corpus show that these changes decrease the latency of the outputs, and improve the detection rates, with a trade-off between these goals. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现代读音错误检测与诊断系统，已经看到了精度显著收益由于引入深的学问。然而，这些系统目前尚未评估在实时运行的能力，在提供快速反馈应用的重要因素。特别地，国家的最先进的用途双向复发性网络，其中的单向网络可以是更合适的。教师与学生的学习是一种自然的方法来使用，以提高单向模式，但使用CTC的目标时，这是通过对证据的输出对准差的限制。我们应对努力为改善我们的模型的比对2点损失而言，这限制。一个亏损是一个“对准损失”一词，鼓励输出只有当功能并不像沉默。其他损耗项采用了单向模型作为教师模型对准双向模式。我们提出的模型使用这些对准双向模型作为教师的模型。在CSLU孩子的语料实验表明，这些变化降低了输出的延迟时间，提高检测率，这些目标之间的权衡。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-04</title>
    <url>/2020/03/04/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-04/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> BATS: Binary ArchitecTure Search <a href="https://arxiv.org/pdf/2003.01711" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Holistically-Attracted Wireframe Parsing <a href="https://arxiv.org/pdf/2003.01663" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Unsupervised Learning of Intrinsic Structural Representation Points <a href="https://arxiv.org/pdf/2003.01661" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Volumetric landmark detection with a multi-scale shift equivariant  neural network <a href="https://arxiv.org/pdf/2003.01639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Deep Multi-Modal Sets <a href="https://arxiv.org/pdf/2003.01607" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Image Matching across Wide Baselines: From Paper to Practice <a href="https://arxiv.org/pdf/2003.01587" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Distilled Hierarchical Neural Ensembles with Adaptive Inference Cost <a href="https://arxiv.org/pdf/2003.01474" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Disentangling Physical Dynamics from Unknown Factors for Unsupervised  Video Prediction <a href="https://arxiv.org/pdf/2003.01460" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Implicit Functions in Feature Space for 3D Shape Reconstruction and  Completion <a href="https://arxiv.org/pdf/2003.01456" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Rethinking Zero-shot Video Classification: End-to-end Training for  Realistic Applications <a href="https://arxiv.org/pdf/2003.01455" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> 3D dynamic hand gestures recognition using the Leap Motion sensor and  convolutional neural networks <a href="https://arxiv.org/pdf/2003.01450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> UDD: An Underwater Open-sea Farm Object Detection Dataset for Underwater  Robot Picking <a href="https://arxiv.org/pdf/2003.01446" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> What's the relationship between CNNs and communication systems? <a href="https://arxiv.org/pdf/2003.01413" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> DeepSperm: A robust and real-time bull sperm-cell detection in densely  populated semen videos <a href="https://arxiv.org/pdf/2003.01395" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Fully Convolutional Networks for Automatically Generating Image Masks to  Train Mask R-CNN <a href="https://arxiv.org/pdf/2003.01383" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> multi-patch aggregation models for resampling detection <a href="https://arxiv.org/pdf/2003.01364" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth  and Ego-motion from Monocular Videos <a href="https://arxiv.org/pdf/2003.01360" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Gastric histopathology image segmentation using a hierarchical  conditional random field <a href="https://arxiv.org/pdf/2003.01302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Data-Free Adversarial Perturbations for Practical Black-Box Attack <a href="https://arxiv.org/pdf/2003.01295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Trained Model Fusion for Object Detection using Gating Network <a href="https://arxiv.org/pdf/2003.01288" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Towards Noise-resistant Object Detection with Noisy Annotations <a href="https://arxiv.org/pdf/2003.01285" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Disrupting DeepFakes: Adversarial Attacks Against Conditional Image  Translation Networks and Facial Manipulation Systems <a href="https://arxiv.org/pdf/2003.01279" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Single-Shot Pose Estimation of Surgical Robot Instruments' Shafts from  Monocular Endoscopic Images <a href="https://arxiv.org/pdf/2003.01267" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud <a href="https://arxiv.org/pdf/2003.01251" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> MVC-Net: A Convolutional Neural Network Architecture for Manifold-Valued  Images With Applications <a href="https://arxiv.org/pdf/2003.01234" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> MRI Super-Resolution with GAN and 3D Multi-Level DenseNet: Smaller,  Faster, and Better <a href="https://arxiv.org/pdf/2003.01217" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Energy-efficient and Robust Cumulative Training with Net2Net  Transformation <a href="https://arxiv.org/pdf/2003.01204" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> DEEVA: A Deep Learning and IoT Based Computer Vision System to Address  Safety and Security of Production Sites in Energy Industry <a href="https://arxiv.org/pdf/2003.01196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> LiDARNet: A Boundary-Aware Domain Adaptation Model for Lidar Point Cloud  Semantic Segmentation <a href="https://arxiv.org/pdf/2003.01174" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Understanding Contexts Inside Robot and Human Manipulation Tasks through  a Vision-Language Model and Ontology System in a Video Stream <a href="https://arxiv.org/pdf/2003.01163" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Unsupervised Domain Adaptation for Mammogram Image Classification: A  Promising Tool for Model Generalization <a href="https://arxiv.org/pdf/2003.01111" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Learning from Suspected Target: Bootstrapping Performance for Breast  Cancer Detection in Mammography <a href="https://arxiv.org/pdf/2003.01109" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Reliable evaluation of adversarial robustness with an ensemble of  diverse parameter-free attacks <a href="https://arxiv.org/pdf/2003.01690" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Compact Surjective Encoding Autoencoder for Unsupervised Novelty  Detection <a href="https://arxiv.org/pdf/2003.01665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> BUSU-Net: An Ensemble U-Net Framework for Medical Image Segmentation <a href="https://arxiv.org/pdf/2003.01581" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> XGPT: Cross-modal Generative Pre-Training for Image Captioning <a href="https://arxiv.org/pdf/2003.01473" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Curriculum By Texture <a href="https://arxiv.org/pdf/2003.01367" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Shape analysis via inconsistent surface registration <a href="https://arxiv.org/pdf/2003.01357" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> DDU-Nets: Distributed Dense Model for 3D MRI Brain Tumor Segmentation <a href="https://arxiv.org/pdf/2003.01337" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Visualizing intestines for diagnostic assistance of ileus based on  intestinal region segmentation from 3D CT images <a href="https://arxiv.org/pdf/2003.01290" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> A Deep learning Approach to Generate Contrast-Enhanced Computerised  Tomography Angiography without the Use of Intravenous Contrast Agents <a href="https://arxiv.org/pdf/2003.01223" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> RandomNet: Towards Fully Automatic Neural Architecture Design for  Multimodal Learning <a href="https://arxiv.org/pdf/2003.01181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. BATS: Binary ArchitecTure Search</b>  <a href="https://arxiv.org/pdf/2003.01711" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bulat%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrian Bulat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martinez%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brais Martinez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tzimiropoulos%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Georgios Tzimiropoulos</a><br>
<font size="3">
Abstract: This paper proposes Binary ArchitecTure Search (BATS), a framework that drastically reduces the accuracy gap between binary neural networks and their real-valued counterparts by means of Neural Architecture Search (NAS). We show that directly applying NAS to the binary domain provides very poor results. To alleviate this, we describe, to our knowledge, for the first time, the 3 key ingredients for successfully applying NAS to the binary domain. Specifically, we (1) introduce and design a novel binary-oriented search space, (2) propose a new mechanism for controlling and stabilising the resulting searched topologies, (3) propose and validate a series of new search strategies for binary networks that lead to faster convergence and lower search times. Experimental results demonstrate the effectiveness of the proposed approach and the necessity of searching in the binary space directly. Moreover, (4) we set a new state-of-the-art for binary neural networks on CIFAR10, CIFAR100 and ImageNet datasets. Code will be made available this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了二进制架构搜索（BATS），一个框架，大大减少由神经结构搜索（NAS）的手段二元神经网络和他们的真实值同行的准确性差距。我们发现，直接将NAS二进制领域提供了非常差的结果。为了缓解这种情况，我们描述，就我们所知，第一次，3个关键因素为成功运用NAS二进制领域。具体来说，我们（1）引入和设计的新颖的面向二进制搜索空间，（2）提出了一种新的机构，用于控制和稳定所得搜索拓扑，（3）提出并验证了一系列新的搜索策略二进制网络，铅以更快的收敛和较低的搜索时间。实验结果表明，该方法的有效性和二进制空间直接搜索的必要性。此外，（4），我们设置了新的国家的最先进的用于在CIFAR10，CIFAR100和ImageNet二进制数据集的神经网络。代码将提供这HTTPS URL</font>
</div>


<hr>
<div id="paper2"> <b>2. Holistically-Attracted Wireframe Parsing</b>  <a href="https://arxiv.org/pdf/2003.01663" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianfu Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Song Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fu-Dong Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gui-Song Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangpei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torr%2C+P+H+S" target="_blank" rel="noopener" style="color:#0000EE;">Philip H.S. Torr</a><br>
<font size="3">
Abstract: This paper presents a fast and parsimonious parsing method to accurately and robustly detect a vectorized wireframe in an input image with a single forward pass. The proposed method is end-to-end trainable, consisting of three components: (i) line segment and junction proposal generation, (ii) line segment and junction matching, and (iii) line segment and junction verification. For computing line segment proposals, a novel exact dual representation is proposed which exploits a parsimonious geometric reparameterization for line segments and forms a holistic 4-dimensional attraction field map for an input image. Junctions can be treated as the "basins" in the attraction field. The proposed method is thus called Holistically-Attracted Wireframe Parser (HAWP). In experiments, the proposed method is tested on two benchmarks, the Wireframe dataset, and the YorkUrban dataset. On both benchmarks, it obtains state-of-the-art performance in terms of accuracy and efficiency. For example, on the Wireframe dataset, compared to the previous state-of-the-art method L-CNN, it improves the challenging mean structural average precision (msAP) by a large margin ($2.8\%$ absolute improvements) and achieves 29.5 FPS on single GPU ($89\%$ relative improvement). A systematic ablation study is performed to further justify the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了一种快速和简约解析方法准确且鲁棒地检测在输入图像中的矢量线框具有单个直传。所提出的方法是端至端可训练，由三个部分组成：（ⅰ）线段和连接点方案生成，（ⅱ）线段和结匹配，和（iii）的线段和连接验证。用于计算线段的提案，一个新颖的确切双重代表，提出了利用对线段和形成整体的4维吸引场的地图为输入图像中的简约几何重新参数化。结可以被视为在吸引场的“盆”。因而所提出的方法被称为整体上-吸引线框分析器（HAWP）。在实验中，所提出的方法是在两个基准，线框数据集，并且数据集YorkUrban测试。在这两个基准，它获得的精度和效率方面的国家的最先进的性能。例如，关于线框数据集，相比以前的状态的最先进的方法L-CNN，它大幅度改善了具有挑战性的平均结构平均精度（MSAP）（$ 2.8 \％$绝对改进）和达到29.5 FPS单GPU（$ 89 \％$相对改善）。系统性消融研究，以进一步证明了该方法。</font>
</div>


<hr>
<div id="paper3"> <b>3. Unsupervised Learning of Intrinsic Structural Representation Points</b>  <a href="https://arxiv.org/pdf/2003.01661" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nenglun Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingjie Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiming Cui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Runnan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ceylan%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duygu Ceylan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changhe Tu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenping Wang</a><br>
<font size="3">
Abstract: Learning structures of 3D shapes is a fundamental problem in the field of computer graphics and geometry processing. We present a simple yet interpretable unsupervised method for learning a new structural representation in the form of 3D structure points. The 3D structure points pro-duced by our method encode the shape structure intrinsi-cally and exhibit semantic consistency across all the shapeinstances with similar structures. This is a challenging goal that has not fully been achieved by other methods. Specifically, our method takes a 3D point cloud as input and encodes it as a set of local features. The local features are then passed through a novel point integration module to produce a set of 3D structure points. The chamfer distance is used as reconstruction loss to ensure the structure points lie close to the input point cloud. Extensive experiments have shown that our method outperforms the state-of-the-art on the semantic shape correspondence task and achieves achieve comparable performance with state-of-the-art on the segmentation label transfer task. Moreover, the PCA based shape embedding built upon consistent structure points demonstrates good performance in preserving the shape structures. Code is available at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：3D形状的学习结构是计算机图形学和几何处理领域的一个基本问题。我们提出了一个简单但在三维结构点的形式学习一种新的结构示意图解释无监督的方法。由我们的方法进行编码的3D结构点亲duced形状结构intrinsi-卡利并表现出语义一致性跨所有shapeinstances具有类似的结构。这是一个尚未完全被其他方法实现一个具有挑战性的目标。具体地，我们的方法需要一个3D点云作为输入并将其编码为一组的局部特征。局部特征，然后通过新颖点积分模块传递以产生一组的3D结构点。倒角距离被用作重建损失，以确保点位于接近输入点云的结构。大量的实验已经表明，我们的方法优于所述状态的最先进的语义形状对应的任务，并实现实现与相当的性能状态的最先进的分割标签转移的任务。此外，在一致的结构点建基于PCA形状嵌入演示了保留形状的结构性能良好。代码可在此HTTPS URL</font>
</div>


<hr>
<div id="paper4"> <b>4. Volumetric landmark detection with a multi-scale shift equivariant  neural network</b>  <a href="https://arxiv.org/pdf/2003.01639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianyu Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ajay Gupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabuncu%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Mert R. Sabuncu</a><br>
<font size="3">
Abstract: Deep neural networks yield promising results in a wide range of computer vision applications, including landmark detection. A major challenge for accurate anatomical landmark detection in volumetric images such as clinical CT scans is that large-scale data often constrain the capacity of the employed neural network architecture due to GPU memory limitations, which in turn can limit the precision of the output. We propose a multi-scale, end-to-end deep learning method that achieves fast and memory-efficient landmark detection in 3D images. Our architecture consists of blocks of shift-equivariant networks, each of which performs landmark detection at a different spatial scale. These blocks are connected from coarse to fine-scale, with differentiable resampling layers, so that all levels can be trained together. We also present a noise injection strategy that increases the robustness of the model and allows us to quantify uncertainty at test time. We evaluate our method for carotid artery bifurcations detection on 263 CT volumes and achieve a better than state-of-the-art accuracy with mean Euclidean distance error of 2.81mm. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络的承诺收益率在广泛的计算机视觉应用，包括标志检测结果。为精确的解剖界标检测在体积图像如临床CT扫描的一个主要挑战是，大规模的数据往往限制所采用的神经网络结构的容量由于GPU存储器的限制，这反过来又可以限制输出的精度。我们提出了一个多尺度，终端到终端的深度学习方法实现了快速和内存高效的标志检测3D图像。我们的体系结构由移位等变网络的块，其中的每一个在不同的空间尺度执行标志检测的。这些块被从粗到细尺度连接，具有可微重新采样的层，从而使所有级别可以训练在一起。我们还提出一个噪声注入策略，提高了模型的鲁棒性，使我们能够量化的测试时间的不确定性。我们评估我们对263 CT卷颈总动脉分叉检测方法，实现了优于国家的最先进的精度2.81毫米的平均欧氏距离误差。</font>
</div>


<hr>
<div id="paper5"> <b>5. Deep Multi-Modal Sets</b>  <a href="https://arxiv.org/pdf/2003.01607" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Reiter%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Austin Reiter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Menglin Jia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pu Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lim%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Ser-Nam Lim</a><br>
<font size="3">
Abstract: Many vision-related tasks benefit from reasoning over multiple modalities to leverage complementary views of data in an attempt to learn robust embedding spaces. Most deep learning-based methods rely on a late fusion technique whereby multiple feature types are encoded and concatenated and then a multi layer perceptron (MLP) combines the fused embedding to make predictions. This has several limitations, such as an unnatural enforcement that all features be present at all times as well as constraining only a constant number of occurrences of a feature modality at any given time. Furthermore, as more modalities are added, the concatenated embedding grows. To mitigate this, we propose Deep Multi-Modal Sets: a technique that represents a collection of features as an unordered set rather than one long ever-growing fixed-size vector. The set is constructed so that we have invariance both to permutations of the feature modalities as well as to the cardinality of the set. We will also show that with particular choices in our model architecture, we can yield interpretable feature performance such that during inference time we can observe which modalities are most contributing to the prediction.With this in mind, we demonstrate a scalable, multi-modal framework that reasons over different modalities to learn various types of tasks. We demonstrate new state-of-the-art performance on two multi-modal datasets (Ads-Parallelity [34] and MM-IMDb [1]). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：许多视觉相关的任务受益于多模态推理到利用数据的补充意见，企图学习强大的嵌入空间。最深基于学习的方法依赖于一个较晚融合技术，由此多个特征类型进行编码和级联然后多层感知器（MLP）结合了融合嵌入进行预测。这有诸多限制，如不自然的强制执行，所有的功能出现在所有的时间以及仅约束功能形态的出现固定数量的在任何给定的时间。此外，随着越来越多的方式加入，级联嵌入增长。为了缓解这种情况，我们提出了深多模式集：代表的功能集合作为一个无序的，而不是一个长期不断增长的固定大小的矢量的技术。该组构造，使我们拥有不变性既特征模式，以及为集合的基数排列。我们也将表明，在我们的模型架构特别的选择，我们可以得到解释的功能，性能，这样在推理时间，我们可以看到它的模式是最有助于prediction.With考虑到这一点，我们展示了一个可扩展的多模态框架即在不同方式的原因，了解不同类型的任务。我们证明在两个多模态数据集新的国家的最先进的性能（ADS-并行性[34]和MM-IMDB [1]）。</font>
</div>


<hr>
<div id="paper6"> <b>6. Image Matching across Wide Baselines: From Paper to Practice</b>  <a href="https://arxiv.org/pdf/2003.01587" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhe Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mishkin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dmytro Mishkin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mishchuk%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anastasiia Mishchuk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Matas%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiri Matas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fua%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pascal Fua</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">Kwang Moo Yi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Trulls%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eduard Trulls</a><br>
<font size="3">
Abstract: We introduce a comprehensive benchmark for local features and robust estimation algorithms, focusing on the downstream task -- the accuracy of the reconstructed camera pose -- as our primary metric. Our pipeline's modular structure allows us to easily integrate, configure, and combine methods and heuristics. We demonstrate this by embedding dozens of popular algorithms and evaluating them, from seminal works to the cutting edge of machine learning research. We show that with proper settings, classical solutions may still outperform the perceived state of the art. Besides establishing the actual state of the art, the experiments conducted in this paper reveal unexpected properties of SfM pipelines that can be exploited to help improve their performance, for both algorithmic and learned methods. Data and code are online this https URL, providing an easy-to-use and flexible framework for the benchmarking of local feature and robust estimation methods, both alongside and against top-performing methods. This work provides the basis for an open challenge on wide-baseline image matching this https URL . </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍了地方特色和强大的估计算法的综合性基准测试，侧重于下游的任务 - 重建相机姿的准确度 - 作为我们的主要指标。我们的管道的模块化结构使我们能够轻松地集成，配置和组合的方法和启发。我们通过嵌入几十种流行的算法，并进行评价，从开创性的工作机器学习研究的前沿证明这一点。我们发现，用适当的设置，经典的解决方案仍可能跑赢艺术的感知状态。除了建立艺术的实际状况，在本文中进行的实验表明，可以被利用来帮助提高其性能，对于算法和教训方法SFM管道出乎意料的特性。数据和代码是上网本HTTPS URL，为地方特色和强大的估计方法，无论是一起反对顶级表现方法的标杆一个易于使用和灵活的框架。这项工作提供了基础的公开挑战的宽基线图像匹配这个HTTPS URL。</font>
</div>


<hr>
<div id="paper7"> <b>7. Distilled Hierarchical Neural Ensembles with Adaptive Inference Cost</b>  <a href="https://arxiv.org/pdf/2003.01474" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruiz%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adria Ruiz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Verbeek%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jakob Verbeek</a><br>
<font size="3">
Abstract: Deep neural networks form the basis of state-of-the-art models across a variety of application domains. Moreover, networks that are able to dynamically adapt the computational cost of inference are important in scenarios where the amount of compute or input data varies over time. In this paper, we propose Hierarchical Neural Ensembles (HNE), a novel framework to embed an ensemble of multiple networks by sharing intermediate layers using a hierarchical structure. In HNE we control the inference cost by evaluating only a subset of models, which are organized in a nested manner. Our second contribution is a novel co-distillation method to boost the performance of ensemble predictions with low inference cost. This approach leverages the nested structure of our ensembles, to optimally allocate accuracy and diversity across the ensemble members. Comprehensive experiments over the CIFAR and ImageNet datasets confirm the effectiveness of HNE in building deep networks with adaptive inference cost for image classification. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络形成国家的最先进的机型在各种应用领域的基础。此外，网络，都能够动态地适应推理的计算成本是重要的场景，其中计算或输入的数据量随时间变化。在本文中，我们提出了分层神经系综（HNE），一种新型框架，以通过共享使用分级结构的中间层中嵌入的合奏多个网络。在HNE我们通过控制仅评估模型，以嵌套的方式组织的一个子集推理成本。我们的第二个贡献是一种新型的共蒸馏方法，以提高具有低推理成本集合预报的性能。这种方法充分利用了我们歌舞团的嵌套结构，整个乐团成员优化配置，准确性和多样性。在CIFAR和ImageNet数据集综合实验证实了HNE在建设深网络与图像分类自适应推断成本有效性。</font>
</div>


<hr>
<div id="paper8"> <b>8. Disentangling Physical Dynamics from Unknown Factors for Unsupervised  Video Prediction</b>  <a href="https://arxiv.org/pdf/2003.01460" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guen%2C+V+L" target="_blank" rel="noopener" style="color:#0000EE;">Vincent Le Guen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thome%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Thome</a><br>
<font size="3">
Abstract: Leveraging physical knowledge described by partial differential equations (PDEs) is an appealing way to improve unsupervised video prediction methods. Since physics is too restrictive for describing the full visual content of generic videos, we introduce PhyDNet, a two-branch deep architecture, which explicitly disentangles PDE dynamics from unknown complementary information. A second contribution is to propose a new recurrent physical cell (PhyCell), inspired from data assimilation techniques, for performing PDE-constrained prediction in latent space. Extensive experiments conducted on four various datasets show the ability of PhyDNet to outperform state-of-the-art methods. Ablation studies also highlight the important gain brought out by both disentanglement and PDE-constrained prediction. Finally, we show that PhyDNet presents interesting features for dealing with missing data and long-term forecasting. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由偏微分方程（PDE的）中描述的利用物理知识是提高无监督视频预测方法的一个有吸引力的方式。由于物理的限制太大，描述的通用视频完整的视觉内容，我们引入PhyDNet，一个有两个分支深结构，其中明确理顺了那些纷繁来自未知的补充信息PDE动态。第二个贡献是提出一种新的经常物理小区（PhyCell），从数据同化技术的启发，用于在潜空间中进行PDE受限预测。在四个不同的数据集进行了广泛的实验表明PhyDNet的超越状态的最先进的方法的能力。切除研究还强调双方解开和PDE受限预测带出的重要收获。最后，我们表明，PhyDNet礼物处理丢失数据和长期预测有趣的功能。</font>
</div>


<hr>
<div id="paper9"> <b>9. Implicit Functions in Feature Space for 3D Shape Reconstruction and  Completion</b>  <a href="https://arxiv.org/pdf/2003.01456" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chibane%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julian Chibane</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alldieck%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thiemo Alldieck</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pons-Moll%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerard Pons-Moll</a><br>
<font size="3">
Abstract: While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然许多作品侧重于从图像三维重建，在本文中，我们专注于三维形状重建和完成从多种3D输入，这是在某些方面不足：低分辨率和高分辨率体素，疏与密的点云，完全或不完全。的这样的3D输入的处理是一个日益重要的问题，因为它们是3D扫描仪，其正变得更容易获得的输出，并且是三维计算机视觉算法的中间输出。近日，得知隐函数都表现出极大的承诺，因为它们产生连续重建。然而，我们确定从3D输入端在重建两个限制：1）详述存在于输入数据不保留，和2）铰接的人类重建差。为了解决这个问题，我们提出了隐式功能网络（IF-篮网），它提供持续的输出，可以处理多种拓扑结构，以及完整的形状缺失或稀疏输入数据保留的最近了解到隐函数的良好特性，但批判他们还可以保留当它的细节存在于输入数据，并且可以重建铰接人类。我们的工作不同于以前的工作在两个关键方面。首先，代替使用单个载体到3D形状编码，我们提取的深特征，这与原来的欧氏空间中嵌入的形状对齐的可学习3维多尺度张量。二，而不是X-Y-Z点坐标直接进行分类，我们深分类功能从连续查询点的张量提取。我们表明，这种力量我们的模型基于全局和局部形状的结构，而不是点坐标，这是在欧氏变换任意作出决定。实验表明，IF-篮网优于在ShapeNet在3D对象重建之前的工作，并取得显著更精确三维人体重建。</font>
</div>


<hr>
<div id="paper10"> <b>10. Rethinking Zero-shot Video Classification: End-to-end Training for  Realistic Applications</b>  <a href="https://arxiv.org/pdf/2003.01455" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Brattoli%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Biagio Brattoli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tighe%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joe Tighe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhdanov%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fedor Zhdanov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Perona%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pietro Perona</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chalupka%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Krzysztof Chalupka</a><br>
<font size="3">
Abstract: Trained on large datasets, deep learning (DL) can accurately classify videos into hundreds of diverse classes. However, video data is expensive to annotate. Zero-shot learning (ZSL) proposes one solution to this problem. ZSL trains a model once, and generalizes to new tasks whose classes are not present in the training dataset. We propose the first end-to-end algorithm for ZSL in video classification. Our training procedure builds on insights from recent video classification literature and uses a trainable 3D CNN to learn the visual features. This is in contrast to previous video ZSL methods, which use pretrained feature extractors. We also extend the current benchmarking paradigm: Previous techniques aim to make the test task unknown at training time but fall short of this goal. We encourage domain shift across training and test data and disallow tailoring a ZSL model to a specific test dataset. We outperform the state-of-the-art by a wide margin. Our code, evaluation procedure and model weights are available at this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：经过培训的大型数据集，深学习（DL）可到数百种不同类别的准确分类的视频。然而，视频数据是昂贵的注释。零次学习（ZSL）提出了一个解决这个问题。 ZSL火车模型一次，并推广到新的任务，其类是不存在的训练数据集。我们建议第一终端到终端的算法在视频分类ZSL。我们的训练过程建立在最近的视频分类文献的见解，并使用可训练的3D CNN学习视觉特征。这与之前的视频ZSL方法，它使用预训练的特征提取。我们还扩展了目前的标杆范例：以前技术的目标是使在训练时间的考验，任务不明，但达不到这个目标。我们鼓励跨训练和测试数据域移位并禁止剪裁ZSL模型到一个特定的测试数据集。我们大幅跑赢了国家的最先进的。我们的代码，评估程序和模型的权重可在此http网址。</font>
</div>


<hr>
<div id="paper11"> <b>11. 3D dynamic hand gestures recognition using the Leap Motion sensor and  convolutional neural networks</b>  <a href="https://arxiv.org/pdf/2003.01450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lupinetti%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Katia Lupinetti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ranieri%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Ranieri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Giannini%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Franca Giannini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Monti%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marina Monti</a><br>
<font size="3">
Abstract: Defining methods for the automatic understanding of gestures is of paramount importance in many application contexts and in Virtual Reality applications for creating more natural and easy-to-use human-computer interaction methods. In this paper, we present a method for the recognition of a set of non-static gestures acquired through the Leap Motion sensor. The acquired gesture information is converted in color images, where the variation of hand joint positions during the gesture are projected on a plane and temporal information is represented with color intensity of the projected points. The classification of the gestures is performed using a deep Convolutional Neural Network (CNN). A modified version of the popular ResNet-50 architecture is adopted, obtained by removing the last fully connected layer and adding a new layer with as many neurons as the considered gesture classes. The method has been successfully applied to the existing reference dataset and preliminary tests have already been performed for the real-time recognition of dynamic gestures performed by users. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：手势的自动理解定义方法是非常重要的在许多应用环境和虚拟现实应用中创造更加自然和容易使用的人机交互方式。在本文中，我们提出了识别一组通过跨越运动传感器所获取的非静态的手势的方法。所获取的姿态信息变换彩色图像，其中手关节位置的手势期间的变化被投影在一个平面上和时间信息被表示与所投影的点的颜色强度。使用深卷积神经网络（CNN）执行手势的分类。流行RESNET-50架构的修改版本被采用，通过去除最后完全连接层，并用尽可能多的神经元为考虑姿势类添加新的层来获得。该方法已成功地应用到现有的引用数据集，并初步测试已经为用户进行动态手势的实时识别进行。</font>
</div>


<hr>
<div id="paper12"> <b>12. UDD: An Underwater Open-sea Farm Object Detection Dataset for Underwater  Robot Picking</b>  <a href="https://arxiv.org/pdf/2003.01446" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhihui Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chongwei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shijie Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulong Tao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Caifei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haojie Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xing Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Fan</a><br>
<font size="3">
Abstract: To promote the development of underwater robot picking in sea farms, we propose an underwater open-sea farm object detection dataset called UDD. Concretely, UDD consists of 3 categories (seacucumber, seaurchin, and scallop) with 2227 images. To the best of our knowledge, it's the first dataset collected in a real open-sea farm for underwater robot picking and we also propose a novel Poisson-blending-embedded Generative Adversarial Network (Poisson GAN) to overcome the class-imbalance and massive small objects issues in UDD. By utilizing Poisson GAN to change the number, position, even size of objects in UDD, we construct a large scale augmented dataset (AUDD) containing 18K images. Besides, in order to make the detector better adapted to the underwater picking environment, a dataset (Pre-trained dataset) for pre-training containing 590K images is also proposed. Finally, we design a lightweight network (UnderwaterNet) to address the problems that detecting small objects from cloudy underwater pictures and meeting the efficiency requirements in robots. Specifically, we design a depth-wise-convolution-based Multi-scale Contextual Features Fusion (MFF) block and a Multi-scale Blursampling (MBP) module to reduce the parameters of the network to 1.3M at 48FPS, without any loss on accuracy. Extensive experiments verify the effectiveness of the proposed UnderwaterNet, Poisson GAN, UDD, AUDD, and Pre-trained datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为促进海养殖场水下机器人采摘的发展，我们提出了一个水下开阔海域称为UDD农场物体检测数据集。具体而言，UDD由3类（海参，海胆，和扇贝）与2227倍的图像。据我们所知，这是一个真正的开放海农场水下机器人采摘中收集到的第一个数据集，我们还提出了一个新颖的泊松混合嵌入式剖成对抗性网络（泊松GAN）克服类失衡和大量小对象UDD的问题。通过利用泊松GAN改变的数量，位置，在UDD对象的大小均匀，我们构建含有18K图像的大规模数据集增强（AUDD）。此外，为了使探测器更好地适应水下环境采摘，数据集（预训练数据集）包含590K的图像前培训还提出。最后，我们设计了一个轻量级的网络（UnderwaterNet），以解决检测从浑浊的水下照片的小物件和满足在机器人的效率要求的问题。具体来说，我们设计了一个基于深度方向的卷积多尺度上下文特征融合（MFF）嵌段和多尺度Blursampling（MBP）模块，以减少网络到48FPS 1.3M的参数，而无需对精度的任何损失。大量的实验验证了该UnderwaterNet，泊松甘，UDD，AUDD，和预先训练数据集的有效性。</font>
</div>


<hr>
<div id="paper13"> <b>13. What's the relationship between CNNs and communication systems?</b>  <a href="https://arxiv.org/pdf/2003.01413" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoguang Tu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanxiang Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mei Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Ma</a><br>
<font size="3">
Abstract: The interpretability of Convolutional Neural Networks (CNNs) is an important topic in the field of computer vision. In recent years, works in this field generally adopt a mature model to reveal the internal mechanism of CNNs, helping to understand CNNs thoroughly. In this paper, we argue the working mechanism of CNNs can be revealed through a totally different interpretation, by comparing the communication systems and CNNs. This paper successfully obtained the corresponding relationship between the modules of the two, and verified the rationality of the corresponding relationship with experiments. Finally, through the analysis of some cutting-edge research on neural networks, we find the inherent relation between these two tasks can be of help in explaining these researches reasonably, as well as helping us discover the correct research direction of neural networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）的可解释性是计算机视觉领域的一个重要课题。近年来，在这一领域的作品通常采用一个成熟的模式，揭示细胞神经网络的内部机制，有助于深入了解细胞神经网络。在本文中，我们认为细胞神经网络的工作机制可以通过一个完全不同的解释显露出来，通过比较通信系统和细胞神经网络。本文成功地获得了两个模块之间的对应关系，并验证了与实验对应关系的合理性。最后，通过神经网络的一些前沿研究的分析，我们发现这两个任务之间的内在关系可以合理解释这些研究，以及帮助我​​们发现神经网络的正确的研究方向有所帮助。</font>
</div>


<hr>
<div id="paper14"> <b>14. DeepSperm: A robust and real-time bull sperm-cell detection in densely  populated semen videos</b>  <a href="https://arxiv.org/pdf/2003.01395" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hidayatullah%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Priyanto Hidayatullah</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xueting Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yamasaki%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Toshihiko Yamasaki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mengko%2C+T+L+E+R" target="_blank" rel="noopener" style="color:#0000EE;">Tati L.E.R. Mengko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Munir%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rinaldi Munir</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barlian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anggraini Barlian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sukmawati%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eros Sukmawati</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Supraptono%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Supraptono Supraptono</a><br>
<font size="3">
Abstract: Background and Objective: Object detection is a primary research interest in computer vision. Sperm-cell detection in a densely populated bull semen microscopic observation video presents challenges such as partial occlusion, vast number of objects in a single video frame, tiny size of the object, artifacts, low contrast, and blurry objects because of the rapid movement of the sperm cells. This study proposes an architecture, called DeepSperm, that solves the aforementioned challenges and is more accurate and faster than state-of-the-art architectures. Methods: In the proposed architecture, we use only one detection layer, which is specific for small object detection. For handling overfitting and increasing accuracy, we set a higher network resolution, use a dropout layer, and perform data augmentation on hue, saturation, and exposure. Several hyper-parameters are tuned to achieve better performance. We compare our proposed method with those of a conventional image processing-based object-detection method, you only look once (YOLOv3), and mask region-based convolutional neural network (Mask R-CNN). Results: In our experiment, we achieve 86.91 mAP on the test dataset and a processing speed of 50.3 fps. In comparison with YOLOv3, we achieve an increase of 16.66 mAP point, 3.26 x faster on testing, and 1.4 x faster on training with a small training dataset, which contains 40 video frames. The weights file size was also reduced significantly, with 16.94 x smaller than that of YOLOv3. Moreover, it requires 1.3 x less graphical processing unit (GPU) memory than YOLOv3. Conclusions: This study proposes DeepSperm, which is a simple, effective, and efficient architecture with its hyper-parameters and configuration to detect bull sperm cells robustly in real time. In our experiment, we surpass the state of the art in terms of accuracy, speed, and resource needs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：背景与目的：目标检测是计算机视觉的主要研究兴趣。在人口密集的牛精子细胞检测精液显微镜观察视频呈现为部分遮挡，在单个视频帧广大数目的对象，对象，构件，低对比度的小尺寸，并且由于快速运动的模糊对象的挑战，例如精子细胞。这项研究提出了一种架构，称为DeepSperm，是解决上述挑战，更准确，更不是国家的最先进的架构更快。方法：在所提出的架构中，我们仅使用一个检测层，这是具体的小物体的检测。处理过拟合，提高精度，提出了更高的网络的分辨率，使用差层，并在色相，饱和度和曝光进行数据扩充。几个超参数调整，以达到更好的性能。我们比较了我们提出的方法与传统的基于图像处理对象的检测方法，你只能看一次（YOLOv3），并掩盖基于区域的卷积神经网络（面膜R-CNN）。结果：在我们的实验中，我们实现了对测试数据集和50.3 fps的处理速度86.91地图。在与YOLOv3比较，我们在测试3.26 X更快达到16.66地图点的增加，和1.4×培训用小训练数据集，其中包含40个视频帧更快。权重文件的大小也显著减少，比YOLOv3小16.94 X。此外，它需要1.3×更少的图形处理单元（GPU）的内存比YOLOv3。结论：本研究提出DeepSperm，这是一种简单，有效和高效的结构与它的超参数和配置在实时检测公牛精子细胞鲁棒。在我们的实验中，我们超越在精度，速度和资源需求方面的技术状态。</font>
</div>


<hr>
<div id="paper15"> <b>15. Fully Convolutional Networks for Automatically Generating Image Masks to  Train Mask R-CNN</b>  <a href="https://arxiv.org/pdf/2003.01383" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Siebert%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Jan Paul Siebert</a><br>
<font size="3">
Abstract: This paper proposes a novel automatically generating image masks method for the state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method achieves the best results in object detection until now, however, it is very time-consuming and laborious to get the object Masks for training, the proposed method is composed by a two-stage design, to automatically generating image masks, the first stage implements a fully convolutional networks (FCN) based segmentation network, the second stage network, a Mask R-CNN based object detection network, which is trained on the object image masks from FCN output, the original input image, and additional label information. Through experimentation, our proposed method can obtain the image masks automatically to train Mask R-CNN, and it can achieve very high classification accuracy with an over 90% mean of average precision (mAP) for segmentation </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种状态的最先进的面膜R-CNN深学习方法的新颖自动生成图像的掩模的方法。面具R-CNN方法实现了在对象检测最好的结果到现在为止，但是，这是非常费时和费力以获取训练对象面具，所提出的方法是通过两阶段的设计构成，以自动生成图像掩模，第一级器具完全卷积网络（FCN）基于分割网络，第二级网络，掩模R-CNN基于物体检测网络，这是在从FCN输出，原始输入图像的对象图像掩模的培训，和附加的标签信息。通过实验，我们提出的方法能够自动获得图像掩模以列车面膜R-CNN，并且它可以实现为分割用平均精确度（MAP）的超过90％的平均非常高的分类精度</font>
</div>


<hr>
<div id="paper16"> <b>16. multi-patch aggregation models for resampling detection</b>  <a href="https://arxiv.org/pdf/2003.01364" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lamba%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Lamba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mitra%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaushik Mitra</a><br>
<font size="3">
Abstract: Images captured nowadays are of varying dimensions with smartphones and DSLR's allowing users to choose from a list of available image resolutions. It is therefore imperative for forensic algorithms such as resampling detection to scale well for images of varying dimensions. However, in our experiments, we observed that many state-of-the-art forensic algorithms are sensitive to image size and their performance quickly degenerates when operated on images of diverse dimensions despite re-training them using multiple image sizes. To handle this issue, we propose a novel pooling strategy called ITERATIVE POOLING. This pooling strategy can dynamically adjust input tensors in a discrete without much loss of information as in ROI Max-pooling. This pooling strategy can be used with any of the existing deep models and for demonstration purposes, we show its utility on Resnet-18 for the case of resampling detection a fundamental operation for any image sought of image manipulation. Compared to existing strategies and Max-pooling it gives up to 7-8% improvement on public datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：今天拍摄的图像以不同的智能手机和数码单反相机的用户允许尺寸从可用图像分辨率的列表中进行选择的。因此，法医算法，如二次采样检测到规模以及对不同尺寸的图像势在必行。然而，在我们的实验中，我们观察到，尽管时再培训他们使用多种大小的图片在不同尺寸的图像操作，许多国家的最先进的法医算法对图像大小和它们的性能迅速退化敏感。为了解决这个问题，我们提出了所谓的迭代汇集了新颖的合并策略。这个池策略可以动态地调整在一个离散的输入张量没有信息多大损失如在ROI最大 - 池。这种合并策略可以与任何现有的深模型和示范的目的可以使用，我们将展示其对RESNET-18实用程序，用于二次采样检测要求图像处理的任何图像的基本操作的情况下。相较于现有的战略和Max-汇集它给公共数据集高达7-8％的提升。</font>
</div>


<hr>
<div id="paper17"> <b>17. DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth  and Ego-motion from Monocular Videos</b>  <a href="https://arxiv.org/pdf/2003.01360" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hualie Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laiyan Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Huang</a><br>
<font size="3">
Abstract: Unsupervised learning of depth and ego-motion from unlabelled monocular videos has recently drawn attention as it has notable advantages than the supervised ones. It uses the photometric errors between the target view and the synthesized views from its adjacent source views as the loss. Although significant progress has been made, the learning still suffers from occlusion and scene dynamics. This paper shows that carefully manipulating photometric errors can tackle these difficulties better. The primary improvement is achieved by masking out the invisible or nonstationary pixels in the photometric error map using a statistical technique. With this outlier masking approach, the depth of objects that move in the opposite direction to the camera can be estimated more accurately. According to our best knowledge, such objects have not been seriously considered in the previous work, even though they pose a higher risk in applications like autonomous driving. We also propose an efficient weighted multi-scale scheme to reduce the artifacts in the predicted depth maps. Extensive experiments on the KITTI dataset show the effectiveness of the proposed approaches. The overall system achieves state-of-the-art performance on both depth and ego-motion estimation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深度和未标记的单目视频中的自我运动的无监督学习最近引起关注，因为它比那些监管显着的优点。它使用目标视图和与其相邻源视图作为损失合成视图之间的测光误差。虽然显著已经取得了进展，学习仍然闭塞和场景的动态受到影响。本文表明精心操作的测光误差可以更好地解决这些困难。的主要改进是通过使用统计技术在测光误差地图屏蔽掉不可见或不稳定的像素来实现。与此异常值掩蔽方法中，对象的深度，在相反的方向上移动到摄像机可以更精确地估算。据我们所知，这样的对象还没有受到严重的前期工作考虑，即使它们对像自动驾驶应用的风险较高。我们还提出了一种高效的加权多尺度方案以降低预测的深度图伪影。在KITTI大量的实验数据集验证了该方法的有效性。整个系统上实现深度和自运动估计状态的最先进的性能。</font>
</div>


<hr>
<div id="paper18"> <b>18. Gastric histopathology image segmentation using a hierarchical  conditional random field</b>  <a href="https://arxiv.org/pdf/2003.01302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changhao Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyan Li</a><br>
<font size="3">
Abstract: In this paper, a Hierarchical Conditional Random Field (HCRF) model based Gastric Histopathology Image Segmentation (GHIS) method is proposed, which can localize abnormal (cancer) regions in gastric histopathology images obtained by optical microscope to assist histopathologists in medical work. First, to obtain pixel-level segmentation information, we retrain a Convolutional Neural Network (CNN) to build up our pixel-level potentials. Then, in order to obtain abundant spatial segmentation information in patch-level, we fine-tune another three CNNs to build up our patch-level potentials. Thirdly, based on the pixel and patch-level potentials, our HCRF model is structured. Finally, graph-based post-processing is applied to further improve our segmentation performance. In the experiment, a segmentation accuracy of 78.91% is achieved on a Hematoxylin and Eosin (H&E) stained gastric histopathological dataset with 560 images, showing the effectiveness and future potential of the proposed GHIS method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，一个分层条件随机场（HCRF）基于模型胃组织病理学图像分割（GHIS）方法提出了一种能够在由光学显微镜获得，以协助医疗工作组织病理学家胃组织病理学图像本地化异常（癌症）的区域。首先，为了获得像素级的细分信息，我们重新训练卷积神经网络（CNN）建立我们的像素级的潜力。然后，为了获得补丁级别的丰富的空间分割的信息，我们微调另外三个细胞神经网络来建立我们的补丁级别的潜力。第三，基于像素和补丁级别的潜力，我们的HCRF模型的结构。最后，基于图的后处理应用，进一步提高我们的分割性能。在实验中，的78.91％分割精度上的苏木精和实现曙红（H＆E）染色的组织病理学胃与数据集560倍的图像，示出所提出的GHIS方法的有效性和未来潜力。</font>
</div>


<hr>
<div id="paper19"> <b>19. Data-Free Adversarial Perturbations for Practical Black-Box Attack</b>  <a href="https://arxiv.org/pdf/2003.01295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">ZhaoXin Huan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulong Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Shang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chilin Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Zhou</a><br>
<font size="3">
Abstract: Neural networks are vulnerable to adversarial examples, which are malicious inputs crafted to fool pre-trained models. Adversarial examples often exhibit black-box attacking transferability, which allows that adversarial examples crafted for one model can fool another model. However, existing black-box attack methods require samples from the training data distribution to improve the transferability of adversarial examples across different models. Because of the data dependence, the fooling ability of adversarial perturbations is only applicable when training data are accessible. In this paper, we present a data-free method for crafting adversarial perturbations that can fool a target model without any knowledge about the training data distribution. In the practical setting of a black-box attack scenario where attackers do not have access to target models and training data, our method achieves high fooling rates on target models and outperforms other universal adversarial perturbation methods. Our method empirically shows that current deep learning models are still at risk even when the attackers do not have access to training data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经网络是容易受到对抗性的例子，这是恶意输入制作的愚弄预先训练模式。对抗性的例子常常表现出黑箱攻击转让，其允许对抗性的例子制作一个模型可以糊弄另一种模式。但是，现有的黑盒攻击方法需要从训练数据分发样本，以提高不同模型对抗例子转移性。由于数据的依赖，当训练数据是可访问的对抗扰动的能力嘴硬只适用。在本文中，我们提出了各具特色的对抗性干扰，可以骗过目标模型没有有关训练数据分布的任何知识无数据的方法。在黑盒攻击场景，攻击者没有获得目标模型和训练数据的实际环境，我们的方法实现对目标模型的高嘴硬率，优于其他通用对抗性的扰动方法。我们的方法经验表明，目前的深度学习模式仍处于即使攻击者没有获得训练数据的风险。</font>
</div>


<hr>
<div id="paper20"> <b>20. Trained Model Fusion for Object Detection using Gating Network</b>  <a href="https://arxiv.org/pdf/2003.01288" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Inoshita%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tetsuo Inoshita</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nakatani%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuichi Nakatani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Takahashi%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Katsuhiko Takahashi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ishii%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asuka Ishii</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nakano%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gaku Nakano</a><br>
<font size="3">
Abstract: The major approaches of transfer learning in computer vision have tried to adapt the source domain to the target domain one-to-one. However, this scenario is difficult to apply to real applications such as video surveillance systems. As those systems have many cameras installed at each location regarded as source domains, it is difficult to identify the proper source domain. In this paper, we introduce a new transfer learning scenario that has various source domains and one target domain, assuming video surveillance system integration. Also, we propose a novel method for automatically producing a high accuracy model by fusing models trained at various source domains. In particular, we show how to apply a gating network to fuse source domains for object detection tasks, which is a new approach. We demonstrate the effectiveness of our method through experiments on traffic surveillance datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在传输计算机视觉学习的主要途径试图源域适应目标域一个对一个。然而，这种情况下很难应用到实际应用，如视频监控系统。由于这些系统都安装在被视为源域的每个位置多台摄像机，它是难以确定适当的源域。在本文中，我们介绍了具有各种源域和一个目标域，假设视频监控系统集成新的传输的学习场景。此外，我们提出了一种用于自动产生由熔化在不同源域训练的模型高精度模型的新方法。特别是，我们将展示如何选通网络适用于保险丝源域物体检测任务，这是一种新的方法。我们通过对交通监控数据集实验证明了该方法的有效性。</font>
</div>


<hr>
<div id="paper21"> <b>21. Towards Noise-resistant Object Detection with Noisy Annotations</b>  <a href="https://arxiv.org/pdf/2003.01285" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junnan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Caiming Xiong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Socher</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hoi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steven Hoi</a><br>
<font size="3">
Abstract: Training deep object detectors requires significant amount of human-annotated images with accurate object labels and bounding box coordinates, which are extremely expensive to acquire. Noisy annotations are much more easily accessible, but they could be detrimental for learning. We address the challenging problem of training object detectors with noisy annotations, where the noise contains a mixture of label noise and bounding box noise. We propose a learning framework which jointly optimizes object labels, bounding box coordinates, and model parameters by performing alternating noise correction and model training. To disentangle label noise and bounding box noise, we propose a two-step noise correction method. The first step performs class-agnostic bounding box correction by minimizing classifier discrepancy and maximizing region objectness. The second step distils knowledge from dual detection heads for soft label correction and class-specific bounding box refinement. We conduct experiments on PASCAL VOC and MS-COCO dataset with both synthetic noise and machine-generated noise. Our method achieves state-of-the-art performance by effectively cleaning both label noise and bounding box noise. Code to reproduce all results will be released. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：培训对象深探测器需要人力标注准确的对象标签和边界框坐标，这是为了获得极其昂贵的图像显著量。噪声注释是更方便，但他们可能是有害的学习。我们解决的培养对象探测器与噪声注释，其中噪声包含标签噪音的混合物和边界框噪声具有挑战性的问题。我们提出了一个学习框架，通过执行交替噪声校正和模型训练联合优化对象的标签，边界框坐标，和模型参数。理清标签噪音和边界框噪音，我们提出了一个两步噪声修正方法。通过最小化分类差异和最大化区域对象性的第一步进行类无关的边界框的校正。从双重检测头第二步下蒸馏知识，为软标签校正和类特定边界框细化。我们进行上PASCAL VOC和MS-COCO数据集实验与噪声产生机器包括合成噪声和。我们的方法有效地清洗两个标签噪音和噪音包围盒实现国家的最先进的性能。复制所有结果代码将被释放。</font>
</div>


<hr>
<div id="paper22"> <b>22. Disrupting DeepFakes: Adversarial Attacks Against Conditional Image  Translation Networks and Facial Manipulation Systems</b>  <a href="https://arxiv.org/pdf/2003.01279" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruiz%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nataniel Ruiz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sclaroff%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stan Sclaroff</a><br>
<font size="3">
Abstract: Face modification systems using deep learning have become increasingly powerful and accessible. Given images of a person's face, such systems can generate new images of that same person under different expressions and poses. Some systems can also modify targeted attributes such as hair color or age. This type of manipulated images and video have been coined DeepFakes. In order to prevent a malicious user from generating modified images of a person without their consent we tackle the new problem of generating adversarial attacks against image translation systems, which disrupt the resulting output image. We call this problem disrupting deepfakes. We adapt traditional adversarial attacks to our scenario. Most image translation architectures are generative models conditioned on an attribute (e.g. put a smile on this person's face). We present class transferable adversarial attacks that generalize to different classes, which means that the attacker does not need to have knowledge about the conditioning vector. In gray-box scenarios, blurring can mount a successful defense against disruption. We present a spread-spectrum adversarial attack, which evades blurring defenses. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：采用深度学习脸型修饰系统已经变得越来越强大和方便。一个人的面部图像给出，这样的系统可以产生在不同的表情和姿势是同一人的新形象。有些系统还可以通过修改目标属性，如头发的颜色或年龄。这种类型的操纵图像和视频已经创造DeepFakes。为了防止恶意用户生成一个人的修改图像未经他们同意我们处理生成对图像翻译系统，其破坏所产生的输出图像的对抗攻击的新问题。我们把这个问题破坏deepfakes。我们适应我们的场景传统对抗性攻击。大多数图像转换架构生成模型空调上的属性（如放在这个人的脸上露出了笑容）。我们认为推广到不同的类别，这意味着攻击者不需要有关于空调向量知识当前类转让对抗性攻击。在灰盒场景，图像模糊的现象发动针对中断一个成功的防守。我们提出了一个扩频敌对攻击，逃避模糊防御。</font>
</div>


<hr>
<div id="paper23"> <b>23. Single-Shot Pose Estimation of Surgical Robot Instruments' Shafts from  Monocular Endoscopic Images</b>  <a href="https://arxiv.org/pdf/2003.01267" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yoshimura%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masakazu Yoshimura</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marinho%2C+M+M" target="_blank" rel="noopener" style="color:#0000EE;">Murilo M. Marinho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Harada%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kanako Harada</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mitsuishi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mamoru Mitsuishi</a><br>
<font size="3">
Abstract: Surgical robots are used to perform minimally invasive surgery and alleviate much of the burden imposed on surgeons. Our group has developed a surgical robot to aid in the removal of tumors at the base of the skull via access through the nostrils. To avoid injuring the patients, a collision-avoidance algorithm that depends on having an accurate model for the poses of the instruments' shafts is used. Given that the model's parameters can change over time owing to interactions between instruments and other disturbances, the online estimation of the poses of the instrument's shaft is essential. In this work, we propose a new method to estimate the pose of the surgical instruments' shafts using a monocular endoscope. Our method is based on the use of an automatically annotated training dataset and an improved pose-estimation deep-learning architecture. In preliminary experiments, we show that our method can surpass state of the art vision-based marker-less pose estimation techniques (providing an error decrease of 55% in position estimation, 64% in pitch, and 69% in yaw) by using artificial images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：外科手术机器人被用来进行微创手术，减轻很多强加给医生的负担。我们小组已经开发了手术机器人经由通过鼻孔进入在头骨底部的切除肿瘤的帮助。为了避免伤及病人中，使用依赖于具有用于仪器的轴的姿势的精确模型的碰撞回避算法。鉴于该模型的参数可由于仪器和其他干扰之间的相互作用随时间而改变，仪器的轴的姿态的在线估计是必不可少的。在这项工作中，我们提出来估算使用单眼内窥镜手术器械轴的姿态的新方法。我们的方法是基于使用自动注释的训练数据集和改进的姿态估计深学习建筑。在初步实验中，我们表明，我们的方法可以通过使用人工超越本领域基于视觉的无标记姿势估计技术（提供的55％的位置的估计误差减小，在间距64％，和69％在偏航）的状态图片。</font>
</div>


<hr>
<div id="paper24"> <b>24. Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud</b>  <a href="https://arxiv.org/pdf/2003.01251" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weijing Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ragunathan" target="_blank" rel="noopener" style="color:#0000EE;">Ragunathan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rajkumar" target="_blank" rel="noopener" style="color:#0000EE;">Rajkumar</a><br>
<font size="3">
Abstract: In this paper, we propose a graph neural network to detect objects from a LiDAR point cloud. Towards this end, we encode the point cloud efficiently in a fixed radius near-neighbors graph. We design a graph neural network, named Point-GNN, to predict the category and shape of the object that each vertex in the graph belongs to. In Point-GNN, we propose an auto-registration mechanism to reduce translation variance, and also design a box merging and scoring operation to combine detections from multiple vertices accurately. Our experiments on the KITTI benchmark show the proposed approach achieves leading accuracy using the point cloud alone and can even surpass fusion-based algorithms. Our results demonstrate the potential of using the graph neural network as a new approach for 3D object detection. The code is available this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个图形的神经网络，从激光雷达点云检测的对象。为此，我们有效编码的点云在固定半径近的邻居图。我们设计的图形神经网络，命名点GNN，预测，图中的每个顶点所属的对象的类别和形状。在点GNN，我们提出了一种自动注册机制来减少翻译偏差，并且还设计了一个盒合并和评分操作检测从多个顶点准确地组合。我们对KITTI基准实验表明，该方法实现单独使用点云领先的精度甚至会超过基于融合的算法。我们的研究结果表明使用图形神经网络作为立体物检测新方法的潜力。代码可以这样HTTPS URL。</font>
</div>


<hr>
<div id="paper25"> <b>25. MVC-Net: A Convolutional Neural Network Architecture for Manifold-Valued  Images With Applications</b>  <a href="https://arxiv.org/pdf/2003.01234" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bouza%2C+J+J" target="_blank" rel="noopener" style="color:#0000EE;">Jose J. Bouza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chun-Hao Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vemuri%2C+B+C" target="_blank" rel="noopener" style="color:#0000EE;">Baba C. Vemuri</a><br>
<font size="3">
Abstract: Geometric deep learning has attracted significant attention in recent years, in part due to the availability of exotic data types for which traditional neural network architectures are not well suited. Our goal in this paper is to generalize convolutional neural networks (CNN) to the manifold-valued image case which arises commonly in medical imaging and computer vision applications. Explicitly, the input data to the network is an image where each pixel value is a sample from a Riemannian manifold. To achieve this goal, we must generalize the basic building block of traditional CNN architectures, namely, the weighted combinations operation. To this end, we develop a tangent space combination operation which is used to define a convolution operation on manifold-valued images that we call, the Manifold-Valued Convolution (MVC). We prove theoretical properties of the MVC operation, including equivariance to the action of the isometry group admitted by the manifold and characterizing when compositions of MVC layers collapse to a single layer. We present a detailed description of how to use MVC layers to build full, multi-layer neural networks that operate on manifold-valued images, which we call the MVC-net. Further, we empirically demonstrate superior performance of the MVC-nets in medical imaging and computer vision tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：几何深度学习吸引显著重视，近年来，部分原因是由于针对传统的神经网络结构不能很好适应异国的数据类型的可用性。我们在本文的目标是推广卷积神经网络（CNN），其在医疗成像和计算机视觉应用中通常出现的歧管值图像的情况。明确地，输入数据到所述网络是其中每个像素值是从黎曼流形的样品的图像。为了实现这一目标，我们必须推广传统CNN架构，即加权组合操作的基本构建块。为此，我们开发这是用来在歧管值图像，我们称之为定义的卷积运算的切线空间组合操作中，歧管子值卷积（MVC）。我们证明了MVC操作的理论性能，包括同变性由歧管承认等距组的作用，并且当MVC层的组合物塌陷于单层表征。我们提出了如何使用MVC层来构建在歧管值图像，我们称之为MVC网运营满，多层神经网络的详细描述。此外，我们经验证明医疗成像和计算机视觉任务的MVC-网的卓越性能。</font>
</div>


<hr>
<div id="paper26"> <b>26. MRI Super-Resolution with GAN and 3D Multi-Level DenseNet: Smaller,  Faster, and Better</b>  <a href="https://arxiv.org/pdf/2003.01217" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhua Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Christodoulou%2C+A+G" target="_blank" rel="noopener" style="color:#0000EE;">Anthony G. Christodoulou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengwei Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yibin Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Debiao Li</a><br>
<font size="3">
Abstract: High-resolution (HR) magnetic resonance imaging (MRI) provides detailed anatomical information that is critical for diagnosis in the clinical application. However, HR MRI typically comes at the cost of long scan time, small spatial coverage, and low signal-to-noise ratio (SNR). Recent studies showed that with a deep convolutional neural network (CNN), HR generic images could be recovered from low-resolution (LR) inputs via single image super-resolution (SISR) approaches. Additionally, previous works have shown that a deep 3D CNN can generate high-quality SR MRIs by using learned image priors. However, 3D CNN with deep structures, have a large number of parameters and are computationally expensive. In this paper, we propose a novel 3D CNN architecture, namely a multi-level densely connected super-resolution network (mDCSRN), which is light-weight, fast and accurate. We also show that with the generative adversarial network (GAN)-guided training, the mDCSRN-GAN provides appealing sharp SR images with rich texture details that are highly comparable with the referenced HR images. Our results from experiments on a large public dataset with 1,113 subjects showed that this new architecture outperformed other popular deep learning methods in recovering 4x resolution-downgraded images in both quality and speed. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：高分辨率（HR）磁共振成像（MRI）提供了详细的解剖信息是用于在临床应用的诊断至关重要。然而，HR MRI通常是以牺牲长的扫描时间，小的空间覆盖范围，和低信噪比（SNR）的成本。最近的研究表明，一个深深的卷积神经网络（CNN），HR一般的图像可以通过从单幅图像超分辨率低分辨率（LR）输入回收（SISR）方法。此外，以前的作品表明了深刻的3D CNN可以利用学到先验图像生成高品质的SR核磁共振。然而，3D CNN与深层结构，有大量的参数，并计算成本。在本文中，我们提出了一种新的3D CNN架构，即多级密集连接的超分辨率网络（mDCSRN），这是重量轻，快速和准确的。我们还表明，与生殖对抗网络（GAN）引导下培养，mDCSRN-GaN提供有吸引力急剧SR图像具有丰富的纹理细节是与引用的HR图像高度相当。我们从上与1113个主题大型公共数据集的实验结果表明，这种新的架构跑赢收回在质量和速度的4倍分辨率降级图像等热门深的学习方法。</font>
</div>


<hr>
<div id="paper27"> <b>27. Energy-efficient and Robust Cumulative Training with Net2Net  Transformation</b>  <a href="https://arxiv.org/pdf/2003.01204" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aosong Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Panda%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Priyadarshini Panda</a><br>
<font size="3">
Abstract: Deep learning has achieved state-of-the-art accuracies on several computer vision tasks. However, the computational and energy requirements associated with training such deep neural networks can be quite high. In this paper, we propose a cumulative training strategy with Net2Net transformation that achieves training computational efficiency without incurring large accuracy loss, in comparison to a model trained from scratch. We achieve this by first training a small network (with lesser parameters) on a small subset of the original dataset, and then gradually expanding the network using Net2Net transformation to train incrementally on larger subsets of the dataset. This incremental training strategy with Net2Net utilizes function-preserving transformations that transfers knowledge from each previous small network to the next larger network, thereby, reducing the overall training complexity. Our experiments demonstrate that compared with training from scratch, cumulative training yields ~2x reduction in computational complexity for training TinyImageNet using VGG19 at iso-accuracy. Besides training efficiency, a key advantage of our cumulative training strategy is that we can perform pruning during Net2Net expansion to obtain a final network with optimal configuration (~0.4x lower inference compute complexity) compared to conventional training from scratch. We also demonstrate that the final network obtained from cumulative training yields better generalization performance and noise robustness. Further, we show that mutual inference from all the networks created with cumulative Net2Net expansion enables improved adversarial input detection. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习取得了几个计算机视觉任务的国家的最先进的精度。然而，这样的训练深层神经网络相关的计算和能源需求可能相当高。在本文中，我们提出用Net2Net转型是实现培训的计算效率，而不会产生大的精度损失，相较于从头开始训练的模型累计培训战略。我们通过对原始数据集的一小部分第一次训练的小型网络（用较少参数）实现这一点，然后逐步扩大使用Net2Net转变到更大的数据集的子集，逐步训练网络。与Net2Net这种渐进培训战略利用功能保留转变，从以前的每一个小型网络到下一个更大的网络传输知识，从而降低了整体训练的复杂性。我们的实验表明，与从头开始培训，累计培训产量〜2倍降低计算复杂性使用VGG19在异准确性训练TinyImageNet比较。除了训练效率，我们的累计培训战略的一个关键优势在于，我们可以执行Net2Net扩张过程中修剪，以获得较从头常规训练的最佳配置（0.4倍〜较低推理的计算复杂度）的最终网络。我们还表明，从累计培训获得最终的网络得到更好的泛化性能和噪音的鲁棒性。此外，我们还表明，所有累积Net2Net扩张创造了网络相互推理能够提高对抗输入检测。</font>
</div>


<hr>
<div id="paper28"> <b>28. DEEVA: A Deep Learning and IoT Based Computer Vision System to Address  Safety and Security of Production Sites in Energy Industry</b>  <a href="https://arxiv.org/pdf/2003.01196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Awalgaonkar%2C+N+M" target="_blank" rel="noopener" style="color:#0000EE;">Nimish M. Awalgaonkar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haining Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gurciullo%2C+C+S" target="_blank" rel="noopener" style="color:#0000EE;">Christopher S. Gurciullo</a><br>
<font size="3">
Abstract: When it comes to addressing the safety/security related needs at different production/construction sites, accurate detection of the presence of workers, vehicles, equipment important and formed an integral part of computer vision-based surveillance systems (CVSS). Traditional CVSS systems focus on the use of different computer vision and pattern recognition algorithms overly reliant on manual extraction of features and small datasets, limiting their usage because of low accuracy, need for expert knowledge and high computational costs. The main objective of this paper is to provide decision makers at sites with a practical yet comprehensive deep learning and IoT based solution to tackle various computer vision related problems such as scene classification, object detection in scenes, semantic segmentation, scene captioning etc. Our overarching goal is to address the central question of What is happening at this site and where is it happening in an automated fashion minimizing the need for human resources dedicated to surveillance. We developed Deep ExxonMobil Eye for Video Analysis (DEEVA) package to handle scene classification, object detection, semantic segmentation and captioning of scenes in a hierarchical approach. The results reveal that transfer learning with the RetinaNet object detector is able to detect the presence of workers, different types of vehicles/construction equipment, safety related objects at a high level of accuracy (above 90%). With the help of deep learning to automatically extract features and IoT technology to automatic capture, transfer and process vast amount of realtime images, this framework is an important step towards the development of intelligent surveillance systems aimed at addressing myriads of open ended problems in the realm of security/safety monitoring, productivity assessments and future decision making. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当谈到解决在不同的生产/建筑工地，工人，车辆，设备的重要形成基于计算机视觉监控系统（CVSS）的一个组成部分存在的精确检测安全/安全性方面的需求。传统的CVSS体系侧重于不同的使用计算机视觉和模式识别算法上的特点和小型数据集手工提取过于依赖，因为限制低精度，需要专业知识和高计算成本的使用。本文的主要目的是在与实用而全面的深度学习和基于物联网解决方案，以应对各种计算机视觉相关的问题，如场景分类，在场景中的物体检测，语义分割，现场字幕等。我们的首要站点提供决策者目标是要解决什么是在这个网站发生的主要问题，哪里是它以自动方式发生最小化专用于监控人力资源的需求。我们开发深埃克森美孚公司以眼视频分析（DEEVA）封装处理场景分类，目标检测，语义分割和分层方法场景的字幕。结果表明与RetinaNet对象检测器传递学习能够检测工人，不同类型的车辆/施工设备，安全相关的对象的存在下，在高精确度（90％以上）。随着深度学习的帮助下，自动提取特征和物联网技术的实时图像的自动采集，传输和处理大量的，这个框架是对旨在解决开放式问题无数的境界智能监控系统发展的重要一步安全性/安全监控，生产力的评估和未来的决策。</font>
</div>


<hr>
<div id="paper29"> <b>29. LiDARNet: A Boundary-Aware Domain Adaptation Model for Lidar Point Cloud  Semantic Segmentation</b>  <a href="https://arxiv.org/pdf/2003.01174" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saripalli%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Srikanth Saripalli</a><br>
<font size="3">
Abstract: We present a boundary-aware domain adaptation model for Lidar point cloud semantic segmentation. Our model is designed to extract both the domain private features and the domain shared features using shared weight. We embedded Gated-SCNN into the shared features extractors to help it learn boundary information while learning other shared features. Besides, the CycleGAN mechanism is imposed for further adaptation. We conducted experiments on real-world datasets. The source domain data is from the Semantic KITTI dataset, and the target domain data is collected from our own platform (a warthog) in off-road as well as urban scenarios. The two datasets have differences in channel distributions, reflectivity distributions, and sensors setup. Using our approach, we are able to get a single model that can work on both domains. The model is capable of achieving the state of art performance on the source domain (Semantic KITTI dataset) and get 44.0\% mIoU on the target domain dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了激光雷达点云语义分割边界感知领域适应性模型。我们的模型的目的是提取域专用功能和使用共享重量域共享特征的。我们的嵌入式门控SCNN到共享特性提取，以帮助IT学习边界信息，同时学习其他共享功能。此外，CycleGAN机制规定的进一步调整。我们进行了真实世界的数据集实验。源域数据是从语义KITTI数据集和目标域数据是从我们自己的平台（疣猪）在越野和城市环境下收集。两个数据集在信道分布，反射率分布和传感器设置差异。使用我们的方法，我们可以得到一个模型，可以在两个领域的工作。该模型能够在源域（语义KITTI数据集）实现的技术性能的状态和在目标域的数据集获得44.0 \％米欧。</font>
</div>


<hr>
<div id="paper30"> <b>30. Understanding Contexts Inside Robot and Human Manipulation Tasks through  a Vision-Language Model and Ontology System in a Video Stream</b>  <a href="https://arxiv.org/pdf/2003.01163" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dehghan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masood Dehghan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jagersand%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Jagersand</a><br>
<font size="3">
Abstract: Manipulation tasks in daily life, such as pouring water, unfold intentionally under specialized manipulation contexts. Being able to process contextual knowledge in these Activities of Daily Living (ADLs) over time can help us understand manipulation intentions, which are essential for an intelligent robot to transition smoothly between various manipulation actions. In this paper, to model the intended concepts of manipulation, we present a vision dataset under a strictly constrained knowledge domain for both robot and human manipulations, where manipulation concepts and relations are stored by an ontology system in a taxonomic manner. Furthermore, we propose a scheme to generate a combination of visual attentions and an evolving knowledge graph filled with commonsense knowledge. Our scheme works with real-world camera streams and fuses an attention-based Vision-Language model with the ontology system. The experimental results demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans. The proposed scheme allows the robot to mimic human-like intentional behaviors by watching real-time videos. We aim to develop this scheme further for real-world robot intelligence in Human-Robot Interaction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在日常生活中操作任务，如倒水，有意展现在专业操作环境。随着时间的推移能够处理语境知识在日常生活（ADL的）这些活动可以帮助我们了解操作的意图，这对各种操作行为之间的智能机器人平稳过渡至关重要。在本文中，操控的预期概念模型，我们提出以下两个机器人和人的操作，在操作概念和关系存储由本体系统中的分类方式约束严格知识领域的远景数据集。此外，我们提出了一个方案，以产生视觉关注的组合，并且充满了常识性知识的一个不断发展的知识图。我们的方案可与真实世界的摄像头流和融合与本体系统的关注，基于视觉的语言模型。实验结果表明，该方案能够成功地表示想要的对象操作过程的演化两个机器人和人类。该方案允许机器人模仿人类般通过观看实时视频故意行为。我们的目标是进一步发展该方案在人机交互真实世界的机器人智能。</font>
</div>


<hr>
<div id="paper31"> <b>31. Unsupervised Domain Adaptation for Mammogram Image Classification: A  Promising Tool for Model Generalization</b>  <a href="https://arxiv.org/pdf/2003.01111" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gongbo Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jacobs%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nathan Jacobs</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoqin Wang</a><br>
<font size="3">
Abstract: Generalization is one of the key challenges in the clinical validation and application of deep learning models to medical images. Studies have shown that such models trained on publicly available datasets often do not work well on real-world clinical data due to the differences in patient population and image device configurations. Also, manually annotating clinical images is expensive. In this work, we propose an unsupervised domain adaptation (UDA) method using Cycle-GAN to improve the generalization ability of the model without using any additional manual annotations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：泛化是在临床验证和深入学习模型应用到医学图像的关键挑战之一。有研究表明，经过训练，可公开获得的数据集，模型往往不能很好地对现实世界的临床数据，由于在患者人群和影像设备配置的不同工作。此外，手动注释临床图像是昂贵的。在这项工作中，我们提出用循环-GaN提高模型的泛化能力，而无需使用任何额外的手动注释无人监管的领域适应性（UDA）方法。</font>
</div>


<hr>
<div id="paper32"> <b>32. Learning from Suspected Target: Bootstrapping Performance for Breast  Cancer Detection in Mammography</b>  <a href="https://arxiv.org/pdf/2003.01109" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheng Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junjun Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunlong Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peifang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Zhao</a><br>
<font size="3">
Abstract: Deep learning object detection algorithm has been widely used in medical image analysis. Currently all the object detection tasks are based on the data annotated with object classes and their bounding boxes. On the other hand, medical images such as mammography usually contain normal regions or objects that are similar to the lesion region, and may be misclassified in the testing stage if they are not taken care of. In this paper, we address such problem by introducing a novel top likelihood loss together with a new sampling procedure to select and train the suspected target regions, as well as proposing a similarity loss to further identify suspected targets from targets. Mean average precision (mAP) according to the predicted targets and specificity, sensitivity, accuracy, AUC values according to classification of patients are adopted for performance comparisons. We firstly test our proposed method on a private dense mammogram dataset. Results show that our proposed method greatly reduce the false positive rate and the specificity is increased by 0.25 on detecting mass type cancer. It is worth mention that dense breast typically has a higher risk for developing breast cancers and also are harder for cancer detection in diagnosis, and our method outperforms a reported result from performance of radiologists. Our method is also validated on the public Digital Database for Screening Mammography (DDSM) dataset, brings significant improvement on mass type cancer detection and outperforms the most state-of-the-art work. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习对象检测算法已广泛应用于医学图像分析。目前，所有的物体检测任务是基于与对象类及其边界框注释的数据。在另一方面，医学图像，如乳房X线照相通常含有正常区域或类似于病变区域，并且可以在测试阶段被错误分类，如果他们不照顾对象。在本文中，我们通过一起引入新颖顶部损失的可能性用新的采样过程来选择和训练可疑目标区域，以及从提出的目标的相似度损失进一步识别可疑目标解决这样的问题。根据所预测的目标和特异性，灵敏度，准确度值平均精度（MAP），根据患者分类AUC值采用对性能比较。我们首先在一个私密的乳房X光检查的数据集测试我们提出的方法。结果表明，我们提出的方法大大降低了假阳性率和特异性上检测质量类型癌症增加0.25。值得一提的是密集的乳房通常具有发展乳腺癌的风险更高，也更难在诊断癌症检测，而我们的方法优于从放射科医生的绩效报告的结果。我们的方法也验证了在公共数字数据库的乳房摄影筛检（DDSM）数据集，带来质量上的类型癌症检测显著改善，优于国家的最先进的最多的工作。</font>
</div>


<hr>
<div id="paper33"> <b>33. Reliable evaluation of adversarial robustness with an ensemble of  diverse parameter-free attacks</b>  <a href="https://arxiv.org/pdf/2003.01690" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Croce%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Francesco Croce</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hein%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Hein</a><br>
<font size="3">
Abstract: The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 40 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\%$, identifying several broken defenses. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：针对敌对攻击防御战略领域已显著增长在过去几年中，但进展是阻碍作为对抗防御系统的评价往往是不够的，因此给出了稳健性的错误印象。很多有前途的防御可在以后打破，使得它难以确定国家的最先进的。在评价频繁缺陷是攻击，梯度混淆或掩盖的超参数的调整不当。在本文中，我们首先提出了PGD-攻击克服的失败是由于次优步长和目标函数的问题，两个扩展。然后，我们用两个互补现有的结合了我们新的发作形成的攻击无参数，计算负担得起的和用户无关的合奏测试对抗性的鲁棒性。我们运用我们的合奏40多个型号从近期顶部的机器学习和计算机视觉场所发表的论文。在所有除的情况下，一个我们超过$ 10 \％$实现比在这些论文中报告了较低的稳健性检验精度，往往，确定几个破防御。</font>
</div>


<hr>
<div id="paper34"> <b>34. Compact Surjective Encoding Autoencoder for Unsupervised Novelty  Detection</b>  <a href="https://arxiv.org/pdf/2003.01665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaewoo Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+Y+G" target="_blank" rel="noopener" style="color:#0000EE;">Yoon Gyo Jung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Teoh%2C+A+B+J" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Beng Jin Teoh</a><br>
<font size="3">
Abstract: In unsupervised novelty detection, a model is trained solely on the in-class data, and infer to single out out-class data. Autoencoder (AE) variants aim to compactly model the in-class data to reconstruct it exclusively, differentiating it from out-class by the reconstruction error. However, it remains challenging for high-dimensional image data in the fully unsupervised setting. For this, we propose Compact Surjective Encoding AE (CSE-AE). In this framework, the encoding of any input is constrained into a compact manifold by exploiting the deep neural net's ignorance of the unknown. Concurrently, the in-class data is surjectively encoded to the compact manifold via AE. The mechanism is realized by both GAN and its ensembled discriminative layers, and results to reconstruct the in-class exclusively. In inference, the reconstruction error of a query is measured using high-level semantics captured by the discriminator. Extensive experiments on image data show that the proposed model gives state-of-the-art performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在无监督新奇检测，模型训练只对在类数据，并推断单出掉级的数据。自动编码器（AE）的变体旨在中级数据紧凑地建模为排他性地重建它，由重建误差从掉级区分它。但是，它仍然在完全无人监管的设定具有挑战性的高维图像数据。为此，我们提出了紧凑满射编码AE（CSE-AE）。在此框架下，任何输入的编码是通过利用未知的深神经网络的无知约束成一个紧凑的歧管。同时，在该级的数据被编码surjectively经由AE紧凑歧管。该机制由GAN二者及其合奏的判别的层，和结果实现重构在级排他。在推理，查询的重建误差，使用由鉴别捕获高级别语义测量。图像数据大量实验表明，该模型给出国家的最先进的性能。</font>
</div>


<hr>
<div id="paper35"> <b>35. BUSU-Net: An Ensemble U-Net Framework for Medical Image Segmentation</b>  <a href="https://arxiv.org/pdf/2003.01581" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Khoong%2C+W+H" target="_blank" rel="noopener" style="color:#0000EE;">Wei Hao Khoong</a><br>
<font size="3">
Abstract: In recent years, convolutional neural networks (CNNs) have revolutionized medical image analysis. One of the most well-known CNN architectures in semantic segmentation is the U-net, which has achieved much success in several medical image segmentation applications. Also more recently, with the rise of autoML ad advancements in neural architecture search (NAS), methods like NAS-Unet have been proposed for NAS in medical image segmentation. In this paper, with inspiration from LadderNet, U-Net, autoML and NAS, we propose an ensemble deep neural network with an underlying U-Net framework consisting of bi-directional convolutional LSTMs and dense connections, where the first (from left) U-Net-like network is deeper than the second (from left). We show that this ensemble network outperforms recent state-of-the-art networks in several evaluation metrics, and also evaluate a lightweight version of this ensemble network, which also outperforms recent state-of-the-art networks in some evaluation metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，卷积神经网络（细胞神经网络）已经彻底改变了医学图像分析。一个在语义分割上最知名的CNN架构是U形网，已在一些医学图像分割的应用取得了很大的成功。而且最近，随着autoML广告进步的神经结构搜索（NAS）的兴起，像NAS-UNET方法已经提出了NAS在医学图像分割。在本文中，与来自LadderNet，U型网，autoML和NAS灵感，我们提出了一种合奏深层神经网络具有由双向卷积LSTMs和密集的连接，其中所述第一（左）U的基本U形.Net框架-net状网络是比所述第二（左起）更深。我们表明，这种集成网络中有多个评价指标优于国家的最先进的最近使用的网络，同时还评估这个乐团的网络，这也优于国家的最先进的网络最近在一些评价指标的轻量级版本。</font>
</div>


<hr>
<div id="paper36"> <b>36. XGPT: Cross-modal Generative Pre-Training for Image Captioning</b>  <a href="https://arxiv.org/pdf/2003.01473" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiaolin Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoyang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongdong Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhifang Sui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Edward Cui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bharti%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taroon Bharti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: While many BERT-based cross-modal pre-trained models produce excellent results on downstream understanding tasks like image-text retrieval and VQA, they cannot be applied to generation tasks directly. In this paper, we propose XGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning that is designed to pre-train text-to-image caption generators through three novel generation tasks, including Image-conditioned Masked Language Modeling (IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned Image Feature Generation (TIFG). As a result, the pre-trained XGPT can be fine-tuned without any task-specific architecture modifications to create state-of-the-art models for image captioning. Experiments show that XGPT obtains new state-of-the-art results on the benchmark datasets, including COCO Captions and Flickr30k Captions. We also use XGPT to generate new image captions as data augmentation for the image retrieval task and achieve significant improvement on all recall metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然许多基于BERT，跨模式预先训练模型产生像图像，文本检索和VQA下游理解任务优异的成绩，他们不能直接应用于生成任务。在本文中，我们提出XGPT，跨模态剖成前的训练中的图像字幕的新方法，旨在通过三种新的生成任务，包括图像的空调蒙面语言模型预列车文字到图片标题发生器（ IMLM），图像空调去噪Autoencoding（IDA），和文本条件的图像特征生成（TIFG）。其结果是，预先训练XGPT可以进行微调而没有任何特定任务的架构修改以用于图像字幕创建状态的最先进的模型。实验表明，XGPT获得国家的最先进的新的基准数据集，包括COCO字幕和字幕Flickr30k结果。我们还使用XGPT产生新的图片说明为图像检索任务数据增强，实现对所有召回的指标显著改善。</font>
</div>


<hr>
<div id="paper37"> <b>37. Curriculum By Texture</b>  <a href="https://arxiv.org/pdf/2003.01367" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sinha%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samarth Sinha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Animesh Garg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Larochelle%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hugo Larochelle</a><br>
<font size="3">
Abstract: Convolutional Neural Networks (CNNs) have shown impressive performance in computer vision tasks such as image classification and segmentation. One factor for the success of CNNs is that they have an inductive bias that assumes a certain type of spatial structure is present in the data. Recent work by Geirhos et al. (2018) shows how learning in CNNs causes the learned CNN models to be biased towards high-frequency textural information, compared to low-frequency shape information in images. Many tasks generally requires both shape and textural information. Hence, we propose a simple curriculum based scheme which improves the ability of CNNs to be less biased towards textural information, and at the same time, being able to represent both the shape and textural information. We propose to augment the training of CNNs by controlling the amount of textural information that is available to the CNNs during the training process, by convolving the output of a CNN layer with a low-pass filter, or simply a Gaussian kernel. By reducing the standard deviation of the Gaussian kernel, we are able to gradually increase the amount of textural information available as training progresses, and hence reduce the texture bias. Such an augmented training scheme significantly improves the performance of CNNs on various image classification tasks, while adding no additional trainable parameters or auxiliary regularization objectives. We also observe significant improvements when using the trained CNNs to perform transfer learning on a different dataset, and transferring to a different task which shows how the learned CNNs using the proposed method act as better feature extractors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）显示在计算机视觉任务的骄人业绩，如图像分类和分割。对于细胞神经网络的成功的一个因素是，它们具有呈现某种类型的空间结构的存在于所述数据的感应偏压。通过Geirhos等人最近的工作。 （2018）示出了如何在细胞神经网络学习使了解到CNN模型朝高频纹理信息被偏压，而在图像的低频形状信息。许多任务通常需要形状和纹理信息。因此，我们提出一种改善细胞神经网络的朝向纹理信息被较少偏置的能力，并且在同一时间，能够表示二者的形状和纹理信息的简单课程基于方案。我们提出通过控制在训练过程中是可用的细胞神经网络的纹理信息的量，通过用低通滤波器，或简称为高斯核进行卷积CNN的层的输出，以增加细胞神经网络的训练。通过降低高斯核的标准偏差，我们能够逐步提高可作为培训进展纹理信息的数量，因此降低了质地偏差。这样的增强的训练方案显著提高细胞神经网络的各种图像分类任务的性能，同时加入没有额外的可训练的参数或辅助正规化目标。用训练细胞神经网络时，在不同的数据集进行迁移学习，并转移到一个不同的任务，显示了使用该方法的行为更好的特征提取的是如何学会细胞神经网络我们也观察到显著的改善。</font>
</div>


<hr>
<div id="paper38"> <b>38. Shape analysis via inconsistent surface registration</b>  <a href="https://arxiv.org/pdf/2003.01357" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+G+P+T" target="_blank" rel="noopener" style="color:#0000EE;">Gary P. T. Choi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Di Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lui%2C+L+M" target="_blank" rel="noopener" style="color:#0000EE;">Lok Ming Lui</a><br>
<font size="3">
Abstract: In this work, we develop a framework for shape analysis using inconsistent surface mapping. Traditional landmark-based geometric morphometrics methods suffer from the limited degrees of freedom, while most of the more advanced non-rigid surface mapping methods rely on a strong assumption of the global consistency of two surfaces. From a practical point of view, given two anatomical surfaces with prominent feature landmarks, it is more desirable to have a method that automatically detects the most relevant parts of the two surfaces and finds the optimal landmark-matching alignment between those parts, without assuming any global 1-1 correspondence between the two surfaces. Our method is capable of solving this problem using inconsistent surface registration based on quasi-conformal theory. It further enables us to quantify the dissimilarity of two shapes using quasi-conformal distortion and differences in mean and Gaussian curvatures, thereby providing a natural way for shape classification. Experiments on Platyrrhine molars demonstrate the effectiveness of our method and shed light on the interplay between function and shape in nature. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们开发使用不一致的表面映射形状分析的框架。传统的基于地标的几何形态测量方法从有限的自由度受到影响，而大多数的更先进的非刚性表面映射方法依赖于两个表面的全局一致性的一个很强的假设。从给定的与突出的特点的地标两个解剖表面的实际角度来看，更希望有一种方法，该方法自动检测两个表面的最相关的部分，并发现这些部分之间的最佳的地标匹配对准，而不承担任何两个表面之间的全局1-1对应。我们的方法是能够利用基于准共形理论不一致表面登记解决这一问题的。它进一步使我们使用均值和高斯曲率准共形失真和差异，从而提供形状分类以自然的方式来量化两个形状的不同之处。上Platyrrhine臼齿实验表明我们的方法的有效性和性质的功能和形状之间的相互作用线索。</font>
</div>


<hr>
<div id="paper39"> <b>39. DDU-Nets: Distributed Dense Model for 3D MRI Brain Tumor Segmentation</b>  <a href="https://arxiv.org/pdf/2003.01337" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanxiao Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingxiong Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Shen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mali Shen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaqi Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guang-Zhong Yang</a><br>
<font size="3">
Abstract: Segmentation of brain tumors and their subregions remains a challenging task due to their weak features and deformable shapes. In this paper, three patterns (cross-skip, skip-1 and skip-2) of distributed dense connections (DDCs) are proposed to enhance feature reuse and propagation of CNNs by constructing tunnels between key layers of the network. For better detecting and segmenting brain tumors from multi-modal 3D MR images, CNN-based models embedded with DDCs (DDU-Nets) are trained efficiently from pixel to pixel with a limited number of parameters. Postprocessing is then applied to refine the segmentation results by reducing the false-positive samples. The proposed method is evaluated on the BraTS 2019 dataset with results demonstrating the effectiveness of the DDU-Nets while requiring less computational cost. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：脑肿瘤和他们分区域的分割仍然是一个具有挑战性的任务，由于其薄弱的特点和可变形的。在本文中，三种模式（横跳，则跳过-1和跳过-2）提出了由网络的关键层之间的隧道构造，以增强细胞神经网络的特征重用和分布式传播密连接（DDC的）的。对于从多模态的3D MR图像更好检测和分割脑肿瘤，CNN的嵌入用的DDC（DDU-网）从像素有效地训练像素与参数的数量有限的模式。然后后处理被施加通过减少假阳性的样品以细化分割结果。所提出的方法在评价臭小子2019数据集的结果表明所述DDU-篮网的效力，同时要求较少的计算成本。</font>
</div>


<hr>
<div id="paper40"> <b>40. Visualizing intestines for diagnostic assistance of ileus based on  intestinal region segmentation from 3D CT images</b>  <a href="https://arxiv.org/pdf/2003.01290" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Oda%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hirohisa Oda</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Nishio%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kohei Nishio</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kitasaka%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takayuki Kitasaka</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Amano%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hizuru Amano</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Takimoto%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aitaro Takimoto</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Uchida%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiroo Uchida</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Suzuki%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kojiro Suzuki</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Itoh%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hayato Itoh</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Oda%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masahiro Oda</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mori%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kensaku Mori</a><br>
<font size="3">
Abstract: This paper presents a visualization method of intestine (the small and large intestines) regions and their stenosed parts caused by ileus from CT volumes. Since it is difficult for non-expert clinicians to find stenosed parts, the intestine and its stenosed parts should be visualized intuitively. Furthermore, the intestine regions of ileus cases are quite hard to be segmented. The proposed method segments intestine regions by 3D FCN (3D U-Net). Intestine regions are quite difficult to be segmented in ileus cases since the inside the intestine is filled with fluids. These fluids have similar intensities with intestinal wall on 3D CT volumes. We segment the intestine regions by using 3D U-Net trained by a weak annotation approach. Weak-annotation makes possible to train the 3D U-Net with small manually-traced label images of the intestine. This avoids us to prepare many annotation labels of the intestine that has long and winding shape. Each intestine segment is volume-rendered and colored based on the distance from its endpoint in volume rendering. Stenosed parts (disjoint points of an intestine segment) can be easily identified on such visualization. In the experiments, we showed that stenosed parts were intuitively visualized as endpoints of segmented regions, which are colored by red or blue. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文呈现肠的可视化方法（小肠和大肠）的区域和它们的狭窄部分从CT体积引起肠梗阻。由于难以对非临床专家发现狭窄部位，小肠及其狭窄部位应直观可视化。此外，肠梗阻病例肠道地区是相当难以分割。所提出的方法的段肠通过3D FCN（3D U形净）区域。肠道地区是相当困难的情况下，肠梗阻被分割，因为肠道内充满液体。这些流体具有与三维CT体积肠壁类似的强度。我们段通过使用3D掌中肠道地区的培训由弱注解方法。弱注释使得可能与肠的小手动追踪标签图像训练3D掌中。这样就避免了我们准备已漫长而曲折的形状肠道的许多注释标签。各肠段是体积渲染并且基于来自其端点在体绘制的距离着色。狭窄部分（肠段的不相交的点）可以容易地识别这样的可视化。在实验中，我们发现，狭窄的部分被直观地显现为分割区域，其中用红色或蓝色的彩色的端点。</font>
</div>


<hr>
<div id="paper41"> <b>41. A Deep learning Approach to Generate Contrast-Enhanced Computerised  Tomography Angiography without the Use of Intravenous Contrast Agents</b>  <a href="https://arxiv.org/pdf/2003.01223" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Chandrashekar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anirudh Chandrashekar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Handa%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashok Handa</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Shivakumar%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Natesh Shivakumar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lapolla%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierfrancesco Lapolla</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Grau%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vicente Grau</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lee%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Regent Lee</a><br>
<font size="3">
Abstract: Contrast-enhanced computed tomography angiograms (CTAs) are widely used in cardiovascular imaging to obtain a non-invasive view of arterial structures. However, contrast agents are associated with complications at the injection site as well as renal toxicity leading to contrast-induced nephropathy (CIN) and renal failure. We hypothesised that the raw data acquired from a non-contrast CT contains sufficient information to differentiate blood and other soft tissue components. We utilised deep learning methods to define the subtleties between soft tissue components in order to simulate contrast enhanced CTAs without contrast agents. Twenty-six patients with paired non-contrast and CTA images were randomly selected from an approved clinical study. Non-contrast axial slices within the AAA from 10 patients (n = 100) were sampled for the underlying Hounsfield unit (HU) distribution at the lumen, intra-luminal thrombus and interface locations. Sampling of HUs in these regions revealed significant differences between all regions (p<0.001 for all comparisons), confirming the intrinsic differences in radiomic signatures between these regions. to generate a large training dataset, paired axial slices from set (n="13)" were augmented produce total of 23,551 2-d images. we trained cycle generative adversarial network (cyclegan) this non-contrast contrast (nc2c) transformation task. accuracy cyclegan output was assessed by comparison image. pipeline is able differentiate visually incoherent soft tissue regions ct ctas generated images bear strong resemblance ground truth. here describe novel application image processing. poised disrupt clinical pathways requiring enhanced imaging. < font>
<br>
<font size="2" style="line-height:30px;">
摘要：对比增强计算机断层造影（的CTA）被广泛用于心血管成像以获得动脉结构的非侵入性的图。然而，造影剂与注射部位的并发症，以及肾毒性导致造影剂肾病（CIN）和肾功能衰竭有关。我们假设从非造影CT采集的原始数据包含足够的信息来区分血液和其它软组织的部件。我们利用深学习方法，以便限定软组织组分之间的细微之处，以模拟造影剂增强的CTA没有造影剂。 26例有配对的非对比度和CTA图像，随机从批准的临床研究选择。非造影从10名患者（n = 100）中的AAA内轴向切片取样用于在该腔，腔内血栓和接口位置底层亨氏单位（HU）的分布。 HUS在这些区域采样揭示所有区域（P <0.001所有的比较）之间显著差异，确认在这些区域之间的radiomic签名的固有差异。产生大的训练数据集，从所述训练集（n = 13）配对的轴向切片扩充以产生总的23551 2-d的图像。我们培养了2-d周期剖成对抗性网络（cyclegan）此非对比对比度（nc2c）改造任务。所述cyclegan输出的精确度是通过比较所述对比图像进行评估。这条管线能够以非造影ct图像在视觉上不连贯的软组织区域之间进行区分。从非对比度的图像生成cta所承受非常相似的地面实况。这里，我们描述剖成对抗性网络在ct图像处理的新的应用。这是准备以破坏需要造影增强的ct成像临床路径。< font>
</0.001所有的比较）之间显著差异，确认在这些区域之间的radiomic签名的固有差异。产生大的训练数据集，从所述训练集（n></font></0.001></font></div>


<hr>
<div id="paper42"> <b>42. RandomNet: Towards Fully Automatic Neural Architecture Design for  Multimodal Learning</b>  <a href="https://arxiv.org/pdf/2003.01181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Alletto%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefano Alletto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shenyang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Francois-Lavet%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vincent Francois-Lavet</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nakata%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yohei Nakata</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rabusseau%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillaume Rabusseau</a><br>
<font size="3">
Abstract: Almost all neural architecture search methods are evaluated in terms of performance (i.e. test accuracy) of the model structures that it finds. Should it be the only metric for a good autoML approach? To examine aspects beyond performance, we propose a set of criteria aimed at evaluating the core of autoML problem: the amount of human intervention required to deploy these methods into real world scenarios. Based on our proposed evaluation checklist, we study the effectiveness of a random search strategy for fully automated multimodal neural architecture search. Compared to traditional methods that rely on manually crafted feature extractors, our method selects each modality from a large search space with minimal human supervision. We show that our proposed random search strategy performs close to the state of the art on the AV-MNIST dataset while meeting the desirable characteristics for a fully automated design process. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：几乎所有的神经结构的搜索方法的模型结构，性能方面（即测试精度）进行评估，它发现。它应该是一个很好的办法autoML的唯一指标？为了考察超越性能方面，我们提出了一系列旨在评估autoML问题的核心标准：人工干预的量所需部署这些方法融入现实世界的场景。根据我们提出的评估清单中，我们研究了完全自动化的多模态神经结构搜索随机搜索策略的有效性。相比于依靠手工制作的特征提取，我们的方法选择以最少的人力监督较大的空间搜索每一种模式的传统方法。我们证明了我们所提出的随机搜索策略进行接近的AV-MNIST数据集中的艺术，同时满足一个完全自动化的设计过程中所需特性的状态。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-03-04</title>
    <url>/2020/03/04/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-04/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Hybrid Generative-Retrieval Transformers for Dialogue Domain Adaptation <a href="https://arxiv.org/pdf/2003.01680" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Improving Uyghur ASR systems with decoders using morpheme-based language  models <a href="https://arxiv.org/pdf/2003.01509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Multi-Task Learning Network for Emotion Recognition in Conversation <a href="https://arxiv.org/pdf/2003.01478" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> XGPT: Cross-modal Generative Pre-Training for Image Captioning <a href="https://arxiv.org/pdf/2003.01473" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Seshat: A tool for managing and verifying annotation campaigns of audio  data <a href="https://arxiv.org/pdf/2003.01472" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Meta-Embeddings Based On Self-Attention <a href="https://arxiv.org/pdf/2003.01371" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> CLUECorpus2020: A Large-scale Chinese Corpus for Pre-trainingLanguage  Model <a href="https://arxiv.org/pdf/2003.01355" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Improving Candidate Generation for Low-resource Cross-lingual Entity  Linking <a href="https://arxiv.org/pdf/2003.01343" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Controllable Time-Delay Transformer for Real-Time Punctuation Prediction  and Disfluency Detection <a href="https://arxiv.org/pdf/2003.01309" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Transfer Learning for Context-Aware Spoken Language Understanding <a href="https://arxiv.org/pdf/2003.01305" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Med7: a transferable clinical natural language processing model for  electronic health records <a href="https://arxiv.org/pdf/2003.01271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Understanding the Prediction Mechanism of Sentiments by XAI  Visualization <a href="https://arxiv.org/pdf/2003.01425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Hierarchical Context Enhanced Multi-Domain Dialogue System for  Multi-domain Task Completion <a href="https://arxiv.org/pdf/2003.01338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>



<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Hybrid Generative-Retrieval Transformers for Dialogue Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2003.01680" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shalyminov%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Igor Shalyminov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sordoni%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessandro Sordoni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Atkinson%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adam Atkinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schulz%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hannes Schulz</a><br>
<font size="3">
Abstract: Domain adaptation has recently become a key problem in dialogue systems research. Deep learning, while being the preferred technique for modeling such systems, works best given massive training data. However, in the real-world scenario, such resources aren't available for every new domain, so the ability to train with a few dialogue examples can be considered essential. Pre-training on large data sources and adapting to the target data has become the standard method for few-shot problems within the deep learning framework. In this paper, we present the winning entry at the fast domain adaptation task of DSTC8, a hybrid generative-retrieval model based on GPT-2 fine-tuned to the multi-domain MetaLWOz dataset. Robust and diverse in response generation, our model uses retrieval logic as a fallback, being SoTA on MetaLWOz in human evaluation (>4% improvement over the 2nd place system) and attaining competitive generalization performance in adaptation to the unseen MultiWOZ dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：域名适应最近已成为对话系统研究的关键问题。深度学习，而被用于模拟这样的系统的首选技术，最适合给大量的训练数据。然而，在实际情况中，这种资源是不可用于每一个新的领域，因此有能力与列车的几个对话的例子可以被认为是必不可少的。在大型数据源和适应目标数据前培训已成为深学习框架内为数不多的射门问题的标准方法。在本文中，我们提出在DSTC8，基于GPT-2微调到多域MetaLWOz数据集混合生成检索模型的快速领域适应性任务的获奖作品。健壮和在响应生成多样化的，我们的模型使用检索逻辑作为备用，上MetaLWOz人评价为Sota株（>在第2个地方系统4％的改善）和实现在适应看不见MultiWOZ数据集竞争性泛化性能。</font>
</div>


<hr>
<div id="paper2"> <b>2. Improving Uyghur ASR systems with decoders using morpheme-based language  models</b>  <a href="https://arxiv.org/pdf/2003.01509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zicheng Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mamut%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Turghunjan Mamut</a><br>
<font size="3">
Abstract: Uyghur is a minority language, and its resources for Automatic Speech Recognition (ASR) research are always insufficient. THUYG-20 is currently the only open-sourced dataset of Uyghur speeches. State-of-the-art results of its clean and noiseless speech test task haven't been updated since the first release, which shows a big gap in the development of ASR between mainstream languages and this http URL this paper, we try to bridge the gap by ultimately optimizing the ASR systems, and by developing a morpheme-based decoder, MLDG-Decoder (Morpheme Lattice Dynamically Generating Decoder for Uyghur DNN-HMM systems), which has long been missing. We have open-sourced the decoder. The MLDG-Decoder employs an algorithm, named as "on-the-fly composition with FEBABOS", to allow the back-off states and transitions to play the role of a relay station in on-the-fly composition. The algorithm empowers the dynamically generated graph to constrain the morpheme sequences in the lattices as effectively as the static and fully composed graph does when a 4-Gram morpheme-based Language Model (LM) is used. We have trained deeper and wider neural network acoustic models, and experimented with three kinds of decoding schemes. The experimental results show that the decoding based on the static and fully composed graph reduces state-of-the-art Word Error Rate (WER) on the clean and noiseless speech test task in THUYG-20 to 14.24%. The MLDG-Decoder reduces the WER to 14.54% while keeping the memory consumption reasonable. Based on the open-sourced MLDG-Decoder, readers can easily reproduce the experimental results in this paper. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：维吾尔语是一种少数民族语言，它的资源自动语音识别（ASR）的研究始终是不够的。 THUYG-20是目前维吾尔语演讲的唯一开源数据集。国家的最先进的清洁和无声的语音测试任务的结果尚未以来的第一个版本更新，这显示了ASR的主流语言的发展有很大的差距，这HTTP URL本文中，我们尝试桥该间隙由最终优化ASR系统，并且通过开发基于词素解码器，MLDG  - 解码器（语素格子动态地生成解码维吾尔族DNN-HMM系统），它一直被丢失。我们已经开源的解码器。该MLDG  - 解码器采用的算法，命名为“上即时组合物与FEBABOS”，允许回退状态和转换在即时成分发挥中继站的角色。该算法授权时使用基于词素-4- gram语言模型（LM）做动态生成图形，以限制在晶格语素序列尽可能有效的静态和完全构成图。我们已经培训了更深，更广的神经网络声学模型，并具有三种解码方案的实验。实验结果表明，基于静态和完全由曲线图中的解码降低状态的最先进的在THUYG-20至14.24％的清洁和无噪声语音测试任务字差错率（WER）。所述MLDG  - 解码器减少到WER 14.54％，同时保持存储器消耗合理。基于开源MLDG  - 解码器，读者可以很容易地复制在本文的实验结果。</font>
</div>


<hr>
<div id="paper3"> <b>3. Multi-Task Learning Network for Emotion Recognition in Conversation</b>  <a href="https://arxiv.org/pdf/2003.01478" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingye Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meishan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Donghong Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yijiang Liu</a><br>
<font size="3">
Abstract: Conversational emotion recognition (CER) has attracted increasing interests in the natural language processing (NLP) community. Different from the vanilla emotion recognition, effective speaker-sensitive utterance representation is one major challenge for CER. In this paper, we exploit speaker identification (SI) as an auxiliary task to enhance the utterance representation in conversations. By this method, we can learn better speaker-aware contextual representations from the additional SI corpus. Experiments on two benchmark datasets demonstrate that the proposed architecture is highly effective for CER, obtaining new state-of-the-art results on two datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：会话情感识别（CER）已经吸引了自然语言处理（NLP）的社区越来越浓厚的兴趣。从香草情感识别不同的，有效的扬声器敏感的话语表示是CER一个重大的挑战。在本文中，我们利用说话人识别（SI）作为辅助任务，以提高对话的话语表示。通过这种方法，我们可以从另外的SI语料更好的演讲者感知上下文表示。两个基准数据集的实验表明，该架构是CER高效，对两个数据集获得国家的最先进的新成果。</font>
</div>


<hr>
<div id="paper4"> <b>4. XGPT: Cross-modal Generative Pre-Training for Image Captioning</b>  <a href="https://arxiv.org/pdf/2003.01473" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiaolin Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoyang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongdong Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhifang Sui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Edward Cui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bharti%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taroon Bharti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: While many BERT-based cross-modal pre-trained models produce excellent results on downstream understanding tasks like image-text retrieval and VQA, they cannot be applied to generation tasks directly. In this paper, we propose XGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning that is designed to pre-train text-to-image caption generators through three novel generation tasks, including Image-conditioned Masked Language Modeling (IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned Image Feature Generation (TIFG). As a result, the pre-trained XGPT can be fine-tuned without any task-specific architecture modifications to create state-of-the-art models for image captioning. Experiments show that XGPT obtains new state-of-the-art results on the benchmark datasets, including COCO Captions and Flickr30k Captions. We also use XGPT to generate new image captions as data augmentation for the image retrieval task and achieve significant improvement on all recall metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然许多基于BERT，跨模式预先训练模型产生像图像，文本检索和VQA下游理解任务优异的成绩，他们不能直接应用于生成任务。在本文中，我们提出XGPT，跨模态剖成前的训练中的图像字幕的新方法，旨在通过三种新的生成任务，包括图像的空调蒙面语言模型预列车文字到图片标题发生器（ IMLM），图像空调去噪Autoencoding（IDA），和文本条件的图像特征生成（TIFG）。其结果是，预先训练XGPT可以进行微调而没有任何特定任务的架构修改以用于图像字幕创建状态的最先进的模型。实验表明，XGPT获得国家的最先进的新的基准数据集，包括COCO字幕和字幕Flickr30k结果。我们还使用XGPT产生新的图片说明为图像检索任务数据增强，实现对所有召回的指标显著改善。</font>
</div>


<hr>
<div id="paper5"> <b>5. Seshat: A tool for managing and verifying annotation campaigns of audio  data</b>  <a href="https://arxiv.org/pdf/2003.01472" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Titeux%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hadrien Titeux</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Riad%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rachid Riad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuan-Nga Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hamilakis%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Hamilakis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Madden%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kris Madden</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cristia%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alejandrina Cristia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bachoud-L%C3%A9vi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anne-Catherine Bachoud-Lévi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dupoux%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emmanuel Dupoux</a><br>
<font size="3">
Abstract: We introduce Seshat, a new, simple and open-source software to efficiently manage annotations of speech corpora. The Seshat software allows users to easily customise and manage annotations of large audio corpora while ensuring compliance with the formatting and naming conventions of the annotated output files. In addition, it includes procedures for checking the content of annotations following specific rules are implemented in personalised parsers. Finally, we propose a double-annotation mode, for which Seshat computes automatically an associated inter-annotator agreement with the $\gamma$ measure taking into account the categorisation and segmentation discrepancies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍塞莎特，一个新的，简单的和开源软件来有效地管理语音语料库的注释。该塞莎特软件允许用户轻松定制并同时确保遵守注释的输出文件的格式和命名规范管理大型音语料库的注释。此外，它还包括用于检查下列特定规则在个性化的解析器实现注释的内容程序。最后，我们提出了一个双注释模式，为此塞莎特自动计算与$ \ $伽玛衡量考虑到分类和分割的差异关联，注释间协议。</font>
</div>


<hr>
<div id="paper6"> <b>6. Meta-Embeddings Based On Self-Attention</b>  <a href="https://arxiv.org/pdf/2003.01371" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qichen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoke Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Li</a><br>
<font size="3">
Abstract: Creating meta-embeddings for better performance in language modelling has received attention lately, and methods based on concatenation or merely calculating the arithmetic mean of more than one separately trained embeddings to perform meta-embeddings have shown to be beneficial. In this paper, we devise a new meta-embedding model based on the self-attention mechanism, namely the Duo. With less than 0.4M parameters, the Duo mechanism achieves state-of-the-art accuracy in text classification tasks such as 20NG. Additionally, we propose a new meta-embedding sequece-to-sequence model for machine translation, which to the best of our knowledge, is the first machine translation model based on more than one word-embedding. Furthermore, it has turned out that our model outperform the Transformer not only in terms of achieving a better result, but also a faster convergence on recognized benchmarks, such as the WMT 2014 English-to-French translation task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：创建元的嵌入在语言模型更好的性能受到了关注最近和方法，基于级联或仅仅计算一个以上的单独训练的嵌入物进行荟萃的嵌入的算术平均值已经证明是有益的。在本文中，我们设计基础上，自注意机制，即双核新的元嵌入模型。少于0.4M参数，二重奏机构达到的状态的最先进的精度在文本分类任务，如20ng的。此外，我们提出了一个新的元嵌入sequece到序列模型机器翻译，这在我们所知的，是基于超过一个字嵌入第一机器翻译模型。此外，它已经证明，我们的模型跑赢变压器不仅实现了较好的效果方面，也对认可的基准，如WMT 2014英语对法语翻译任务较快的收敛。</font>
</div>


<hr>
<div id="paper7"> <b>7. CLUECorpus2020: A Large-scale Chinese Corpus for Pre-trainingLanguage  Model</b>  <a href="https://arxiv.org/pdf/2003.01355" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuanwei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianqian Dong</a><br>
<font size="3">
Abstract: In this paper, we introduce the Chinese corpus from CLUE organization, CLUECorpus2020, a large-scale corpus that can be used directly for self-supervised learning such as pre-training of a language model, or language generation. It has 100G raw corpus with 35 billion Chinese characters, which is retrieved from Common Crawl. To better understand this corpus, we conduct language understanding experiments on both small and large scale, and results show that the models trained on this corpus can achieve excellent performance on Chinese. We release a new Chinese vocabulary with a size of 8K, which is only one-third of the vocabulary size used in Chinese Bert released by Google. It saves computational cost and memory while works as good as original vocabulary. We also release both large and tiny versions of the pre-trained model on this corpus. The former achieves the state-of-the-art result, and the latter retains most precision while accelerating training and prediction speed for eight times compared to Bert-base. To facilitate future work on self-supervised learning on Chinese, we release our dataset, new vocabulary, codes, and pre-trained models on Github. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们将介绍从CLUE组织，CLUECorpus2020，可直接用于大规模语料库的中国语料库自我监督学习，如语言模型前培训或语言的产生。它拥有100G 35个十亿中国字，这是从常见的爬行检索生语料库。为了更好地理解这个语料库，我们在两个小规模和大规模开展语言理解实验，结果表明，经过训练这个语料库的车型可以实现在中国的出色表现。我们发布了新的中国的词汇，大小为8K，这是只有三分之一由谷歌发布了中国伯特使用的词汇量的大小的。它节省了计算成本和内存，而作品不如原来的词汇。我们还发布关于这个语料库大型和微型版本的预先训练模式。前者达到国家的最先进的结果，后者则保留了大部分精度，同时加速训练和预测速度的八倍相比伯特基。为了便于对中国自我监督学习今后的工作中，我们发布的数据集，新的词汇，代码和预先训练Github上的模型。</font>
</div>


<hr>
<div id="paper8"> <b>8. Improving Candidate Generation for Low-resource Cross-lingual Entity  Linking</b>  <a href="https://arxiv.org/pdf/2003.01343" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuyan Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rijhawani%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shruti Rijhawani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wieting%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Wieting</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carbonell%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaime Carbonell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Graham Neubig</a><br>
<font size="3">
Abstract: Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages (HRL), but these do not extend well to low-resource languages (LRL) with few, if any, Wikipedia pages. Recently, transfer learning methods have been shown to reduce the demand for resources in the LRL by utilizing resources in closely-related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective: we experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9% in Top-30 gold candidate recall, compared to state-of-the-art baselines. Our improved model also yields an average gain of 7.9% in in-KB accuracy of end-to-end XEL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨语种实体链接（XEL）是在目标语言知识基础提到从源语言文本中提取找到参照物（KB）的任务。 （X）EL的第一步是候选生成，它检索来自目标语言KB对每个提似是而非候选实体的列表。但是基于维基百科资源相对较高的资源语言（HRL）的领域证明是成功的，但这些并没有很好地低资源语言（LRL）很少，如果有的话，维基百科页面扩展。近日，传递学习方法已被证明在密切相关的语言运用资源，以减少LRL对资源的需求，但性能仍远远落后于他们的高资源同行。在本文中，我们首先评估低资源XEL面临当前实体候选生成方法存在的问题，则提出了三种改善：（1）降低断开之间实体提到和KB条目，和（2）提高了模型的鲁棒性以低资源场景。方法很简单，但有效的：我们尝试与我们的七个XEL数据集的方法，并发现它们产生了16.9％的前30金的候选人召回的平均收益，相较于国家的最先进的基线。我们的改进的模型也产生的7.9％在-KB的端至端XEL的精度的平均增益。</font>
</div>


<hr>
<div id="paper9"> <b>9. Controllable Time-Delay Transformer for Real-Time Punctuation Prediction  and Disfluency Detection</b>  <a href="https://arxiv.org/pdf/2003.01309" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengzhe Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Wang</a><br>
<font size="3">
Abstract: With the increased applications of automatic speech recognition (ASR) in recent years, it is essential to automatically insert punctuation marks and remove disfluencies in transcripts, to improve the readability of the transcripts as well as the performance of subsequent applications, such as machine translation, dialogue systems, and so forth. In this paper, we propose a Controllable Time-delay Transformer (CT-Transformer) model that jointly completes the punctuation prediction and disfluency detection tasks in real time. The CT-Transformer model facilitates freezing partial outputs with controllable time delay to fulfill the real-time constraints in partial decoding required by subsequent applications. We further propose a fast decoding strategy to minimize latency while maintaining competitive performance. Experimental results on the IWSLT2011 benchmark dataset and an in-house Chinese annotated dataset demonstrate that the proposed approach outperforms the previous state-of-the-art models on F-scores and achieves a competitive inference speed. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，随着自动语音识别（ASR）的申请越来越多，这是至关重要的自动插入标点符号和成绩单删除不流利，以提高转录本的可读性，以及随后的应用程序，如机器的性能翻译，对话系统，等等。在本文中，我们提出了共同完成实时标点符号预测和不流利的检测任务的可控延时互感器（CT变压器）模型。的CT-Transformer模型有利于冷冻可控时间延迟部分输出到履行由后续应用所需部分解码的实时约束。我们进一步提出了一个快速解码策略，以减少等待时间，同时保持具有竞争力的表现。在IWSLT2011基准数据集和内部注解中国数据集表明，该方法比对F-分数以前的状态的最先进的车型，并实现了有竞争力的推理速度实验结果。</font>
</div>


<hr>
<div id="paper10"> <b>10. Transfer Learning for Context-Aware Spoken Language Understanding</b>  <a href="https://arxiv.org/pdf/2003.01305" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuo%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhu Zhuo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiuyun Xu</a><br>
<font size="3">
Abstract: Spoken language understanding (SLU) is a key component of task-oriented dialogue systems. SLU parses natural language user utterances into semantic frames. Previous work has shown that incorporating context information significantly improves SLU performance for multi-turn dialogues. However, collecting a large-scale human-labeled multi-turn dialogue corpus for the target domains is complex and costly. To reduce dependency on the collection and annotation effort, we propose a Context Encoding Language Transformer (CELT) model facilitating exploiting various context information for SLU. We explore different transfer learning approaches to reduce dependency on data collection and annotation. In addition to unsupervised pre-training using large-scale general purpose unlabeled corpora, such as Wikipedia, we explore unsupervised and supervised adaptive training approaches for transfer learning to benefit from other in-domain and out-of-domain dialogue corpora. Experimental results demonstrate that the proposed model with the proposed transfer learning approaches achieves significant improvement on the SLU performance over state-of-the-art models on two large-scale single-turn dialogue benchmarks and one large-scale multi-turn dialogue benchmark. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：口语理解（SLU）是面向任务的对话系统的关键组成部分。 SLU解析自然语言的用户话语到语义框架。以前的研究表明，结合上下文信息显著提高了多圈的对话SLU性能。然而，收集大规模人标记的多匝对话语料库用于目标域是复杂和昂贵的。为了减少对收集和注释工作的依赖性，我们提出了一个上下文编语言变压器（CELT）模型有利于开发用于SLU各种上下文信息。我们在探索不同的传输学习方法，以减少对数据收集和注解的依赖。除了使用大型通用语料库未标注，如维基百科监督的训练前，我们探索无监督和监督适应性训练转移学习其它域内和外的域对话语料库方法中受益。实验结果表明，该模型所提出的迁移学习方法实现了对两个大型单圈对话基准和一个大型多转对话标杆SLU表现在国家的最先进的车型显著的改善。</font>
</div>


<hr>
<div id="paper11"> <b>11. Med7: a transferable clinical natural language processing model for  electronic health records</b>  <a href="https://arxiv.org/pdf/2003.01271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kormilitzin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrey Kormilitzin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vaci%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nemanja Vaci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nevado-Holgado%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alejo Nevado-Holgado</a><br>
<font size="3">
Abstract: The field of clinical natural language processing has been advanced significantly since the introduction of deep learning models. The self-supervised representation learning and the transfer learning paradigm became the methods of choice in many natural language processing application, in particular in the settings with the dearth of high quality manually annotated data. Electronic health record systems are ubiquitous and the majority of patients' data are now being collected electronically and in particular in the form of free text. Identification of medical concepts and information extraction is a challenging task, yet important ingredient for parsing unstructured data into structured and tabulated format for downstream analytical tasks. In this work we introduced a named-entity recognition model for clinical natural language processing. The model is trained to recognise seven categories: drug names, route, frequency, dosage, strength, form, duration. The model was first self-supervisedly pre-trained by predicting the next word, using a collection of 2 million free-text patients' records from MIMIC-III corpora and then fine-tuned on the named-entity recognition task. The model achieved a lenient (strict) micro-averaged F1 score of 0.957 (0.893) across all seven categories. Additionally, we evaluated the transferability of the developed model using the data from the Intensive Care Unit in the US to secondary care mental health records (CRIS) in the UK. A direct application of the trained NER model to CRIS data resulted in reduced performance of F1=0.762, however after fine-tuning on a small sample from CRIS, the model achieved a reasonable performance of F1=0.944. This demonstrated that despite a close similarity between the data sets and the NER tasks, it is essential to fine-tune on the target domain data in order to achieve more accurate results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：临床自然语言处理领域一直以来引进深学习模式的显著进展。自我监督表示学习和转移学习模式成为首选的方法在许多自然语言处理的应用，特别是在高品质的手工标注的数据缺乏的设置。电子病历系统是普遍存在的，大多数病人的数据，目前正在收集电子，特别是在自由文本形式。医学概念和信息提取鉴定是一项艰巨的任务，用于分析非结构化数据转换成结构和表格形式进行下游分析任务而重要的组成部分。在这项工作中，我们介绍了临床自然语言处理命名实体识别模型。该模型被训练识别七类：药品名称，路径，频率，用量，强度，形式，时间。该模型是第一个自supervisedly通过预测下一个字，用200万自由文本病人从MIMIC-III语料库记录，然后在命名实体识别任务微调集合预先训练。该模型在所有七个类别取得了宽松（严格）的微平均F1的0.957（0.893）得分。此外，我们评估使用从重症监护病房，在美国的数据在英国的二级医疗机构的心理健康记录（CRIS）开发的模型的转移性。经训练的模型NER到CRIS数据的直接应用导致F1 = 0.762的性能下降，但之后从CRIS一个小样本的微调，模型取得的F1 = 0.944一个合理的性能。这表明，尽管该数据集和NER任务之间有密切的相似性，它是在目标域数据微调必要的，以便实现更精确的结果。</font>
</div>


<hr>
<div id="paper12"> <b>12. Understanding the Prediction Mechanism of Sentiments by XAI  Visualization</b>  <a href="https://arxiv.org/pdf/2003.01425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=So%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaehan So</a><br>
<font size="3">
Abstract: People often rely on online reviews to make purchase decisions. The present work aimed to gain an understanding of a machine learning model's prediction mechanism by visualizing the effect of sentiments extracted from online hotel reviews with explainable AI (XAI) methodology. Study 1 used the extracted sentiments as features to predict the review ratings by five machine learning algorithms (knn, CART decision trees, support vector machines, random forests, gradient boosting machines) and identified random forests as best algorithm. Study 2 analyzed the random forests model by feature importance and revealed the sentiments joy, disgust, positive and negative as the most predictive features. Furthermore, the visualization of additive variable attributions and their prediction distribution showed correct prediction in direction and effect size for the 5-star rating but partially wrong direction and insufficient effect size for the 1-star rating. These prediction details were corroborated by a what-if analysis for the four top features. In conclusion, the prediction mechanism of a machine learning model can be uncovered by visualization of particular observations. Comparing instances of contrasting ground truth values can draw a differential picture of the prediction mechanism and inform decisions for model improvement. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人们通常依赖于网上评论做出购买决定。目前的工作的目的是通过从可视化的在线酒店评论与解释的AI（XAI）方法提取情绪的效果，以获得一个机器学习模型的预测机制的理解。研究1使用提取的情感为特征五个机器学习算法（KNN，CART决策树，支持向量机，随机森林，助推梯度机器），并确定随机森林是最好的算法预测评价评分。研究2分析了功能重要性的随机森林模型，揭示了感情的喜悦，厌恶，积极和消极作为最有预测功能。此外，添加剂可变归因和它们的预测分布的可视化显示，在方向和效果大小正确预测对5-星级，而是部分错误的方向和作用大小不足1星评级。这些预测资料由一个假设分析的四大功能确证。最后，机器学习模型的预测机构可以通过特定的观察可视化被发现。比较对比地面真值的情况下，可以得出预测机制的差分图像，并告知决策模型的改进。</font>
</div>


<hr>
<div id="paper13"> <b>13. Hierarchical Context Enhanced Multi-Domain Dialogue System for  Multi-domain Task Completion</b>  <a href="https://arxiv.org/pdf/2003.01338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingyuan Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuzhao Mao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiwei Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weiguo Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haiqin Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianping Shen</a><br>
<font size="3">
Abstract: Task 1 of the DSTC8-track1 challenge aims to develop an end-to-end multi-domain dialogue system to accomplish complex users' goals under tourist information desk settings. This paper describes our submitted solution, Hierarchical Context Enhanced Dialogue System (HCEDS), for this task. The main motivation of our system is to comprehensively explore the potential of hierarchical context for sufficiently understanding complex dialogues. More specifically, we apply BERT to capture token-level information and employ the attention mechanism to capture sentence-level information. The results listed in the leaderboard show that our system achieves first place in automatic evaluation and the second place in human evaluation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：DSTC8-TRACK1挑战目标的任务1，开发一个终端到终端的多领域对话系统来完成在旅游信息服务台设置复杂的用户的目标。本文介绍了我们的解决方案提出，阶层上下文加强对话系统（HCEDS），这个任务。我们的系统的主要动机是为全面探索分层上下文的潜力充分理解复杂的对话。更具体地讲，我们应用BERT捕捉标记级别信息和使用注意机制，捕捉句子级别的信息。结果在排行榜显示，我们的系统实现了自动评估和人工评估的第二位第一位上市。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-03</title>
    <url>/2020/03/03/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-03/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Learn2Perturb: an End-to-end Feature Perturbation Learning to Improve  Adversarial Robustness <a href="https://arxiv.org/pdf/2003.01090" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual  Odometry <a href="https://arxiv.org/pdf/2003.01060" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Plug &amp; Play Convolutional Regression Tracker for Video Object Detection <a href="https://arxiv.org/pdf/2003.00981" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro  Hand Gestures and a Real-Time Recognition Framework <a href="https://arxiv.org/pdf/2003.00951" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Always Look on the Bright Side of the Field: Merging Pose and Contextual  Data to Estimate Orientation of Soccer Players <a href="https://arxiv.org/pdf/2003.00943" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Deep Meditations: Controlled navigation of latent space <a href="https://arxiv.org/pdf/2003.00910" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Learning Fast and Robust Target Models for Video Object Segmentation <a href="https://arxiv.org/pdf/2003.00908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Learning to See: You Are What You See <a href="https://arxiv.org/pdf/2003.00902" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Revisiting Convolutional Neural Networks for Urban Flow Analytics <a href="https://arxiv.org/pdf/2003.00895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Gated Fusion Network for Degraded Image Super Resolution <a href="https://arxiv.org/pdf/2003.00893" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Instance Separation Emerges from Inpainting <a href="https://arxiv.org/pdf/2003.00891" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> 3D Object Detection From LiDAR Data Using Distance Depended Feature  Extraction <a href="https://arxiv.org/pdf/2003.00888" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color  Space <a href="https://arxiv.org/pdf/2003.00883" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> The perceptual boost of visual attention is task-dependent in  naturalistic settings <a href="https://arxiv.org/pdf/2003.00882" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Introducing Fuzzy Layers for Deep Learning <a href="https://arxiv.org/pdf/2003.00880" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Estimating a Null Model of Scientific Image Reuse to Support Research  Integrity Investigations <a href="https://arxiv.org/pdf/2003.00878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> A Multi-view Perspective of Self-supervised Learning <a href="https://arxiv.org/pdf/2003.00877" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Predicting TUG score from gait characteristics based on video analysis  and machine learning <a href="https://arxiv.org/pdf/2003.00875" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Few-shot Learning with Weakly-supervised Object Localization <a href="https://arxiv.org/pdf/2003.00874" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> AlignSeg: Feature-Aligned Segmentation Networks <a href="https://arxiv.org/pdf/2003.00872" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Learning Texture Invariant Representation for Domain Adaptation of  Semantic Segmentation <a href="https://arxiv.org/pdf/2003.00867" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Exposing Backdoors in Robust Machine Learning Models <a href="https://arxiv.org/pdf/2003.00865" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Triangle-Net: Towards Robustness in Point Cloud Classification <a href="https://arxiv.org/pdf/2003.00856" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Deep Learning on Radar Centric 3D Object Detection <a href="https://arxiv.org/pdf/2003.00851" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Learning to Deblur and Generate High Frame Rate Video with an Event  Camera <a href="https://arxiv.org/pdf/2003.00847" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> FPGA Implementation of Minimum Mean Brightness Error Bi-Histogram  Equalization <a href="https://arxiv.org/pdf/2003.00840" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> A Machine Learning Framework for Data Ingestion in Document Images <a href="https://arxiv.org/pdf/2003.00838" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> On Parameter Tuning in Meta-learning for Computer Vision <a href="https://arxiv.org/pdf/2003.00837" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Marine life through You Only Look Once's perspective <a href="https://arxiv.org/pdf/2003.00836" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Deep Variational Luenberger-type Observer for Stochastic Video  Prediction <a href="https://arxiv.org/pdf/2003.00835" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> CALVIS: chest, waist and pelvis circumference from 3D human body meshes  as ground truth for deep learning <a href="https://arxiv.org/pdf/2003.00834" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> CNN Hyperparameter tuning applied to Iris Liveness Detection <a href="https://arxiv.org/pdf/2003.00833" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> An End-to-End Visual-Audio Attention Network for Emotion Recognition in  User-Generated Videos <a href="https://arxiv.org/pdf/2003.00832" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Character Segmentation in Asian Collector's Seal Imprints: An Attempt to  Retrieval Based on Ancient Character Typeface <a href="https://arxiv.org/pdf/2003.00831" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> GSANet: Semantic Segmentation with Global and Selective Attention <a href="https://arxiv.org/pdf/2003.00830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Verifying Deep Learning-based Decisions for Facial Expression  Recognition <a href="https://arxiv.org/pdf/2003.00828" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> CheXclusion: Fairness gaps in deep chest X-ray classifiers <a href="https://arxiv.org/pdf/2003.00827" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Realistic River Image Synthesis using Deep Generative Adversarial  Networks <a href="https://arxiv.org/pdf/2003.00826" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> SIP-SegNet: A Deep Convolutional Encoder-Decoder Network for Joint  Semantic Segmentation and Extraction of Sclera, Iris and Pupil based on  Periocular Region Suppression <a href="https://arxiv.org/pdf/2003.00825" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Multi-Scale Representation Learning for Spatial Feature Distributions  using Grid Cells <a href="https://arxiv.org/pdf/2003.00824" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Breast Cancer Histopathology Image Classification and Localization using  Multiple Instance Learning <a href="https://arxiv.org/pdf/2003.00823" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> MADAN: Multi-source Adversarial Domain Aggregation Network for Domain  Adaptation <a href="https://arxiv.org/pdf/2003.00820" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Recognizing Handwritten Mathematical Expressions as LaTex Sequences  Using a Multiscale Robust Neural Network <a href="https://arxiv.org/pdf/2003.00817" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Deepfakes for Medical Video De-Identification: Privacy Protection and  Diagnostic Information Preservation <a href="https://arxiv.org/pdf/2003.00813" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Medicine Strip Identification using 2-D Cepstral Feature Extraction and  Multiclass Classification Methods <a href="https://arxiv.org/pdf/2003.00810" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> Vision based body gesture meta features for Affective Computing <a href="https://arxiv.org/pdf/2003.00809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> A Convolutional Baseline for Person Re-Identification Using Vision and  Language Descriptions <a href="https://arxiv.org/pdf/2003.00808" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> Firearm Detection and Segmentation Using an Ensemble of Semantic Neural  Networks <a href="https://arxiv.org/pdf/2003.00805" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Task Augmentation by Rotating for Meta-Learning <a href="https://arxiv.org/pdf/2003.00804" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> Hypernetwork approach to generating point clouds <a href="https://arxiv.org/pdf/2003.00802" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> Real-Time target detection in maritime scenarios based on YOLOv3 model <a href="https://arxiv.org/pdf/2003.00800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> Preventing Clean Label Poisoning using Gaussian Mixture Loss <a href="https://arxiv.org/pdf/2003.00798" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> Identity Recognition in Intelligent Cars with Behavioral Data and  LSTM-ResNet Classifier <a href="https://arxiv.org/pdf/2003.00770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion  from 3D Geometry <a href="https://arxiv.org/pdf/2003.00766" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> Learning Depth via Interaction <a href="https://arxiv.org/pdf/2003.00752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> Long Short-Term Sample Distillation <a href="https://arxiv.org/pdf/2003.00739" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Towards Unconstrained Palmprint Recognition on Consumer Devices: a  Literature Review <a href="https://arxiv.org/pdf/2003.00737" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
<div id="title58">
<b>58.</b> A-TVSNet: Aggregated Two-View Stereo Network for Multi-View Stereo Depth  Estimation <a href="https://arxiv.org/pdf/2003.00711" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper58" style="color:#0000EE;">摘要</a><br></div>
<div id="title59">
<b>59.</b> Learned Enrichment of Top-View Grid Maps Improves Object Detection <a href="https://arxiv.org/pdf/2003.00710" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper59" style="color:#0000EE;">摘要</a><br></div>
<div id="title60">
<b>60.</b> Unbiased Mean Teacher for Cross Domain Object Detection <a href="https://arxiv.org/pdf/2003.00707" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper60" style="color:#0000EE;">摘要</a><br></div>
<div id="title61">
<b>61.</b> GPU-Accelerated Mobile Multi-view Style Transfer <a href="https://arxiv.org/pdf/2003.00706" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper61" style="color:#0000EE;">摘要</a><br></div>
<div id="title62">
<b>62.</b> Relational Deep Feature Learning for Heterogeneous Face Recognition <a href="https://arxiv.org/pdf/2003.00697" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper62" style="color:#0000EE;">摘要</a><br></div>
<div id="title63">
<b>63.</b> Deep Image Spatial Transformation for Person Image Generation <a href="https://arxiv.org/pdf/2003.00696" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper63" style="color:#0000EE;">摘要</a><br></div>
<div id="title64">
<b>64.</b> SketchGCN: Semantic Sketch Segmentation with Graph Convolutional  Networks <a href="https://arxiv.org/pdf/2003.00678" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper64" style="color:#0000EE;">摘要</a><br></div>
<div id="title65">
<b>65.</b> Global Context-Aware Progressive Aggregation Network for Salient Object  Detection <a href="https://arxiv.org/pdf/2003.00651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper65" style="color:#0000EE;">摘要</a><br></div>
<div id="title66">
<b>66.</b> VAE/WGAN-Based Image Representation Learning For Pose-Preserving  Seamless Identity Replacement In Facial Images <a href="https://arxiv.org/pdf/2003.00641" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper66" style="color:#0000EE;">摘要</a><br></div>
<div id="title67">
<b>67.</b> A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view  Stereo Reconstruction from An Open Aerial Dataset <a href="https://arxiv.org/pdf/2003.00637" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper67" style="color:#0000EE;">摘要</a><br></div>
<div id="title68">
<b>68.</b> Matching Neuromorphic Events and Color Images via Adversarial Learning <a href="https://arxiv.org/pdf/2003.00636" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper68" style="color:#0000EE;">摘要</a><br></div>
<div id="title69">
<b>69.</b> Extremely Dense Point Correspondences using a Learned Feature Descriptor <a href="https://arxiv.org/pdf/2003.00619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper69" style="color:#0000EE;">摘要</a><br></div>
<div id="title70">
<b>70.</b> 3D Point Cloud Processing and Learning for Autonomous Driving <a href="https://arxiv.org/pdf/2003.00601" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper70" style="color:#0000EE;">摘要</a><br></div>
<div id="title71">
<b>71.</b> Rethinking Fully Convolutional Networks for the Analysis of  Photoluminescence Wafer Images <a href="https://arxiv.org/pdf/2003.00594" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper71" style="color:#0000EE;">摘要</a><br></div>
<div id="title72">
<b>72.</b> Fast Lidar Clustering by Density and Connectivity <a href="https://arxiv.org/pdf/2003.00575" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper72" style="color:#0000EE;">摘要</a><br></div>
<div id="title73">
<b>73.</b> The Sloop System for Individual Animal Identification with Deep Learning <a href="https://arxiv.org/pdf/2003.00559" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper73" style="color:#0000EE;">摘要</a><br></div>
<div id="title74">
<b>74.</b> Soft-Root-Sign Activation Function <a href="https://arxiv.org/pdf/2003.00547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper74" style="color:#0000EE;">摘要</a><br></div>
<div id="title75">
<b>75.</b> ZoomNet: Part-Aware Adaptive Zooming Neural Network for 3D Object  Detection <a href="https://arxiv.org/pdf/2003.00529" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper75" style="color:#0000EE;">摘要</a><br></div>
<div id="title76">
<b>76.</b> Deep Attention Aware Feature Learning for Person Re-Identification <a href="https://arxiv.org/pdf/2003.00517" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper76" style="color:#0000EE;">摘要</a><br></div>
<div id="title77">
<b>77.</b> MonoPair: Monocular 3D Object Detection Using Pairwise Spatial  Relationships <a href="https://arxiv.org/pdf/2003.00504" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper77" style="color:#0000EE;">摘要</a><br></div>
<div id="title78">
<b>78.</b> PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks  with Adaptive Sampling <a href="https://arxiv.org/pdf/2003.00492" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper78" style="color:#0000EE;">摘要</a><br></div>
<div id="title79">
<b>79.</b> State-Aware Tracker for Real-Time Video Object Segmentation <a href="https://arxiv.org/pdf/2003.00482" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper79" style="color:#0000EE;">摘要</a><br></div>
<div id="title80">
<b>80.</b> STC-Flow: Spatio-temporal Context-aware Optical Flow Estimation <a href="https://arxiv.org/pdf/2003.00434" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper80" style="color:#0000EE;">摘要</a><br></div>
<div id="title81">
<b>81.</b> Deep Learning for Content-based Personalized Viewport Prediction of  360-Degree VR Videos <a href="https://arxiv.org/pdf/2003.00429" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper81" style="color:#0000EE;">摘要</a><br></div>
<div id="title82">
<b>82.</b> Learning When and Where to Zoom with Deep Reinforcement Learning <a href="https://arxiv.org/pdf/2003.00425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper82" style="color:#0000EE;">摘要</a><br></div>
<div id="title83">
<b>83.</b> Towards Automatic Face-to-Face Translation <a href="https://arxiv.org/pdf/2003.00418" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper83" style="color:#0000EE;">摘要</a><br></div>
<div id="title84">
<b>84.</b> PF-Net: Point Fractal Network for 3D Point Cloud Completion <a href="https://arxiv.org/pdf/2003.00410" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper84" style="color:#0000EE;">摘要</a><br></div>
<div id="title85">
<b>85.</b> FMT:Fusing Multi-task Convolutional Neural Network for Person Search <a href="https://arxiv.org/pdf/2003.00406" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper85" style="color:#0000EE;">摘要</a><br></div>
<div id="title86">
<b>86.</b> Cops-Ref: A new Dataset and Task on Compositional Referring Expression  Comprehension <a href="https://arxiv.org/pdf/2003.00403" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper86" style="color:#0000EE;">摘要</a><br></div>
<div id="title87">
<b>87.</b> Intelligent Home 3D: Automatic 3D-House Design from Linguistic  Descriptions Only <a href="https://arxiv.org/pdf/2003.00397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper87" style="color:#0000EE;">摘要</a><br></div>
<div id="title88">
<b>88.</b> Deep Active Learning for Biased Datasets via Fisher Kernel  Self-Supervision <a href="https://arxiv.org/pdf/2003.00393" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper88" style="color:#0000EE;">摘要</a><br></div>
<div id="title89">
<b>89.</b> Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning <a href="https://arxiv.org/pdf/2003.00392" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper89" style="color:#0000EE;">摘要</a><br></div>
<div id="title90">
<b>90.</b> Joint Wasserstein Distribution Matching <a href="https://arxiv.org/pdf/2003.00389" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper90" style="color:#0000EE;">摘要</a><br></div>
<div id="title91">
<b>91.</b> Say As You Wish: Fine-grained Control of Image Caption Generation with  Abstract Scene Graphs <a href="https://arxiv.org/pdf/2003.00387" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper91" style="color:#0000EE;">摘要</a><br></div>
<div id="title92">
<b>92.</b> Emotion Recognition System from Speech and Visual Information based on  Convolutional Neural Networks <a href="https://arxiv.org/pdf/2003.00351" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper92" style="color:#0000EE;">摘要</a><br></div>
<div id="title93">
<b>93.</b> Learning Cross-domain Generalizable Features by Representation  Disentanglement <a href="https://arxiv.org/pdf/2003.00321" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper93" style="color:#0000EE;">摘要</a><br></div>
<div id="title94">
<b>94.</b> Grounded and Controllable Image Completion by Incorporating Lexical  Semantics <a href="https://arxiv.org/pdf/2003.00303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper94" style="color:#0000EE;">摘要</a><br></div>
<div id="title95">
<b>95.</b> Representations, Metrics and Statistics For Shape Analysis of Elastic  Graphs <a href="https://arxiv.org/pdf/2003.00287" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper95" style="color:#0000EE;">摘要</a><br></div>
<div id="title96">
<b>96.</b> Augmenting Visual Place Recognition with Structural Cues <a href="https://arxiv.org/pdf/2003.00278" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper96" style="color:#0000EE;">摘要</a><br></div>
<div id="title97">
<b>97.</b> Reusing Discriminators for Encoding Towards Unsupervised Image-to-Image  Translation <a href="https://arxiv.org/pdf/2003.00273" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper97" style="color:#0000EE;">摘要</a><br></div>
<div id="title98">
<b>98.</b> Joint Face Completion and Super-resolution using Multi-scale Feature  Relation Learning <a href="https://arxiv.org/pdf/2003.00255" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper98" style="color:#0000EE;">摘要</a><br></div>
<div id="title99">
<b>99.</b> NAS-Count: Counting-by-Density with Neural Architecture Search <a href="https://arxiv.org/pdf/2003.00217" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper99" style="color:#0000EE;">摘要</a><br></div>
<div id="title100">
<b>100.</b> Channel Equilibrium Networks for Learning Deep Representation <a href="https://arxiv.org/pdf/2003.00214" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper100" style="color:#0000EE;">摘要</a><br></div>
<div id="title101">
<b>101.</b> Cross-Spectrum Dual-Subspace Pairing for RGB-infrared Cross-Modality  Person Re-Identification <a href="https://arxiv.org/pdf/2003.00213" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper101" style="color:#0000EE;">摘要</a><br></div>
<div id="title102">
<b>102.</b> Learning to Compare Relation: Semantic Alignment for Few-Shot Learning <a href="https://arxiv.org/pdf/2003.00210" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper102" style="color:#0000EE;">摘要</a><br></div>
<div id="title103">
<b>103.</b> VideoSSL: Semi-Supervised Learning for Video Classification <a href="https://arxiv.org/pdf/2003.00197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper103" style="color:#0000EE;">摘要</a><br></div>
<div id="title104">
<b>104.</b> First Order Motion Model for Image Animation <a href="https://arxiv.org/pdf/2003.00196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper104" style="color:#0000EE;">摘要</a><br></div>
<div id="title105">
<b>105.</b> Robust 6D Object Pose Estimation by Learning RGB-D Features <a href="https://arxiv.org/pdf/2003.00188" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper105" style="color:#0000EE;">摘要</a><br></div>
<div id="title106">
<b>106.</b> Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image  Translation <a href="https://arxiv.org/pdf/2003.00187" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper106" style="color:#0000EE;">摘要</a><br></div>
<div id="title107">
<b>107.</b> HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection <a href="https://arxiv.org/pdf/2003.00186" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper107" style="color:#0000EE;">摘要</a><br></div>
<div id="title108">
<b>108.</b> Attention-aware fusion RGB-D face recognition <a href="https://arxiv.org/pdf/2003.00168" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper108" style="color:#0000EE;">摘要</a><br></div>
<div id="title109">
<b>109.</b> Towards Using Count-level Weak Supervision for Crowd Counting <a href="https://arxiv.org/pdf/2003.00164" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper109" style="color:#0000EE;">摘要</a><br></div>
<div id="title110">
<b>110.</b> Self-supervised Representation Learning for Ultrasound Video <a href="https://arxiv.org/pdf/2003.00105" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper110" style="color:#0000EE;">摘要</a><br></div>
<div id="title111">
<b>111.</b> Transferring Dense Pose to Proximal Animal Classes <a href="https://arxiv.org/pdf/2003.00080" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper111" style="color:#0000EE;">摘要</a><br></div>
<div id="title112">
<b>112.</b> Bio-Inspired Modality Fusion for Active Speaker Detection <a href="https://arxiv.org/pdf/2003.00063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper112" style="color:#0000EE;">摘要</a><br></div>
<div id="title113">
<b>113.</b> Learning Nonparametric Human Mesh Reconstruction from a Single Image  without Ground Truth Meshes <a href="https://arxiv.org/pdf/2003.00052" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper113" style="color:#0000EE;">摘要</a><br></div>
<div id="title114">
<b>114.</b> Unlimited Resolution Image Generation with R2D2-GANs <a href="https://arxiv.org/pdf/2003.01063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper114" style="color:#0000EE;">摘要</a><br></div>
<div id="title115">
<b>115.</b> Constrained Nonnegative Matrix Factorization for Blind Hyperspectral  Unmixing incorporating Endmember Independence <a href="https://arxiv.org/pdf/2003.01041" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper115" style="color:#0000EE;">摘要</a><br></div>
<div id="title116">
<b>116.</b> Evaluating Temporal Queries Over Video Feeds <a href="https://arxiv.org/pdf/2003.00953" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper116" style="color:#0000EE;">摘要</a><br></div>
<div id="title117">
<b>117.</b> Multi-View Learning for Vision-and-Language Navigation <a href="https://arxiv.org/pdf/2003.00857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper117" style="color:#0000EE;">摘要</a><br></div>
<div id="title118">
<b>118.</b> Addressing target shift in zero-shot learning using grouped adversarial  learning <a href="https://arxiv.org/pdf/2003.00845" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper118" style="color:#0000EE;">摘要</a><br></div>
<div id="title119">
<b>119.</b> Quantized Neural Network Inference with Precision Batching <a href="https://arxiv.org/pdf/2003.00822" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper119" style="color:#0000EE;">摘要</a><br></div>
<div id="title120">
<b>120.</b> Convolutional Sparse Support Estimator Network (CSEN) From energy  efficient support estimation to learning-aided Compressive Sensing <a href="https://arxiv.org/pdf/2003.00768" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper120" style="color:#0000EE;">摘要</a><br></div>
<div id="title121">
<b>121.</b> MVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation <a href="https://arxiv.org/pdf/2003.00667" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper121" style="color:#0000EE;">摘要</a><br></div>
<div id="title122">
<b>122.</b> PSF--NET: A Non-parametric Point Spread Function Model for Ground Based  Optical Telescopes <a href="https://arxiv.org/pdf/2003.00615" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper122" style="color:#0000EE;">摘要</a><br></div>
<div id="title123">
<b>123.</b> 3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via  Coupled Feature Selection <a href="https://arxiv.org/pdf/2003.00535" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper123" style="color:#0000EE;">摘要</a><br></div>
<div id="title124">
<b>124.</b> Dimensionality reduction to maximize prediction generalization  capability <a href="https://arxiv.org/pdf/2003.00470" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper124" style="color:#0000EE;">摘要</a><br></div>
<div id="title125">
<b>125.</b> Environment-agnostic Multitask Learning for Natural Language Grounded  Navigation <a href="https://arxiv.org/pdf/2003.00443" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper125" style="color:#0000EE;">摘要</a><br></div>
<div id="title126">
<b>126.</b> Why is the Mahalanobis Distance Effective for Anomaly Detection? <a href="https://arxiv.org/pdf/2003.00402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper126" style="color:#0000EE;">摘要</a><br></div>
<div id="title127">
<b>127.</b> Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI  Components by Deep Learning <a href="https://arxiv.org/pdf/2003.00380" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper127" style="color:#0000EE;">摘要</a><br></div>
<div id="title128">
<b>128.</b> Understanding the Intrinsic Robustness of Image Distributions using  Conditional Generative Models <a href="https://arxiv.org/pdf/2003.00378" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper128" style="color:#0000EE;">摘要</a><br></div>
<div id="title129">
<b>129.</b> An Evaluation of Knowledge Graph Embeddings for Autonomous Driving Data:  Experience and Practice <a href="https://arxiv.org/pdf/2003.00344" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper129" style="color:#0000EE;">摘要</a><br></div>
<div id="title130">
<b>130.</b> An estimation-based method to segment PET images <a href="https://arxiv.org/pdf/2003.00317" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper130" style="color:#0000EE;">摘要</a><br></div>
<div id="title131">
<b>131.</b> Unsupervised Dictionary Learning for Anomaly Detection <a href="https://arxiv.org/pdf/2003.00293" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper131" style="color:#0000EE;">摘要</a><br></div>
<div id="title132">
<b>132.</b> Image Hashing by Minimizing Independent Relaxed Wasserstein Distance <a href="https://arxiv.org/pdf/2003.00134" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper132" style="color:#0000EE;">摘要</a><br></div>
<div id="title133">
<b>133.</b> Inexpensive surface electromyography sleeve with consistent electrode  placement enables dexterous and stable prosthetic control through deep  learning <a href="https://arxiv.org/pdf/2003.00070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper133" style="color:#0000EE;">摘要</a><br></div>
<div id="title134">
<b>134.</b> SeismiQB -- a novel framework for deep learning with seismic data <a href="https://arxiv.org/pdf/2001.06416" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper134" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a><p></p>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Learn2Perturb: an End-to-end Feature Perturbation Learning to Improve  Adversarial Robustness</b>  <a href="https://arxiv.org/pdf/2003.01090" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jeddi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmadreza Jeddi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shafiee%2C+M+J" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Javad Shafiee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Karg%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michelle Karg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scharfenberger%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Scharfenberger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Wong</a><br>
<font size="3">
Abstract: While deep neural networks have been achieving state-of-the-art performance across a wide variety of applications, their vulnerability to adversarial attacks limits their widespread deployment for safety-critical applications. Alongside other adversarial defense approaches being investigated, there has been a very recent interest in improving adversarial robustness in deep neural networks through the introduction of perturbations during the training process. However, such methods leverage fixed, pre-defined perturbations and require significant hyper-parameter tuning that makes them very difficult to leverage in a general fashion. In this study, we introduce Learn2Perturb, an end-to-end feature perturbation learning approach for improving the adversarial robustness of deep neural networks. More specifically, we introduce novel perturbation-injection modules that are incorporated at each layer to perturb the feature space and increase uncertainty in the network. This feature perturbation is performed at both the training and the inference stages. Furthermore, inspired by the Expectation-Maximization, an alternating back-propagation training algorithm is introduced to train the network and noise parameters consecutively. Experimental results on CIFAR-10 and CIFAR-100 datasets show that the proposed Learn2Perturb method can result in deep neural networks which are $4-7\%$ more robust on $l_{\infty}$ FGSM and PDG adversarial attacks and significantly outperforms the state-of-the-art against $l_2$ $C\&W$ attack and a wide range of well-known black-box attacks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管深层神经网络已经在各种不同的应用实现国家的最先进的性能，他们的对抗攻击的弱点限制了其广泛应用的安全关键型应用。与其他对抗性的防守办法正在调查中，出现了通过在训练过程中引入扰动的改善深层神经网络对抗的鲁棒性非常最近的兴趣。然而，这种方法利用固定的，预定义的干扰，需要显著超参数调整，使得利用它们很难在一般的时尚。在这项研究中，我们介绍Learn2Perturb，可以改进神经网络的鲁棒性对抗的结束到终端的功能扰动的学习方法。更具体地说，我们引入在每个层中扰动特征空间和增加了网络中的不确定性微扰新颖喷射模块。此功能扰动在训练和推理阶段都进行。此外，激发由期望最大化，交替反向传播训练算法被引入到连续训练网络和噪声参数。上CIFAR-10和CIFAR-100数据集实验结果表明，所提出的Learn2Perturb方法可导致深的神经网络，其是$ 4-7 \％$上$升_ {\ infty} $ FGSM和PDG对抗攻击更加稳健和显著优于国家的最先进的打击$ L_2 $ $ C \＆W $攻击力和广泛的知名黑箱攻击。</font>
</div>


<hr>
<div id="paper2"> <b>2. D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual  Odometry</b>  <a href="https://arxiv.org/pdf/2003.01060" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Stumberg%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lukas von Stumberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cremers%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Cremers</a><br>
<font size="3">
Abstract: We propose D3VO as a novel framework for monocular visual odometry that exploits deep networks on three levels -- deep depth, pose and uncertainty estimation. We first propose a novel self-supervised monocular depth estimation network trained on stereo videos without any external supervision. In particular, it aligns the training image pairs into similar lighting condition with predictive brightness transformation parameters. Besides, we model the photometric uncertainties of pixels on the input images, which improves the depth estimation accuracy and provides a learned weighting function for the photometric residuals in direct (feature-less) visual odometry. Evaluation results show that the proposed network outperforms state-of-the-art self-supervised depth estimation networks. D3VO tightly incorporates the predicted depth, pose and uncertainty into a direct visual odometry method to boost both the front-end tracking as well as the back-end non-linear optimization. We evaluate D3VO in terms of monocular visual odometry on both the KITTI odometry benchmark and the EuRoC MAV dataset. The results show that D3VO outperforms state-of-the-art traditional monocular VO methods by a large margin. It also achieves comparable results to state-of-the-art stereo/LiDAR odometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC MAV, while using only a single camera. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出D3VO为单眼视觉测程是在三个层面上利用深网络一种新型的框架 - 深度较深，姿态和不确定性估计。我们首先提出的培训立体声影片新颖的自我监督单眼深度估计网络，无需任何外部监督。特别是，它对准训练图像对与预测亮度变换参数类似的光照条件。此外，我们在输入的图像，从而提高了深度估计精度和直接（特征更少）视觉里程计提供了学习的加权函数用于光度残差像素的光度的不确定性模型。评价结果表明，该网络性能优于国家的最先进的自我监督的深度估计网络。 D3VO紧紧结合预测的深度，姿势和不确定性的直接视觉测距方法，以提高两个前端跟踪以及后端非线性优化。我们在上KITTI测距标杆和EuRoC MAV数据集都单眼视觉测程方面评估D3VO。结果表明，D3VO性能优于国家的最先进的传统单眼VO方法大幅度。它也实现了上KITTI并就EuRoC MAV所述状态的最先进的视觉惯性测程比较的结果，国家的最先进的立体声/激光雷达测距，而只使用一个单一的相机。</font>
</div>


<hr>
<div id="paper3"> <b>3. Plug &amp; Play Convolutional Regression Tracker for Video Object Detection</b>  <a href="https://arxiv.org/pdf/2003.00981" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ye Lyu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+M+Y" target="_blank" rel="noopener" style="color:#0000EE;">Michael Ying Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vosselman%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">George Vosselman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gui-Song Xia</a><br>
<font size="3">
Abstract: Video object detection targets to simultaneously localize the bounding boxes of the objects and identify their classes in a given video. One challenge for video object detection is to consistently detect all objects across the whole video. As the appearance of objects may deteriorate in some frames, features or detections from the other frames are commonly used to enhance the prediction. In this paper, we propose a Plug & Play scale-adaptive convolutional regression tracker for the video object detection task, which could be easily and compatibly implanted into the current state-of-the-art detection networks. As the tracker reuses the features from the detector, it is a very light-weighted increment to the detection network. The whole network performs at the speed close to a standard object detector. With our new video object detection pipeline design, image object detectors can be easily turned into efficient video object detectors without modifying any parameters. The performance is evaluated on the large-scale ImageNet VID dataset. Our Plug & Play design improves mAP score for the image detector by around 5% with only little speed drop. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频对象检测目标同时定位的对象的包围盒，并确定在给定的视频课堂。视频对象检测的一个挑战是始终如一地检测在整个视频中的所有对象。作为对象的外观可在一些帧恶化，特征或从其它帧检测通常用于增强预测。在本文中，我们提出了一个即插即用尺度自适应卷积回归跟踪的视频对象检测任务，这可能是很容易和兼容植入到当前国家的最先进的检测网络。由于跟踪器从检测器的重复使用的特性，这是一个非常光加权增量到检测网络。整个网络进行在速度接近标准的对象检测器。与我们的新的视频对象检测流水线设计，图像对象检测器可以很容易地转变成有效的视频对象的探测器，而无需修改任何参数。性能上大规模ImageNet VID数据集进行评估。我们的即插即用设计提高了地图分数由5％左右，只有很少的速度下降到图像检测器。</font>
</div>


<hr>
<div id="paper4"> <b>4. DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro  Hand Gestures and a Real-Time Recognition Framework</b>  <a href="https://arxiv.org/pdf/2003.00951" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=K%C3%B6p%C3%BCkl%C3%BC%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Okan Köpüklü</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ledwon%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Ledwon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yao Rong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kose%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Neslihan Kose</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rigoll%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerhard Rigoll</a><br>
<font size="3">
Abstract: The use of hand gestures provides a natural alternative to cumbersome interface devices for Human-Computer Interaction (HCI) systems. However, real-time recognition of dynamic micro hand gestures from video streams is challenging for in-vehicle scenarios since (i) the gestures should be performed naturally without distracting the driver, (ii) micro hand gestures occur within very short time intervals at spatially constrained areas, (iii) the performed gesture should be recognized only once, and (iv) the entire architecture should be designed lightweight as it will be deployed to an embedded system. In this work, we propose an HCI system for dynamic recognition of driver micro hand gestures, which can have a crucial impact in automotive sector especially for safety related issues. For this purpose, we initially collected a dataset named Driver Micro Hand Gestures (DriverMHG), which consists of RGB, depth and infrared modalities. The challenges for dynamic recognition of micro hand gestures have been addressed by proposing a lightweight convolutional neural network (CNN) based architecture which operates online efficiently with a sliding window approach. For the CNN model, several 3-dimensional resource efficient networks are applied and their performances are analyzed. Online recognition of gestures has been performed with 3D-MobileNetV2, which provided the best offline accuracy among the applied networks with similar computational complexities. The final architecture is deployed on a driver simulator operating in real-time. We make DriverMHG dataset and our source code publicly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：使用手势提供了用于人机交互（HCI）的系统笨重接口设备的天然替代。然而，由于（i）所述的手势应当自然而不分心驾驶员执行从视频流动态微手势的实时识别被用于车载场景挑战，（ii）在空间上很短的时间间隔内发生微手势狭窄的区域，（iii）所述进行手势应当仅一次认识到的，以及（iv）在整个体系结构的设计应轻便，因为它会被部署到嵌入式系统。在这项工作中，我们提出了动态识别驾驶员微手势，它可以在汽车行业尤其是对安全相关问题的关键影响的人机交互系统。为此，我们最初收集到一个名为驱动微手势（DriverMHG）数据集，其中包括RGB，深度和红外模式的。微手势的动态识别的难题已经得到解决，通过提出与滑动窗口方法网上有效地运行一个轻量级的卷积神经网络（CNN）的基础架构。对于CNN模型，几个3维资源高效的网络应用，并对其性能进行了分析。手势识别在线已经与3D-MobileNetV2，它提供了类似的计算复杂的应用网络中最好的离线准确性进行。最终的架构部署在驾驶模拟器的实时操作。我们做DriverMHG数据集和我们的源代码公开。</font>
</div>


<hr>
<div id="paper5"> <b>5. Always Look on the Bright Side of the Field: Merging Pose and Contextual  Data to Estimate Orientation of Soccer Players</b>  <a href="https://arxiv.org/pdf/2003.00943" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Arbu%C3%A9s-Sang%C3%BCesa%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrià Arbués-Sangüesa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mart%C3%ADn%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrián Martín</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Javier Fernández</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rodr%C3%ADguez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Rodríguez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haro%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gloria Haro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ballester%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Coloma Ballester</a><br>
<font size="3">
Abstract: Although orientation has proven to be a key skill of soccer players in order to succeed in a broad spectrum of plays, body orientation is a yet-little-explored area in sports analytics' research. Despite being an inherently ambiguous concept, player orientation can be defined as the projection (2D) of the normal vector placed in the center of the upper-torso of players (3D). This research presents a novel technique to obtain player orientation from monocular video recordings by mapping pose parts (shoulders and hips) in a 2D field by combining OpenPose with a super-resolution network, and merging the obtained estimation with contextual information (ball position). Results have been validated with players-held EPTS devices, obtaining a median error of 27 degrees/player. Moreover, three novel types of orientation maps are proposed in order to make raw orientation data easy to visualize and understand, thus allowing further analysis at team- or player-level. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管定位已被证明是足球运动员的一个重要技巧，才能在戏剧的广泛成功，身体方向是运动分析研究尚未小面积探索。尽管是一种固有的模糊的概念，玩家取向可以被定义为放置在玩家上部躯干（3D）的中心处的法线矢量的投影（2D）。本研究提出了一种新技术，通过OpenPose与超分辨率网络相结合，和合并与上下文信息（球的位置）得到的估算，以获得从通过在2D场映射姿态份（肩部和臀部）单眼录像播放器取向。结果进行了验证，玩家持有的EPTS设备，获得27度/播放机的平均误差。此外，三种新类型的定向图，提出为了使原始方位数据易于可视化和理解，因此允许在team-或玩家级进一步分析。</font>
</div>


<hr>
<div id="paper6"> <b>6. Deep Meditations: Controlled navigation of latent space</b>  <a href="https://arxiv.org/pdf/2003.00910" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Akten%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Memo Akten</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fiebrink%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rebecca Fiebrink</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grierson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mick Grierson</a><br>
<font size="3">
Abstract: We introduce a method which allows users to creatively explore and navigate the vast latent spaces of deep generative models. Specifically, our method enables users to \textit{discover} and \textit{design} \textit{trajectories} in these high dimensional spaces, to construct stories, and produce time-based media such as videos---\textit{with meaningful control over narrative}. Our goal is to encourage and aid the use of deep generative models as a medium for creative expression and story telling with meaningful human control. Our method is analogous to traditional video production pipelines in that we use a conventional non-linear video editor with proxy clips, and conform with arrays of latent space vectors. Examples can be seen at \url{this http URL}. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍了一种方法，可以让用户创造性地探索和导航深生成模型的巨大潜在空间。具体来说，我们的方法，使用户能够\ textit {}发现和\ {textit设计} \ {textit轨迹}在这些高维空间，构造故事，以及基于时间的媒体，例如视频--- \ textit {有意义在控制叙述}。我们的目标是鼓励和帮助使用深生成模式作为创造性的表达和讲故事有意义的人类控制的媒体。在我们使用具有代理剪辑常规的非线性视频编辑器，并与潜在空间矢量的阵列符合我们的方法类似于传统的视频制作流程。例如可以在\网址可以看到这个{HTTP URL}。</font>
</div>


<hr>
<div id="paper7"> <b>7. Learning Fast and Robust Target Models for Video Object Segmentation</b>  <a href="https://arxiv.org/pdf/2003.00908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Robinson%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andreas Robinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lawin%2C+F+J" target="_blank" rel="noopener" style="color:#0000EE;">Felix Järemo Lawin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Danelljan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Danelljan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+F+S" target="_blank" rel="noopener" style="color:#0000EE;">Fahad Shahbaz Khan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Felsberg%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Felsberg</a><br>
<font size="3">
Abstract: Video object segmentation (VOS) is a highly challenging problem since the initial mask, defining the target object, is only given at test-time. The main difficulty is to effectively handle appearance changes and similar background objects, while maintaining accurate segmentation. Most previous approaches fine-tune segmentation networks on the first frame, resulting in impractical frame-rates and risk of overfitting. More recent methods integrate generative target appearance models, but either achieve limited robustness or require large amounts of training data. We propose a novel VOS architecture consisting of two network components. The target appearance model consists of a light-weight module, learned during the inference stage using fast optimization techniques to predict a coarse but robust target segmentation. The segmentation model is exclusively trained offline, designed to process the coarse scores into high quality segmentation masks. Our method is fast, easily trainable and remains is highly effective in cases of limited training data. We perform extensive experiments on the challenging YouTube-VOS and DAVIS datasets. Our network achieves favorable performance, while operating at significantly higher frame-rates compared to state-of-the-art. Code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频对象分割（VOS）是由于初始掩模的高度挑战性的问题，定义目标对象，仅在测试时间给出。主要的困难是有效地处理外观变化和相似的背景对象，同时保持准确的分割。大多数以前的办法在第一帧微调分割网络，造成不切实际的帧速率和过度拟合的风险。最近的集成方法生成的目标外观模型，但无论是实现有限的稳健性或需要大量的训练数据。我们提出了一个新颖的VOS架构由两个网络组件。目标外观模型由重量轻的模块，期间使用快速优化技术来预测一个粗略但健壮目标分割推论阶段了解到的。细分模型是专门离线训练，旨在处理粗分数成高品质的分割掩码。我们的方法是快速，轻松地训练的和遗体是在有限的训练数据的情况下非常有效。我们进行的挑战YouTube上，VOS和戴维斯的数据集广泛的实验。我们的网络实现了良好的性能，而在显著更高的帧速率运行相比，国家的最先进的。代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper8"> <b>8. Learning to See: You Are What You See</b>  <a href="https://arxiv.org/pdf/2003.00902" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Akten%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Memo Akten</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fiebrink%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rebecca Fiebrink</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grierson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mick Grierson</a><br>
<font size="3">
Abstract: The authors present a visual instrument developed as part of the creation of the artwork Learning to See. The artwork explores bias in artificial neural networks and provides mechanisms for the manipulation of specifically trained for real-world representations. The exploration of these representations acts as a metaphor for the process of developing a visual understanding and/or visual vocabulary of the world. These representations can be explored and manipulated in real time, and have been produced in such a way so as to reflect specific creative perspectives that call into question the relationship between how both artificial neural networks and humans may construct meaning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：作者提出发展为创造艺术品习见的一部分可视仪器。艺术品探讨偏向于人工神经网络，并提供了专门训练的真实世界表示的操作机制。这些表述的探索充当开发一种直观的了解和/或世界的视觉词汇的过程中的隐喻。这些表示可以探索和实时操作，并以这样的方式已经产生，以反映特定创造性的观点是质疑两者间人工神经网络和人类如何可以建构意义的关系。</font>
</div>


<hr>
<div id="paper9"> <b>9. Revisiting Convolutional Neural Networks for Urban Flow Analytics</b>  <a href="https://arxiv.org/pdf/2003.00895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxuan Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ouyang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Ouyang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junbo Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosenblum%2C+D+S" target="_blank" rel="noopener" style="color:#0000EE;">David S. Rosenblum</a><br>
<font size="3">
Abstract: Convolutional Neural Networks (CNNs) have been widely adopted in raster-based urban flow analytics by virtue of their capability in capturing nearby spatial context. By revisiting CNN-based methods for different analytics tasks, we expose two common critical drawbacks in the existing uses: 1) inefficiency in learning global context, and 2) overlooking latent region functions. To tackle these challenges, in this paper we present a novel framework entitled DeepLGR that can be easily generalized to address various urban flow analytics problems. This framework consists of three major parts: 1) a local context module to learn local representations of each region; 2) a global context module to extract global contextual priors and upsample them to generate the global features; and 3) a region-specific predictor based on tensor decomposition to provide customized predictions for each region, which is very parameter-efficient compared to previous methods. Extensive experiments on two typical urban analytics tasks demonstrate the effectiveness, stability, and generality of our framework. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）已经被广泛应用在基于栅格的城市流动分析凭借着自己的能力，在拍摄附近的空间环境中采用。通过重新审视不同的分析任务的基于CNN的方法，我们暴露出现有用途两种常见的关键缺点：俯瞰潜在区域功能1）在学习全球范围内的低效率，以及2）。为了应对这些挑战，在本文中我们提出题为DeepLGR一个新的框架，可以很容易地推广到地址不同城市流动分析问题。这个框架由三个主要部分组成：1）地方背景模块，以了解每个地区的本地表示; 2）一个全局上下文模块以提取上下文全球先验和上采样它们来生成全局特征;和3）的基础上张量分解的特定区域预测器提供定制针对每个区域，相比于以前的方法，这是非常参数效率的预测。两个典型的城市分析任务大量实验证明的有效性，稳定性，和我们的框架的通用性。</font>
</div>


<hr>
<div id="paper10"> <b>10. Gated Fusion Network for Degraded Image Super Resolution</b>  <a href="https://arxiv.org/pdf/2003.00893" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Sheng Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Hsuan Yang</a><br>
<font size="3">
Abstract: Single image super resolution aims to enhance image quality with respect to spatial content, which is a fundamental task in computer vision. In this work, we address the task of single frame super resolution with the presence of image degradation, e.g., blur, haze, or rain streaks. Due to the limitations of frame capturing and formation processes, image degradation is inevitable, and the artifacts would be exacerbated by super resolution methods. To address this problem, we propose a dual-branch convolutional neural network to extract base features and recovered features separately. The base features contain local and global information of the input image. On the other hand, the recovered features focus on the degraded regions and are used to remove the degradation. Those features are then fused through a recursive gate module to obtain sharp features for super resolution. By decomposing the feature extraction step into two task-independent streams, the dual-branch model can facilitate the training process by avoiding learning the mixed degradation all-in-one and thus enhance the final high-resolution prediction results. We evaluate the proposed method in three degradation scenarios. Experiments on these scenarios demonstrate that the proposed method performs more efficiently and favorably against the state-of-the-art approaches on benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：单张超分辨率的目标，以提高图像质量相对于空间的内容，这是计算机视觉的基本任务。在这项工作中，我们要解决单帧超解像与图像质量下降，例如，模糊，霾，雨或条纹的存在的任务。由于帧捕捉和形成工艺的局限性，图像劣化是不可避免的，工件将由超分辨率方法而加剧。为了解决这个问题，我们提出了一个双分支卷积神经网络来提取基地的功能和单独回收功能。该基地的功能将包含输入图像的局部和全局的信息。在另一方面，回收的特征着眼于退化区域和用于除去的降解。那些特征，然后通过递归栅极模块融合以获得超分辨率尖锐特征。通过分解特征提取步骤分为两个任务无关的流，双分支模型可以方便通过避免学习的混合降解的所有功能于一身，从而提高最终的高分辨率的预测结果的训练过程。我们评估在3分劣化的情况所提出的方法。在这些情况下的实验表明，所提出的方法进行更有效和更有利地抵靠状态的最先进的上基准数据集的方法。</font>
</div>


<hr>
<div id="paper11"> <b>11. Instance Separation Emerges from Inpainting</b>  <a href="https://arxiv.org/pdf/2003.00891" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wolf%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steffen Wolf</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hamprecht%2C+F+A" target="_blank" rel="noopener" style="color:#0000EE;">Fred A. Hamprecht</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Funke%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Funke</a><br>
<font size="3">
Abstract: Deep neural networks trained to inpaint partially occluded images show a deep understanding of image composition and have even been shown to remove objects from images convincingly. In this work, we investigate how this implicit knowledge of image composition can be leveraged for fully self-supervised instance separation. We propose a measure for the independence of two image regions given a fully self-supervised inpainting network and separate objects by maximizing this independence. We evaluate our method on two microscopy image datasets and show that it reaches similar segmentation performance to fully supervised methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：训练部分补绘深层神经网络遮挡图像显示的图像组成的深刻理解和甚至表现出令人信服地从图像中删除对象。在这项工作中，我们研究如何图像组成的这个隐性知识可以利用的完全自我监督的情况下分离。我们提出给予完全的两个图像区域的独立性衡量自我监督最大化这种独立性去水印网络和独立的对象。我们评估我们在两个显微镜图像数据集的方法和显示它达到类似的分割性能全面监督的方法。</font>
</div>


<hr>
<div id="paper12"> <b>12. 3D Object Detection From LiDAR Data Using Distance Depended Feature  Extraction</b>  <a href="https://arxiv.org/pdf/2003.00888" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Engels%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guus Engels</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Aranjuelo%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nerea Aranjuelo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arganda-Carreras%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ignacio Arganda-Carreras</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nieto%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marcos Nieto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Otaegui%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oihana Otaegui</a><br>
<font size="3">
Abstract: This paper presents a new approach to 3D object detection that leverages the properties of the data obtained by a LiDAR sensor. State-of-the-art detectors use neural network architectures based on assumptions valid for camera images. However, point clouds obtained from LiDAR are fundamentally different. Most detectors use shared filter kernels to extract features which do not take into account the range dependent nature of the point cloud features. To show this, different detectors are trained on two splits of the KITTI dataset: close range (points up to 25 meters from LiDAR) and long-range. Top view images are generated from point clouds as input for the networks. Combined results outperform the baseline network trained on the full dataset with a single backbone. Additional research compares the effect of using different input features when converting the point cloud to image. The results indicate that the network focuses on the shape and structure of the objects, rather than exact values of the input. This work proposes an improvement for 3D object detectors by taking into account the properties of LiDAR point clouds over distance. Results show that training separate networks for close-range and long-range objects boosts performance for all KITTI benchmark difficulties. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文呈现给立体物检测的一种新方法，它利用由激光雷达传感器获得的数据的属性。国家的最先进的检测器使用基于假设有效的摄像机图像的神经网络结构。然而，从激光雷达得到的点云是根本不同的。大多数检测器使用共享滤波器内核于不考虑的点群的特征的范围内依赖性质提取特征。为了证明这一点，不同的探测器在KITTI数据集的两个裂口的培训：近距离（点最多从激光雷达25米）和远射。俯视图图像是从点云生成作为用于网络的输入。合并后的结果优于训练有素的完整数据集与单个骨干基线网络。另外的研究比较了点云转换为图像时，使用不同的输入特性的效果。结果表明，该网络的重点是物体的形状和结构，而不是输入的精确值。这项工作提出了通过考虑随着距离的LiDAR点云的属性为3D对象检测器的改进。结果表明，培养独立的网络进行近距离和远距离对象的性能提升为所有KITTI基准困难。</font>
</div>


<hr>
<div id="paper13"> <b>13. Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color  Space</b>  <a href="https://arxiv.org/pdf/2003.00883" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pestana%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Camilo Pestana</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Akhtar%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naveed Akhtar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Glance%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Glance</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ajmal Mian</a><br>
<font size="3">
Abstract: Deep learning offers state of the art solutions for image recognition. However, deep models are vulnerable to adversarial perturbations in images that are subtle but significantly change the model's prediction. In a white-box attack, these perturbations are generally learned for deep models that operate on RGB images and, hence, the perturbations are equally distributed in the RGB color space. In this paper, we show that the adversarial perturbations prevail in the Y-channel of the YCbCr space. Our finding is motivated from the fact that the human vision and deep models are more responsive to shape and texture rather than color. Based on our finding, we propose a defense against adversarial images. Our defence, coined ResUpNet, removes perturbations only from the Y-channel by exploiting ResNet features in an upsampling framework without the need for a bottleneck. At the final stage, the untouched CbCr-channels are combined with the refined Y-channel to restore the clean image. Note that ResUpNet is model agnostic as it does not modify the DNN structure. ResUpNet is trained end-to-end in Pytorch and the results are compared to existing defence techniques in the input transformation category. Our results show that our approach achieves the best balance between defence against adversarial attacks such as FGSM, PGD and DDN and maintaining the original accuracies of VGG-16, ResNet50 and DenseNet121 on clean images. We perform another experiment to show that learning adversarial perturbations only for the Y-channel results in higher fooling rates for the same perturbation magnitude. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图像识别的技术解决方案的深度学习报价状态。然而，深模型是在那些微妙的，但显著变化模型的预测图像容易受到敌对扰动。在白盒攻击，这些扰动一般都学了，关于RGB图像，因此，扰动均匀分布在RGB色彩空间中操作深模型。在本文中，我们证明了对抗扰动在YCbCr空间的Y通道为准。我们的发现是从一个事实，即人类视觉和深模型更加适应的形状和质地，而不是颜色的动机。根据我们的发现，我们提出反对对抗图像的防御。我们的防守，创造ResUpNet，只能从Y通道通过利用RESNET消除干扰功能在采样架构，而不需要一个瓶颈。在最后阶段，未触摸CBCR-通道与精制Y通道结合以恢复清洁的图像。需要注意的是ResUpNet是因为它不修改DNN结构模型无关。 ResUpNet训练端至端Pytorch和结果相比，在输入变换类现有的防御技术。我们的研究结果表明，我们的方法实现了对敌对攻击，如FGSM，PGD和DDN防御和维护清洁图像VGG-16，ResNet50和DenseNet121原有精度之间的最佳平衡。我们进行另一项实验中，只显示了更高的愚弄率Y通道结果相同的扰动幅度，学习对抗扰动。</font>
</div>


<hr>
<div id="paper14"> <b>14. The perceptual boost of visual attention is task-dependent in  naturalistic settings</b>  <a href="https://arxiv.org/pdf/2003.00882" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+F+B" target="_blank" rel="noopener" style="color:#0000EE;">Freddie Bickford Smith</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoliang Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roads%2C+B+D" target="_blank" rel="noopener" style="color:#0000EE;">Brett D. Roads</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Love%2C+B+C" target="_blank" rel="noopener" style="color:#0000EE;">Bradley C. Love</a><br>
<font size="3">
Abstract: Attentional modulation of neural representations is known to enhance processing of task-relevant visual information. Is the resulting perceptual boost task-dependent in naturalistic settings? We aim to answer this with a large-scale computational experiment. First we design a series of visual tasks, each consisting of classifying images from a particular task set (group of image categories). The nature of a given task is determined by which categories are included in the task set. Then on each task we compare the accuracy of an attention-augmented neural network to that of an attention-free counterpart. We show that, all else being equal, the performance impact of attention is stronger with increasing task-set difficulty, weaker with increasing task-set size, and weaker with increasing perceptual similarity within a task set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经表征的注意瞬调制已知能够提高的任务相关的视觉信息处理。是导致感性升压任务依赖于自然的设置？我们的目标是与大规模的计算实验来回答这个问题。首先，我们设计了一系列的视觉任务，每个由分类图像从特定的任务组（图像类别的组）。通过该类别包括在设定的任务是确定的给定任务的性质。然后在每个任务中，我们的注意力，增强神经网络的精度比较，一个免费的注意力对应的。我们表明，在其他条件相同，人们关注的性能的影响是随着任务难度集，随着任务集大小弱更强，与任务组内增加感知相似弱。</font>
</div>


<hr>
<div id="paper15"> <b>15. Introducing Fuzzy Layers for Deep Learning</b>  <a href="https://arxiv.org/pdf/2003.00880" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Price%2C+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Stanton R. Price</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Price%2C+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Steven R. Price</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+D+T" target="_blank" rel="noopener" style="color:#0000EE;">Derek T. Anderson</a><br>
<font size="3">
Abstract: Many state-of-the-art technologies developed in recent years have been influenced by machine learning to some extent. Most popular at the time of this writing are artificial intelligence methodologies that fall under the umbrella of deep learning. Deep learning has been shown across many applications to be extremely powerful and capable of handling problems that possess great complexity and difficulty. In this work, we introduce a new layer to deep learning: the fuzzy layer. Traditionally, the network architecture of neural networks is composed of an input layer, some combination of hidden layers, and an output layer. We propose the introduction of fuzzy layers into the deep learning architecture to exploit the powerful aggregation properties expressed through fuzzy methodologies, such as the Choquet and Sugueno fuzzy integrals. To date, fuzzy approaches taken to deep learning have been through the application of various fusion strategies at the decision level to aggregate outputs from state-of-the-art pre-trained models, e.g., AlexNet, VGG16, GoogLeNet, Inception-v3, ResNet-18, etc. While these strategies have been shown to improve accuracy performance for image classification tasks, none have explored the use of fuzzified intermediate, or hidden, layers. Herein, we present a new deep learning strategy that incorporates fuzzy strategies into the deep learning architecture focused on the application of semantic segmentation using per-pixel classification. Experiments are conducted on a benchmark data set as well as a data set collected via an unmanned aerial system at a U.S. Army test site for the task of automatic road segmentation, and preliminary results are promising. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来开发了许多国家的先进技术已经被机器学习的影响在一定程度上。最流行的在写这篇文章的时候是深度学习的保护伞下下降的人工智能方法。深度学习已经在多种应用中证明是非常强大，能够处理具有很大的复杂性和难度的问题。在这项工作中，我们引入了一个新的层，深度学习：模糊层。传统上，神经网络的网络体系结构由一个输入层，隐藏层的一些组合，以及输出层构成。我们建议引入模糊层的进深学习结构利用通过模糊的方法，如Choquet模糊和Sugueno模糊积分表达了强大的聚集性质。迄今为止，采取深度学习模糊方法已经通过各种融合策略应用在决策层面，从国家的最先进的预先训练模型总产出，例如，AlexNet，VGG16，GoogLeNet，启-V3， RESNET-18，等。虽然这些策略已被证实可以改善用于图像分类任务的精度性能，没有已经探索了使用模糊化的中间体，或隐藏，层。在此，我们提出合并模糊战略进入深学习架构专注于语义分割的使用每像素分类中的应用新的深度学习策略。实验是在基准数据集以及在美国陆军试验场进行自动分割道路的任务，通过无人机系统所收集的数据集进行，初步结果是有希望的。</font>
</div>


<hr>
<div id="paper16"> <b>16. Estimating a Null Model of Scientific Image Reuse to Support Research  Integrity Investigations</b>  <a href="https://arxiv.org/pdf/2003.00878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Acuna%2C+D+E" target="_blank" rel="noopener" style="color:#0000EE;">Daniel E. Acuna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziyue Xiang</a><br>
<font size="3">
Abstract: When there is a suspicious figure reuse case in science, research integrity investigators often find it difficult to rebut authors claiming that "it happened by chance". In other words, when there is a "collision" of image features, it is difficult to justify whether it appears rarely or not. In this article, we provide a method to predict the rarity of an image feature by statistically estimating the chance of it randomly occurring across all scientific imagery. Our method is based on high-dimensional density estimation of ORB features using 7+ million images in the PubMed Open Access Subset dataset. We show that this method can lead to meaningful feedback during research integrity investigations by providing a null hypothesis for scientific image reuse and thus a p-value during deliberations. We apply the model to a sample of increasingly complex imagery and confirm that it produces decreasingly smaller p-values as expected. We discuss applications to research integrity investigations as well as future work. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当有科学可疑人物重用的情况下，科研诚信调查发现很难反驳的作者声称“它的发生是偶然”。换句话说，当有图像特征的“碰撞”，这是很难证明它是否出现很少或没有。在这篇文章中，我们提供的统计估计在所有科学影像它随机发生的可能性进行预测的图像特征的稀有性的方法。我们的方法是基于ORB的高维密度估计功能使用的考研开放存取子集数据集7+万张图片。我们表明，这种方法可以通过提供科学图像重复使用，因而审议中的p值的零假设在科研诚信调查导致有意义的反馈。我们的模型适用于日益复杂的图像和确认的样品，它预期会产生递减的p值较小。我们讨论的应用研究诚信调查以及今后的工作。</font>
</div>


<hr>
<div id="paper17"> <b>17. A Multi-view Perspective of Self-supervised Learning</b>  <a href="https://arxiv.org/pdf/2003.00877" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuanxing Geng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenghao Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Songcan Chen</a><br>
<font size="3">
Abstract: As a newly emerging unsupervised learning paradigm, self-supervised learning (SSL) recently gained widespread attention, which usually introduces a pretext task without manual annotation of data. With its help, SSL effectively learns the feature representation beneficial for downstream tasks. Thus the pretext task plays a key role. However, the study of its design, especially its essence currently is still open. In this paper, we borrow a multi-view perspective to decouple a class of popular pretext tasks into a combination of view data augmentation (VDA) and view label classification (VLC), where we attempt to explore the essence of such pretext task while providing some insights into its design. Specifically, a simple multi-view learning framework is specially designed (SSL-MV), which assists the feature learning of downstream tasks (original view) through the same tasks on the augmented views. SSL-MV focuses on VDA while abandons VLC, empirically uncovering that it is VDA rather than generally considered VLC that dominates the performance of such SSL. Additionally, thanks to replacing VLC with VDA tasks, SSL-MV also enables an integrated inference combining the predictions from the augmented views, further improving the performance. Experiments on several benchmark datasets demonstrate its advantages. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：作为一种新兴的无监督学习的范式，自我监督学习（SSL）最近获得了广泛的关注，这通常不引入数据的人工注释的借口任务。有了它的帮助，SSL有效学习的特征表现为下游的任务是有益的。因此为借口任务起着关键的作用。然而，它的设计的研究，特别是它的本质目前仍处于打开状态。在本文中，我们借用一个多视图的角度来解耦一类流行的借口，任务到哪里，我们试图探讨这样的借口任务的精髓，同时提供视图数据增强（VDA）和查看标签分类（VLC）的组合更深入地了解它的设计。具体地，一个简单的多视图学习框架是专门设计（SSL-MV），这有助于下游任务（原始视图）通过对增强视图中的相同任务的特征的学习。 SSL-MV的重点，而VDA退让VLC，经验揭示，这是VDA，而不是通常认为VLC支配这种SSL的性能。此外，由于与VDA任务更换VLC，SSL-MV也使综合推理结合从增强的观点预测，进一步提高性能。在几个基准数据集实验证明它的优点。</font>
</div>


<hr>
<div id="paper18"> <b>18. Predicting TUG score from gait characteristics based on video analysis  and machine learning</b>  <a href="https://arxiv.org/pdf/2003.00875" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Ma</a><br>
<font size="3">
Abstract: Fall is a leading cause of death which suffers the elderly and society. Timed Up and Go (TUG) test is a common tool for fall risk assessment. In this paper, we propose a method for predicting TUG score from gait characteristics extracted from video based on computer vision and machine learning technologies. First, 3D pose is estimated from video captured with 2D and 3D cameras during human motion and then a group of gait characteristics are computed from 3D pose series. After that, copula entropy is used to select those characteristics which are mostly associated with TUG score. Finally, the selected characteristics are fed into the predictive models to predict TUG score. Experiments on real world data demonstrated the effectiveness of the proposed method. As a byproduct, the associations between TUG score and several gait characteristics are discovered, which laid the scientific foundation of the proposed method and make the predictive models such built interpretable to clinical users. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：秋天是死亡的主要原因，其遭受的老人和社会。计时起立行走（TUG）测试是秋季的风险评估的常用工具。在本文中，我们提出了预测基于计算机视觉和机器学习技术，从视频中提取的步态特征TUG得分的方法。首先，三维姿态从人体运动，然后一组步态特征从三维姿态系列计算期间与2D和3D照相机拍摄的视频进行估计。在这之后，连接函数熵用于选择基本都是用TUG得分相关联的那些特性。最后，选择的特性被送入预测模型来预测TUG得分。对现实世界的数据的实验验证了该方法的有效性。作为一个副产品，TUG得分和几个步态特征之间的关联被发现，从而奠定了该方法的科学依据，使预测模型建立等可解释临床用户。</font>
</div>


<hr>
<div id="paper19"> <b>19. Few-shot Learning with Weakly-supervised Object Localization</b>  <a href="https://arxiv.org/pdf/2003.00874" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinfu Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojian He</a><br>
<font size="3">
Abstract: Few-shot learning (FSL) aims to learn novel visual categories from very few samples, which is a challenging problem in real-world applications. Many data generation methods have improved the performance of FSL models, but require lots of annotated images to train a specialized network (e.g., GAN) dedicated to hallucinate new samples. We argue that localization is a more efficient approach because it provides the most discriminative regions without using extra samples. In this paper, we propose a novel method to address the FSL task by achieving weakly-supervised object localization within performing few-shot classification. To this end, we design (i) a triplet-input module to obtain the initial object seeds and (ii) an Image-To-Class-Distance (ITCD) based localizer to activate the deep descriptors of the key objects, thus obtaining the more discriminative representations used to perform few-shot classification. Extensive experiments show our method outperforms the state-of-the-art methods on benchmark datasets under various settings. Besides, our method achieves superior performance over previous methods when training the model on miniImageNet and evaluating it on the different datasets (e.g., Stanford Dogs), demonstrating its superior generalization capacity. Extra visualization shows the proposed method can localize the key objects accurately. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：很少次学习（FSL）的目的是从很少的样本，这是在现实应用中一个具有挑战性的问题，学习新的视觉类。许多数据生成方法提高了FSL车型的性能，但需要大量注释的图像的培养，致力于幻觉新样本专门的网络（例如，GAN）。我们认为，本土化是一个更有效的方法，因为它提供了最鉴别的区域，而无需使用额外的样品。在本文中，我们提出通过实现内进行几拍分类弱监督的对象定位于解决FSL任务的新方法。为此，我们设计（ⅰ）的三重态 - 输入模块，以获得初始目标种子，和（ii）一个图像 - 类距离（ITCD）基于定位以激活键的对象的深描述符，从而获得更有辨别力表示用于执行为数不多的镜头分类。大量的实验证明我们的方法优于对下各种设置基准数据集的国家的最先进的方法。此外，我们的方法训练的时候就miniImageNet模型以及对不同的数据集（例如，斯坦福大学的狗）评价它，展示了其卓越的泛化能力达到了以前的方法优越的性能。额外的可视化示出所提出的方法可以精确定位的关键对象。</font>
</div>


<hr>
<div id="paper20"> <b>20. AlignSeg: Feature-Aligned Segmentation Networks</b>  <a href="https://arxiv.org/pdf/2003.00872" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zilong Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunchao Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinggang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Honghui Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenyu Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+T+S" target="_blank" rel="noopener" style="color:#0000EE;">Thomas S. Huang</a><br>
<font size="3">
Abstract: Aggregating features in terms of different convolutional blocks or contextual embeddings has been proven to be an effective way to strengthen feature representations for semantic segmentation. However, most of the current popular network architectures tend to ignore the misalignment issues during the feature aggregation process caused by 1) step-by-step downsampling operations, and 2) indiscriminate contextual information fusion. In this paper, we explore the principles in addressing such feature misalignment issues and inventively propose Feature-Aligned Segmentation Networks (AlignSeg). AlignSeg consists of two primary modules, i.e., the Aligned Feature Aggregation (AlignFA) module and the Aligned Context Modeling (AlignCM) module. First, AlignFA adopts a simple learnable interpolation strategy to learn transformation offsets of pixels, which can effectively relieve the feature misalignment issue caused by multiresolution feature aggregation. Second, with the contextual embeddings in hand, AlignCM enables each pixel to choose private custom contextual information in an adaptive manner, making the contextual embeddings aligned better to provide appropriate guidance. We validate the effectiveness of our AlignSeg network with extensive experiments on Cityscapes and ADE20K, achieving new state-of-the-art mIoU scores of 82.6% and 45.95%, respectively. Our source code will be made available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在不同的卷积块或上下文的嵌入方面汇总的功能已经被证明是加强对语义分割特征表示的有效途径。然而，大多数当前流行的网络体系结构的倾向于忽略期​​间由1特征聚集过程的错位的问题）一步一步下采样操作，以及2）不加选择的上下文信息融合。在本文中，我们在处理这些功能错位问题探索的原则，创造性地提出了功能对齐分割网络（AlignSeg）。 AlignSeg包括两个主要模块，即，对齐特征聚合（AlignFA）模块和对齐上下文建模（AlignCM）模块。首先，AlignFA采用一个简单的可以学习插值战略学会像素的改造偏移，可有效缓解由多分辨率功能的聚集功能错位问题。其次，在手上下文的嵌入，AlignCM使每个像素可以选择以自适应的方式私人定制上下文信息，从而更好地对齐，以提供适当的指导上下文的嵌入。我们验证与城市景观和ADE20K广泛的实验，实现了分别为82.6％和45.95％，新的国家的最先进的米欧分数我们AlignSeg网络的有效性。我们的源代码将提供。</font>
</div>


<hr>
<div id="paper21"> <b>21. Learning Texture Invariant Representation for Domain Adaptation of  Semantic Segmentation</b>  <a href="https://arxiv.org/pdf/2003.00867" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Myeongjin Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Byun%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyeran Byun</a><br>
<font size="3">
Abstract: Since annotating pixel-level labels for semantic segmentation is laborious, leveraging synthetic data is an attractive solution. However, due to the domain gap between synthetic domain and real domain, it is challenging for a model trained with synthetic data to generalize to real data. In this paper, considering the fundamental difference between the two domains as the texture, we propose a method to adapt to the texture of the target domain. First, we diversity the texture of synthetic images using a style transfer algorithm. The various textures of generated images prevent a segmentation model from overfitting to one specific (synthetic) texture. Then, we fine-tune the model with self-training to get direct supervision of the target texture. Our results achieve state-of-the-art performance and we analyze the properties of the model trained on the stylized dataset with extensive experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由于标注像素级标签语义分割是费力的，利用合成数据是一个有吸引力的解决方案。然而，由于合成结构域和实际结构域之间的结构域间隙，它被用于与合成数据训练的模型的挑战推广到实际数据。在本文中，考虑到两个域的纹理之间的根本区别，我们提出了一个适应目标域的纹理的方法。首先，我们利用多元化风格转换算法合成图像​​的纹理。生成的图像的各种纹理防止分割模型过度拟合到一个特定的（合成的）纹理。然后，我们微调与自我培养模式，以获得目标质地的直接监督。我们的研究结果达到国家的最先进的性能和我们分析的培训与广泛的实验程式化数据集模型的属性。</font>
</div>


<hr>
<div id="paper22"> <b>22. Exposing Backdoors in Robust Machine Learning Models</b>  <a href="https://arxiv.org/pdf/2003.00865" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Soremekun%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ezekiel Soremekun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Udeshi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sakshi Udeshi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chattopadhyay%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sudipta Chattopadhyay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeller%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andreas Zeller</a><br>
<font size="3">
Abstract: The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we demonstrate that adversarially robust models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this is leveraged to detect backdoor-infected models. Specifically, we use feature clustering to effectively detect backdoor-infected robust Deep Neural Networks (DNNs). In our evaluation of major classification tasks, our approach effectively detects robust DNNs infected with backdoors. Our investigation reveals that salient features of adversarially robust DNNs break the stealthy nature of backdoor attacks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍的稳健优化推动了国家的最先进的防御敌对攻击。然而，这种优化的行为还没有被研究了一个根本不同类的攻击称为后门的光。在本文中，我们证明了adversarially可靠的模型很容易受到后门攻击。随后，我们观察到后门反映在这些模型的特征表示。然后，将其利用，以检测后门感染模型。具体而言，我们使用特征聚类有效地检测后门程序感染的强劲深层神经网络（DNNs）。在我们的主要任务分类评价，我们的方法有效地检测感染了后门强劲DNNs。我们的调查显示adversarially稳健DNNs的那几个突出的特点打破后门攻击的隐蔽性质。</font>
</div>


<hr>
<div id="paper23"> <b>23. Triangle-Net: Towards Robustness in Point Cloud Classification</b>  <a href="https://arxiv.org/pdf/2003.00856" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenxi Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wachs%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juan Wachs</a><br>
<font size="3">
Abstract: 3D object recognition is becoming a key desired capability for many computer vision systems such as autonomous vehicles, service robots and surveillance drones to operate more effectively in unstructured environments. These real-time systems require effective classification methods that are robust to sampling resolution, measurement noise, and pose configuration of the objects. Previous research has shown that sparsity, rotation and positional variance of points can lead to a significant drop in the performance of point cloud based classification techniques. In this regard, we propose a novel approach for 3D classification that takes sparse point clouds as input and learns a model that is robust to rotational and positional variance as well as point sparsity. To this end, we introduce new feature descriptors which are fed as an input to our proposed neural network in order to learn a robust latent representation of the 3D object. We show that such latent representations can significantly improve the performance of object classification and retrieval. Further, we show that our approach outperforms PointNet and 3DmFV by 34.4% and 27.4% respectively in classification tasks using sparse point clouds of only 16 points under arbitrary SO(3) rotation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：3D物体识别逐渐成为许多计算机视觉系统，如自动驾驶汽车，服务机器人和侦察机在非结构化环境中更有效地运作的关键所需的能力。这些实时系统需要有效的分类方法是稳健的采样分辨率，测量噪声，和姿势的对象的配置。以前的研究已经表明，点稀疏，旋转和位置变化可能导致基于点云分类技术性能有显著下降。在这方面，我们提出了3D分类该需要稀疏点云作为输入和学习的模型是鲁棒的旋转和位置变化以及点稀疏性的新方法。为此，我们介绍这是为了学习3D对象的一个​​强大的潜在表示供给作为输入到我们提出的神经网络新功能描述。我们发现，这些潜在的表示可以显著提高对象分类和检索的性能。此外，我们证明了我们的方法了34.4％和27.4％，分别使用在任意SO（3）旋转的只有16点稀疏的点云优于PointNet和3DmFV在分类任务。</font>
</div>


<hr>
<div id="paper24"> <b>24. Deep Learning on Radar Centric 3D Object Detection</b>  <a href="https://arxiv.org/pdf/2003.00851" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seungjun Lee</a><br>
<font size="3">
Abstract: Even though many existing 3D object detection algorithms rely mostly on camera and LiDAR, camera and LiDAR are prone to be affected by harsh weather and lighting conditions. On the other hand, radar is resistant to such conditions. However, research has found only recently to apply deep neural networks on radar data. In this paper, we introduce a deep learning approach to 3D object detection with radar only. To the best of our knowledge, we are the first ones to demonstrate a deep learning-based 3D object detection model with radar only that was trained on the public radar dataset. To overcome the lack of radar labeled data, we propose a novel way of making use of abundant LiDAR data by transforming it into radar-like point cloud data and aggressive radar augmentation techniques. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管许多现有的三维物体检测算法主要依靠相机和激光雷达，摄像机和激光雷达是容易受到恶劣天气和照明条件的影响。在另一方面，雷达是这样的条件有抗性。然而，研究发现，最近才在雷达数据应用深层神经网络。在本文中，我们介绍了与单独雷达深刻的学习方法，以立体物检测。据我们所知，我们是第一批来证明与单独雷达这是对公众的雷达数据集训练了深刻的学习型立体物检测模型。为了克服缺少雷达标记的数据，我们建议将其转化为类似雷达点云数据和积极的雷达增强技术，利用丰富的LiDAR数据的一种新方法。</font>
</div>


<hr>
<div id="paper25"> <b>25. Learning to Deblur and Generate High Frame Rate Video with an Event  Camera</b>  <a href="https://arxiv.org/pdf/2003.00847" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Haoyu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Haoyu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Minggui%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Teng Minggui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Boxin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shi Boxin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=YIzhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wang YIzhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tiejun%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huang Tiejun</a><br>
<font size="3">
Abstract: Event cameras are bio-inspired cameras which can measure the change of intensity asynchronously with high temporal resolution. One of the event cameras' advantages is that they do not suffer from motion blur when recording high-speed scenes. In this paper, we formulate the deblurring task on traditional cameras directed by events to be a residual learning one, and we propose corresponding network architectures for effective learning of deblurring and high frame rate video generation tasks. We first train a modified U-Net network to restore a sharp image from a blurry image using corresponding events. Then we train another similar network with different downsampling blocks to generate high frame rate video using the restored sharp image and events. Experiment results show that our method can restore sharper images and videos than state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事件相机仿生相机能够以高时间分辨率异步测量强度的变化。其中一个的情况下，相机的优点是记录高速场景时，他们不从运动模糊苦。在本文中，我们制定的事件，定向为剩余学习一个传统相机的去模糊任务，并提出了相应的网络架构的去模糊和高帧率视频生成任务的有效的学习。首先，我们培养了变形的U-Net网络来使用相应的事件模糊图像恢复清晰的图像。然后，我们培养具有不同下采样块另一个类似的网络使用恢复的清晰图像和事件来生成高帧率视频。实验结果表明，该方法能比国家的最先进的方法可以还原更清晰的图像和视频。</font>
</div>


<hr>
<div id="paper26"> <b>26. FPGA Implementation of Minimum Mean Brightness Error Bi-Histogram  Equalization</b>  <a href="https://arxiv.org/pdf/2003.00840" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saroha%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhishek Saroha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rakesh%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avichal Rakesh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tripathi%2C+R+K" target="_blank" rel="noopener" style="color:#0000EE;">Rajiv Kumar Tripathi</a><br>
<font size="3">
Abstract: Histogram Equalization (HE) is a popular method for contrast enhancement. Generally, mean brightness is not conserved in Histogram Equalization. Initially, Bi-Histogram Equalization (BBHE) was proposed to enhance contrast while maintaining a the mean brightness. However, when mean brightness is primary concern, Minimum Mean Brightness Error Bi-Histogram Equalization (MMBEBHE) is the best technique. There are several implementations of Histogram Equalization on FPGA, however to our knowledge MMBEBHE has not been implemented on FPGAs before. Therefore, we present an implementation of MMBEBHE on FPGA. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：直方图均衡（HE）是用于对比度增强的常用方法。一般情况下，平均亮度不直方图均衡保守。最初，双直方图均衡（BBHE）提出了以提高对比度，同时保持平均亮度。然而，当平均亮度是主要关心的，最小均亮度误差双直方图均衡（MMBEBHE）是最好的技术。有直方图均衡的FPGA的几种实现，但据我们所知MMBEBHE尚未在FPGA中前完成。因此，我们提出MMBEBHE对FPGA的实现。</font>
</div>


<hr>
<div id="paper27"> <b>27. A Machine Learning Framework for Data Ingestion in Document Images</b>  <a href="https://arxiv.org/pdf/2003.00838" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunyu Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuo Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianling Sun</a><br>
<font size="3">
Abstract: Paper documents are widely used as an irreplaceable channel of information in many fields, especially in financial industry, fostering a great amount of demand for systems which can convert document images into structured data representations. In this paper, we present a machine learning framework for data ingestion in document images, which processes the images uploaded by users and return fine-grained data in JSON format. Details of model architectures, design strategies, distinctions with existing solutions and lessons learned during development are elaborated. We conduct abundant experiments on both synthetic and real-world data in State Street. The experimental results indicate the effectiveness and efficiency of our methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：纸质文件被广泛用作信息在许多领域具有不可替代的渠道，尤其是在金融行业，促进需求的巨大量能的文档图像转换成结构化的数据表示系统。在本文中，我们提出了在文档图像数据摄取，处理由用户和返回细粒度的数据以JSON格式上传影像机器学习框架。模型架构，设计策略，与现有的解决方案和开发过程中的经验教训区别的详细阐述。我们开展对州街合成和真实世界的数据丰富的实验。实验结果表明我们的方法的有效性和效率。</font>
</div>


<hr>
<div id="paper28"> <b>28. On Parameter Tuning in Meta-learning for Computer Vision</b>  <a href="https://arxiv.org/pdf/2003.00837" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohammadi%2C+F+G" target="_blank" rel="noopener" style="color:#0000EE;">Farid Ghareh Mohammadi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Amini%2C+M+H" target="_blank" rel="noopener" style="color:#0000EE;">M. Hadi Amini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arabnia%2C+H+R" target="_blank" rel="noopener" style="color:#0000EE;">Hamid R. Arabnia</a><br>
<font size="3">
Abstract: Learning to learn plays a pivotal role in meta-learning (MTL) to obtain an optimal learning model. In this paper, we investigate mage recognition for unseen categories of a given dataset with limited training information. We deploy a zero-shot learning (ZSL) algorithm to achieve this goal. We also explore the effect of parameter tuning on performance of semantic auto-encoder (SAE). We further address the parameter tuning problem for meta-learning, especially focusing on zero-shot learning. By combining different embedded parameters, we improved the accuracy of tuned-SAE. Advantages and disadvantages of parameter tuning and its application in image classification are also explored. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：学会学习戏剧荟萃学习（MTL）了举足轻重的作用，以获得最佳的学习模式。在本文中，我们探讨看不见的类别与有限的培训信息给定数据集的法师认可。我们部署了一个零次学习（ZSL）算法来实现这一目标。我们还探讨了参数调整的语义自动编码器（SAE）的性能的影响。我们进一步地址元学习的参数调整问题，特别是侧重于零射门学习。通过组合不同的嵌入式参数，我们改进调谐-SAE的准确性。优点和参数整定及其在图像分类应用的优缺点进行了探讨。</font>
</div>


<hr>
<div id="paper29"> <b>29. Marine life through You Only Look Once's perspective</b>  <a href="https://arxiv.org/pdf/2003.00836" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Stavelin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Herman Stavelin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rasheed%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adil Rasheed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=San%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Omer San</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hestnes%2C+A+J" target="_blank" rel="noopener" style="color:#0000EE;">Arne Johan Hestnes</a><br>
<font size="3">
Abstract: With the rise of focus on man made changes to our planet and wildlife therein, more and more emphasis is put on sustainable and responsible gathering of resources. In an effort to preserve maritime wildlife the Norwegian government has decided that it is necessary to create an overview over the presence and abundance of various species of wildlife in the Norwegian fjords and oceans. In this paper we apply and analyze an object detection scheme that detects fish in camera images. The data is sampled from a submerged data station at Fulehuk in Norway. We implement You Only Look Once (YOLO) version 3 and create a dataset consisting of 99,961 images with a mAP of $\sim 0.88$. We also investigate intermediate results within YOLO, gaining insight into how it performs object detection. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：重点对人为改变我们的星球和野生动物在其中的兴起，越来越多的重点放在资源的可持续和负责任的聚会。在努力维护海洋野生动物挪威政府已决定，有必要建立在存在和野生动物的不同种类的丰富在挪威峡湾和海洋的概述。在本文中，我们应用和分析检测相机图像鱼对象检测方案。数据是从在挪威在Fulehuk浸没数据站采样。我们实施仅看一次3（永乐）版本，并创建一个由99961个图像与$ \卡$ 0.88地图的数据集。我们也调查中YOLO中间结果，深入了解它是如何进行目标检测。</font>
</div>


<hr>
<div id="paper30"> <b>30. Deep Variational Luenberger-type Observer for Stochastic Video  Prediction</b>  <a href="https://arxiv.org/pdf/2003.00835" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dong Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guang Yao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zongxuan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wennan Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cewu Lu</a><br>
<font size="3">
Abstract: Considering the inherent stochasticity and uncertainty, predicting future video frames is exceptionally challenging. In this work, we study the problem of video prediction by combining interpretability of stochastic state space models and representation learning of deep neural networks. Our model builds upon an variational encoder which transforms the input video into a latent feature space and a Luenberger-type observer which captures the dynamic evolution of the latent features. This enables the decomposition of videos into static features and dynamics in an unsupervised manner. By deriving the stability theory of the nonlinear Luenberger-type observer, the hidden states in the feature space become insensitive with respect to the initial values, which improves the robustness of the overall model. Furthermore, the variational lower bound on the data log-likelihood can be derived to obtain the tractable posterior prediction distribution based on the variational principle. Finally, the experiments such as the Bouncing Balls dataset and the Pendulum dataset are provided to demonstrate the proposed model outperforms concurrent works. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：考虑到固有的随机性和不确定性，预测未来的视频帧是极具挑战性。在这项工作中，我们结合随机状态空间模型，并表示学习深层神经网络的解释性研究视频预测的问题。我们的模型建立在其中将输入视频转换成潜特征空间和其捕获的潜特征动态演化一个的Luenberger型观察者的变分编码器。这使得视频分为静态特征和动态，无人监督的方式分解。通过导出非线性的Luenberger型观察者的稳定性理论，在特征空间中的隐藏的状态变得不敏感相对于该初始值，这提高了整体模型的稳健性。此外，变低的数据数似然结合的可衍生以获得基于变分原理的易处理后预测分布。最后，提供数据集中表明了该模型的实验，如弹弹球的数据集与钟摆优于并行工程。</font>
</div>


<hr>
<div id="paper31"> <b>31. CALVIS: chest, waist and pelvis circumference from 3D human body meshes  as ground truth for deep learning</b>  <a href="https://arxiv.org/pdf/2003.00834" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tejeda%2C+Y+G" target="_blank" rel="noopener" style="color:#0000EE;">Yansel Gonzalez Tejeda</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mayer%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Helmut Mayer</a><br>
<font size="3">
Abstract: In this paper we present CALVIS, a method to calculate $\textbf{C}$hest, w$\textbf{A}$ist and pe$\textbf{LVIS}$ circumference from 3D human body meshes. Our motivation is to use this data as ground truth for training convolutional neural networks (CNN). Previous work had used the large scale CAESAR dataset or determined these anthropometrical measurements $\textit{manually}$ from a person or human 3D body meshes. Unfortunately, acquiring these data is a cost and time consuming endeavor. In contrast, our method can be used on 3D meshes automatically. We synthesize eight human body meshes and apply CALVIS to calculate chest, waist and pelvis circumference. We evaluate the results qualitatively and observe that the measurements can indeed be used to estimate the shape of a person. We then asses the plausibility of our approach by generating ground truth with CALVIS to train a small CNN. After having trained the network with our data, we achieve competitive validation error. Furthermore, we make the implementation of CALVIS publicly available to advance the field. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中我们本CALVIS，一种方法来计算$ \ textbf {C} $ HEST，瓦特$ \ textbf {A} $ IST和PE $ \ textbf {LVIS}从三维人体网格$圆周。我们的动机是为了用这个数据作为地面实况训练卷积神经网络（CNN）。以前的研究中使用的大型数据集凯撒或确定从一个人或人体三维人体网格，这些人体测量学的测量$ \ {textit手动} $。不幸的是，获得这些数据是一个成本和耗时的努力。相比之下，我们的方法可以在3D网格自动使用。我们合成8个个人体网格，并应用到CALVIS计算胸部，腰部和骨盆周围。我们定性评估结果，并观察测量确实可以用来估算一个人的形状。然后，我们驴我们的方法通过生成地面真相与CALVIS训练小CNN的合理性。在已经训练了与我们的数据网络，我们获得竞争验证错误。此外，我们做CALVIS实行公开可用来推动该领域。</font>
</div>


<hr>
<div id="paper32"> <b>32. CNN Hyperparameter tuning applied to Iris Liveness Detection</b>  <a href="https://arxiv.org/pdf/2003.00833" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kimura%2C+G+Y" target="_blank" rel="noopener" style="color:#0000EE;">Gabriela Y. Kimura</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lucio%2C+D+R" target="_blank" rel="noopener" style="color:#0000EE;">Diego R. Lucio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Britto%2C+A+S" target="_blank" rel="noopener" style="color:#0000EE;">Alceu S. Britto Jr.</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Menotti%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Menotti</a><br>
<font size="3">
Abstract: The iris pattern has significantly improved the biometric recognition field due to its high level of stability and uniqueness. Such physical feature has played an important role in security and other related areas. However, presentation attacks, also known as spoofing techniques, can be used to bypass the biometric system with artifacts such as printed images, artificial eyes, and textured contact lenses. To improve the security of these systems, many liveness detection methods have been proposed, and the first Internacional Iris Liveness Detection competition was launched in 2013 to evaluate their effectiveness. In this paper, we propose a hyperparameter tuning of the CASIA algorithm, submitted by the Chinese Academy of Sciences to the third competition of Iris Liveness Detection, in 2017. The modifications proposed promoted an overall improvement, with an 8.48% Attack Presentation Classification Error Rate (APCER) and 0.18% Bonafide Presentation Classification Error Rate (BPCER) for the evaluation of the combined datasets. Other threshold values were evaluated in an attempt to reduce the trade-off between the APCER and the BPCER on the evaluated datasets and worked out successfully. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虹膜图案生物统计识别领域已显著改善由于其高水平的稳定性和唯一的。这种物理特性起到了安全和其他相关领域发挥重要作用。但是，呈现的攻击，也称为欺骗技术，可用于与旁路工件诸如打印的图像，人工眼睛，纹理隐形眼镜的生物测定系统。为了提高这些系统的安全性，许多活跃度检测方法已经被提出，并在第一INTERNACIONAL虹膜活跃度检测竞赛在2013年推出，以评估其有效性。在本文中，我们提出了中科院自动化所算法的超参数调整，提出中国社科院的科学到虹膜活跃度检测的第三次比赛，在2017年的修改提出了促进全面提高，有8.48％的攻击演示分类错误率（APCER）和0.18％Bonafide演示分类错误率（BPCER）用于组合的数据集的评估。其他的阈值，以试图减少对评估数据集的APCER和BPCER之间的权衡进行了评价，并成功地摸索出。</font>
</div>


<hr>
<div id="paper33"> <b>33. An End-to-End Visual-Audio Attention Network for Emotion Recognition in  User-Generated Videos</b>  <a href="https://arxiv.org/pdf/2003.00832" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sicheng Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunsheng Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Gu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jufeng Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tengfei Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Runbo Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chai%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hua Chai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kurt Keutzer</a><br>
<font size="3">
Abstract: Emotion recognition in user-generated videos plays an important role in human-centered computing. Existing methods mainly employ traditional two-stage shallow pipeline, i.e. extracting visual and/or audio features and training classifiers. In this paper, we propose to recognize video emotions in an end-to-end manner based on convolutional neural networks (CNNs). Specifically, we develop a deep Visual-Audio Attention Network (VAANet), a novel architecture that integrates spatial, channel-wise, and temporal attentions into a visual 3D CNN and temporal attentions into an audio 2D CNN. Further, we design a special classification loss, i.e. polarity-consistent cross-entropy loss, based on the polarity-emotion hierarchy constraint to guide the attention generation. Extensive experiments conducted on the challenging VideoEmotion-8 and Ekman-6 datasets demonstrate that the proposed VAANet outperforms the state-of-the-art approaches for video emotion recognition. Our source code is released at: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在用户生成的视频情感识别在人类为中心的计算中起重要作用。现有的方法主要采用传统的两段式浅管道，即提取视觉和/或音频功能和训练分类。在本文中，我们提出了一种基于卷积神经网络（细胞神经网络）来识别一个终端到终端的方式视频情绪。具体而言，我们开发了一个深的视觉音频注意网络（VAANet），一种新型结构，集成了空间，信道的角度来看，和时间关注成可视3D CNN和时间关注到音频2D CNN。此外，我们还设计了一个特殊的分类损失，即极性一致的跨熵损失的基础上，极性情感层次的约束，引导关注的产生。在挑战VideoEmotion-8和埃克曼-6的数据集进行了广泛的实验表明，该VAANet优于状态的最先进的方法用于视频情绪识别。此HTTPS URL：我们的源代码是在释放。</font>
</div>


<hr>
<div id="paper34"> <b>34. Character Segmentation in Asian Collector's Seal Imprints: An Attempt to  Retrieval Based on Ancient Character Typeface</b>  <a href="https://arxiv.org/pdf/2003.00831" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kangying Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Batjargal%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Biligsaikhan Batjargal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Maeda%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Akira Maeda</a><br>
<font size="3">
Abstract: Collector's seals provide important clues about the ownership of a book. They contain much information pertaining to the essential elements of ancient materials and also show the details of possession, its relation to the book, the identity of the collectors and their social status and wealth, amongst others. Asian collectors have typically used artistic ancient characters rather than modern ones to make their seals. In addition to the owner's name, several other words are used to express more profound meanings. A system that automatically recognizes these characters can help enthusiasts and professionals better understand the background information of these seals. However, there is a lack of training data and labelled images, as samples of some seals are scarce and most of them are degraded images. It is necessary to find new ways to make full use of such scarce data. While these data are available online, they do not contain information on the characters'position. The goal of this research is to provide retrieval tools assist in obtaining more information from Asian collector's seals imprints without consuming a lot of computational resources. In this paper, a character segmentation method is proposed to predict the candidate characters'area without any labelled training data that contain character coordinate information. A retrieval-based recognition system that focuses on a single character is also proposed to support seal retrieval and matching. The experimental results demonstrate that the proposed character segmentation method performs well on Asian collector's seals, with 92% of the test data being correctly segmented. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：收藏家的印章提供关于一本书的所有权的重要线索。它们包含与古材料的基本要素多的信息，也显示出藏的细节，它关系到的书，收藏家的身份和他们的社会地位和财富，等等。亚洲收藏家通常用于艺术的古字，而不是现代的，以使他们的密封件。除了主人的名字，其他的几个单词来表达更深刻的含义。自动识别这些字符可以帮助爱好者和专业人士更好地了解这些密封的后台信息系统。然而，缺乏训练数据和标记的图像，如一些密封的样本很少，其中大部分是退化图像。有必要寻找新的方法来充分利用这种稀缺数据。虽然这些数据可在网上，它们不包含在characters'position信息。这项研究的目的是提供检索工具协助获得亚洲收藏家的印章印的详细信息，而无需耗费大量的计算资源。在本文中，一个字符分割方法，提出了预测候选characters'area不包含字符坐标信息的任何标记的训练数据。侧重于单个字符一个基于检索识别系统还提议支持密封检索和匹配。实验结果表明，以及对亚洲集电极的密封件所提议的字符分割方法进行，与所述测试数据的92％被正确分割。</font>
</div>


<hr>
<div id="paper35"> <b>35. GSANet: Semantic Segmentation with Global and Selective Attention</b>  <a href="https://arxiv.org/pdf/2003.00830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingfeng Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=El-Khamy%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mostafa El-Khamy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongwoon Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jungwon Lee</a><br>
<font size="3">
Abstract: This paper proposes a novel deep learning architecture for semantic segmentation. The proposed Global and Selective Attention Network (GSANet) features Atrous Spatial Pyramid Pooling (ASPP) with a novel sparsemax global attention and a novel selective attention that deploys a condensation and diffusion mechanism to aggregate the multi-scale contextual information from the extracted deep features. A selective attention decoder is also proposed to process the GSA-ASPP outputs for optimizing the softmax volume. We are the first to benchmark the performance of semantic segmentation networks with the low-complexity feature extraction network (FXN) MobileNetEdge, that is optimized for low latency on edge devices. We show that GSANet can result in more accurate segmentation with MobileNetEdge, as well as with strong FXNs, such as Xception. GSANet improves the state-of-art semantic segmentation accuracy on both the ADE20k and the Cityscapes datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种语义分割一个新的深度学习建筑。提议的全球和选择性注意网络（GSANet）功能Atrous空间金字塔池（ASPP）具有新颖sparsemax全球瞩目和部署一个凝聚和扩散机制，聚集来自提取深特征的多尺度的上下文信息的新的选择性注意。甲选择性注意解码器，还提出以处理GSA-ASPP输出用于优化SOFTMAX体积。我们是第一个到基准与低复杂度特征提取网络（FXN）MobileNetEdge，即对于上边缘设备低延迟优化语义分割网络的性能。我们表明，GSANet可能导致更准确的分割与MobileNetEdge，以及具有较强FXNs，如Xception。 GSANet提高两者上ADE20k和都市风景数据集的状态的最先进的语义分割精度。</font>
</div>


<hr>
<div id="paper36"> <b>36. Verifying Deep Learning-based Decisions for Facial Expression  Recognition</b>  <a href="https://arxiv.org/pdf/2003.00828" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rieger%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ines Rieger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kollmann%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rene Kollmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Finzel%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bettina Finzel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Seuss%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dominik Seuss</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schmid%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Ute Schmid</a><br>
<font size="3">
Abstract: Neural networks with high performance can still be biased towards non-relevant features. However, reliability and robustness is especially important for high-risk fields such as clinical pain treatment. We therefore propose a verification pipeline, which consists of three steps. First, we classify facial expressions with a neural network. Next, we apply layer-wise relevance propagation to create pixel-based explanations. Finally, we quantify these visual explanations based on a bounding-box method with respect to facial regions. Although our results show that the neural network achieves state-of-the-art results, the evaluation of the visual explanations reveals that relevant facial regions may not be considered. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经网络，高性能仍然可以对非相关特征有偏差。然而，可靠性和耐用性是高风险的领域尤其重要，如临床疼痛治疗。因此，我们提出了一个验证管道，其中包括三个步骤。首先，我们用分类神经网络的面部表情。接下来，我们采用逐层传播的相关性来创建基于像素的解释。最后，我们量化基于一个包围盒方法对于面部区域这些视觉解释。尽管我们的结果表明，该神经网络实现了国家的先进成果，视觉解释的评估表明，有关面部区域可以不考虑。</font>
</div>


<hr>
<div id="paper37"> <b>37. CheXclusion: Fairness gaps in deep chest X-ray classifiers</b>  <a href="https://arxiv.org/pdf/2003.00827" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Seyyed-Kalantari%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laleh Seyyed-Kalantari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guanxiong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McDermott%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew McDermott</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghassemi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marzyeh Ghassemi</a><br>
<font size="3">
Abstract: Machine learning systems have received much attention recently for their ability to achieve expert-level performance on clinical tasks, particularly in medical imaging. Here, we examine the extent to which state-of-the-art deep learning classifiers trained to yield diagnostic labels from X-ray images are biased with respect to protected attributes. We train convolution neural networks to predict 14 diagnostic labels in three prominent public chest X-ray datasets: MIMIC-CXR, Chest-Xray8, and CheXpert. We then evaluate the TPR disparity - the difference in true positive rates (TPR) and - underdiagnosis rate - the false positive rate of a non-diagnosis - among different protected attributes such as patient sex, age, race, and insurance type. We demonstrate that TPR disparities exist in the state-of-the-art classifiers in all datasets, for all clinical tasks, and all subgroups. We find that TPR disparities are most commonly not significantly correlated with a subgroup's proportional disease burden; further, we find that some subgroups and subsection of the population are chronically underdiagnosed. Such performance disparities have real consequences as models move from papers to products, and should be carefully audited prior to deployment. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器学习系统备受关注最近他们实现临床任务的专家级的性能，特别是在医疗成像能力。在这里，我们检查状态的最先进的深学习训练从X射线图像产生诊断标签分类器，其被偏置相对于受保护的属性的程度。我们训练卷积神经网络在三个重要公共胸部X射线数据集来预测14个诊断标签：MIMIC-CXR，胸Xray8和CheXpert。然后，我们评估TPR差距 - 在真正的阳性率（TPR）的差异， - 漏诊率 - 非诊断的假阳性率 - 不同的保护属性，如患者性别，年龄，种族，保险类型之一。我们表明，在国家的最先进的分类中的所有数据集存在TPR差距，所有临床任务，所有子组。我们发现，TPR差距是最常见的不与群的比例疾病负担显著相关;此外，我们发现，一些人口群与小节中长期漏诊。这样的性能差距有真正的后果，因为模型从报纸转向产品，并应在部署之前进行仔细审核。</font>
</div>


<hr>
<div id="paper38"> <b>38. Realistic River Image Synthesis using Deep Generative Adversarial  Networks</b>  <a href="https://arxiv.org/pdf/2003.00826" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gautam%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Akshat Gautam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sit%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Muhammed Sit</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Demir%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ibrahim Demir</a><br>
<font size="3">
Abstract: In this paper, we investigate an application of image generation for river satellite imagery. Specifically, we propose a generative adversarial network (GAN) model capable of generating high-resolution and realistic river images that can be used to support models in surface water estimation, river meandering, wetland loss and other hydrological research studies. First, we summarized an augmented, diverse repository of overhead river images to be used in training. Second, we incorporate the Progressive Growing GAN (PGGAN), a network architecture that iteratively trains smaller-resolution GANs to gradually build up to a very high resolution, to generate 256x256 river satellite imagery. With conventional GAN architectures, difficulties soon arise in terms of exponential increase of training time and vanishing/exploding gradient issues, which the PGGAN implementation seems to significantly reduce. Our preliminary results show great promise in capturing the detail of river flow and green areas present in river satellite images that can be used for supporting hydroinformatics studies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们研究了图像生成的河流卫星影像的应用程序。具体来说，我们提出了一个生成对抗网络（GAN）能够产生高清晰度和可用于支持机型地表水估计，河流蜿蜒，湿地减少等水文调查研究现实河流的图像模式。首先，我们总结了在训练中使用的开销河图像的增强，多样化的存储库。其次，我们结合了渐进式生长GaN（PGGAN），网络架构，反复训练较小的分辨率甘斯，逐步建立以非常高的分辨率，256x256的生成河流卫星图像。与传统的GAN架构，困难很快在训练时间和消失/爆炸梯度问题的指数增长，其中PGGAN实施似乎显著减少条款出现。我们的初步结果表明，捕获河流流量和当前在可用于支持水文信息学研究河流的卫星图像的绿色区域的细节巨大潜力。</font>
</div>


<hr>
<div id="paper39"> <b>39. SIP-SegNet: A Deep Convolutional Encoder-Decoder Network for Joint  Semantic Segmentation and Extraction of Sclera, Iris and Pupil based on  Periocular Region Suppression</b>  <a href="https://arxiv.org/pdf/2003.00825" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hassan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bilal Hassan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ahmed%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ramsha Ahmed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hassan%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taimur Hassan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Werghi%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naoufel Werghi</a><br>
<font size="3">
Abstract: The current developments in the field of machine vision have opened new vistas towards deploying multimodal biometric recognition systems in various real-world applications. These systems have the ability to deal with the limitations of unimodal biometric systems which are vulnerable to spoofing, noise, non-universality and intra-class variations. In addition, the ocular traits among various biometric traits are preferably used in these recognition systems. Such systems possess high distinctiveness, permanence, and performance while, technologies based on other biometric traits (fingerprints, voice etc.) can be easily compromised. This work presents a novel deep learning framework called SIP-SegNet, which performs the joint semantic segmentation of ocular traits (sclera, iris and pupil) in unconstrained scenarios with greater accuracy. The acquired images under these scenarios exhibit purkinje reflexes, specular reflections, eye gaze, off-angle shots, low resolution, and various occlusions particularly by eyelids and eyelashes. To address these issues, SIP-SegNet begins with denoising the pristine image using denoising convolutional neural network (DnCNN), followed by reflection removal and image enhancement based on contrast limited adaptive histogram equalization (CLAHE). Our proposed framework then extracts the periocular information using adaptive thresholding and employs the fuzzy filtering technique to suppress this information. Finally, the semantic segmentation of sclera, iris and pupil is achieved using the densely connected fully convolutional encoder-decoder network. We used five CASIA datasets to evaluate the performance of SIP-SegNet based on various evaluation metrics. The simulation results validate the optimal segmentation of the proposed SIP-SegNet, with the mean f1 scores of 93.35, 95.11 and 96.69 for the sclera, iris and pupil classes respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在机器视觉领域的最新发展已经对各种现实应用中部署多模态生物特征识别系统，开辟了新的前景。这些系统必须处理单峰生物识别系统，其很容易受到欺骗，噪音小，无普遍性和类内变化的局限性的能力。此外，各种生物统计性状的眼性状在这些识别系统被优选使用。这种系统具有较高的显着性，持久性，并且在性能的基础上，其他的生物统计学特征（指纹，语音等）的技术可以很容易地受到损害。这项工作提出一种新颖的深学习框架被叫SIP-SegNet，其执行在更准确不受约束场景眼性状（巩膜，虹膜和瞳孔）的联合语义分割。这些情景下所获得的图像特别是通过眼睑和睫毛呈现浦肯野反射，镜面反射，眼睛注视，偏角拍摄，低分辨率和各种闭塞。为了解决这些问题，SIP-SegNet与去噪使用去噪卷积神经网络（DnCNN）原始图像开始，随后反射去除和基于对比度的图像增强限于自适应直方图均衡（CLAHE）。我们提出的框架然后提取使用自适应阈值化的眼周信息和采用模糊滤波技术抑制此信息。最后，巩膜，虹膜和瞳孔的语义分割使用密集连接完全卷积编码器 - 解码器网络来实现。我们使用的五种自动化所数据集来评估SIP-SegNet的基于各种评价指标的表现。仿真结果验证了该SIP-SegNet的最佳分割，具有93.35，95.11和96.69的平均F1分数分别巩膜，虹膜和瞳孔类。</font>
</div>


<hr>
<div id="paper40"> <b>40. Multi-Scale Representation Learning for Spatial Feature Distributions  using Grid Cells</b>  <a href="https://arxiv.org/pdf/2003.00824" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mai%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gengchen Mai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Janowicz%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Krzysztof Janowicz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Ling Cai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lao%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ni Lao</a><br>
<font size="3">
Abstract: Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for a multitude of tasks. However, no such general model exists to date beyond simply applying discretization or feed-forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Therefore, we propose a representation learning model called Space2Vec to encode the absolute positions and spatial relationships of places. We conduct experiments on two real-world geographic data for two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification leveraging their geo-locations. Results show that because of its multi-scale representations, Space2Vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. Detailed analysis shows that all baselines can at most well handle distribution at one scale but show poor performances in other scales. In contrast, Space2Vec's multi-scale representation can handle distributions at different scales. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督文本编码模型最近助长了NLP实质性进展。关键思想是利用神经网络的话在文本转换为基于句子中的单词位置和他们的环境，适合于终端到终端的培训下游任务矢量空间表示。我们看到在空间分析，其重点是结合两者的绝对位置和地理对象的空间环境下，比如兴趣点到模型中惊人相似的情况。空间通用表示模型是多项任务有价值。但是，没有这样的一般模型存在至今超越了简单的应用离散或前馈网，坐标，举手之劳已投入联合分布建模与完全不同的特性，这通常由GIS数据出现。同时，获得诺贝尔奖的神经科学的研究表明，在哺乳动物中网格单元提供了一个多尺度的周期性表现，其功能如同一个度量位置编码，并识别名额和路径整合的关键。因此，我们提出了一个名为Space2Vec表示学习模式编码的绝对位置和场所的空间关系。我们进行了两个真实世界的地理数据实验的两个不同的任务：1）预测给出自己的立场和背景种类的POI，2）图像分类利用其地缘位置。结果表明，由于其多尺度交涉，Space2Vec优于成熟的ML方法，如RBF内核，多层前馈网，和位置建模和图像分类任务瓷砖嵌入方法。详细的分析表明，所有基准最多能处理好分布在某一尺度，但显示其他尺度表现不佳。相比之下，Space2Vec的多尺度表示可以处理不同尺度分布。</font>
</div>


<hr>
<div id="paper41"> <b>41. Breast Cancer Histopathology Image Classification and Localization using  Multiple Instance Learning</b>  <a href="https://arxiv.org/pdf/2003.00823" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Patil%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhijeet Patil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tamboli%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dipesh Tamboli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meena%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Swati Meena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anand%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deepak Anand</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sethi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amit Sethi</a><br>
<font size="3">
Abstract: Breast cancer has the highest mortality among cancers in women. Computer-aided pathology to analyze microscopic histopathology images for diagnosis with an increasing number of breast cancer patients can bring the cost and delays of diagnosis down. Deep learning in histopathology has attracted attention over the last decade of achieving state-of-the-art performance in classification and localization tasks. The convolutional neural network, a deep learning framework, provides remarkable results in tissue images analysis, but lacks in providing interpretation and reasoning behind the decisions. We aim to provide a better interpretation of classification results by providing localization on microscopic histopathology images. We frame the image classification problem as weakly supervised multiple instance learning problem where an image is collection of patches i.e. instances. Attention-based multiple instance learning (A-MIL) learns attention on the patches from the image to localize the malignant and normal regions in an image and use them to classify the image. We present classification and localization results on two publicly available BreakHIS and BACH dataset. The classification and visualization results are compared with other recent techniques. The proposed method achieves better localization results without compromising classification accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：乳腺癌在女性癌症中死亡率最高。计算机辅助病理分析诊断显微组织病理学图像与越来越多的乳腺癌患者可以带来成本和诊断下来的延迟。在病理组织学深学已引起重视了实现分类和定位任务的国家的最先进的性能的最后十年。卷积神经网络，深学习框架，提供了组织图像分析了显着成绩，但在提供解释和推理的决定背后的缺乏。我们的目标是通过对微观病理图像提供本地化提供分类结果更好的诠释。作为弱监督多示例学习问题，其中的图像是补丁即实例的集合，我们帧图像分类问题。关注基于多示例学习（A-MIL）学习从图像补丁注意本地化恶性和正常区域的图像中并用它们来对图像进行分类。我们两个公开可用BreakHIS和巴赫的数据集目前的分类和定位结果。分类和可视化结果与最近的其他技术相比。所提出的方法实现更好的定位结果不影响分类的准确性。</font>
</div>


<hr>
<div id="paper42"> <b>42. MADAN: Multi-source Adversarial Domain Aggregation Network for Domain  Adaptation</b>  <a href="https://arxiv.org/pdf/2003.00820" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sicheng Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yue%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyu Yue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kurt Keutzer</a><br>
<font size="3">
Abstract: Domain adaptation aims to learn a transferable model to bridge the domain shift between one labeled source domain and another sparsely labeled or unlabeled target domain. Since the labeled data may be collected from multiple sources, multi-source domain adaptation (MDA) has attracted increasing attention. Recent MDA methods do not consider the pixel-level alignment between sources and target or the misalignment across different sources. In this paper, we propose a novel MDA framework to address these challenges. Specifically, we design an end-to-end Multi-source Adversarial Domain Aggregation Network (MADAN). First, an adapted domain is generated for each source with dynamic semantic consistency while aligning towards the target at the pixel-level cycle-consistently. Second, sub-domain aggregation discriminator and cross-domain cycle discriminator are proposed to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and the target domain while training the task network. For the segmentation adaptation, we further enforce category-level alignment and incorporate context-aware generation, which constitutes MADAN+. We conduct extensive MDA experiments on digit recognition, object classification, and simulation-to-real semantic segmentation. The results demonstrate that the proposed MADAN and MANDA+ models outperform state-of-the-art approaches by a large margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：域名适应旨在学习转移模型来弥补一个标记源域和另一个稀疏标注或未标注的目标域之间的域转移。由于标记的数据可从多个源收集，多源域的适应（MDA）已经吸引了越来越多的关注。最近MDA方法没有考虑源和目标或在不同源的偏差之间的像素级的定位。在本文中，我们提出了一个新颖的MDA框架来应对这些挑战。具体来说，我们设计了一个端至端的多源对抗性域聚合网络（马丹）。首先，用于与同时在像素级对所述目标对准周期一致地动态语义一致性每个源产生的适应域。其次，子域名聚集鉴别和跨域循环标识符都提出了不同的适应领域更加紧密地聚集做。最后，聚集区和同时培养任务的网络目标域之间进行功能级别的对齐。对于分割适应，我们进一步执行类别级对准并结合上下文感知代，其构成马丹+。我们对数字识别，对象分类，并模拟到真实语义分割进行广泛的MDA的实验。结果表明，所提出的麻石和MANDA +车型超越国家的最先进的大幅度接近。</font>
</div>


<hr>
<div id="paper43"> <b>43. Recognizing Handwritten Mathematical Expressions as LaTex Sequences  Using a Multiscale Robust Neural Network</b>  <a href="https://arxiv.org/pdf/2003.00817" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongyu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guangcun Shan</a><br>
<font size="3">
Abstract: In this paper, a robust multiscale neural network is proposed to recognize handwritten mathematical expressions and output LaTeX sequences, which can effectively and correctly focus on where each step of output should be concerned and has a positive effect on analyzing the two-dimensional structure of handwritten mathematical expressions and identifying different mathematical symbols in a long expression. With the addition of visualization, the model's recognition process is shown in detail. In addition, our model achieved 49.459% and 46.062% ExpRate on the public CROHME 2014 and CROHME 2016 datasets. The present model results suggest that the state-of-the-art model has better robustness, fewer errors, and higher accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，一个强大的多尺度神经网络提出了识别手写的数学表达式和输出乳胶序列，其可有效地和正确地集中于其中输出的每一步骤应该关注，并且具有在分析所述二维结构的积极作用的手写数学表达式，并确定在长表达不同的数学符号。由于增加的可视化，该模型的识别处理中详细示出。此外，我们的模型实现了49.459％，并在公共CROHME 2014年和CROHME 2016数据集46.062％ExpRate。本模型结果表明，国家的最先进的模型具有较好的稳健性，更少的错误，以及更高的精度。</font>
</div>


<hr>
<div id="paper44"> <b>44. Deepfakes for Medical Video De-Identification: Privacy Protection and  Diagnostic Information Preservation</b>  <a href="https://arxiv.org/pdf/2003.00813" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bingquan Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Fang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanan Sui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luming Li</a><br>
<font size="3">
Abstract: Data sharing for medical research has been difficult as open-sourcing clinical data may violate patient privacy. Traditional methods for face de-identification wipe out facial information entirely, making it impossible to analyze facial behavior. Recent advancements on whole-body keypoints detection also rely on facial input to estimate body keypoints. Both facial and body keypoints are critical in some medical diagnoses, and keypoints invariability after de-identification is of great importance. Here, we propose a solution using deepfake technology, the face swapping technique. While this swapping method has been criticized for invading privacy and portraiture right, it could conversely protect privacy in medical video: patients' faces could be swapped to a proper target face and become unrecognizable. However, it remained an open question that to what extent the swapping de-identification method could affect the automatic detection of body keypoints. In this study, we apply deepfake technology to Parkinson's disease examination videos to de-identify subjects, and quantitatively show that: face-swapping as a de-identification approach is reliable, and it keeps the keypoints almost invariant, significantly better than traditional methods. This study proposes a pipeline for video de-identification and keypoint preservation, clearing up some ethical restrictions for medical data sharing. This work could make open-source high quality medical video datasets more feasible and promote future medical research that benefits our society. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据共享为医学研究一直是困难的，因为开放式采购的临床数据可能侵犯病人隐私。面部去标识的传统方法完全祛除面部信息，因此无法分析脸部行为。对全身的关键点的最新进展检测还要靠面部输入估计身体的关键点。无论脸部和身体的关键点是在一些医疗诊断的关键，去标识后的关键点不变是非常重要的。在这里，我们建议使用deepfake技术，脸部交换技术的解决方案。虽然这种互换方法已经被批评为侵犯隐私和肖像权，它可以在医疗视频反过来保护隐私：患者的面部可能被交换到正确目标的脸，变得面目全非。但是，它仍然是一个悬而未决的问题是在何种程度上交换去识别方法可能会影响身体的关键点的自动检测。在这项研究中，我们应用deepfake技术，帕金森氏病检查视频去标识科目，并定量地表明：面对面交换作为去识别方法是可靠的，它不断的关键点几乎不变，显著优于传统方法。这项研究提出了视频去识别和关键点保存管道，清理医疗数据共享一些道德限制。这项工作可以使开源高品质的医疗影像数据集的可行性，并促进未来医学研究有利于我们的社会。</font>
</div>


<hr>
<div id="paper45"> <b>45. Medicine Strip Identification using 2-D Cepstral Feature Extraction and  Multiclass Classification Methods</b>  <a href="https://arxiv.org/pdf/2003.00810" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title45" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Itagi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anirudh Itagi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sil%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ritam Sil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohapatra%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Saurav Mohapatra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rout%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subham Rout</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%2C+B+K" target="_blank" rel="noopener" style="color:#0000EE;">Bharath K P</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=R%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karthik R</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Muthu%2C+R+K" target="_blank" rel="noopener" style="color:#0000EE;">Rajesh Kumar Muthu</a><br>
<font size="3">
Abstract: Misclassification of medicine is perilous to the health of a patient, more so if the said patient is visually impaired or simply did not recognize the color, shape or type of medicine strip. This paper proposes a method for identification of medicine strips by 2-D cepstral analysis of their images followed by performing classification that has been done using the K-Nearest Neighbor (KNN), Support Vector Machine (SVM) and Logistic Regression (LR) Classifiers. The 2-D cepstral features extracted are extremely distinct to a medicine strip and consequently make identifying them exceptionally accurate. This paper also proposes the Color Gradient and Pill shape Feature (CGPF) extraction procedure and discusses the Binary Robust Invariant Scalable Keypoints (BRISK) algorithm as well. The mentioned algorithms were implemented and their identification results have been compared. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：医学误判是危险的患者的健康，更是这样，如果说患者有视力障碍或根本不认识颜色，形状或药条的类型。本文提出了一种由它们的图像的2-d倒频谱分析鉴定药带的方法，接着进行已使用k近邻（KNN）进行分类，支持向量机（SVM）和Logistic回归（LR）的分类器。提取出的2-d倒谱特征是极为不同的药条，并因此使鉴定它们极其精确。本文还提出了颜色梯度和丸形状特征（CGPF）提取方法，并讨论了二进制鲁棒不变可伸缩关键点（BRISK）算法为好。所提到的算法得以实施，他们的鉴定结果进行了比较。</font>
</div>


<hr>
<div id="paper46"> <b>46. Vision based body gesture meta features for Affective Computing</b>  <a href="https://arxiv.org/pdf/2003.00809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title46" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Orton%2C+I+J+D" target="_blank" rel="noopener" style="color:#0000EE;">Indigo J. D. Orton</a><br>
<font size="3">
Abstract: Early detection of psychological distress is key to effective treatment. Automatic detection of distress, such as depression, is an active area of research. Current approaches utilise vocal, facial, and bodily modalities. Of these, the bodily modality is the least investigated, partially due to the difficulty in extracting bodily representations from videos, and partially due to the lack of viable datasets. Existing body modality approaches use automatic categorization of expressions to represent body language as a series of specific expressions, much like words within natural language. In this dissertation I present a new type of feature, within the body modality, that represents meta information of gestures, such as speed, and use it to predict a non-clinical depression label. This differs to existing work by representing overall behaviour as a small set of aggregated meta features derived from a person's movement. In my method I extract pose estimation from videos, detect gestures within body parts, extract meta information from individual gestures, and finally aggregate these features to generate a small feature vector for use in prediction tasks. I introduce a new dataset of 65 video recordings of interviews with self-evaluated distress, personality, and demographic labels. This dataset enables the development of features utilising the whole body in distress detection tasks. I evaluate my newly introduced meta-features for predicting depression, anxiety, perceived stress, somatic stress, five standard personality measures, and gender. A linear regression based classifier using these features achieves a 82.70% F1 score for predicting depression within my novel dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：心理困扰的早期发现是关键，有效的治疗方法。自动检测困扰，如抑郁症，是一个活跃的研究领域。目前的方法利用声音，面部，和身体的方式。其中，身体形态是最少的调查，部分原因是由于难以从视频中提取的身体表示，部分由于缺乏可行的数据集。现有的身体形态的方法使用表达式的自动分类来表示的肢体语言为一系列具体表述，很像自然语言中的单词。在本文我提出了一种新类型的特征，身体形态中，表示的手势，如速度的元信息，并用它来预测非临床抑郁症的标签。这不同于由代表整体行为为一小部分从一个人的运动衍生聚集元功能现有的工作。在从视频中我的方法我提取姿态估计，检测手势的身体部位，从单个手势提取元数据信息中，最后汇总这些功能来生成用于预测任务使用小特征向量。我介绍的与自我评价的困扰，个性和人口标签采访的65个视频录制新的数据集。该数据集能够利用的整个身体处于困境的检测任务功能的开发。我评估我的新推出的元功能预测抑郁，焦虑，心理压力，躯体应激，五个标准的个性措施，和性别。使用这些功能的线性回归的分类器实现了82.70％的F1的成绩。我的小说集内预测抑郁症。</font>
</div>


<hr>
<div id="paper47"> <b>47. A Convolutional Baseline for Person Re-Identification Using Vision and  Language Descriptions</b>  <a href="https://arxiv.org/pdf/2003.00808" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title47" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Farooq%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ammarah Farooq</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Awais%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Muhammad Awais</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kittler%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Josef Kittler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Akbari%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Akbari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khalid%2C+S+S" target="_blank" rel="noopener" style="color:#0000EE;">Syed Safwan Khalid</a><br>
<font size="3">
Abstract: Classical person re-identification approaches assume that a person of interest has appeared across different cameras and can be queried by one of the existing images. However, in real-world surveillance scenarios, frequently no visual information will be available about the queried person. In such scenarios, a natural language description of the person by a witness will provide the only source of information for retrieval. In this work, person re-identification using both vision and language information is addressed under all possible gallery and query scenarios. A two stream deep convolutional neural network framework supervised by cross entropy loss is presented. The weights connecting the second last layer to the last layer with class probabilities, i.e., logits of softmax layer are shared in both networks. Canonical Correlation Analysis is performed to enhance the correlation between the two modalities in a joint latent embedding space. To investigate the benefits of the proposed approach, a new testing protocol under a multi modal ReID setting is proposed for the test split of the CUHK-PEDES and CUHK-SYSU benchmarks. The experimental results verify the merits of the proposed system. The learnt visual representations are more robust and perform 22\% better during retrieval as compared to a single modality system. The retrieval with a multi modal query greatly enhances the re-identification capability of the system quantitatively as well as qualitatively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：经典的人重新鉴定方法假设的权益的人已经在不同的摄像机出现，并且可以通过现有的图像之一进行查询。然而，在现实世界中的场景监控，经常没有视觉信息将有关查询人。在这样的情况下，通过证人的人的自然语言描述将提供用于检索信息的唯一来源。在这项工作中，同时使用视觉和语言信息的人重新鉴定是在所有可能的画廊和查询场景讨论。两流深卷积神经网络架构监督交叉熵损失提出。倒数第二层连接到与类概率，即最后的层的权重，SOFTMAX层的logits在两个网络共享。典型相关分析进行，以提高在合资潜在嵌入空间的两种模式之间的相关性。为了研究该方法的优点，在多模态里德设置一个新的测试方案，提出了中大 - 德斯和香港中文大学 - 中山大学基准测试分裂。实验结果验证了该系统的优点。博学的视觉表现更健壮，相比于单模态系统恢复过程中执行22 \％更好。具有多模态查询检索极大地提高了系统的重新识别能力定量以及定性。</font>
</div>


<hr>
<div id="paper48"> <b>48. Firearm Detection and Segmentation Using an Ensemble of Semantic Neural  Networks</b>  <a href="https://arxiv.org/pdf/2003.00805" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title48" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Egiazarov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Egiazarov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mavroeidis%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasileios Mavroeidis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zennaro%2C+F+M" target="_blank" rel="noopener" style="color:#0000EE;">Fabio Massimo Zennaro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vishi%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kamer Vishi</a><br>
<font size="3">
Abstract: In recent years we have seen an upsurge in terror attacks around the world. Such attacks usually happen in public places with large crowds to cause the most damage possible and get the most attention. Even though surveillance cameras are assumed to be a powerful tool, their effect in preventing crime is far from clear due to either limitation in the ability of humans to vigilantly monitor video surveillance or for the simple reason that they are operating passively. In this paper, we present a weapon detection system based on an ensemble of semantic Convolutional Neural Networks that decomposes the problem of detecting and locating a weapon into a set of smaller problems concerned with the individual component parts of a weapon. This approach has computational and practical advantages: a set of simpler neural networks dedicated to specific tasks requires less computational resources and can be trained in parallel; the overall output of the system given by the aggregation of the outputs of individual networks can be tuned by a user to trade-off false positives and false negatives; finally, according to ensemble theory, the output of the overall system will be robust and reliable even in the presence of weak individual models. We evaluated our system running simulations aimed at assessing the accuracy of individual networks and the whole system. The results on synthetic data and real-world data are promising, and they suggest that our approach may have advantages compared to the monolithic approach based on a single deep convolutional neural network. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，我们看到世界各地的恐怖袭击的热潮。这种攻击通常在公共场所与大量人群发生造成最大的伤害可能并得到了最多的关注。尽管监控摄像机被认为是一个强大的工具，其在预防犯罪的效果还远未明朗因人的能力，警惕地监视视频监控或原因很简单，他们正在操作被动或者限制。在本文中，我们提出基于语义卷积神经网络的集合会分解的检测和定位的武器成一组涉及一种武器的各个组成部件较小的问题的问题的武器检测系统。这种方法具有计算和实用的优点：一个专用于特定任务的一组简单的神经网络需要较少的计算资源可以并行进行培训;通过各个网络的输出的聚集给出的系统的总输出可以由用户被调谐到权衡假阳性和假阴性;最后，根据合奏理论，整个系统的输出，即使在微弱的个别型号的存在是稳定可靠的。我们评估我们的系统旨在评估单个网络的精度和整个系统的仿真。在模拟数据和真实数据的结果是令人鼓舞的，他们认为我们的做法可能具有的优势与基于单一的深卷积神经网络的整体办法。</font>
</div>


<hr>
<div id="paper49"> <b>49. Task Augmentation by Rotating for Meta-Learning</b>  <a href="https://arxiv.org/pdf/2003.00804" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title49" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jialin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Chao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chih-Min Lin</a><br>
<font size="3">
Abstract: Data augmentation is one of the most effective approaches for improving the accuracy of modern machine learning models, and it is also indispensable to train a deep model for meta-learning. In this paper, we introduce a task augmentation method by rotating, which increases the number of classes by rotating the original images 90, 180 and 270 degrees, different from traditional augmentation methods which increase the number of images. With a larger amount of classes, we can sample more diverse task instances during training. Therefore, task augmentation by rotating allows us to train a deep network by meta-learning methods with little over-fitting. Experimental results show that our approach is better than the rotation for increasing the number of images and achieves state-of-the-art performance on miniImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. The code is available on \url{this http URL}. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据隆胸是最有效的办法对于提高现代机器学习模型的准确性之一，它也是必不可少的训练对元学习了深刻的模型。在本文中，我们引入通过旋转任务增强方法，该方法通过旋转原始图像90，180和270度，与传统的增强方法，其增加的图像的数量不同增加了类的数量。随着类用量较大，我们可以在训练中品尝更多样化的任务实例。因此，通过旋转任务增加允许我们训练由元的学习方法了深刻的网络，有点过度拟合。实验结果表明，该方法比旋转更好地为提高图像的数量，并实现对miniImageNet，CIFAR-FS国家的最先进的性能和FC100几拍的学习标杆。该代码可以在\ {URL这个HTTP URL}。</font>
</div>


<hr>
<div id="paper50"> <b>50. Hypernetwork approach to generating point clouds</b>  <a href="https://arxiv.org/pdf/2003.00802" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title50" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Spurek%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Przemysław Spurek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Winczowski%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastian Winczowski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tabor%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jacek Tabor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zamorski%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maciej Zamorski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zi%C4%99ba%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maciej Zięba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Trzci%C5%84ski%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomasz Trzciński</a><br>
<font size="3">
Abstract: In this work, we propose a novel method for generating 3D point clouds that leverage properties of hyper networks. Contrary to the existing methods that learn only the representation of a 3D object, our approach simultaneously finds a representation of the object and its 3D surface. The main idea of our HyperCloud method is to build a hyper network that returns weights of a particular neural network (target network) trained to map points from a uniform unit ball distribution into a 3D shape. As a consequence, a particular 3D shape can be generated using point-by-point sampling from the assumed prior distribution and transforming sampled points with the target network. Since the hyper network is based on an auto-encoder architecture trained to reconstruct realistic 3D shapes, the target network weights can be considered a parametrization of the surface of a 3D shape, and not a standard representation of point cloud usually returned by competitive approaches. The proposed architecture allows finding mesh-based representation of 3D objects in a generative manner while providing point clouds en pair in quality with the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们提出了产生三维点云是超网络的杠杆性质的新方法。与此相反学习3D对象的仅表示现有的方法，我们的方法同时找到对象和它的3D表面的表示。我们的方法HyperCloud的主要思想是建立一个超网络训练以从均匀单元球分布为3D形状的点映射的特定神经网络（目标网络）的回报的权重。因此，可使用逐点从采样假定先验分布和用所述目标网络采样的点来产生特定的3D形状。由于超网络是基于一个自动编码器架构训练来重建逼真的三维形状，所述目标网络的权值可被认为是三维形状的表面的参数化，而不是点云的标准表示通常通过竞争性方法返回。所提出的架构允许找到3D的基于网格的表示在生成对象的方式，而在质量提供点云连接一对与国家的最先进的方法。</font>
</div>


<hr>
<div id="paper51"> <b>51. Real-Time target detection in maritime scenarios based on YOLOv3 model</b>  <a href="https://arxiv.org/pdf/2003.00800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title51" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Betti%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessandro Betti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Michelozzi%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benedetto Michelozzi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bracci%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Bracci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Masini%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Masini</a><br>
<font size="3">
Abstract: In this work a novel ships dataset is proposed consisting of more than 56k images of marine vessels collected by means of web-scraping and including 12 ship categories. A YOLOv3 single-stage detector based on Keras API is built on top of this dataset. Current results on four categories (cargo ship, naval ship, oil ship and tug ship) show Average Precision up to 96% for Intersection over Union (IoU) of 0.5 and satisfactory detection performances up to IoU of 0.8. A Data Analytics GUI service based on QT framework and Darknet-53 engine is also implemented in order to simplify the deployment process and analyse massive amount of images even for people without Data Science expertise. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中数据集提出由借助于收集海洋船舶的多于56K的图像的新颖的一般Web的刮削和包括12个船类别。基于Keras API甲YOLOv3单级检测器是建立在此数据集的顶部。四类（货船，军舰，油船和船舶拖船）当前结果表明平均精确到0.5和满意的检测性能96％交叉口超过联盟（IOU）高达0.8 IOU。基于Qt框架和暗网-53发动机的数据分析GUI服务，以简化部署流程和分析图像的巨量甚至对人没有数据科学的专业知识也可以实现。</font>
</div>


<hr>
<div id="paper52"> <b>52. Preventing Clean Label Poisoning using Gaussian Mixture Loss</b>  <a href="https://arxiv.org/pdf/2003.00798" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title52" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yaseen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Muhammad Yaseen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Aadil%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Muneeb Aadil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sargsyan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maria Sargsyan</a><br>
<font size="3">
Abstract: Since 2014 when Szegedy et al. showed that carefully designed perturbations of the input can lead Deep Neural Networks (DNNs) to wrongly classify its label, there has been an ongoing research to make DNNs more robust to such malicious perturbations. In this work, we consider a poisoning attack called Clean Labeling poisoning attack (CLPA). The goal of CLPA is to inject seemingly benign instances which can drastically change decision boundary of the DNNs due to which subsequent queries at test time can be mis-classified. We argue that a strong defense against CLPA can be embedded into the model during the training by imposing features of the network to follow a Large Margin Gaussian Mixture distribution in the penultimate layer. By having such a prior knowledge, we can systematically evaluate how unusual the example is, given the label it is claiming to be. We demonstrate our builtin defense via experiments on MNIST and CIFAR datasets. We train two models on each dataset: one trained via softmax, another via LGM. We show that using LGM can substantially reduce the effectiveness of CLPA while having no additional overhead of data sanitization. The code to reproduce our results is available online. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从2014年时Szegedy等。结果表明，输入精心设计的扰动会导致深层神经网络（DNNs）错误地归类它的标签，出现了持续的研究，使DNNs更强大的这种恶意干扰。在这项工作中，我们认为所谓的清洁标签中毒攻击中毒攻击（CLPA）。 CLPA的目标是注入看似良性的情况下，可以彻底改变DNNs由于其在测试时后续查询可以误分类的决策边界。我们认为，对CLPA强大的防御可以通过实施网络的功能，遵循在倒数第二层大幅度高斯混合分布的训练过程中嵌入到模型中。通过具有这样的先验知识，我们可以系统地评价如何不同寻常的例子，给了它声称是标签。我们证明通过对MNIST和CIFAR数据集实验我们内置的防御。我们培养对每个数据集两种模式：一是通过SOFTMAX训练有素，另一个通过LGM。我们表明，使用LGM可以同时具有数据消毒的无额外的开销大大减少CLPA的有效性。重现我们的结果的代码是在网上提供。</font>
</div>


<hr>
<div id="paper53"> <b>53. Identity Recognition in Intelligent Cars with Behavioral Data and  LSTM-ResNet Classifier</b>  <a href="https://arxiv.org/pdf/2003.00770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title53" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hammann%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Hammann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kraus%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maximilian Kraus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shafaei%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sina Shafaei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Knoll%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alois Knoll</a><br>
<font size="3">
Abstract: Identity recognition in a car cabin is a critical task nowadays and offers a great field of applications ranging from personalizing intelligent cars to suit drivers physical and behavioral needs to increasing safety and security. However, the performance and applicability of published approaches are still not suitable for use in series cars and need to be improved. In this paper, we investigate Human Identity Recognition in a car cabin with Time Series Classification (TSC) and deep neural networks. We use gas and brake pedal pressure as input to our models. This data is easily collectable during driving in everyday situations. Since our classifiers have very little memory requirements and do not require any input data preproccesing, we were able to train on one Intel i5-3210M processor only. Our classification approach is based on a combination of LSTM and ResNet. The network trained on a subset of NUDrive outperforms the ResNet and LSTM models trained solely by 35.9 % and 53.85 % accuracy respectively. We reach a final accuracy of 79.49 % on a 10-drivers subset of NUDrive and 96.90 % on a 5-drivers subset of UTDrive. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在汽车驾驶室身份识别是时下一个重要的任务，并提供应用软件，从个性化的智能汽车，以适应驾驶员的身体和行为需要增加安全保障有很大场。但是，性能和公布方式的适用性仍然不适合在系列轿车的使用和需要改进。在本文中，我们在汽车舱时间序列分类（TSC）和深层神经网络调查人身份识别。我们使用天然气和制动踏板的压力输入到我们的模型。这个数据是在日常生活中驾驶过程中容易收藏。由于我们的分类有很少的内存需求，并且不需要任何输入数据preproccesing，我们能够在只有一个英特尔i5-3210M处理器训练。我们的分类方法是根据LSTM和RESNET的组合。培训了NUDrive的一个子集的网络优于RESNET和LSTM模型35.9％和53.85％的准确度分别单独训练。我们到达79.49％的NUDrive的10集司机和96.90％的UTDrive子集5驱动程序最终精度。</font>
</div>


<hr>
<div id="paper54"> <b>54. Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion  from 3D Geometry</b>  <a href="https://arxiv.org/pdf/2003.00766" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title54" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guangming Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hesheng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingchuan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinlei Wang</a><br>
<font size="3">
Abstract: In autonomous driving, monocular sequences contain lots of information. Monocular depth estimation, camera ego-motion estimation and optical flow estimation in consecutive frames are high-profile concerns recently. By analyzing tasks above, pixels in the first frame are modeled into three parts: the rigid region, the non-rigid region, and the occluded region. In joint unsupervised training of depth and pose, we can segment the occluded region explicitly. The occlusion information is used in unsupervised learning of depth, pose and optical flow, as the image reconstructed by depth, pose and flow will be invalid in occluded regions. A less-than-mean mask is designed to further exclude the mismatched pixels which are interfered with motion or illumination change in the training of depth and pose networks. This method is also used to exclude some trivial mismatched pixels in the training of the flow net. Maximum normalization is proposed for smoothness term of depth-pose networks to restrain degradation in textureless regions. In the occluded region, as depth and camera motion can provide more reliable motion estimation, they can be used to instruct unsupervised learning of flow. Our experiments in KITTI dataset demonstrate that the model based on three regions, full and explicit segmentation of occlusion, rigid region and non-rigid region with corresponding unsupervised losses can improve performance on three tasks significantly. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在自主驾驶，单眼序列含有大量的信息。单眼深度估计，相机自运动估计和光流估计在连续的帧是高调关注最近。通过分析上述任务，在所述第一帧中的像素被建模为三个部分：刚性区域，非刚性区域和遮挡区域。在深度和姿态的联合监督的培训，我们可以细分闭塞的区域明确。遮蔽信息在深度，姿势和光流的无监督学习使用的，作为图像重建由深度，姿势和流量将是无效的遮挡区域。甲低于平均掩模被设计成进一步排除这些干扰在深度和姿势网络的训练运动或光照变化不匹配的像素。这种方法也可以用来排除在流动净额的训练一些琐碎的不匹配的像素。最大的标准化提出了深度姿势网络的平滑项约束的无纹理区域的退化。在咬合区，作为深度和摄像机运动可以提供更可靠的运动估计，它们可以被用于指示流的无监督学习。我们在KITTI实验数据集表明，基于三个区域，闭塞的充分和明确的细分，刚性区和非刚性区域与相应的监督的损失会显著提高三个任务性能的模型。</font>
</div>


<hr>
<div id="paper55"> <b>55. Learning Depth via Interaction</b>  <a href="https://arxiv.org/pdf/2003.00752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title55" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Loquercio%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonio Loquercio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dosovitskiy%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexey Dosovitskiy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scaramuzza%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Scaramuzza</a><br>
<font size="3">
Abstract: Motivated by the astonishing capabilities of natural intelligent agents and inspired by theories from psychology, this paper explores the idea that perception gets coupled to 3D properties of the world via interaction with the environment. Existing works for depth estimation require either massive amounts of annotated training data or some form of hard-coded geometrical constraint. This paper explores a new approach to learning depth perception requiring neither of those. Specifically, we train a specialized global-local network architecture with what would be available to a robot interacting with the environment: from extremely sparse depth measurements down to even a single pixel per image. From a pair of consecutive images, our proposed network outputs a latent representation of the observer's motion between the images and a dense depth map. Experiments on several datasets show that, when ground truth is available even for just one of the image pixels, the proposed network can learn monocular dense depth estimation up to 22.5% more accurately than state-of-the-art approaches. We believe that this work, despite its scientific interest, lays the foundations to learn depth from extremely sparse supervision, which can be valuable to all robotic systems acting under severe bandwidth or sensing constraints. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由天然智能代理的惊人能力的启发，并从心理学理论的启发，本文探讨这种看法得到通过与环境的交互耦合到世界的3D性能的想法。对于深度估计现有作品需要注释的训练数据或某种形式的硬编码的几何约束的任大量。本文探讨了一种新的方法来学习的深度知觉既不需要这些的。具体来说，我们培养什么将提供给机器人与环境交互的专业全球和当地的网络架构：从极其稀疏深度测量低至每幅图像甚至单个像素。从一对连续图像的，我们所提出的网络输出的图像和稠密深度图之间的观察者的运动的潜表示。几个数据集的实验表明，当基础事实可即使对图像的像素只有一个，所提出的网络可以更准确地了解单眼密集深度估计达22.5％，比国家的最先进的方法。我们认为，这一工作，尽管它的科学兴趣，奠定了基础，从极其稀疏的监督，这可能是下严重的带宽或传感约束作用的所有机器人系统的宝贵学习的深度。</font>
</div>


<hr>
<div id="paper56"> <b>56. Long Short-Term Sample Distillation</b>  <a href="https://arxiv.org/pdf/2003.00739" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title56" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zujie Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongping Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yafang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Melo%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerard de Melo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangzhuang Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaxing Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolong Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Qi</a><br>
<font size="3">
Abstract: In the past decade, there has been substantial progress at training increasingly deep neural networks. Recent advances within the teacher--student training paradigm have established that information about past training updates show promise as a source of guidance during subsequent training steps. Based on this notion, in this paper, we propose Long Short-Term Sample Distillation, a novel training policy that simultaneously leverages multiple phases of the previous training process to guide the later training updates to a neural network, while efficiently proceeding in just one single generation pass. With Long Short-Term Sample Distillation, the supervision signal for each sample is decomposed into two parts: a long-term signal and a short-term one. The long-term teacher draws on snapshots from several epochs ago in order to provide steadfast guidance and to guarantee teacher--student differences, while the short-term one yields more up-to-date cues with the goal of enabling higher-quality updates. Moreover, the teachers for each sample are unique, such that, overall, the model learns from a very diverse set of teachers. Comprehensive experimental results across a range of vision and NLP tasks demonstrate the effectiveness of this new training method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在过去的十年里，在训练越来越深层神经网络已取得实质性进展。教师中的最新进展 - 学生培养模式已经建立了关于过去的培训更新的信息显示承诺为指导的在随后的训练步骤的来源。基于这个概念，在本文中，我们提出了长短期样品蒸馏，一种新颖的培训政策，即同时利用了以前的培训过程中的多个阶段，以指导以后的训练更新神经网络，而只是一个单一有效地进行代传。随着长短期样品蒸馏，每个样品的监管信号被分解为两个部分：一个长期的信号和短期的一个。长期的教师借鉴了几个时代的快照前，以提供坚定的指导和保障教师 - 学生的差异，而短期收益率一个更先进的最新线索有，可实现更高质量的更新的目标。此外，教师对每个样品是唯一的，这样，总体而言，从一个非常多样化的教师模型获悉。在一系列的视觉和NLP任务的综合实验结果表明，这种新的训练方法的有效性。</font>
</div>


<hr>
<div id="paper57"> <b>57. Towards Unconstrained Palmprint Recognition on Consumer Devices: a  Literature Review</b>  <a href="https://arxiv.org/pdf/2003.00737" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title57" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ungureanu%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrian-S. Ungureanu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Salahuddin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Saqib Salahuddin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Corcoran%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Corcoran</a><br>
<font size="3">
Abstract: As a biometric palmprints have been largely under-utilized, but they offer some advantages over fingerprints and facial biometrics. Recent improvements in imaging capabilities on handheld and wearable consumer devices have re-awakened interest in the use fo palmprints. The aim of this paper is to provide a comprehensive review of state-of-the-art methods for palmprint recognition including Region of Interest extraction methods, feature extraction approaches and matching algorithms along with overview of available palmprint datasets in order to understand the latest trends and research dynamics in the palmprint recognition field. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生物识别掌纹已基本得到充分利用，但他们提供了指纹和脸部生物特征一定的优势。在手持设备和可穿戴式消费设备的成像能力的最新改进在使用FO掌纹重新唤醒兴趣。本文的目的是为掌纹识别其中的感兴趣区域提取方法，特征提取方法和匹配算法，以便了解最新趋势的现有掌纹数据集的概述一起提供的先进设备，最先进的方法进行全面审查和研究动态的掌纹识别领域。</font>
</div>


<hr>
<div id="paper58"> <b>58. A-TVSNet: Aggregated Two-View Stereo Network for Multi-View Stereo Depth  Estimation</b>  <a href="https://arxiv.org/pdf/2003.00711" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title58" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sizhang Dai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weibing Huang</a><br>
<font size="3">
Abstract: We propose a learning-based network for depth map estimation from multi-view stereo (MVS) images. Our proposed network consists of three sub-networks: 1) a base network for initial depth map estimation from an unstructured stereo image pair, 2) a novel refinement network that leverages both photometric and geometric information, and 3) an attentional multi-view aggregation framework that enables efficient information exchange and integration among different stereo image pairs. The proposed network, called A-TVSNet, is evaluated on various MVS datasets and shows the ability to produce high quality depth map that outperforms competing approaches. Our code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了从多视点立体（MVS）图像深度图估计基于学习的网络。我们提出的网络由三个子网络：1）一种基础网络用于初始深度图估计从非结构化立体图像对，2）一种新颖的改进的网络，它利用光度和几何信息，以及3）一个所注意的多视图聚合框架，使不同的立体图像对之间高效的信息交换和集成。所提出的网络，称为A-TVSNet，在各种数据集MVS和节目进行评估，以生产高品质的深度图性能优于竞争的方法的能力。我们的代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper59"> <b>59. Learned Enrichment of Top-View Grid Maps Improves Object Detection</b>  <a href="https://arxiv.org/pdf/2003.00710" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title59" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wirges%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sascha Wirges</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ye Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Richter%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sven Richter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hahohao Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stiller%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christoph Stiller</a><br>
<font size="3">
Abstract: We propose an object detector for top-view grid maps which is additionally trained to generate an enriched version of its input. Our goal in the joint model is to improve generalization by regularizing towards structural knowledge in form of a map fused from multiple adjacent range sensor measurements. This training data can be generated in an automatic fashion, thus does not require manual annotations. We present an evidential framework to generate training data, investigate different model architectures and show that predicting enriched inputs as an additional task can improve object detection performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了其附加训练以产生其输入的富集版本顶视图网格地图的对象检测器。我们在关节模型的目标是通过在朝着从多个相邻的范围传感器测量稠合的映射的形式的结构知识正则化，以改善泛化。此训练数据可以以自动的方式来产生，因此不需要手动注释。我们提出的证据框架来生成训练数据，研究不同模型架构和显示，预测丰富的输入作为一个额外的任务可以提高目标探测性能。</font>
</div>


<hr>
<div id="paper60"> <b>60. Unbiased Mean Teacher for Cross Domain Object Detection</b>  <a href="https://arxiv.org/pdf/2003.00707" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title60" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinhong Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhua Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lixin Duan</a><br>
<font size="3">
Abstract: Cross domain object detection is challenging, because object detection model is often vulnerable to data variance, especially to the considerable domain shift in cross domain scenarios. In this paper, we propose a new approach called Unbiased Mean Teacher (UMT) for cross domain object detection. While the simple mean teacher (MT) model exhibits good robustness to small data variance, it can also become easily biased in cross domain scenarios. We thus improve it with several simple yet highly effective strategies. In particular, we firstly propose a novel cross domain distillation for MT to maximally exploit the expertise of the teacher model. Then, we further alleviate the bias in the student model by augmenting training samples with pixel-level adaptation. The feature level adversarial training is also incorporated to learn domain-invariant representation. Those strategies can be implemented easily into MT and leads to our unbiased MT model. Our model surpasses the existing state-of-the-art models in large margins on benchmark datasets, which demonstrates the effectiveness of our approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨域对象检测是具有挑战性的，因为对象检测模型往往易受数据方差，特别是在跨域场景中的相当大的域的转变。在本文中，我们提出了跨域对象检测叫做无偏的平均老师（UMT）的新方法。而简单平均老师（MT）模型显示出良好的鲁棒性较小的数据的方差，就可以也成为容易在跨域场景偏置。因此，我们与几个简单但非常有效的策略，提高它。特别是，我们首先提出一种新的跨域蒸馏MT最大限度地利用教师模型的专业知识。然后，我们进一步通过增加训练样本与像素级自适应减轻学生模型的偏差。该功能级别的对抗训练也纳入到学习领域不变的表示。这些策略可以很容易地实现到MT和潜在客户对我们的偏见MT模型。我们的模型优于现有的国家的最先进车型在标准数据集，这证明了我们方法的有效性大的利润。</font>
</div>


<hr>
<div id="paper61"> <b>61. GPU-Accelerated Mobile Multi-view Style Transfer</b>  <a href="https://arxiv.org/pdf/2003.00706" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title61" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kohli%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Puneet Kohli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gunaseelan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Saravana Gunaseelan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Orozco%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Orozco</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiwen Hua</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Edward Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dahlquist%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Dahlquist</a><br>
<font size="3">
Abstract: An estimated 60% of smartphones sold in 2018 were equipped with multiple rear cameras, enabling a wide variety of 3D-enabled applications such as 3D Photos. The success of 3D Photo platforms (Facebook 3D Photo, Holopix, etc) depend on a steady influx of user generated content. These platforms must provide simple image manipulation tools to facilitate content creation, akin to traditional photo platforms. Artistic neural style transfer, propelled by recent advancements in GPU technology, is one such tool for enhancing traditional photos. However, naively extrapolating single-view neural style transfer to the multi-view scenario produces visually inconsistent results and is prohibitively slow on mobile devices. We present a GPU-accelerated multi-view style transfer pipeline which enforces style consistency between views with on-demand performance on mobile platforms. Our pipeline is modular and creates high quality depth and parallax effects from a stereoscopic image pair. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：估计在2018年出售的智能手机60％配备有多个后置摄像头，可实现多种多样的3D功能的应用程序，例如3D照片。的3D照片平台（脸谱3D照片，Holopix等）的成功依赖于用户产生的内容的稳定涌入。这些平台必须提供简单的图像处理工具，以促进内容创作，类似于传统照片的平台。艺术风格的神经传递，最近的进步在GPU技术推动，是提高传统的照片一个这样的工具。然而，推断天真单视图神经样式转移到多视点方案中产生视觉不一致的结果和令人望而却步缓慢移动设备上。我们提出了一个GPU加速的多视图风格传递管道，其强制对移动平台的按需性能视图之间风格的一致性。我们的管道是模块化的，创建高品质的深度和立体图像对视差效果。</font>
</div>


<hr>
<div id="paper62"> <b>62. Relational Deep Feature Learning for Heterogeneous Face Recognition</b>  <a href="https://arxiv.org/pdf/2003.00697" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title62" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">MyeongAh Cho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taeoh Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ig-Jae Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sangyoun Lee</a><br>
<font size="3">
Abstract: Heterogeneous Face Recognition (HFR) is a task that matches faces across two different domains such as VIS (visible light), NIR (near-infrared), or the sketch domain. In contrast to face recognition in visual spectrum, because of the domain discrepancy, this task requires to extract domain-invariant feature or common space projection learning. To bridge this domain gap, we propose a graph-structured module that focuses on facial relational information to reduce the fundamental differences in domain characteristics. Since relational information is domain independent, our Relational Graph Module (RGM) performs relation modeling from node vectors that represent facial components such as lips, nose, and chin. Propagation of the generated relational graph then reduces the domain difference by transitioning from spatially correlated CNN (convolutional neural network) features to inter-dependent relational features. In addition, we propose a Node Attention Unit (NAU) that performs node-wise recalibration to focus on the more informative nodes arising from the relation-based propagation. Furthermore, we suggest a novel conditional-margin loss function (C-Softmax) for efficient projection learning on the common latent space of the embedding vector. Our module can be plugged into any pre-trained face recognition network to help overcome the limitations of a small HFR database. The proposed method shows superior performance on three different HFR databases CAISA NIR-VIS 2.0, IIIT-D Sketch, and BUAA-VisNir in various pre-trained networks. Furthermore, we explore our C-Softmax loss boosts HFR performance and also apply our loss to the large-scale visual face database LFW(Labeled Faces in Wild) by learning inter-class margins adaptively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：异构人脸识别（HFR）是一个任务跨越两个不同的域的匹配面如VIS（可见光），NIR（近红外）或草图域。在可见光谱对比脸部识别，因为域差异，此任务需要提取域不变特征或共同空间投影的学习。为了弥补这个差距域，我们建议侧重于面部关系信息，以减少域特性的根本差异的图形结构模块。由于关系信息是域独立的，我们的关系图模块（RGM）执行从关系表示面部组件如嘴唇，鼻子和下巴节点矢量建模。所产生的关系图的传播然后减少了由从空间相关的CNN（卷积神经网络）过渡域差设有于互相依赖的关系特性。此外，我们提出了一个节点注意单元（NAU），该执行节点明智重新校准聚焦从基于关系的传播所产生的更多的信息节点上。此外，我们建议用于在嵌入矢量的共同潜在空间高效投影学习一种新的有条件利润损失函数（C-使用SoftMax）。我们的模块可以插入任何预先训练脸部识别网络，以帮助克服小HFR数据库的限制。在三个不同的HFR所提出的方法示出了优越的性能数据库CAISA NIR-VIS 2.0，IIIT-d草图，和北航-VisNir各种预先训练网络。此外，我们探索我们的C-使用SoftMax损失HFR提升性能，还通过自适应学习班际利润率运用我们的损失，大型视觉人脸数据库LFW（野生标记面）。</font>
</div>


<hr>
<div id="paper63"> <b>63. Deep Image Spatial Transformation for Person Image Generation</b>  <a href="https://arxiv.org/pdf/2003.00696" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title63" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yurui Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoming Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junming Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+T+H" target="_blank" rel="noopener" style="color:#0000EE;">Thomas H. Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Ge Li</a><br>
<font size="3">
Abstract: Pose-guided person image generation is to transform a source person image to a target pose. This task requires spatial manipulations of source data. However, Convolutional Neural Networks are limited by lacking the ability to spatially transform the inputs. In this paper, we propose a differentiable global-flow local-attention framework to reassemble the inputs at the feature level. Specifically, our model first calculates the global correlations between sources and targets to predict flow fields. Then, the flowed local patch pairs are extracted from the feature maps to calculate the local attention coefficients. Finally, we warp the source features using a content-aware sampling method with the obtained local attention coefficients. The results of both subjective and objective experiments demonstrate the superiority of our model. Besides, additional results in video animation and view synthesis show that our model is applicable to other tasks requiring spatial transformation. Our source code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：姿态引导人的图像生成是一个源人图像转换到目标姿势。此任务需要的源数据的空间操作。然而，卷积神经网络是由缺乏在空间上变换的输入的能力的限制。在本文中，我们提出了一个微全球流动局部注意力框架在功能层面重新组合的输入。具体来说，我们的模型首先计算源和目标之间的相关性的全球预测流场。然后，流入当地的补丁对提取从特征映射到计算局部注意力系数。最后，我们用经编用所获得的本地注意力系数的内容感知抽样法源功能。主观和客观实验的结果表明我们的模型的优越性。此外，在视频动画和视图合成表明我们的模型是适用于需要空间变换等任务的其他结果。我们的源代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper64"> <b>64. SketchGCN: Semantic Sketch Segmentation with Graph Convolutional  Networks</b>  <a href="https://arxiv.org/pdf/2003.00678" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title64" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lumin Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiajie Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongbo Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youyi Zheng</a><br>
<font size="3">
Abstract: We introduce SketchGCN, a graph convolutional neural network for semantic segmentation and labeling of free-hand sketches. We treat an input sketch as a 2D pointset, and encode the stroke structure information into graph node/edge representations. To predict the per-point labels, our SketchGCN uses graph convolution and a global-local branching network architecture to extract both intra-stroke and inter-stroke features. SketchGCN significantly improves the accuracy of the state-of-the-art methods for semantic sketch segmentation (by 11.4% in the pixel-basedmetric and 18.2% in the component-based metric over a large-scale challenging SPG dataset) and has magnitudes fewer parameters than both image-based and sequence-based methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍SketchGCN，对于语义分割和手绘草图标注图形卷积神经网络。我们把一个输入草图作为2D点集，并编码笔画结构信息转换成图形节点/边缘表示。为了预测每点的标签，我们SketchGCN使用图形的卷积和全球本地分支网络架构，同时抽取内部行程和笔划间的功能。 SketchGCN显著改善的国家的最先进的方法语义草图分割的精确度（在11.4％的像素basedmetric在基于组件在大规模度量挑战SPG的数据集和18.2％），并且具有幅度更少参数比都基于序列基于图像和方法。</font>
</div>


<hr>
<div id="paper65"> <b>65. Global Context-Aware Progressive Aggregation Network for Salient Object  Detection</b>  <a href="https://arxiv.org/pdf/2003.00651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title65" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zuyao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianqian Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cong%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Runmin Cong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingming Huang</a><br>
<font size="3">
Abstract: Deep convolutional neural networks have achieved competitive performance in salient object detection, in which how to learn effective and comprehensive features plays a critical role. Most of the previous works mainly adopted multiple level feature integration yet ignored the gap between different features. Besides, there also exists a dilution process of high-level features as they passed on the top-down pathway. To remedy these issues, we propose a novel network named GCPANet to effectively integrate low-level appearance features, high-level semantic features, and global context features through some progressive context-aware Feature Interweaved Aggregation (FIA) modules and generate the saliency map in a supervised way. Moreover, a Head Attention (HA) module is used to reduce information redundancy and enhance the top layers features by leveraging the spatial and channel-wise attention, and the Self Refinement (SR) module is utilized to further refine and heighten the input features. Furthermore, we design the Global Context Flow (GCF) module to generate the global context information at different stages, which aims to learn the relationship among different salient regions and alleviate the dilution effect of high-level features. Experimental results on six benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-art methods both quantitatively and qualitatively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深卷积神经网络已取得显着的物体探测竞争力的性能，在如何学习有效和全面的功能起着至关重要的作用。大部分以前的作品主要采用多级功能集成还忽略不同功能之间的差距。此外，还存在，因为他们在自上而下的途径通过高层次的稀释处理功能。为了解决这些问题，我们提出了一个名为GCPANet有效整合低级别的外观特征，高层语义特征的新型网络，以及全球范围内通过一些渐进的上下文感知功能交织聚合（FIA）模块的功能，并产生显着图一个监督方式。此外，头注意（HA）模块用于减少信息冗余和提高的顶层通过利用空间和通道明智关注特征和自我细化（SR）模块被用来进一步细化，并且可以提高输入的功能。此外，我们设计的全球背景下的流量（GCF）模块，以产生不同阶段的全球环境信息，其目的是了解不同的显着区域之间的关系，并缓解高层次功能的稀释效应。在六个基准数据集的实验结果表明，所提出的方法在数量和质量优于国家的最先进的方法。</font>
</div>


<hr>
<div id="paper66"> <b>66. VAE/WGAN-Based Image Representation Learning For Pose-Preserving  Seamless Identity Replacement In Facial Images</b>  <a href="https://arxiv.org/pdf/2003.00641" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title66" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kawai%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiroki Kawai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiawei Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ishwar%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prakash Ishwar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Konrad%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Janusz Konrad</a><br>
<font size="3">
Abstract: We present a novel variational generative adversarial network (VGAN) based on Wasserstein loss to learn a latent representation from a face image that is invariant to identity but preserves head-pose information. This facilitates synthesis of a realistic face image with the same head pose as a given input image, but with a different identity. One application of this network is in privacy-sensitive scenarios; after identity replacement in an image, utility, such as head pose, can still be recovered. Extensive experimental validation on synthetic and real human-face image datasets performed under 3 threat scenarios confirms the ability of the proposed network to preserve head pose of the input image, mask the input identity, and synthesize a good-quality realistic face image of a desired identity. We also show that our network can be used to perform pose-preserving identity morphing and identity-preserving pose morphing. The proposed method improves over a recent state-of-the-art method in terms of quantitative metrics as well as synthesized image quality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提出了一种基于Wasserstein的损失要学会从人脸图像是不变的身份，但保持净空，姿态信息的潜表示一种新型变生成对抗网络（VGAN）。这有利于合成具有相同的头部姿势作为给定的输入图像逼真面部图像的，但具有不同的标识。这个网络的一个应用是隐私敏感的情况;标识替换在图像，实用程序后，如头部的姿势，仍可以恢复。 3个威胁情景确认执行对合成和真实的人脸图像数据集大量实验验证所提出的网络来保存输入图像的头部姿势，屏蔽输入身份和合成所需的优质逼真的人脸图像的能力身份。我们还表明，我们的网络可以用于执行姿势保留身份变形和身份保持姿势变形。所提出的方法改进了在定量的度量以及合成图像质量方面最近的状态的最先进的方法。</font>
</div>


<hr>
<div id="paper67"> <b>67. A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view  Stereo Reconstruction from An Open Aerial Dataset</b>  <a href="https://arxiv.org/pdf/2003.00637" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title67" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shunping Ji</a><br>
<font size="3">
Abstract: A great deal of research has demonstrated recently that multi-view stereo (MVS) matching can be solved with deep learning methods. However, these efforts were focused on close-range objects and only a very few of the deep learning-based methods were specifically designed for large-scale 3D urban reconstruction due to the lack of multi-view aerial image benchmarks. In this paper, we present a synthetic aerial dataset, called the WHU dataset, we created for MVS tasks, which, to our knowledge, is the first large-scale multi-view aerial dataset. It was generated from a highly accurate 3D digital surface model produced from thousands of real aerial images with precise camera parameters. We also introduce in this paper a novel network, called RED-Net, for wide-range depth inference, which we developed from a recurrent encoder-decoder structure to regularize cost maps across depths and a 2D fully convolutional network as framework. RED-Net's low memory requirements and high performance make it suitable for large-scale and highly accurate 3D Earth surface reconstruction. Our experiments confirmed that not only did our method exceed the current state-of-the-art MVS methods by more than 50% mean absolute error (MAE) with less memory and computational cost, but its efficiency as well. It outperformed one of the best commercial software programs based on conventional methods, improving their efficiency 16 times over. Moreover, we proved that our RED-Net model pre-trained on the synthetic WHU dataset can be efficiently transferred to very different multi-view aerial image datasets without any fine-tuning. Dataset are available at this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：研究了大量已经证明最近，多视点立体（MVS）匹配可以用深的学习方法来解决。然而，这些努力都集中在近距离的物体，只有极少数的，由于缺乏深基于学习的方法是专门为大型3D城市改造设计的多视角航空影像基准。在本文中，我们提出了一个合成的空中数据集，叫做WHU数据集，我们为MVS任务，其中，据我们所知，是第一次大规模的多视图空中数据集创建。它是从从数千具有精确的相机参数真实空间图像的产生高度精确的3D数字表面模型生成的。我们还介绍了在本文提出了一种新颖的网络，所谓的RED-Net的，对于大范围的深度推断，这是我们从一个经常性的编码器，解码器结构，开发成本正规化跨深度和二维卷积完全网络架构映射。 RED-Net的低内存需求和高性能使其适用于大型和高度精确的3D地球表面重建。我们的实验证实，不仅没有我们的方法，超过50％以上，平均绝对误差（MAE），使用较少的内存和计算成本，但它的效率，以及当前国家的最先进的MVS方法。它优于基于传统的方法最好的商业软件之一，提高其效率超过16倍。此外，我们证明了在合成WHU数据集可以被有效地传递到非常不同的多视点空中影像的数据集而没有任何的微调，我们的RED-Net的预训练的模型。数据集可在此http网址。</font>
</div>


<hr>
<div id="paper68"> <b>68. Matching Neuromorphic Events and Color Images via Adversarial Learning</b>  <a href="https://arxiv.org/pdf/2003.00636" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title68" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shijie Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dengxin Dai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gui-song Xia</a><br>
<font size="3">
Abstract: The event camera has appealing properties: high dynamic range, low latency, low power consumption and low memory usage, and thus provides complementariness to conventional frame-based cameras. It only captures the dynamics of a scene and is able to capture almost "continuous" motion. However, different from frame-based camera that reflects the whole appearance as scenes are, the event camera casts away the detailed characteristics of objects, such as texture and color. To take advantages of both modalities, the event camera and frame-based camera are combined together for various machine vision tasks. Then the cross-modal matching between neuromorphic events and color images plays a vital and essential role. In this paper, we propose the Event-Based Image Retrieval (EBIR) problem to exploit the cross-modal matching task. Given an event stream depicting a particular object as query, the aim is to retrieve color images containing the same object. This problem is challenging because there exists a large modality gap between neuromorphic events and color images. We address the EBIR problem by proposing neuromorphic Events-Color image Feature Learning (ECFL). Particularly, the adversarial learning is employed to jointly model neuromorphic events and color images into a common embedding space. We also contribute to the community N-UKbench and EC180 dataset to promote the development of EBIR problem. Extensive experiments on our datasets show that the proposed method is superior in learning effective modality-invariant representation to link two different modalities. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事件相机具有吸引人的性能：高动态范围，低等待时间，低功率消耗和低的内存使用情况，并因此提供互补性常规的基于帧的摄像机。它仅捕获场景的动态，并能捕捉到几乎是“连续”的议案。然而，从反映整个外观场景是基于帧的相机不同，事件相机擅自抛弃对象，例如质地和颜色特征的详细资料。采取两种模态的优点，事件照相机和基于帧的相机被用于各种机器视觉任务结合在一起。然后，神经运动赛事和彩色图像之间的跨模态匹配起着至关重要和必不可少的作用。在本文中，我们提出了基于事件的图像检索（EBIR）问题，利用跨模态匹配任务。给定的事件流描绘的特定对象作为查询，其目的是检索包含相同的对象的彩色图像。这个问题是有挑战性的，因为存在神经运动赛事和彩色图像之间存在较大的差距形态。我们通过提出神经运动活动彩色图像地物学习（ECFL）解决EBIR问题。特别是，对抗性学习采用联合模型神经运动赛事和彩色图像到一个共同的嵌入空间。我们回馈社会的N- UKbench和EC180数据集，以促进EBIR问题的发展。我们的数据集，大量实验表明，该方法是在学习有效方式不变表示连接两个不同的模式优越。</font>
</div>


<hr>
<div id="paper69"> <b>69. Extremely Dense Point Correspondences using a Learned Feature Descriptor</b>  <a href="https://arxiv.org/pdf/2003.00619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title69" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xingtong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiping Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Killeen%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin Killeen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ishii%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masaru Ishii</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hager%2C+G+D" target="_blank" rel="noopener" style="color:#0000EE;">Gregory D. Hager</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Taylor%2C+R+H" target="_blank" rel="noopener" style="color:#0000EE;">Russell H. Taylor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Unberath%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mathias Unberath</a><br>
<font size="3">
Abstract: High-quality 3D reconstructions from endoscopy video play an important role in many clinical applications, including surgical navigation where they enable direct video-CT registration. While many methods exist for general multi-view 3D reconstruction, these methods often fail to deliver satisfactory performance on endoscopic video. Part of the reason is that local descriptors that establish pair-wise point correspondences, and thus drive reconstruction, struggle when confronted with the texture-scarce surface of anatomy. Learning-based dense descriptors usually have larger receptive fields enabling the encoding of global information, which can be used to disambiguate matches. In this work, we present an effective self-supervised training scheme and novel loss design for dense descriptor learning. In direct comparison to recent local and dense descriptors on an in-house sinus endoscopy dataset, we demonstrate that our proposed dense descriptor can generalize to unseen patients and scopes, thereby largely improving the performance of Structure from Motion (SfM) in terms of model density and completeness. We also evaluate our method on a public dense optical flow dataset and a small-scale SfM public dataset to further demonstrate the effectiveness and generality of our method. The source code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：高品质的视频内窥镜三维重建在许多临床应用，包括外科手术导航，他们能直接视频-CT登记中发挥重要作用。虽然一般的多视图三维重建中存在许多方法，这些方法往往不能兑现内窥镜检查视频表现令人满意。的部分原因是，当与解剖结构的纹理表面稀缺面临的是建立成对点对应，因此局部描述符驱动重建，斗争。学习基于致密描述符通常具有较大的感受野使全球信息，它可以用来消除匹配的编码。在这项工作中，我们提出了密集的描述符学习有效的自我指导训练方案和新颖的设计损失。在直接比较近期本地和密集的描述在一个内部的鼻窦内窥镜数据集，我们证明了我们提出的密集描述符可以推广到看不见的患者和范围，从而大大提高了模型密度方面结构从运动（SFM）的性能和完整性。我们也评估我们在公共密集光流数据集和小规模的SFM公共数据集的方法来进一步证明了该方法的有效性和普遍性。源代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper70"> <b>70. 3D Point Cloud Processing and Learning for Autonomous Driving</b>  <a href="https://arxiv.org/pdf/2003.00601" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title70" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baoan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vallespi-Gonzalez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Vallespi-Gonzalez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wellington%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carl Wellington</a><br>
<font size="3">
Abstract: We present a review of 3D point cloud processing and learning for autonomous driving. As one of the most important sensors in autonomous vehicles, light detection and ranging (LiDAR) sensors collect 3D point clouds that precisely record the external surfaces of objects and scenes. The tools for 3D point cloud processing and learning are critical to the map creation, localization, and perception modules in an autonomous vehicle. While much attention has been paid to data collected from cameras, such as images and videos, an increasing number of researchers have recognized the importance and significance of LiDAR in autonomous driving and have proposed processing and learning algorithms to exploit 3D point clouds. We review the recent progress in this research area and summarize what has been tried and what is needed for practical and safe autonomous vehicles. We also offer perspectives on open issues that are needed to be solved in the future. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出三维点云处理的回顾和学习自主驾驶。作为自主车，光探测和测距的最重要的传感器之一（LIDAR）传感器采集的三维点云能够精确记录物体和场景的外表面。三维点云处理和学习的工具是地图制作，本地化，并在自动车辆感知模块的关键。而备受人们关注已经支付给从摄像头采集到的数据，如图像和视频，越来越多的研究人员已经认识到激光雷达的重要性和意义的自主驾驶，并提出了处理和学习算法利用三维点云。我们回顾在这一研究领域的最新进展，并总结一下已经尝试过的，什么是需要的实用，安全的自动驾驶汽车。我们还提供所需要在未来要解决的开放性问题的观点。</font>
</div>


<hr>
<div id="paper71"> <b>71. Rethinking Fully Convolutional Networks for the Analysis of  Photoluminescence Wafer Images</b>  <a href="https://arxiv.org/pdf/2003.00594" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title71" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M+L" target="_blank" rel="noopener" style="color:#0000EE;">Maike Lorena Stern</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lindberg%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hans Lindberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meyer-Wegener%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Klaus Meyer-Wegener</a><br>
<font size="3">
Abstract: The manufacturing of light-emitting diodes is a complex semiconductor-manufacturing process, interspersed with different measurements. Among the employed measurements, photoluminescence imaging has several advantages, namely being a non-destructive, fast and thus cost-effective measurement. On a photoluminescence measurement image of an LED wafer, every pixel corresponds to an LED chip's brightness after photo-excitation, revealing chip performance information. However, generating a chip-fine defect map of the LED wafer, based on photoluminescence images, proves challenging for multiple reasons: on the one hand, the measured brightness values vary from image to image, in addition to local spots of differing brightness. On the other hand, certain defect structures may assume multiple shapes, sizes and brightness gradients, where salient brightness values may correspond to defective LED chips, measurement artefacts or non-defective structures. In this work, we revisit the creation of chip-fine defect maps using fully convolutional networks and show that the problem of segmenting objects at multiple scales can be improved by the incorporation of densely connected convolutional blocks and atrous spatial pyramid pooling modules. We also share implementation details and our experiences with training networks with small datasets of measurement images. The proposed architecture significantly improves the segmentation accuracy of highly variable defect structures over our previous version. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：发光二极管的制造是复杂的半导体制造过程中，随着不同的测量穿插。其中所采用的测量结果，光致发光成像具有几个优点，即是一个非破坏性的，快速并且因此成本有效的测量。在LED晶片的光致发光测定图像，每个像素对应于一个LED芯片的亮度光激发，揭示芯片的性能信息之后。然而，产生该LED晶片的芯片微细缺陷映射表，基于光致发光图像，证明出于多种原因挑战：在一方面，所测量的亮度值从图像变化到图像中，除了不同的亮度的局部斑点。在另一方面，某些缺陷结构可以采取多种形状，尺寸和亮度梯度，其中凸亮度值可以对应于缺陷的LED芯片，测量假象或无缺陷的结构。在这项工作中，我们重新访问的芯片微细缺陷创建使用完全卷积网络和显示，分割在多个尺度的对象的问题可以通过密集地连接卷积块与àtrous空间金字塔池模块的掺入来提高地图。我们还份额实现的细节，我们与测量图像的小型数据集培训网络体验。所提出的架构显著提升高度可变的缺陷结构在原有版本的分割精度。</font>
</div>


<hr>
<div id="paper72"> <b>72. Fast Lidar Clustering by Density and Connectivity</b>  <a href="https://arxiv.org/pdf/2003.00575" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title72" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hasecke%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Frederik Hasecke</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hahn%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lukas Hahn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kummert%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anton Kummert</a><br>
<font size="3">
Abstract: Lidar sensors are widely used in various applications, ranging from scientific fields over industrial use to integration in consumer products. With an ever growing number of different driver assistance systems, they have been introduced to automotive series production in recent years and are considered an important building block for the practical realisation of autonomous driving. However, due to the potentially large amount of Lidar points per scan, tailored algorithms are required to identify objects (e.g. pedestrians or vehicles) with high precision in a very short time. In this work, we propose an algorithmic approach for real-time instance segmentation of Lidar sensor data. We show how our method leverages the properties of the Euclidean distance to retain three-dimensional measurement information, while being narrowed down to a two-dimensional representation for fast computation. We further introduce what we call skip connections, to make our approach robust against over-segmentation and improve assignment in cases of partial occlusion. Through detailed evaluation on public data and comparison with established methods, we show how these aspects enable state-of-the-art performance and runtime on a single CPU core. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：激光雷达传感器被广泛用于各种应用，从对工业用在消费类产品整合科研领域。随着数量不断增长的不同的驾驶辅助系统，它们已经被引入到汽车系列产品的生产，近年来，被认为是为自主驾驶的实际实现的一个重要组成部分。然而，由于潜在的大量的每次扫描激光雷达点，需要定制算法来识别在一个很短的时间高精度对象（例如行人或车辆）。在这项工作中，我们提出了激光雷达传感器数据的实时情况下分割的算法方法。我们证明我们的方法如何利用欧氏距离保持三维测量信息的特性，同时缩小到了快速计算二维表示。我们进一步介绍我们称之为跳过连接，使我们的方法对过度分割健壮，提高部分遮挡的情况下分配。通过对公共数据和既定方法的比较详细的评测中，我们展示这些方面如何能够在一个单一的CPU核状态的最先进的性能和运行。</font>
</div>


<hr>
<div id="paper73"> <b>73. The Sloop System for Individual Animal Identification with Deep Learning</b>  <a href="https://arxiv.org/pdf/2003.00559" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title73" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bakliwal%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kshitij Bakliwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ravela%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sai Ravela</a><br>
<font size="3">
Abstract: The MIT Sloop system indexes and retrieves photographs from databases of non-stationary animal population distributions. To do this, it adaptively represents and matches generic visual feature representations using sparse relevance feedback from experts and crowds. Here, we describe the Sloop system and its application, then compare its approach to a standard deep learning formulation. We then show that priming with amplitude and deformation features requires very shallow networks to produce superior recognition results. Results suggest that relevance feedback, which enables Sloop's high-recall performance may also be essential for deep learning approaches to individual identification to deliver comparable results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：MIT桅系统的索引和检索照片从非平稳动物种群分布的数据库。要做到这一点，它适应性代表，并使用专家和人群稀疏的相关反馈通用的视觉特征相匹配的表示。在这里，我们描述的单桅帆船系统及其应用，那么它的方法比较标准的深度学习配方。然后，我们显示出与幅度吸和变形功能需要很浅的网络生产出卓越的识别结果。结果表明，相关反馈，使桅高召回的表现也可能对深学习必不可少接近个体识别提供可比较的结果。</font>
</div>


<hr>
<div id="paper74"> <b>74. Soft-Root-Sign Activation Function</b>  <a href="https://arxiv.org/pdf/2003.00547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title74" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dandan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuwei Huo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kung%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sun-Yuan Kung</a><br>
<font size="3">
Abstract: The choice of activation function in deep networks has a significant effect on the training dynamics and task performance. At present, the most effective and widely-used activation function is ReLU. However, because of the non-zero mean, negative missing and unbounded output, ReLU is at a potential disadvantage during optimization. To this end, we introduce a novel activation function to manage to overcome the above three challenges. The proposed nonlinearity, namely "Soft-Root-Sign" (SRS), is smooth, non-monotonic, and bounded. Notably, the bounded property of SRS distinguishes itself from most state-of-the-art activation functions. In contrast to ReLU, SRS can adaptively adjust the output by a pair of independent trainable parameters to capture negative information and provide zero-mean property, which leading not only to better generalization performance, but also to faster learning speed. It also avoids and rectifies the output distribution to be scattered in the non-negative real number space, making it more compatible with batch normalization (BN) and less sensitive to initialization. In experiments, we evaluated SRS on deep networks applied to a variety of tasks, including image classification, machine translation and generative modelling. Our SRS matches or exceeds models with ReLU and other state-of-the-art nonlinearities, showing that the proposed activation function is generalized and can achieve high performance across tasks. Ablation study further verified the compatibility with BN and self-adaptability for different initialization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：激活功能的深层网络选择对的培训力度和任务性能显著的效果。目前，最有效和广泛使用的活化功能RELU。但是，因为非零均值，负面失踪，无限产出，RELU是在优化过程中一个潜在的缺点。为此，我们引入了一个新的激活功能，以设法克服上述三大挑战。所提出的非线性，即“软根符号”（SRS），光滑，非单调和有界的。值得注意的是，SRS的有界属性从国家的最先进的最激活函数中脱颖而出。与此相反RELU，SRS可以自适应由一对独立的可训练参数，以获取负面信息调整输出，并提供零均值特性，这不仅导致更好的泛化性能，而且能够更快的学习速度。它还避免和整流被散射在非负实数空间中的输出分配，使得它与批标准化（BN），以及初始化不太敏感更相容。在实验中，我们评估了适用于多种任务，包括图像分类，机器翻译和生成模拟的深网络SRS。我们的SRS匹配或超过机型RELU和国家的最先进的非线性等，表明所提出的激活功能是广义的，可以实现跨任务的高性能。消融研究进一步验证与国阵不同的初始化的相容性和自适应性。</font>
</div>


<hr>
<div id="paper75"> <b>75. ZoomNet: Part-Aware Adaptive Zooming Neural Network for 3D Object  Detection</b>  <a href="https://arxiv.org/pdf/2003.00529" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title75" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenbo Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoqing Ye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shilei Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Errui Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ajin Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liusheng Huang</a><br>
<font size="3">
Abstract: 3D object detection is an essential task in autonomous driving and robotics. Though great progress has been made, challenges remain in estimating 3D pose for distant and occluded objects. In this paper, we present a novel framework named ZoomNet for stereo imagery-based 3D detection. The pipeline of ZoomNet begins with an ordinary 2D object detection model which is used to obtain pairs of left-right bounding boxes. To further exploit the abundant texture cues in RGB images for more accurate disparity estimation, we introduce a conceptually straight-forward module -- adaptive zooming, which simultaneously resizes 2D instance bounding boxes to a unified resolution and adjusts the camera intrinsic parameters accordingly. In this way, we are able to estimate higher-quality disparity maps from the resized box images then construct dense point clouds for both nearby and distant objects. Moreover, we introduce to learn part locations as complementary features to improve the resistance against occlusion and put forward the 3D fitting score to better estimate the 3D detection quality. Extensive experiments on the popular KITTI 3D detection dataset indicate ZoomNet surpasses all previous state-of-the-art methods by large margins (improved by 9.4% on APbv (IoU=0.7) over pseudo-LiDAR). Ablation study also demonstrates that our adaptive zooming strategy brings an improvement of over 10% on AP3d (IoU=0.7). In addition, since the official KITTI benchmark lacks fine-grained annotations like pixel-wise part locations, we also present our KFG dataset by augmenting KITTI with detailed instance-wise annotations including pixel-wise part location, pixel-wise disparity, etc.. Both the KFG dataset and our codes will be publicly available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：3D物体的检测是在自动驾驶和机器人的一项重要任务。虽然巨大的已经取得了进展，挑战依然存在估计3D姿势的遥远而闭塞的对象。在本文中，我们提出了一个名为ZoomNet立体图像为基础的，3D检测一个新的框架。 ZoomNet的流水线开始其用于获得对左右边界框普通2D对象检测模型。为了进一步发挥RGB图像丰富的纹理线索更精确的视差估计，我们引入一个概念直接的模块 - 自适应缩放，它同时重新调整2D情况下边框为统一的分辨率和内在的参数进行相应调节摄像机。通过这种方式，我们可以从调整大小后的图片框估计更高质量的差异图然后构造致密的点云的附近和远处的两个对象。此外，我们介绍学习部分位置的补充功能，以提高对闭塞性，并提出了3D拟合得分，以更好地估计三维检测质量。上流行的3D KITTI检测数据集广泛的实验表明ZoomNet超过先前的所有国家的最先进的方法，通过大边距（9.4％上APbv（IOU = 0.7）以上的伪激光雷达改善）。消融研究还表明，我们的自适应缩放策略带来了超过10％的AP3d（IOU = 0.7）的改善。此外，由于官方KITTI基准缺乏细粒度的注解像逐像素的部分地区，我们也有详细的实例明智的注解，包括逐像素部分的位置，像素方面的差距，增强等KITTI提出我们的KFG数据集..无论是KFG数据集，我们的代码将公开可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper76"> <b>76. Deep Attention Aware Feature Learning for Person Re-Identification</b>  <a href="https://arxiv.org/pdf/2003.00517" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title76" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yifan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolu Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Fan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chu Tang</a><br>
<font size="3">
Abstract: Visual attention has proven to be effective in improving the performance of person re-identification. Most existing methods apply visual attention heuristically by learning an additional attention map to re-weight the feature maps for person re-identification. However, this kind of methods inevitably increase the model complexity and inference time. In this paper, we propose to incorporate the attention learning as additional objectives in a person ReID network without changing the original structure, thus maintain the same inference time and model size. Two kinds of attentions have been considered to make the learned feature maps being aware of the person and related body parts respectively. Globally, a holistic attention branch (HAB) makes the feature maps obtained by backbone focus on persons so as to alleviate the influence of background. Locally, a partial attention branch (PAB) makes the extracted features be decoupled into several groups and be separately responsible for different body parts (i.e., keypoints), thus increasing the robustness to pose variation and partial occlusion. These two kinds of attentions are universal and can be incorporated into existing ReID networks. We have tested its performance on two typical networks (TriNet and Bag of Tricks) and observed significant performance improvement on five widely used datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉注意力已被证明是有效地改善人重新鉴定的性能。大多数现有的方法通过学习额外注意图重新重量的功能对于人重新鉴定映射启发式应用视觉注意力。然而，这种方法不可避免地增加了模型的复杂性和推理时间。在本文中，我们建议纳入关注学习作为一个人里德网络的其他目标，在不改变原有结构，从而保持相同的推理时间和模型的大小。两种关注已考虑到使学习地物地图分别为了解人及相关身体部位。在全球范围内，全方位关注分支（HAB）使得特征映射由骨干重点人员所获得的，以减轻背景的影响。局部，局部关注分支（PAB），使所提取的特征被解耦成几组，并对于不同的身体部位（即，关键点）分别负责，因此增加了鲁棒性姿势变化和部分遮挡。这两种关注的是普遍的，可纳入现有的里德网络。我们已经测试了两种典型的网络性能（TriNet和技巧的袋），并在五个广泛使用的数据集观测显著的性能提升。</font>
</div>


<hr>
<div id="paper77"> <b>77. MonoPair: Monocular 3D Object Detection Using Pairwise Spatial  Relationships</b>  <a href="https://arxiv.org/pdf/2003.00504" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title77" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongjian Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tai%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Tai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingyang Li</a><br>
<font size="3">
Abstract: Monocular 3D object detection is an essential component in autonomous driving while challenging to solve, especially for those occluded samples which are only partially visible. Most detectors consider each 3D object as an independent training target, inevitably resulting in a lack of useful information for occluded samples. To this end, we propose a novel method to improve the monocular 3D object detection by considering the relationship of paired samples. This allows us to encode spatial constraints for partially-occluded objects from their adjacent neighbors. Specifically, the proposed detector computes uncertainty-aware predictions for object locations and 3D distances for the adjacent object pairs, which are subsequently jointly optimized by nonlinear least squares. Finally, the one-stage uncertainty-aware prediction structure and the post-optimization module are dedicatedly integrated for ensuring the run-time efficiency. Experiments demonstrate that our method yields the best performance on KITTI 3D detection benchmark, by outperforming state-of-the-art competitors by wide margins, especially for the hard samples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：单眼立体物检测是在自动驾驶的必要成分而具有挑战性的解决，特别是对那些遮挡样品其是仅部分可见。大多数检测器考虑的每个3D对象作为一个独立的训练目标，不可避免地导致缺乏用于遮挡样品的有用信息。为此，我们提出了通过考虑成对样品的关系来改善单眼立体物检测的新方法。这使我们能够编码空间限制从它们相邻的邻居部分遮挡的对象。具体地，所提出的检测器计算用于物体位置和3D距离为相邻对象对，其随后通过共同非线性最小二乘优化不确定感知预测。最后，一个阶段的不确定性感知预测结构和后优化模块的专用地集成，以确保运行时的效率。实验表明，我们的方法产生的KITTI 3D检测基准性能最好，通过广泛的利润率跑赢国家的最先进的竞争对手，尤其是对硬盘的样品。</font>
</div>


<hr>
<div id="paper78"> <b>78. PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks  with Adaptive Sampling</b>  <a href="https://arxiv.org/pdf/2003.00492" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title78" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xu Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaoda Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sheng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuguang Cui</a><br>
<font size="3">
Abstract: Raw point clouds data inevitably contains outliers or noise through acquisition from 3D sensors or reconstruction algorithms. In this paper, we present a novel end-to-end network for robust point clouds processing, named PointASNL, which can deal with point clouds with noise effectively. The key component in our approach is the adaptive sampling (AS) module. It first re-weights the neighbors around the initial sampled points from farthest point sampling (FPS), and then adaptively adjusts the sampled points beyond the entire point cloud. Our AS module can not only benefit the feature learning of point clouds, but also ease the biased effect of outliers. To further capture the neighbor and long-range dependencies of the sampled point, we proposed a local-nonlocal (L-NL) module inspired by the nonlocal operation. Such L-NL module enables the learning process insensitive to noise. Extensive experiments verify the robustness and superiority of our approach in point clouds processing tasks regardless of synthesis data, indoor data, and outdoor data with or without noise. Specifically, PointASNL achieves state-of-the-art robust performance for classification and segmentation tasks on all datasets, and significantly outperforms previous methods on real-world outdoor SemanticKITTI dataset with considerate noise. Our code is released through this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：原始点云数据不可避免地含有异常值或噪声通过从三维传感器或重建算法获取。在本文中，我们提出了一个新颖的端至端的网络的鲁棒点云处理，命名PointASNL，其可以处理点云与噪声有效。在我们的方法的关键组件是自适应采样（AS）模块。它首先重新权重绕初始采样点邻居从最远点取样（FPS），然后自适应地调整超出整个点云的采样的点。我们作为模块既可以造福点云的特征学习，还能缓解异常值的偏差影响。为了进一步捕获采样点的邻居和远程相关性，我们提出了通过非局部操作的启发局部非局部（L-NL）模块。这种L-NL模块使学习过程对噪声不敏感。大量的实验验证我们的方法的稳健性和优越性在点云有无杂音的处理任务，无论合成数据，室内数据，和室外的数据。具体来说，PointASNL实现对所有数据集的分类和分割任务的国家的最先进的强劲表现，以及显著优于现实世界与体贴噪音以前的方法户外SemanticKITTI数据集。我们的代码是通过这个HTTPS URL释放。</font>
</div>


<hr>
<div id="paper79"> <b>79. State-Aware Tracker for Real-Time Video Object Segmentation</b>  <a href="https://arxiv.org/pdf/2003.00482" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title79" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xi Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zuoxin Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ye Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gang Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianxin Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Donglian Qi</a><br>
<font size="3">
Abstract: In this work, we address the task of semi-supervised video object segmentation(VOS) and explore how to make efficient use of video property to tackle the challenge of semi-supervision. We propose a novel pipeline called State-Aware Tracker(SAT), which can produce accurate segmentation results with real-time speed. For higher efficiency, SAT takes advantage of the inter-frame consistency and deals with each target object as a tracklet. For more stable and robust performance over video sequences, SAT gets awareness for each state and makes self-adaptation via two feedback loops. One loop assists SAT in generating more stable tracklets. The other loop helps to construct a more robust and holistic target representation. SAT achieves a promising result of 72.3% J&F mean with 39 FPS on DAVIS2017-Val dataset, which shows a decent trade-off between efficiency and accuracy. Code will be released at this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们要解决半监督视频对象分割（VOS）的任务，并探讨如何有效利用视频性能的解决半监督的挑战。我们提出了一个新颖的管道叫状态感知追踪（SAT），能够生成实时的速度准确的分割结果。对于更高的效率，SAT利用了帧间的一致性以及与每个目标对象作为tracklet交易。对于更稳定，在视频序列强劲的性能，SAT取得意识，为每个状态，并通过两个反馈回路使自适应性。一个回路辅助SAT的产生更稳定tracklets。其他环有助于构建一个更强大和全面的目标表示。 SAT实现了72.3％有希望的结果J＆F平均39 FPS上DAVIS2017-VAL数据集，其示出了一个体面折衷效率和准确性之间。代码将在这个HTTP URL被释放。</font>
</div>


<hr>
<div id="paper80"> <b>80. STC-Flow: Spatio-temporal Context-aware Optical Flow Estimation</b>  <a href="https://arxiv.org/pdf/2003.00434" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title80" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuyang Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingyu Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cuiling Lan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenjun Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiahao Li</a><br>
<font size="3">
Abstract: In this paper, we propose a spatio-temporal contextual network, STC-Flow, for optical flow estimation. Unlike previous optical flow estimation approaches with local pyramid feature extraction and multi-level correlation, we propose a contextual relation exploration architecture by capturing rich long-range dependencies in spatial and temporal dimensions. Specifically, STC-Flow contains three key context modules - pyramidal spatial context module, temporal context correlation module and recurrent residual contextual upsampling module, to build the relationship in each stage of feature extraction, correlation, and flow reconstruction, respectively. Experimental results indicate that the proposed scheme achieves the state-of-the-art performance of two-frame based methods on the Sintel dataset and the KITTI 2012/2015 datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个时空情境网络，STC流量，用于光流估计。不同于以往的光流估计与当地的金字塔特征提取和多层次的相关办法，我们提出通过捕捉空间和时间维度丰富的远射依赖上下文关系的探索架构。具体地讲，STC-流包含三个密钥上下文模块 - 锥体空间上下文模块，时间上下文相关模块和复发性残余上下文上采样模块，建立在特征提取，相关的各阶段的关系，并分别流重建。实验结果表明，所提出的方案实现了对辛特尔数据集和KITTI二千○一十五分之二千○十二数据集两帧为基础的方法的状态的最先进的性能。</font>
</div>


<hr>
<div id="paper81"> <b>81. Deep Learning for Content-based Personalized Viewport Prediction of  360-Degree VR Videos</b>  <a href="https://arxiv.org/pdf/2003.00429" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title81" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinwei Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kasgari%2C+A+T+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ali Taleb Zadeh Kasgari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saad%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Walid Saad</a><br>
<font size="3">
Abstract: In this paper, the problem of head movement prediction for virtual reality videos is studied. In the considered model, a deep learning network is introduced to leverage position data as well as video frame content to predict future head movement. For optimizing data input into this neural network, data sample rate, reduced data, and long-period prediction length are also explored for this model. Simulation results show that the proposed approach yields 16.1\% improvement in terms of prediction accuracy compared to a baseline approach that relies only on the position data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，头部运动预测虚拟现实视频的问题进行了研究。在所考虑的模型，深度学习网络引入杠杆位置数据以及视频帧的内容来预测未来的头部运动。为了优化数据输入到该神经网络，数据采样率，减少的数据，和长周期预测长度也探索了这种模式。仿真结果表明，在预测精度方面提出的方法的产率16.1 \％的改进相比于仅依赖于所述位置数据的基线的方法。</font>
</div>


<hr>
<div id="paper82"> <b>82. Learning When and Where to Zoom with Deep Reinforcement Learning</b>  <a href="https://arxiv.org/pdf/2003.00425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title82" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Uzkent%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Burak Uzkent</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ermon%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefano Ermon</a><br>
<font size="3">
Abstract: While high resolution images contain semantically more useful information than their lower resolution counterparts, processing them is computationally more expensive, and in some applications, e.g. remote sensing, they can be much more expensive to acquire. For these reasons, it is desirable to develop an automatic method to selectively use high resolution data when necessary while maintaining accuracy and reducing acquisition/run-time cost. In this direction, we propose PatchDrop a reinforcement learning approach to dynamically identify when and where to use/acquire high resolution data conditioned on the paired, cheap, low resolution images. We conduct experiments on CIFAR10, CIFAR100, ImageNet and fMoW datasets where we use significantly less high resolution data while maintaining similar accuracy to models which use full high resolution images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然高分辨率图像包含比其更低的分辨率对应语义更加有用的信息，处理它们在计算上更昂贵的，并且在一些应用中，例如遥感，它们可以是更昂贵的获取。由于这些原因，理想的是选择性地使用高分辨率数据，同时保持精确度和减少采集/运行时间成本，必要时，开发的自动方法。在这个方向，我们提出PatchDrop增强学习的方法来动态识别何时何地使用空调，在一对，价格便宜，低分辨率图像/获取高分辨率的数据。我们对CIFAR10，CIFAR100，ImageNet和fMoW数据集进行实验，我们用显著较少的高分辨率数据，同时保持同样的准确度，其使用完整的高分辨率图像模式。</font>
</div>


<hr>
<div id="paper83"> <b>83. Towards Automatic Face-to-Face Translation</b>  <a href="https://arxiv.org/pdf/2003.00418" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title83" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=R%2C+P+K" target="_blank" rel="noopener" style="color:#0000EE;">Prajwal K R</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mukhopadhyay%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rudrabha Mukhopadhyay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Philip%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jerin Philip</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jha%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhishek Jha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Namboodiri%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vinay Namboodiri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jawahar%2C+C+V" target="_blank" rel="noopener" style="color:#0000EE;">C.V. Jawahar</a><br>
<font size="3">
Abstract: In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as "Face-to-Face Translation". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact on multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards "Face-to-Face Translation" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available. Demo video: this https URL Code and models: this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在光的自动机器翻译系统近期的突破，我们提出了一种新的方法，我们术语“觌翻译”。由于今天的数字通信变得越来越直观，我们认为有必要对，可以自动翻译语言话语与现实的口形同步目标B语言的人的视频系统。在这项工作中，我们创建了这一问题的自动流水线，并展示其在多个现实世界的应用程序的影响。首先，我们通过把从言语和语言多个现有的模块组合在一起建立一个工作语音到语音翻译系统。然后，我们通过引入一个新的视觉模块，LipGAN从翻译的音频生成拟真的脸朝着“觌翻译”之举。在标准LRW测试仪显示其显著优于现有LipGAN的定量评价在所有标准度量方法。我们也受到我们的脸对脸翻译管道，以多种人类评估和表明，它可以显著提高消费在跨语言多式联运内容交互的整体用户体验。代码，模型和演示视频被公之于众。演示视频：此HTTPS URL代码和模型：该HTTPS URL</font>
</div>


<hr>
<div id="paper84"> <b>84. PF-Net: Point Fractal Network for 3D Point Cloud Completion</b>  <a href="https://arxiv.org/pdf/2003.00410" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title84" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zitian Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yikuan Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiawen Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Ni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Le%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyi Le</a><br>
<font size="3">
Abstract: In this paper, we propose a Point Fractal Network (PF-Net), a novel learning-based approach for precise and high-fidelity point cloud completion. Unlike existing point cloud completion networks, which generate the overall shape of the point cloud from the incomplete point cloud and always change existing points and encounter noise and geometrical loss, PF-Net preserves the spatial arrangements of the incomplete point cloud and can figure out the detailed geometrical structure of the missing region(s) in the prediction. To succeed at this task, PF-Net estimates the missing point cloud hierarchically by utilizing a feature-points-based multi-scale generating network. Further, we add up multi-stage completion loss and adversarial loss to generate more realistic missing region(s). The adversarial loss can better tackle multiple modes in the prediction. Our experiments demonstrate the effectiveness of our method for several challenging point cloud completion tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个分形点网（PF-网），精确和高保真点云完成一种新型的基于学习的方法。不像现有的点云完成网络，其从残缺点云生成点云的整体形状和总是改变现有分和遭遇的噪声和几何损失，PF-Net的蜜饯不完全点云和空间排列可以计算出在预测中详述的丢失区域的几何结构（一个或多个）。在这个任务取得成功，PF-Net的利用基于特征点的多尺度生成网络分层估计丢失的点云。此外，我们加起来多阶段完成损失和对抗性的损失，产生更逼真的失踪区域（一个或多个）。对抗损失可以更好地应对在预测多种模式。我们的实验证明我们的方法的一些具有挑战性的点云完成任务的有效性。</font>
</div>


<hr>
<div id="paper85"> <b>85. FMT:Fusing Multi-task Convolutional Neural Network for Person Search</b>  <a href="https://arxiv.org/pdf/2003.00406" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title85" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sulan Zhai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shunqiang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jin Tang</a><br>
<font size="3">
Abstract: Person search is to detect all persons and identify the query persons from detected persons in the image without proposals and bounding boxes, which is different from person re-identification. In this paper, we propose a fusing multi-task convolutional neural network(FMT-CNN) to tackle the correlation and heterogeneity of detection and re-identification with a single convolutional neural network. We focus on how the interplay of person detection and person re-identification affects the overall performance. We employ person labels in region proposal network to produce features for person re-identification and person detection network, which can improve the accuracy of detection and re-identification simultaneously. We also use a multiple loss to train our re-identification network. Experiment results on CUHK-SYSU Person Search dataset show that the performance of our proposed method is superior to state-of-the-art approaches in both mAP and top-1. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人的搜索是为了检测所有的人，并确定从图像中检测到的人查询者不建议和边框，这是由人重新鉴定不同。在本文中，我们提出了一个融合多任务卷积神经网络（FMT-CNN），以解决检测和重新鉴定的相关性和异质性与单个卷积神经网络。我们专注于人检测和人重新鉴定的相互作用如何影响整体性能。我们目前在区域方案网络人标签，以生产为特征的人重新鉴定和人员检测网络，可以同时提高检测和重新鉴定的准确性。我们还使用多损失训练我们的重新鉴定的网络。在香港中文大学，中山大学人搜索数据集上的实验结果，我们提出的方法的性能优于国家的最先进的两种地图和顶级1接近。</font>
</div>


<hr>
<div id="paper86"> <b>86. Cops-Ref: A new Dataset and Task on Compositional Referring Expression  Comprehension</b>  <a href="https://arxiv.org/pdf/2003.00403" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title86" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenfang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+K+K" target="_blank" rel="noopener" style="color:#0000EE;">Kwan-Yee K. Wong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Wu</a><br>
<font size="3">
Abstract: Referring expression comprehension (REF) aims at identifying a particular object in a scene by a natural language expression. It requires joint reasoning over the textual and visual domains to solve the problem. Some popular referring expression datasets, however, fail to provide an ideal test bed for evaluating the reasoning ability of the models, mainly because 1) their expressions typically describe only some simple distinctive properties of the object and 2) their images contain limited distracting information. To bridge the gap, we propose a new dataset for visual reasoning in context of referring expression comprehension with two main features. First, we design a novel expression engine rendering various reasoning logics that can be flexibly combined with rich visual properties to generate expressions with varying compositionality. Second, to better exploit the full reasoning chain embodied in an expression, we propose a new test setting by adding additional distracting images containing objects sharing similar properties with the referent, thus minimising the success rate of reasoning-free cross-domain alignment. We evaluate several state-of-the-art REF models, but find none of them can achieve promising performance. A proposed modular hard mining strategy performs the best but still leaves substantial room for improvement. We hope this new dataset and task can serve as a benchmark for deeper visual reasoning analysis and foster the research on referring expression comprehension. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在由自然语言表达识别场景中的特定对象参照表达理解（REF）的目标。它需要在文本和可视化领域的联合推理来解决问题。一些流行的参照表达数据集，但是，不能用于评估模型的推理能力提供一个理想的试验台，主要是因为：1）其表达通常描述对象和2的只有一些简单的独特的属性）他们的图像包含有限分心信息。为了缩小差距，我们提出了视觉推理的新数据集在提到表达理解有两个主要特征的情况下。首先，我们设计了一个新的表达引擎渲染可以与富视觉特性来灵活地组合，以产生具有不同组合性表达式各种推理逻辑。第二，更好地利用体现在表达完整的推理链，我们通过将含有对象与指示对象共享类似属性的其他分散注意力的图像，从而最大限度地减少自由推理-跨域对准的成功率提出了一种新的测试设置。我们评估的国家的最先进的几种REF车型，但他们发现没有人能达到承诺的性能。拟议的模块化硬开采战略执行最好的，但仍有改进的余地相当大。我们希望这个新的数据集和任务可以作为更深入的视觉推理分析的基准，并推动指表达理解的研究。</font>
</div>


<hr>
<div id="paper87"> <b>87. Intelligent Home 3D: Automatic 3D-House Design from Linguistic  Descriptions Only</b>  <a href="https://arxiv.org/pdf/2003.00397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title87" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuai Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingkui Tan</a><br>
<font size="3">
Abstract: Home design is a complex task that normally requires architects to finish with their professional skills and tools. It will be fascinating that if one can produce a house plan intuitively without knowing much knowledge about home design and experience of using complex designing tools, for example, via natural language. In this paper, we formulate it as a language conditioned visual content generation problem that is further divided into a floor plan generation and an interior texture (such as floor and wall) synthesis task. The only control signal of the generation process is the linguistic expression given by users that describe the house details. To this end, we propose a House Plan Generative Model (HPGM) that first translates the language input to a structural graph representation and then predicts the layout of rooms with a Graph Conditioned Layout Prediction Network (GC LPN) and generates the interior texture with a Language Conditioned Texture GAN (LCT-GAN). With some post-processing, the final product of this task is a 3D house model. To train and evaluate our model, we build the first Text-to-3D House Model dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：家居设计是一个复杂的任务，通常需要建筑师与自己的专业技能和工具完成。这将是有趣的，如果一个人可以直观地产生一个房子的计划，而无需了解家居设计，并使用复杂的设计工具，例如经验多的知识，通过自然语言。在本文中，我们配制它作为一种语言空调可视内容产生问题被进一步划分成一个平面图生成和内部质地（如地板和墙壁）的合成任务。生成过程的唯一控制信号是由描述该房子细节用户给出的语言表达。为此，我们提出了一个房子的计划生成模型（HPGM），首先转换的输入结构图表示的语言，然后预测客房配有图表空调布局预测网（GC LPN）的布局，并产生内部的质地配合语言空调纹理甘（LCT-GAN）。对于一些后期处理，这个任务的最终产品是一个三维房屋模型。要培养和评价我们的模型，我们所建立的第一个文本到3D房屋模型数据集。</font>
</div>


<hr>
<div id="paper88"> <b>88. Deep Active Learning for Biased Datasets via Fisher Kernel  Self-Supervision</b>  <a href="https://arxiv.org/pdf/2003.00393" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title88" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gudovskiy%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Denis Gudovskiy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hodgkinson%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alec Hodgkinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yamaguchi%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takuya Yamaguchi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tsukizawa%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sotaro Tsukizawa</a><br>
<font size="3">
Abstract: Active learning (AL) aims to minimize labeling efforts for data-demanding deep neural networks (DNNs) by selecting the most representative data points for annotation. However, currently used methods are ill-equipped to deal with biased data. The main motivation of this paper is to consider a realistic setting for pool-based semi-supervised AL, where the unlabeled collection of train data is biased. We theoretically derive an optimal acquisition function for AL in this setting. It can be formulated as distribution shift minimization between unlabeled train data and weakly-labeled validation dataset. To implement such acquisition function, we propose a low-complexity method for feature density matching using self-supervised Fisher kernel (FK) as well as several novel pseudo-label estimators. Our FK-based method outperforms state-of-the-art methods on MNIST, SVHN, and ImageNet classification while requiring only 1/10th of processing. The conducted experiments show at least 40% drop in labeling efforts for the biased class-imbalanced data compared to existing methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：主动学习（AL）的目标，以尽量减少数据要求苛刻的深层神经网络（DNNs）通过选择最有代表性的数据点标注标签的努力。然而，目前使用的方法都没有能力应付偏置数据。本文的主要动机是考虑基于池的半监督AL，其中列车数据的收集未标记偏向真实的场景。从理论上推导AL最佳的采集功能在此设置。它可以被配制为无标签的训练数据和弱标记的验证数据集之间分配移最小化。为了实现这样的采集功能，我们采用自我监督费舍尔内核（FK）以及几个新的伪标签估计提出特征密度匹配的低复杂度的方法。我们的FK-基于状态的的最先进的方法优于上MNIST，SVHN和ImageNet分类方法同时需要处理的只有1/10。所进行的实验显示在标记为相对于现有方法偏置类不平衡数据的努力至少40％的下降。</font>
</div>


<hr>
<div id="paper89"> <b>89. Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning</b>  <a href="https://arxiv.org/pdf/2003.00392" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title89" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shizhe Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yida Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qin Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Wu</a><br>
<font size="3">
Abstract: Cross-modal retrieval between videos and texts has attracted growing attentions due to the rapid emergence of videos on the web. The current dominant approach for this problem is to learn a joint embedding space to measure cross-modal similarities. However, simple joint embeddings are insufficient to represent complicated visual and textual details, such as scenes, objects, actions and their compositions. To improve fine-grained video-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model, which decomposes video-text matching into global-to-local levels. To be specific, the model disentangles texts into hierarchical semantic graph including three levels of events, actions, entities and relationships across levels. Attention-based graph reasoning is utilized to generate hierarchical textual embeddings, which can guide the learning of diverse and hierarchical video representations. The HGR model aggregates matchings from different video-text levels to capture both global and local details. Experimental results on three video-text datasets demonstrate the advantages of our model. Such hierarchical decomposition also enables better generalization across datasets and improves the ability to distinguish fine-grained semantic differences. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频和文本之间的跨模态获取引起了越来越多的关注，由于视频在网络上迅速崛起。对于这个问题目前主要的方式就是学会联合嵌入空间来衡量跨模式的相似性。然而，简单的嵌入关节都不足以表示复杂的视觉和文本的详细信息，如场景，物体，行动和他们的作品。为了提高细粒度视频文本检索，我们提出了一个层次图推理（HGR）模型，分解视频，文本匹配到全球到本地水平。具体而言，该模式理顺了那些纷繁文本翻译成层次语义图包括三个层次的事件，行动，实体并通过水平的关系。基于关注-图形推理被用于产生分级文本的嵌入，从而可以指导多样化和分级视频表示的学习。从不同的视频，文字水平的HGR模型聚集匹配数来捕捉全局和局部细节。三视频，文本数据集实验结果表明我们的模型的优点。这样的分层分解也可以跨数据集更好的泛化和提高分辨细粒度语义差异的能力。</font>
</div>


<hr>
<div id="paper90"> <b>90. Joint Wasserstein Distribution Matching</b>  <a href="https://arxiv.org/pdf/2003.00389" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title90" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">JieZhang Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mo%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Langyuan Mo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qing Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peilin Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junzhou Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingkui Tan</a><br>
<font size="3">
Abstract: Joint distribution matching (JDM) problem, which aims to learn bidirectional mappings to match joint distributions of two domains, occurs in many machine learning and computer vision applications. This problem, however, is very difficult due to two critical challenges: (i) it is often difficult to exploit sufficient information from the joint distribution to conduct the matching; (ii) this problem is hard to formulate and optimize. In this paper, relying on optimal transport theory, we propose to address JDM problem by minimizing the Wasserstein distance of the joint distributions in two domains. However, the resultant optimization problem is still intractable. We then propose an important theorem to reduce the intractable problem into a simple optimization problem, and develop a novel method (called Joint Wasserstein Distribution Matching (JWDM)) to solve it. In the experiments, we apply our method to unsupervised image translation and cross-domain video synthesis. Both qualitative and quantitative comparisons demonstrate the superior performance of our method over several state-of-the-arts. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：共同配送匹配（JDM）的问题，其目的是学习的双向映射，以匹配两个域的联合分布，发生在许多机器学习和计算机视觉应用。这个问题，但是，是由于两个关键的挑战非常困难：（i）其往往难以利用从联合分布足够的信息来进行匹配; （ⅱ）此问题是难以配制和优化。在本文中，依靠优化输运理论，我们在两个域最小化联合分布的Wasserstein的距离提出地址JDM问题。然而，得到的优化问题依然棘手。然后，我们提出了一个重要的定理，以减少棘手的问题变成一个简单的优化问题，并制定新的方法（称为联合华的配送匹配（JWDM））来解决它。在实验中，我们应用我们的方法无监督图像平移和跨域视频合成。定性和定量的比较，证明在几个国家的的艺术了该方法的优越性能。</font>
</div>


<hr>
<div id="paper91"> <b>91. Say As You Wish: Fine-grained Control of Image Caption Generation with  Abstract Scene Graphs</b>  <a href="https://arxiv.org/pdf/2003.00387" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title91" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shizhe Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qin Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Wu</a><br>
<font size="3">
Abstract: Humans are able to describe image contents with coarse to fine details as they wish. However, most image captioning models are intention-agnostic which can not generate diverse descriptions according to different user intentions initiatively. In this work, we propose the Abstract Scene Graph (ASG) structure to represent user intention in fine-grained level and control what and how detailed the generated description should be. The ASG is a directed graph consisting of three types of \textbf{abstract nodes} (object, attribute, relationship) grounded in the image without any concrete semantic labels. Thus it is easy to obtain either manually or automatically. From the ASG, we propose a novel ASG2Caption model, which is able to recognise user intentions and semantics in the graph, and therefore generate desired captions according to the graph structure. Our model achieves better controllability conditioning on ASGs than carefully designed baselines on both VisualGenome and MSCOCO datasets. It also significantly improves the caption diversity via automatically sampling diverse ASGs as control signals. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类能够按照自己的意愿来形容与粗到细的细节图像内容。然而，大多数图像字幕模型是意图无关根据不同用户的意图主动无法生成多样的描述。在这项工作中，我们提出了抽象场景图（ASG）结构来表示的细粒级和控制生成的描述应该是什么，以及如何详细的用户意图。在ASG是由三种类型的\ textbf图像中接地{抽象节点}（对象，属性，关系），而没有任何具体的语义标签的有向图。因此，它是很容易手动或自动获得任一。从ASG，我们提出了一个新颖的ASG2Caption模型，其能够识别用户的意图和语义图中的，因此根据图结构产生期望的标题。我们的模型上取得助理秘书长比都VisualGenome和MSCOCO数据集精心设计的基准更好的可控调节。它还显著提高通过自动采样助理秘书长多样作为控制信号的标题的多样性。</font>
</div>


<hr>
<div id="paper92"> <b>92. Emotion Recognition System from Speech and Visual Information based on  Convolutional Neural Networks</b>  <a href="https://arxiv.org/pdf/2003.00351" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title92" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ristea%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolae-Catalin Ristea</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dutu%2C+L+C" target="_blank" rel="noopener" style="color:#0000EE;">Liviu Cristian Dutu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Radoi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anamaria Radoi</a><br>
<font size="3">
Abstract: Emotion recognition has become an important field of research in the human-computer interactions domain. The latest advancements in the field show that combining visual with audio information lead to better results if compared to the case of using a single source of information separately. From a visual point of view, a human emotion can be recognized by analyzing the facial expression of the person. More precisely, the human emotion can be described through a combination of several Facial Action Units. In this paper, we propose a system that is able to recognize emotions with a high accuracy rate and in real time, based on deep Convolutional Neural Networks. In order to increase the accuracy of the recognition system, we analyze also the speech data and fuse the information coming from both sources, i.e., visual and audio. Experimental results show the effectiveness of the proposed scheme for emotion recognition and the importance of combining visual with audio data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：情感识别已成为人机交互领域研究的一个重要领域。在该领域的最新进展表明，如果相比于单独使用的单一信息来源的情况下，与音频信息铅结合视觉更好的结果。从一个视点，一个人的情感可以通过分析人的脸部表情识别。更确切地说，人的情感可以通过以下几种面部动作单元的组合来描述。在本文中，我们提出了一个系统，该系统能够识别的情绪具有高准确率和实时的基础上，深卷积神经网络。为了提高识别系统的准确度，我们也分析了语音数据和熔断器从两个来源的信息，即，视觉和听觉。实验结果表明，对于情感识别所提出的方案的有效性，并结合视觉与音频数据的重要性。</font>
</div>


<hr>
<div id="paper93"> <b>93. Learning Cross-domain Generalizable Features by Representation  Disentanglement</b>  <a href="https://arxiv.org/pdf/2003.00321" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title93" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingjie Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rueckert%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Rueckert</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kainz%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernhard Kainz</a><br>
<font size="3">
Abstract: Deep learning models exhibit limited generalizability across different domains. Specifically, transferring knowledge from available entangled domain features(source/target domain) and categorical features to new unseen categorical features in a target domain is an interesting and difficult problem that is rarely discussed in the current literature. This problem is essential for many real-world applications such as improving diagnostic classification or prediction in medical imaging. To address this problem, we propose Mutual-Information-based Disentangled Neural Networks (MIDNet) to extract generalizable features that enable transferring knowledge to unseen categorical features in target domains. The proposed MIDNet is developed as a semi-supervised learning paradigm to alleviate the dependency on labeled data. This is important for practical applications where data annotation requires rare expertise as well as intense time and labor. We demonstrate our method on handwritten digits datasets and a fetal ultrasound dataset for image classification tasks. Experiments show that our method outperforms the state-of-the-art and achieve expected performance with sparsely labeled data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习模型表现出在不同的领域普遍性有限。具体而言，从可用的缠结域特征（源/目标域）和分类特征在目标域知识转移到新看不见类别特征是在目前的文献中很少讨论了一个有趣的和困难的问题。这个问题对于许多实际应用，例如提高诊断分类或预测在医学成像中是必不可少的。为了解决这个问题，我们提出了一种基于互信息化解开的神经网络（MIDNet）提取，使在目标域转移的知识看不见的类别特征概括的特点。所提出的MIDNet被开发为一个半监督学习模式，以减轻标签数据的依赖。这对于数据注解需要罕见的专业知识，以及强烈的时间和劳力的实际应用很重要。我们证明手写数字数据集和图像分类任务的胎儿超声数据集我们的方法。实验表明，我们的方法优于国家的最先进的，并实现与稀疏标签的数据预期的表现。</font>
</div>


<hr>
<div id="paper94"> <b>94. Grounded and Controllable Image Completion by Incorporating Lexical  Semantics</b>  <a href="https://arxiv.org/pdf/2003.00303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title94" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shengyu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tan Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghao Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqi Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhou Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siliang Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jin Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongxia Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a><br>
<font size="3">
Abstract: In this paper, we present an approach, namely Lexical Semantic Image Completion (LSIC), that may have potential applications in art, design, and heritage conservation, among several others. Existing image completion procedure is highly subjective by considering only visual context, which may trigger unpredictable results which are plausible but not faithful to a grounded knowledge. To permit both grounded and controllable completion process, we advocate generating results faithful to both visual and lexical semantic context, i.e., the description of leaving holes or blank regions in the image (e.g., hole description). One major challenge for LSIC comes from modeling and aligning the structure of visual-semantic context and translating across different modalities. We term this process as structure completion, which is realized by multi-grained reasoning blocks in our model. Another challenge relates to the unimodal biases, which occurs when the model generates plausible results without using the textual description. This can be true since the annotated captions for an image are often semantically equivalent in existing datasets, and thus there is only one paired text for a masked image in training. We devise an unsupervised unpaired-creation learning path besides the over-explored paired-reconstruction path, as well as a multi-stage training strategy to mitigate the insufficiency of labeled data. We conduct extensive quantitative and qualitative experiments as well as ablation studies, which reveal the efficacy of our proposed LSIC. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个方法，即词汇语义图像修复（大规模集成电路），可能有其中几个人在艺术，设计和文物保护，潜在的应用。现有的图像修复方法是只考虑视觉背景下，这可能会引发哪些是合理的，但不是忠实于一个接地的知识不可预知的结果非常主观。为了允许这两个接地的和可控的完成过程，我们主张生成结果忠实于视觉和词汇语义语境，即，留下孔或空白区域的图像中（例如，孔的描述）的描述。对于LSIC一个主要的挑战来自于模拟和调整视觉语义上下文的结构和不同的模式转换。我们称这个过程为结构完成，这是在我们的模型多晶推理模块实现。另一个挑战涉及单峰偏差，当模型生成合理的结果，而无需使用文本描述时发生。因为对于图像的注解字幕经常在现有的数据集语义上等价这可能是真实的，因此只有一个在训练遮盖的图像配对的文本。我们设计除了过探讨配对重建路径无监督未成创造学习之路，以及多级培训战略，以减轻标记数据的不足。我们进行了广泛的定量和定性实验以及消融研究，其中揭示了我们所提出的大规模集成电路的功效。</font>
</div>


<hr>
<div id="paper95"> <b>95. Representations, Metrics and Statistics For Shape Analysis of Elastic  Graphs</b>  <a href="https://arxiv.org/pdf/2003.00287" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title95" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyang Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Srivastava%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anuj Srivastava</a><br>
<font size="3">
Abstract: Past approaches for statistical shape analysis of objects have focused mainly on objects within the same topological classes, e.g., scalar functions, Euclidean curves, or surfaces, etc. For objects that differ in more complex ways, the current literature offers only topological methods. This paper introduces a far-reaching geometric approach for analyzing shapes of graphical objects, such as road networks, blood vessels, brain fiber tracts, etc. It represents such objects, exhibiting differences in both geometries and topologies, as graphs made of curves with arbitrary shapes (edges) and connected at arbitrary junctions (nodes). To perform statistical analyses, one needs mathematical representations, metrics and other geometrical tools, such as geodesics, means, and covariances. This paper utilizes a quotient structure to develop efficient algorithms for computing these quantities, leading to useful statistical tools, including principal component analysis and analytical statistical testing and modeling of graphical shapes. The efficacy of this framework is demonstrated using various simulated as well as the real data from neurons and brain arterial networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对象的统计形状分析过去的方法主要集中于相同的拓扑的类，例如，标量函数，欧几里德曲线，或表面等中的对象对于在更复杂的方式，目前的文献仅提供拓扑方法不同对象。本文介绍了用于分析的图形对象，诸如道路网络，血管，脑纤维束等，这表示这样的对象，表现出两个几何和拓扑结构的差异的形状的深远几何方法，如由具有任意曲线的曲线图形状（边缘）和在任意的结（节点）连接。为了进行统计分析，一个需要数学表达式，度量和其他几何工具，如测地线，手段，和协方差。本文使用的商结构来开发有效的算法，用于计算这些量，导致有用的统计工具，包括主成分分析和分析的统计检验和图形的形状建模。该框架的功效使用从神经元和脑动脉网络的各种模拟以及实际数据证实。</font>
</div>


<hr>
<div id="paper96"> <b>96. Augmenting Visual Place Recognition with Structural Cues</b>  <a href="https://arxiv.org/pdf/2003.00278" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title96" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Oertel%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amadeus Oertel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cieslewski%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Titus Cieslewski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scaramuzza%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Scaramuzza</a><br>
<font size="3">
Abstract: In this paper, we propose to augment image-based place recognition with structural cues. Specifically, these structural cues are obtained using structure-from-motion, such that no additional sensors are needed for place recognition. This is achieved by augmenting the 2D convolutional neural network (CNN) typically used for image-based place recognition with a 3D CNN that takes as input a voxel grid derived from the structure-from-motion point cloud. We evaluate different methods for fusing the 2D and 3D features and obtain best performance with global average pooling and simple concatenation. The resulting descriptor exhibits superior recognition performance compared to descriptors extracted from only one of the input modalities, including state-of-the-art image-based descriptors. Especially at low descriptor dimensionalities, we outperform state-of-the-art descriptors by up to 90%. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了以增强基于图像的地方识别与结构线索。具体而言，使用结构从运动，使得需要对地方识别没有额外的传感器获得这些结构的线索。这是通过增强通常用于基于图像的地方识别与3D CNN说作为输入从该结构从 - 运动点云衍生的体素网格的2D卷积神经网络（CNN）来实现的。我们评估上述用于融合2D和3D功能，并获得与全球平均水平池和简单的串联最佳性能的不同方法。相比仅从输入模态的一个提取描述符，包括国家的最先进的基于图像的描述符的描述符得到的具有优异的识别性能。特别是在低维度描述符，我们通过高达90％优于状态的最先进的描述符。</font>
</div>


<hr>
<div id="paper97"> <b>97. Reusing Discriminators for Encoding Towards Unsupervised Image-to-Image  Translation</b>  <a href="https://arxiv.org/pdf/2003.00273" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title97" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Runfa Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenbing Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Binghui Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fuchun Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Fang</a><br>
<font size="3">
Abstract: Unsupervised image-to-image translation is a central task in computer vision. Current translation frameworks will abandon the discriminator once the training process is completed. This paper contends a novel role of the discriminator by reusing it for encoding the images of the target domain. The proposed architecture, termed as NICE-GAN, exhibits two advantageous patterns over previous approaches: First, it is more compact since no independent encoding component is required; Second, this plug-in encoder is directly trained by the adversary loss, making it more informative and trained more effectively if a multi-scale discriminator is applied. The main issue in NICE-GAN is the coupling of translation with discrimination along the encoder, which could incur training inconsistency when we play the min-max game via GAN. To tackle this issue, we develop a decoupled training strategy by which the encoder is only trained when maximizing the adversary loss while keeping frozen otherwise. Extensive experiments on four popular benchmarks demonstrate the superior performance of NICE-GAN over state-of-the-art methods in terms of FID, KID, and also human preference. Comprehensive ablation studies are also carried out to isolate the validity of each proposed component. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督图像到影像转换为计算机视觉的中心任务。一旦训练过程完成当前的翻译框架将放弃鉴别。本文争辩通过重用它用于编码目标结构域的图像中的鉴别器的新的作用。所提出的架构，称为NICE-GAN，具有两个有利模式在先前的方法：首先，它是更紧凑，因为没有独立编码组件是必需的;其次，此插件编码器直接被对手损失的培训，使之更翔实，更有效，如果应用了多尺度鉴别培训。在NICE-GaN的主要问题是翻译与沿编码器，它可能招致训练不一致时，我们通过发挥GAN最小 - 最大游戏歧视的耦合。为了解决这个问题，我们开发了一个分离的培训策略，通过最大化对手损失时，同时保持冻结，否则编码器只训练。四个流行的基准测试大量的实验证明NICE-GaN超国家的最先进的方法FID，KID方面的卓越性能，同时还人的偏好。综合消融研究也进行了各自提出的组件的有效性隔离。</font>
</div>


<hr>
<div id="paper98"> <b>98. Joint Face Completion and Super-resolution using Multi-scale Feature  Relation Learning</b>  <a href="https://arxiv.org/pdf/2003.00255" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title98" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhilei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunpeng Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Le Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cuicui Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baoyuan Wu</a><br>
<font size="3">
Abstract: Previous research on face restoration often focused on repairing a specific type of low-quality facial images such as low-resolution (LR) or occluded facial images. However, in the real world, both the above-mentioned forms of image degradation often coexist. Therefore, it is important to design a model that can repair LR occluded images simultaneously. This paper proposes a multi-scale feature graph generative adversarial network (MFG-GAN) to implement the face restoration of images in which both degradation modes coexist, and also to repair images with a single type of degradation. Based on the GAN, the MFG-GAN integrates the graph convolution and feature pyramid network to restore occluded low-resolution face images to non-occluded high-resolution face images. The MFG-GAN uses a set of customized losses to ensure that high-quality images are generated. In addition, we designed the network in an end-to-end format. Experimental results on the public-domain CelebA and Helen databases show that the proposed approach outperforms state-of-the-art methods in performing face super-resolution (up to 4x or 8x) and face completion simultaneously. Cross-database testing also revealed that the proposed approach has good generalizability. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面部恢复以前的研究往往侧重于修复低质量的面部图像的具体类型，如低分辨率（LR）或遮挡面部图像。然而，在现实世界中，无论是图像质量下降的上述形式往往并存。因此，设计出可以修复LR同时遮挡图像的模型是非常重要的。本文提出了一种多尺度特征图表生成对抗网络（MFG-GAN）来实现图像，其中两个退化模式共存，并且还修图像的面恢复与单一类型的退化。基于在GaN中，MFG-GaN集成了图形卷积和功能金字塔网络阻塞低分辨率人脸图像恢复到非封闭的高分辨率人脸图像。所述MFG-GAN使用一组定制的损失，以确保高品质的被生成的图像。此外，我们还设计了一个终端到终端的格式的网络。在公共领域的实验结果CelebA和海伦的数据库表明，该方法比国家的最先进的方法进行面部超分辨率（高达4倍或8倍）和脸部结束的同时。跨数据库测试还透露，该方法具有良好的推广性。</font>
</div>


<hr>
<div id="paper99"> <b>99. NAS-Count: Counting-by-Density with Neural Architecture Search</b>  <a href="https://arxiv.org/pdf/2003.00217" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title99" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yutao Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolong Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuhui Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baochang Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jungong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xianbin Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Doermann%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Doermann</a><br>
<font size="3">
Abstract: Most of the recent advances in crowd counting have evolved from hand-designed density estimation networks, where multi-scale features are leveraged to address scale variation, but at the expense of demanding design efforts. In this work, we automate the design of counting models with Neural Architecture Search (NAS) and introduce an end-to-end searched encoder-decoder architecture, Automatic Multi-Scale Network (AMSNet). The encoder and decoder in AMSNet are composed of different cells discovered from counting-specific search spaces, each dedicated to extracting and aggregating multi-scale features adaptively. To resolve the pixel-level isolation issue in training density estimation models, AMSNet is optimized with a novel Scale Pyramid Pooling Loss (SPPLoss), which exploits a pyramidal architecture to achieve structural supervision at multiple scales. During training time, AMSNet and SPPLoss are searched end-to-end efficiently with differentiable NAS techniques. When testing, AMSNet produces state-of-the-art results that are considerably better than hand-designed models on four challenging datasets, fully demonstrating the efficacy of NAS-Count. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数人群计数的最新进展，已经从手工设计密度估计网络中，多尺度功能利用到地址尺度变化发展而来，但在高标准的设计工作的费用。在这项工作中，我们自动计数与神经结构搜索（NAS）车型的设计和引入终端到终端的搜索编码器，解码器架构，自动多尺度网络（AMSNet）。在AMSNet编码器和解码器是由来自特定计数搜索空间不同发现小区，每个专用于提取和聚合多尺度自适应功能。要解决训练密度估算模型的像素级的隔离问题，AMSNet与新颖的刻度金字塔池损失（SPPLoss），它利用了一个金字塔结构，实现多尺度结构优化的监督。在培训时间，AMSNet和SPPLoss被高效地搜索端至端与NAS区分的技术。测试时，AMSNet产生国家的最先进的效果都大大超过手设计的车型更好的四个挑战数据集，充分展示了NAS-Count的功效。</font>
</div>


<hr>
<div id="paper100"> <b>100. Channel Equilibrium Networks for Learning Deep Representation</b>  <a href="https://arxiv.org/pdf/2003.00214" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title100" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenqi Shao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shitao Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xingang Pan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Ping Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaogang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Ping Luo</a><br>
<font size="3">
Abstract: Convolutional Neural Networks (CNNs) are typically constructed by stacking multiple building blocks, each of which contains a normalization layer such as batch normalization (BN) and a rectified linear function such as ReLU. However, this work shows that the combination of normalization and rectified linear function leads to inhibited channels, which have small magnitude and contribute little to the learned feature representation, impeding the generalization ability of CNNs. Unlike prior arts that simply removed the inhibited channels, we propose to "wake them up" during training by designing a novel neural building block, termed Channel Equilibrium (CE) block, which enables channels at the same layer to contribute equally to the learned representation. We show that CE is able to prevent inhibited channels both empirically and theoretically. CE has several appealing benefits. (1) It can be integrated into many advanced CNN architectures such as ResNet and MobileNet, outperforming their original networks. (2) CE has an interesting connection with the Nash Equilibrium, a well-known solution of a non-cooperative game. (3) Extensive experiments show that CE achieves state-of-the-art performance on various challenging benchmarks such as ImageNet and COCO. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）通常是通过堆叠多个积木，其中每个都包含一个归一化层，例如批标准化（BN）和诸如RELU整流的线性函数构成。然而，这项工作表明，规范化的组合和纠正线性函数导致抑制信道，其中有小幅度和学习地物表示一点贡献，阻碍细胞神经网络的泛化能力。不同于简单地除去抑制通道现有技术中，我们通过设计一种新的神经积木训练期间建议“唤醒他们”，称为信道均衡（CE）块，这使得信道在相同的层，以同样的学习表示有助于。我们证明了CE能够防止抑制两种渠道经验和理论。 CE有几个吸引人的优点。 （1）它可以被集成到许多先进CNN架构，诸如RESNET和MobileNet，超越其原始网络。 （2）CE具有与Nash平衡，非合作游戏的公知的溶液一个有趣的连接。 （3）广泛的实验表明，CE实现各种挑战基准如ImageNet和COCO状态的最先进的性能。</font>
</div>


<hr>
<div id="paper101"> <b>101. Cross-Spectrum Dual-Subspace Pairing for RGB-infrared Cross-Modality  Person Re-Identification</b>  <a href="https://arxiv.org/pdf/2003.00213" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title101" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xing Fan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Jiang</a><br>
<font size="3">
Abstract: Due to its potential wide applications in video surveillance and other computer vision tasks like tracking, person re-identification (ReID) has become popular and been widely investigated. However, conventional person re-identification can only handle RGB color images, which will fail at dark conditions. Thus RGB-infrared ReID (also known as Infrared-Visible ReID or Visible-Thermal ReID) is proposed. Apart from appearance discrepancy in traditional ReID caused by illumination, pose variations and viewpoint changes, modality discrepancy produced by cameras of the different spectrum also exists, which makes RGB-infrared ReID more difficult. To address this problem, we focus on extracting the shared cross-spectrum features of different modalities. In this paper, a novel multi-spectrum image generation method is proposed and the generated samples are utilized to help the network to find discriminative information for re-identifying the same person across modalities. Another challenge of RGB-infrared ReID is that the intra-person (images from the same person) discrepancy is often larger than the inter-person (images from different persons) discrepancy, so a dual-subspace pairing strategy is proposed to alleviate this problem. Combining those two parts together, we also design a one-stream neural network combining the aforementioned methods to extract compact representations of person images, called Cross-spectrum Dual-subspace Pairing (CDP) model. Furthermore, during the training process, we also propose a Dynamic Hard Spectrum Mining method to automatically mine more hard samples from hard spectrum based on the current model state to further boost the performance. Extensive experimental results on two public datasets, SYSU-MM01 with RGB + near-infrared images and RegDB with RGB + far-infrared images, have demonstrated the efficiency and generality of our proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由于视频监控等计算机视觉任务，如追踪其潜在的广泛应用，人重新鉴定（里德）已成为流行，并得到了广泛的研究。然而，传统的人重新鉴定只能处理RGB彩色图像，这将在黑暗条件下失败。因此RGB红外里德（也称为红外可见光里德或可见光热里德）提出。除了引起照明传统REID外观差异，姿势变化和视点的变化，由不同的频谱也存在，这使得RGB红外里德更困难的摄像机产生的形态差异。为了解决这个问题，我们专注于提取不同模态的共享交叉谱特征。在本文中，一种新颖的多光谱图像生成方法，提出和所产生的样本被用于帮助网络找到用于重新识别跨模态是同一人的区别信息。 RGB红外里德的另一个挑战是，内部人（来自同一人的图像）的差异往往比人物间（来自不同人的图像）差异较大，所以双子空间配对策略，提出了缓解这一问题。这两部分结合在一起，我们还设计了一个流神经网络相结合上述方法提取的人物图像，称为跨谱双子空间配对（CDP）模式的简洁表示。此外，在训练过程中，我们还提出了一种基于当前模型状态，进一步提高性能，从硬频谱动态频谱硬采矿方法自动矿更硬的样品。在两个公共数据集大量的实验结果，我校-MM01与RGB +近红外图像和RegDB与RGB +远红外图像，证明我们提出的方法的有效性和普遍性。</font>
</div>


<hr>
<div id="paper102"> <b>102. Learning to Compare Relation: Semantic Alignment for Few-Shot Learning</b>  <a href="https://arxiv.org/pdf/2003.00210" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title102" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Congqi Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanning Zhang</a><br>
<font size="3">
Abstract: Few-shot learning is a fundamental and challenging problem since it requires recognizing novel categories from only a few examples. The objects for recognition have multiple variants and can locate anywhere in images. Directly comparing query images with example images can not handle content misalignment. The representation and metric for comparison are critical but challenging to learn due to the scarcity and wide variation of the samples in few-shot learning. In this paper, we present a novel semantic alignment model to compare relations, which is robust to content misalignment. We propose to add two key ingredients to existing few-shot learning frameworks for better feature and metric learning ability. First, we introduce a semantic alignment loss to align the relation statistics of the features from samples that belong to the same category. And second, local and global mutual information maximization is introduced, allowing for representations that contain locally-consistent and intra-class shared information across structural locations in an image. Thirdly, we introduce a principled approach to weigh multiple loss functions by considering the homoscedastic uncertainty of each stream. We conduct extensive experiments on several few-shot learning datasets. Experimental results show that the proposed method is capable of comparing relations with semantic alignment strategies, and achieves state-of-the-art performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：很少拍学习是根本，具有挑战性的问题，因为它需要从几个例子承认新类别。为表彰对象有多种变体，并且可以在任何地方的图像定位。直接比较与示例图像查询图像不能处理的内容错位。代表性和指标进行比较是重要的，但具有挑战性的学习由于几拍的学习样本的稀缺性和很大的差异。在本文中，我们提出了一种新的语义对准模型来比较的关系，这是强大的内容错位。我们建议两个关键成分添加到现有的几拍的学习框架，更好的功能和指标的学习能力。首先，我们引入了语义对准损失使从样本特征的关系的统计属于同一类别。第二，本地和全球互信息最大化的引入，允许包含在图像中跨越构造位置本地一致，类内共享的信息的表示。第三，我们引入原则的做法，考虑每个流的不确定性同方差衡量多种损失函数。我们进行了几个为数不多的射门学习数据集大量的实验。实验结果表明，所提出的方法能够比较语义对准策略的关系，并实现国家的最先进的性能。</font>
</div>


<hr>
<div id="paper103"> <b>103. VideoSSL: Semi-Supervised Learning for Video Classification</b>  <a href="https://arxiv.org/pdf/2003.00197" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title103" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jing%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Longlong Jing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Parag%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Toufiq Parag</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingli Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongcheng Wang</a><br>
<font size="3">
Abstract: We propose a semi-supervised learning approach for video classification, VideoSSL, using convolutional neural networks (CNN). Like other computer vision tasks, existing supervised video classification methods demand a large amount of labeled data to attain good performance. However, annotation of a large dataset is expensive and time consuming. To minimize the dependence on a large annotated dataset, our proposed semi-supervised method trains from a small number of labeled examples and exploits two regulatory signals from unlabeled data. The first signal is the pseudo-labels of unlabeled examples computed from the confidences of the CNN being trained. The other is the normalized probabilities, as predicted by an image classifier CNN, that captures the information about appearances of the interesting objects in the video. We show that, under the supervision of these guiding signals from unlabeled examples, a video classification CNN can achieve impressive performances utilizing a small fraction of annotated examples on three publicly available datasets: UCF101, HMDB51 and Kinetics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了视频分类，VideoSSL半监督学习方法，使用卷积神经网络（CNN）。像其他的计算机视觉任务，目前受监管的视频分类方法需要大量的标签数据以获得良好的性能。然而，大数据集的注释是昂贵和费时。为了最大限度地减少在一个大型注释的数据集的相关性，从少量的标记的例子我们提出的半监督方法列车和利用来自未标记数据的两个调节信号。该第一信号是被训练的从CNN的置信度计算的未标记的实施例中的伪标签。另一种是归一化的概率，由分类器CNN的图像作为预测，即捕获关于在视频中的有趣的物体的外观的信息。我们表明，与未标记示例中，这些引导信号的监督下，视频分类CNN可以实现利用的注释实例一小部分的三个公开可用的数据集出色的表现：UCF101，HMDB51和动力学。</font>
</div>


<hr>
<div id="paper104"> <b>104. First Order Motion Model for Image Animation</b>  <a href="https://arxiv.org/pdf/2003.00196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title104" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Siarohin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aliaksandr Siarohin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lathuili%C3%A8re%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stéphane Lathuilière</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tulyakov%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergey Tulyakov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ricci%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elisa Ricci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sebe%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicu Sebe</a><br>
<font size="3">
Abstract: Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (\eg \ faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories. Our source code is publicly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图片动画包括使源图像中的对象是根据驱动视频的运动动画生成视频序列的。我们的框架，而无需使用任何注释或对特定对象进行动画信息之前解决这个问题。一旦一组描绘同一类别的对象视频的培训（\例如\脸，人体），我们的方法可以适用于该类的任何对象。为了实现这一目标，我们使用分离自监督配方的外观和运动信息。为了支持复杂的运动，我们使用由一组了解到关键点的表示与当地的仿射变换一起。期间目标动作和联合机从源图像和从驱动视频导出的运动中提取的外观引起的发电机网络模型闭塞。我们的框架成绩最好的多样化基准和对各种对象的类别。我们的源代码是公开的。</font>
</div>


<hr>
<div id="paper105"> <b>105. Robust 6D Object Pose Estimation by Learning RGB-D Features</b>  <a href="https://arxiv.org/pdf/2003.00188" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title105" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meng Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Pan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ang%2C+M+H" target="_blank" rel="noopener" style="color:#0000EE;">Marcelo H Ang Jr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+G+H" target="_blank" rel="noopener" style="color:#0000EE;">Gim Hee Lee</a><br>
<font size="3">
Abstract: Accurate 6D object pose estimation is fundamental to robotic manipulation and grasping. Previous methods follow a local optimization approach which minimizes the distance between closest point pairs to handle the rotation ambiguity of symmetric objects. In this work, we propose a novel discrete-continuous formulation for rotation regression to resolve this local-optimum problem. We uniformly sample rotation anchors in SO(3), and predict a constrained deviation from each anchor to the target, as well as uncertainty scores for selecting the best prediction. Additionally, the object location is detected by aggregating point-wise vectors pointing to the 3D center. Experiments on two benchmarks: LINEMOD and YCB-Video, show that the proposed method outperforms state-of-the-art approaches. Our code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：准确6D对象姿态估计是机器人操作和把握的基础。先前的方法遵循的最接近点对处理对象对称的旋转不确定性之间的距离最小的局部优化的方法。在这项工作中，我们提出了一个新颖的离散连续配方旋转回归来解决这个局部优化问题。我们在SO（3）均匀样品旋转锚，并预测从各锚固到目标受约束的偏差，以及用于选择所述最佳预测不确定性分数。此外，该对象位置通过聚集指向3D中心逐点矢量检测。两个基准测试实验：LINEMOD和YCB视频，表明该方法优于国家的最先进的方法。我们的代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper106"> <b>106. Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image  Translation</b>  <a href="https://arxiv.org/pdf/2003.00187" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title106" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ohkawa%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takehiko Ohkawa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Inoue%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naoto Inoue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kataoka%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hirokatsu Kataoka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Inoue%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nakamasa Inoue</a><br>
<font size="3">
Abstract: Unpaired image-to-image (I2I) translation has received considerable attention in pattern recognition and computer vision because of recent advancements in generative adversarial networks (GANs). However, due to the lack of explicit supervision, unpaired I2I models often fail to generate realistic images, especially in challenging datasets with different backgrounds and poses. Hence, stabilization is indispensable for real-world applications and GANs. Herein, we propose Augmented Cyclic Consistency Regularization (ACCR), a novel regularization method for unpaired I2I translation. Our main idea is to enforce consistency regularization originating from semi-supervised learning on the discriminators leveraging real, fake, reconstructed, and augmented samples. We regularize the discriminators to output similar predictions when fed pairs of original and perturbed images. We qualitatively clarify the generation property between unpaired I2I models and standard GANs, and explain why consistency regularization on fake and reconstructed samples works well. Quantitatively, our method outperforms the consistency regularized GAN (CR-GAN) in digit translations and demonstrates efficacy against several data augmentation variants and cycle-consistent constraints. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：不成对图像到图像（121）的翻译已经收到，因为在生成对抗网络（甘斯）最新发展的模式识别和计算机视觉相当的重视。然而，由于缺乏明确的监管，未成I2I模型通常不能产生逼真的图像，尤其是在挑战不同的背景和姿态数据集。因此，稳定是现实世界的应用和甘斯不可或缺的。在此，我们建议增强循环一致性正则（ACCR），未配对I2I进行翻译正则化方法。我们的主要想法是执行从上鉴别利用真实的，假的，重建和增强样品半监督学习一致性正规化始发。喂养对原有的时候和不安图像我们正规化鉴输出类似的预测。我们定性澄清未成I2I模型和标准甘斯之间产生的财产，并解释为什么假冒重构样品的一致性正规化运作良好。定量地说，我们的方法优于一致性正规化GAN（CR-GAN）在数字平移和证明了对抗几个数据扩张变体和周期一致的约束效果。</font>
</div>


<hr>
<div id="paper107"> <b>107. HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection</b>  <a href="https://arxiv.org/pdf/2003.00186" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title107" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maosheng Ye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuangjie Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tongyi Cao</a><br>
<font size="3">
Abstract: We present Hybrid Voxel Network (HVNet), a novel one-stage unified network for point cloud based 3D object detection for autonomous driving. Recent studies show that 2D voxelization with per voxel PointNet style feature extractor leads to accurate and efficient detector for large 3D scenes. Since the size of the feature map determines the computation and memory cost, the size of the voxel becomes a parameter that is hard to balance. A smaller voxel size gives a better performance, especially for small objects, but a longer inference time. A larger voxel can cover the same area with a smaller feature map, but fails to capture intricate features and accurate location for smaller objects. We present a Hybrid Voxel network that solves this problem by fusing voxel feature encoder (VFE) of different scales at point-wise level and project into multiple pseudo-image feature maps. We further propose an attentive voxel feature encoding that outperforms plain VFE and a feature fusion pyramid network to aggregate multi-scale information at feature map level. Experiments on the KITTI benchmark show that a single HVNet achieves the best mAP among all existing methods with a real time inference speed of 31Hz. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本杂交体素网（HVNet），一种新型单级统一网络点云基于立体物检测为自主驾驶。最近的研究表明，2D素化，每个体素PointNet风格特征提取导致准确，高效的检测器为大型3D场景。由于特征地图的大小决定的计算和存储器成本，体素的尺寸变的参数是很难平衡。较小的体素尺寸给出了一个更好的性能，特别是对于小对象，但较长的推理时间。较大的体素可以覆盖具有较小特征地图相同的面积，但不能捕获复杂的特征和用于更小的物体精确位置。我们提出了一个混合体素的网络，解决了这个问题，通过在逐点的水平和项目划分成多个虚拟图像特征图融合不同尺度的体素的功能编码器（VFE）。我们进一步提出了一个贴心的功能体素编码是性能优于普通VFE和特征融合金字塔网络聚合的多尺度信息的特征图的水平。在KITTI基准显示，一个HVNet实现了与31Hz的实时推理速度所有现有的方法中最好的地图实验。</font>
</div>


<hr>
<div id="paper108"> <b>108. Attention-aware fusion RGB-D face recognition</b>  <a href="https://arxiv.org/pdf/2003.00168" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title108" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Uppal%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hardik Uppal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sepas-Moghaddam%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alireza Sepas-Moghaddam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Greenspan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Greenspan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Etemad%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Etemad</a><br>
<font size="3">
Abstract: A novel attention aware method is proposed to fuse two image modalities, RGB and depth, for enhanced RGB-D facial recognition. The proposed method uses two attention layers, the first focused on the fused feature maps generated by convolution layers, and the second focused on the spatial features of those maps. The training database is preprocessed and augmented through a set of geometric transformations, and the learning process is further aided using transfer learning from a pure 2D RGB image training process. Comparative evaluations demonstrate that the proposed method outperforms other state-of-the-art approaches, including both traditional and deep neural network-based methods, on the challenging CurtinFaces and IIIT-D RGB-D benchmark databases, achieving classification accuracies over 98:2% and 99:3% respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一种新的关注感知方法，提出了熔丝的两个图像模态，RGB和深度，以增强RGB-d面部识别。所提出的方法使用了两个关注的层，所述第一聚焦在熔融特征图通过卷积层中产生，并且第二集中于这些地图的空间特征。训练数据库被预处理并通过一组几何变换的增强，和学习过程是使用转印学习从纯2D RGB图像训练过程进一步辅助。比较评价表明，该方法优于其他国家的最先进的方法，包括传统的和深基于神经网络的方法中，对具有挑战性CurtinFaces和IIIT-d RGB-d基准数据库，超过98实现分类精确度：2 ％和99：分别为3％。</font>
</div>


<hr>
<div id="paper109"> <b>109. Towards Using Count-level Weak Supervision for Crowd Counting</b>  <a href="https://arxiv.org/pdf/2003.00164" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title109" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinjie Lei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pingping Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingqiao Liu</a><br>
<font size="3">
Abstract: Most existing crowd counting methods require object location-level annotation, i.e., placing a dot at the center of an object. While being simpler than the bounding-box or pixel-level annotation, obtaining this annotation is still labor-intensive and time-consuming especially for images with highly crowded scenes. On the other hand, weaker annotations that only know the total count of objects can be almost effortless in many practical scenarios. Thus, it is desirable to develop a learning method that can effectively train models from count-level annotations. To this end, this paper studies the problem of weakly-supervised crowd counting which learns a model from only a small amount of location-level annotations (fully-supervised) but a large amount of count-level annotations (weakly-supervised). To perform effective training in this scenario, we observe that the direct solution of regressing the integral of density map to the object count is not sufficient and it is beneficial to introduce stronger regularizations on the predicted density map of weakly-annotated images. We devise a simple-yet-effective training strategy, namely Multiple Auxiliary Tasks Training (MATT), to construct regularizes for restricting the freedom of the generated density maps. Through extensive experiments on existing datasets and a newly proposed dataset, we validate the effectiveness of the proposed weakly-supervised method and demonstrate its superior performance over existing solutions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数现有的人群计数方法需要对象位置级注释，即，放置在点的对象的中心。虽然比包围盒或像素级别注解简单，获得这个注释仍然是劳动密集和费时尤其是对于高度拥挤的场景画面。在另一方面，弱的注解只知道对象的总数可以毫不费力在许多实际场景。因此，希望开发一种学习的方法，可以有效地从数量级的注解训练模式。为此，本文研究了从只有少量的位置级别的注解（全面监督），但大量的计数级别的注解学习的典范弱监督的人群计数的问题（弱监督）。在这种情况下进行有效的培训，我们观察到，倒退密度图的积分对象计数的直接解决方案是不够的，它是有益的弱注释的图像的预测的密度地图上引进更强的正则化。我们设计一个简单但有效的培训策略，即多个辅助任务训练（MATT），构建​​规则化限制所产生的密度图的自由。通过对现有数据集大量的实验和新提出的数据集，我们验证了该弱监督方法的有效性，并证明其对现有解决方案的卓越性能。</font>
</div>


<hr>
<div id="paper110"> <b>110. Self-supervised Representation Learning for Ultrasound Video</b>  <a href="https://arxiv.org/pdf/2003.00105" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title110" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianbo Jiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Droste%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Droste</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Drukker%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lior Drukker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Papageorghiou%2C+A+T" target="_blank" rel="noopener" style="color:#0000EE;">Aris T. Papageorghiou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Noble%2C+J+A" target="_blank" rel="noopener" style="color:#0000EE;">J. Alison Noble</a><br>
<font size="3">
Abstract: Recent advances in deep learning have achieved promising performance for medical image analysis, while in most cases ground-truth annotations from human experts are necessary to train the deep model. In practice, such annotations are expensive to collect and can be scarce for medical imaging applications. Therefore, there is significant interest in learning representations from unlabelled raw data. In this paper, we propose a self-supervised learning approach to learn meaningful and transferable representations from medical imaging video without any type of human annotation. We assume that in order to learn such a representation, the model should identify anatomical structures from the unlabelled data. Therefore we force the model to address anatomy-aware tasks with free supervision from the data itself. Specifically, the model is designed to correct the order of a reshuffled video clip and at the same time predict the geometric transformation applied to the video clip. Experiments on fetal ultrasound video show that the proposed approach can effectively learn meaningful and strong representations, which transfer well to downstream tasks like standard plane detection and saliency prediction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在深度学习的最新进展，实现了医学图像分析看好的表现，而在大多数情况下，人类专家地面真注解是必要的训练深层模型。在实践中，这样的注释是收集昂贵的并且可以是稀少的医疗成像应用。因此，在学习来自未标记的原始数据表示显著的兴趣。在本文中，我们提出了一个自我监督的学习方法，学习从医疗成像视频有意义的，可转让的表示，而没有任何型人的注释。我们假设，为了学习这样的表示，模型应确认未标记的数据解剖结构。因此，我们迫使模型地址解剖感知任务，从数据本身无监督。具体地，该模型被设计成校正改组视频剪辑的顺序，并在同一时间预测施加到视频片段中的几何变换。对胎儿超声视频显示，该方法能有效地学习有意义和强烈的抗议，后者传以及像标准平面检测和显着性预测下游任务实验。</font>
</div>


<hr>
<div id="paper111"> <b>111. Transferring Dense Pose to Proximal Animal Classes</b>  <a href="https://arxiv.org/pdf/2003.00080" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title111" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sanakoyeu%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Artsiom Sanakoyeu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khalidov%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasil Khalidov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McCarthy%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Maureen S. McCarthy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vedaldi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Vedaldi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neverova%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Natalia Neverova</a><br>
<font size="3">
Abstract: Recent contributions have demonstrated that it is possible to recognize the pose of humans densely and accurately given a large dataset of poses annotated in detail. In principle, the same approach could be extended to any animal class, but the effort required for collecting new annotations for each case makes this strategy impractical, despite important applications in natural conservation, science and business. We show that, at least for proximal animal classes such as chimpanzees, it is possible to transfer the knowledge existing in dense pose recognition for humans, as well as in more general object detectors and segmenters, to the problem of dense pose recognition in other classes. We do this by (1) establishing a DensePose model for the new animal which is also geometrically aligned to humans (2) introducing a multi-head R-CNN architecture that facilitates transfer of multiple recognition tasks between classes, (3) finding which combination of known classes can be transferred most effectively to the new animal and (4) using self-calibrated uncertainty heads to generate pseudo-labels graded by quality for training a model for this class. We also introduce two benchmark datasets labelled in the manner of DensePose for the class chimpanzee and use them to evaluate our approach, showing excellent transfer learning performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的捐款已经表明，它可以识别人类的姿势密集，准确地给出一个大的数据集详细标注的姿势。原则上，同样的方法可以扩展到任何动物类，但对于收集新的注解，每种情况下所需的努力，使这一战略不切实际，尽管在自然保护，科学和商业的重要应用。我们表明，至少对于近端动物类，如黑猩猩，它可以传送现有的在密集的姿态识别对人类的知识，以及在更广泛的对象检测器和分割器，在其他类密集姿态识别的问题。我们通过（1）建立用于将多头R-CNN架构便于传输的类之间的多重识别任务的新的动物其也几何对齐，以人类（2）DensePose模型，（3）发现该组合要这样做的公知的类可以最有效地利用自校准不确定度头，以产生通过质量用于训练模型此类分级伪标签被转移到新的动物和（4）。我们还介绍了标记在DensePose为类黑猩猩的方式用两个标准数据集，并利用它们来评估我们的做法，表现出优异的转印学习表现。</font>
</div>


<hr>
<div id="paper112"> <b>112. Bio-Inspired Modality Fusion for Active Speaker Detection</b>  <a href="https://arxiv.org/pdf/2003.00063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title112" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Assun%C3%A7%C3%A3o%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gustavo Assunção</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gon%C3%A7alves%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nuno Gonçalves</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Menezes%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paulo Menezes</a><br>
<font size="3">
Abstract: Human beings have developed fantastic abilities to integrate information from various sensory sources exploring their inherent complementarity. Perceptual capabilities are therefore heightened enabling, for instance, the well known "cocktail party" and McGurk effects, i.e. speech disambiguation from a panoply of sound signals. This fusion ability is also key in refining the perception of sound source location, as in distinguishing whose voice is being heard in a group conversation. Furthermore, Neuroscience has successfully identified the superior colliculus region in the brain as the one responsible for this modality fusion, with a handful of biological models having been proposed to approach its underlying neurophysiological process. Deriving inspiration from one of these models, this paper presents a methodology for effectively fusing correlated auditory and visual information for active speaker detection. Such an ability can have a wide range of applications, from teleconferencing systems to social robotics. The detection approach initially routes auditory and visual information through two specialized neural network structures. The resulting embeddings are fused via a novel layer based on the superior colliculus, whose topological structure emulates spatial neuron cross-mapping of unimodal perceptual fields. The validation process employed two publicly available datasets, with achieved results confirming and greatly surpassing initial expectations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类已经开发出梦幻般的能力，整合来自各种感官的来源探索其内在的互补性信息。因此感知能力被增强，使得能够，例如，公知的“鸡尾酒会”和McGurk效果，即语音消歧从声音信号的一整套。这种融合的能力也是炼声源位置的感知，为在区分谁的声音被听到群组会话密钥。此外，神经科学，成功确认上丘区在大脑中的一个负责这种方式融合，具有生物模型的极少数已经提出接近其潜在的神经生理过程。从这些模型的一个派生的启发，本文提出了一种有效的融合相关的听觉和视觉信息的有源音箱检测的方法。这种能力可以有广泛的应用，从电话会议系统向社会机器人。最初检测的方针路线听觉和视觉信息通过两个专门的神经网络结构。将所得的嵌入是通过基于上丘，其拓扑结构模拟单峰感知领域的空间神经元跨映射的新的稠合层。验证过程使用两种可公开获得的数据集，用取得的结果确认，并大大超过了最初的期望。</font>
</div>


<hr>
<div id="paper113"> <b>113. Learning Nonparametric Human Mesh Reconstruction from a Single Image  without Ground Truth Meshes</b>  <a href="https://arxiv.org/pdf/2003.00052" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title113" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lijuan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ying Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zicheng Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Ting Sun</a><br>
<font size="3">
Abstract: Nonparametric approaches have shown promising results on reconstructing 3D human mesh from a single monocular image. Unlike previous approaches that use a parametric human model like skinned multi-person linear model (SMPL), and attempt to regress the model parameters, nonparametric approaches relax the heavy reliance on the parametric space. However, existing nonparametric methods require ground truth meshes as their regression target for each vertex, and obtaining ground truth mesh labels is very expensive. In this paper, we propose a novel approach to learn human mesh reconstruction without any ground truth meshes. This is made possible by introducing two new terms into the loss function of a graph convolutional neural network (Graph CNN). The first term is the Laplacian prior that acts as a regularizer on the reconstructed mesh. The second term is the part segmentation loss that forces the projected region of the reconstructed mesh to match the part segmentation. Experimental results on multiple public datasets show that without using 3D ground truth meshes, the proposed approach outperforms the previous state-of-the-art approaches that require ground truth meshes for training. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：非参数方法已显示在重建的三维人体从单个单眼图像啮合有希望的结果。与使用参数化人体模型像皮肤多人线性模型（SMPL）以前的办法，并试图回归模型参数，非参数方法放宽对参数空间的严重依赖。但是，现有的非参数方法需要地面实况网作为自己回归的目标为每个顶点，并获得地面实况网标签是非常昂贵的。在本文中，我们提出了一种新的方法来学习的人网改造没有任何地面实况网格。这是通过引入两个新的术语成图形的卷积神经网络（CNN图形）的损失函数成为可能。第一项是一个用作对重构的啮合的正则拉普拉斯之前。第二项是所述部分分割损失力的投影区域重建啮合以匹配部分分割。在多个公开数据集实验结果表明，在不使用3D地面实况网格，所提出的方法比需要的训练场真理网格以前的状态的最先进的方法。</font>
</div>


<hr>
<div id="paper114"> <b>114. Unlimited Resolution Image Generation with R2D2-GANs</b>  <a href="https://arxiv.org/pdf/2003.01063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title114" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Jegorova%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marija Jegorova</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Karjalainen%2C+A+I" target="_blank" rel="noopener" style="color:#0000EE;">Antti Ilari Karjalainen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Vazquez%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jose Vazquez</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hospedales%2C+T+M" target="_blank" rel="noopener" style="color:#0000EE;">Timothy M. Hospedales</a><br>
<font size="3">
Abstract: In this paper we present a novel simulation technique for generating high quality images of any predefined resolution. This method can be used to synthesize sonar scans of size equivalent to those collected during a full-length mission, with across track resolutions of any chosen magnitude. In essence, our model extends Generative Adversarial Networks (GANs) based architecture into a conditional recursive setting, that facilitates the continuity of the generated images. The data produced is continuous, realistically-looking, and can also be generated at least two times faster than the real speed of acquisition for the sonars with higher resolutions, such as EdgeTech. The seabed topography can be fully controlled by the user. The visual assessment tests demonstrate that humans cannot distinguish the simulated images from real. Moreover, experimental results suggest that in the absence of real data the autonomous recognition systems can benefit greatly from training with the synthetic data, produced by the R2D2-GANs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出用于产生任何预定义的分辨率的高品质的图像的新的模拟技术。此方法可用于在任何所选大小的轨道分辨率合成等效尺寸的声纳扫描那些全长任务期间收集，用。从本质上说，我们的模型延伸剖成对抗性网络（甘斯）体系结构的成有条件的递归设置，有助于所生成的图像的连续性。所产生的数据是连续的，现实外观的，并且也可以产生至少两倍高于采集用于与更高的分辨率，诸如EdgeTech的声纳的实际速度。海底地形可以由用户被完全控制。视觉评估测试证明，人类无法真正区分模拟图像。此外，实验结果表明，在没有实际数据的自主识别系统可以从训练用合成数据，由R2D2-Gans的产生极大地受益。</font>
</div>


<hr>
<div id="paper115"> <b>115. Constrained Nonnegative Matrix Factorization for Blind Hyperspectral  Unmixing incorporating Endmember Independence</b>  <a href="https://arxiv.org/pdf/2003.01041" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title115" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Ekanayake%2C+E+M+M+B" target="_blank" rel="noopener" style="color:#0000EE;">E.M.M.B. Ekanayake</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Rathnayake%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bhathiya Rathnayake</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Godaliyadda%2C+G+M+R+I" target="_blank" rel="noopener" style="color:#0000EE;">G.M.R.I. Godaliyadda</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Herath%2C+H+M+V+R" target="_blank" rel="noopener" style="color:#0000EE;">H.M.V.R. Herath</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ekanayake%2C+M+P+B" target="_blank" rel="noopener" style="color:#0000EE;">M.P.B. Ekanayake</a><br>
<font size="3">
Abstract: Hyperspectral image (HSI) analysis has become a key area in the field of remote sensing as a result of its ability to exploit richer information in the form of multiple spectral bands. The study of hyperspectral unmixing (HU) is important in HSI analysis due to the insufficient spatial resolution of customary imaging spectrometers. The endmembers of an HSI are more likely to be generated by independent sources and be mixed in a macroscopic degree before arriving at the sensor element of the imaging spectrometer as mixed spectra. Over the past few decades, many attempts have focused on imposing auxiliary constraints on the conventional nonnegative matrix factorization (NMF) framework in order to effectively unmix these mixed spectra. Signifying a step forward toward finding an optimum constraint to extract endmembers, this paper presents a novel blind HU algorithm, referred to as Kurtosis-based Smooth Nonnegative Matrix Factorization (KbSNMF) which incorporates a novel constraint-based on the statistical independence of the probability density functions of endmembers. Imposing this constraint on the conventional NMF framework promotes the extraction of independent endmembers while further enhancing the parts-based representation of data. The proposed algorithm manages to outperform several state of the art NMF-based algorithms in terms of extracting endmembers from hyperspectral remote sensing data, hence could uplift the performance of recent deep learning HU methods which utilizes the endmembers as supervisory data for abundance extraction. Keywords: Hyperspectral unmixing (HU), blind source separation, kurtosis, constrained, Gaussianity, endmember independence, nonnegative matrix factorization (NMF). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：高光谱图像（HSI）分析已成为遥感领域作为其利用在多个光谱带的形式更丰富的信息的能力的结果的一个关键区域。高光谱解混（HU）的研究是在HSI分析重要由于常规成像光谱的空间分辨率不足。的HSI的端元更容易由独立的源产生并在成像光谱仪作为混合光谱的传感器元件到达之前的宏观程度混合。在过去的几十年中，许多努力都集中在有效UNMIX这些混合光谱强加给传统的非负矩阵分解（NMF）框架辅助约束顺序。意的一个步骤向前朝着找到最佳约束来提取端元，本文提出了一种新颖的盲目HU算法中，被称为基于峰度光滑非负矩阵分解（KbSNMF），其结合了新颖的约束基于概率密度的统计独立性端元的功能。强加给常规NMF框架此约束促进独立端元的提取的同时进一步提高数据的基于部分的表示。所提出的算法设法优于在从高光谱遥感数据提取端元的术语的基于NMF技术算法的几个状态，因此，可以提升的，它利用了端元作为丰度提取监控数据最近深度学习HU方法的性能。关键词：高光谱解混（HU），盲源分离，峰度，约束，高斯，端元独立，非负矩阵分解（NMF）。</font>
</div>


<hr>
<div id="paper116"> <b>116. Evaluating Temporal Queries Over Video Feeds</b>  <a href="https://arxiv.org/pdf/2003.00953" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title116" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yueting Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaohui Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koudas%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nick Koudas</a><br>
<font size="3">
Abstract: Recent advances in Computer Vision and Deep Learning made possible the efficient extraction of a schema from frames of streaming video. As such, a stream of objects and their associated classes along with unique object identifiers derived via object tracking can be generated, providing unique objects as they are captured across frames. In this paper we initiate a study of temporal queries involving objects and their co-occurrences in video feeds. For example, queries that identify video segments during which the same two red cars and the same two humans appear jointly for five minutes are of interest to many applications ranging from law enforcement to security and safety. We take the first step and define such queries in a way that they incorporate certain physical aspects of video capture such as object occlusion. We present an architecture consisting of three layers, namely object detection/tracking, intermediate data generation and query evaluation. We propose two techniques,MFS and SSG, to organize all detected objects in the intermediate data generation layer, which effectively, given the queries, minimizes the number of objects and frames that have to be considered during query evaluation. We also introduce an algorithm called State Traversal (ST) that processes incoming frames against the SSG and efficiently prunes objects and frames unrelated to query evaluation, while maintaining all states required for succinct query evaluation. We present the results of a thorough experimental evaluation utilizing both real and synthetic data establishing the trade-offs between MFS and SSG. We stress various parameters of interest in our evaluation and demonstrate that the proposed query evaluation methodology coupled with the proposed algorithms is capable to evaluate temporal queries over video feeds efficiently, achieving orders of magnitude performance benefits. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在计算机视觉和深度学习最新进展成为可能的模式的高效提取从视频流的帧。这样，可产生对象和它们的相关联的通过目标跟踪产生的独特的对象标识符沿着类的流，提供独特的对象，因为它们是在帧之间捕获。在本文中，我们开始涉及对象和他们共同出现在视频供稿时间查询的研究。例如，识别视频片段查询在此期间，相同的两个红色轿车和相同的两个人共同出现五分钟感兴趣的许多应用范围从执法的安全性和安全性。我们采取的第一个步骤，并定义在某种程度上这样的查询，他们将视频捕获的某些物理方面，如物体遮挡。我们提出由三层组成，分别是物体检测/跟踪，中间数据生成和查询评估的架构。我们提出两种技术，MFS和SSG，组织在中间数据生成层，从而有效地，给定的查询，最小化对象和具有查询求值过程要考虑的帧的数量的所有检测到的对象。我们还介绍了被称为国家遍历（ST）的算法来处理对SSG的数据帧，有效地修剪对象和无关的查询评估框架，同时保持了简洁的查询评估所需的所有状态。我们提出一个彻底的实验评估的利用真实和合成数据建立MFS和SSG之间的权衡结果。我们强调各种感兴趣的参数在我们的评价，并表明加上该算法的提出查询评估方法能够评估时间查询了视频饲料效率，实现数量级的性能好处订单。</font>
</div>


<hr>
<div id="paper117"> <b>117. Multi-View Learning for Vision-and-Language Navigation</b>  <a href="https://arxiv.org/pdf/2003.00857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title117" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiaolin Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiujun Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunyuan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yonatan Bisk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhifang Sui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yejin Choi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A" target="_blank" rel="noopener" style="color:#0000EE;">Noah A. Smith</a><br>
<font size="3">
Abstract: Learning to navigate in a visual environment following natural language instructions is a challenging task because natural language instructions are highly variable, ambiguous, and under-specified. In this paper, we present a novel training paradigm, Learn from EveryOne (LEO), which leverages multiple instructions (as different views) for the same trajectory to resolve language ambiguity and improve generalization. By sharing parameters across instructions, our approach learns more effectively from limited training data and generalizes better in unseen environments. On the recent Room-to-Room (R2R) benchmark dataset, LEO achieves 16% improvement (absolute) over a greedy agent as the base agent (25.3% $\rightarrow$ 41.4%) in Success Rate weighted by Path Length (SPL). Further, LEO is complementary to most existing models for vision-and-language navigation, allowing for easy integration with the existing techniques, leading to LEO+, which creates the new state of the art, pushing the R2R benchmark to 62% (9% absolute improvement). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：学习在一个可视化的环境中导航以下的自然语言指令是一项具有挑战性的任务，因为自然语言指令是高度可变的，暧昧的，并在指定的。在本文中，我们提出了一个新颖的培训模式，从每个人（LEO），它采用多种指令（如不同的看法）对同一轨迹决心语言歧义学习和提高泛化。通过从有限的训练数据，概括了在看不见的环境更好更有效地跨越指令共享参数，我们的方法可以学习。在最近的房间到房间（R2R）基准数据集，LEO达到16％的改善（绝对值）在贪婪剂为基剂（25.3％$ \ RIGHTARROW $ 41.4％）成功率的路径长度（SPL）加权。此外，LEO是为视觉和语言导航大多数现有车型的补充，允许与现有技术易于集成，导致LEO +，这创造了新的艺术状态，推R2R基准，以62％（9％绝对改善）。</font>
</div>


<hr>
<div id="paper118"> <b>118. Addressing target shift in zero-shot learning using grouped adversarial  learning</b>  <a href="https://arxiv.org/pdf/2003.00845" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title118" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chemmengath%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Saneem Ahmed Chemmengath</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bharadwaj%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samarth Bharadwaj</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Soumava Paul</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Samanta%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suranjana Samanta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sankaranarayanan%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karthik Sankaranarayanan</a><br>
<font size="3">
Abstract: In this paper, we present a new paradigm to zero-shot learning (ZSL) that is trained by utilizing additional information (such as attribute-class mapping) for specific set of unseen classes. We conjecture that such additional information about unseen classes is more readily available than unsupervised image sets. Further, on close examination of the underlying attribute predictors of popular ZSL algorithms, we find that they often leverage attribute correlations to make predictions. While attribute correlations that remain intact in the unseen classes (test) benefit the prediction of difficult attributes, change in correlations can have an adverse effect on ZSL performance. For example, detecting an attribute 'brown' may be the same as detecting 'fur' over an animals' image dataset captured in the tropics. However, such a model might fail on unseen images of Arctic animals. To address this effect, termed target-shift in ZSL, we utilize our proposed framework to design grouped adversarial learning. We introduce grouping of attributes to enable the model to continue to benefit from useful correlations, while restricting cross-group correlations that may be harmful for generalization. Our analysis shows that it is possible to not only constrain the model from leveraging unwanted correlations, but also adjust them to specific test setting using only the additional information (the already available attribute-class mapping). We show empirical results for zero-shot predictions on standard benchmark datasets, namely, aPY, AwA2, SUN and CUB datasets. We further introduce to the research community, a new experimental train-test split that maximizes target-shift to further study its effects. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个新的模式，以零射门学习（ZSL），它通过利用特定的一套看不见类的附加信息（例如属性级映射）培训。我们猜想大概看不见类的，这样的附加信息更容易比无监督图像集可用。此外，在流行ZSL算法的基本属性预测的仔细检查，我们发现，他们经常利用属性相关性进行预测。同时，在看不见类（试验）保持完整属性相关受益难以属性的预测，在相关性的变化可能对性能ZSL有不利的影响。例如，当检测的属性“棕色”可以是相同的检测“皮毛”过度在热带捕获的动物的图像数据集。然而，这种模式可能会失败北极动物看不见图像。为了解决这个问题的影响，被称为在ZSL目标的转变，我们利用我们提出的框架设计分组对抗的学习。我们引入分组属性，使模型继续从有用的相关利益，同时限制跨组相关性，可能是有害的概括。我们的分析表明，它是可能的，不仅从限制不必要的借力相关的模式，但也仅使用附加信息（已经提供的属性级映射）调整到特定的测试设置。我们显示标准基准数据集，即，APY，AwA2，SUN和CUB数据集零爆破预测实证结果。我们进一步介绍给研究界，一个新的实验列车试验分裂最大化目标转向进一步研究其影响。</font>
</div>


<hr>
<div id="paper119"> <b>119. Quantized Neural Network Inference with Precision Batching</b>  <a href="https://arxiv.org/pdf/2003.00822" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title119" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maximilian Lam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yedidia%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zachary Yedidia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Banbury%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Colby Banbury</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reddi%2C+V+J" target="_blank" rel="noopener" style="color:#0000EE;">Vijay Janapa Reddi</a><br>
<font size="3">
Abstract: We present PrecisionBatching, a quantized inference algorithm for speeding up neural network execution on traditional hardware platforms at low bitwidths without the need for retraining or recalibration. PrecisionBatching decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching not only facilitates quantized inference at low bitwidths (< 8 bits) without the need for retraining/recalibration, but also 1) enables traditional hardware platforms the ability to realize inference speedups at a finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. Across a variety of applications (MNIST, language modeling, natural language inference) and neural network architectures (fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of over 8x on a GPU within a < 1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5x-2x at the same error tolerance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出PrecisionBatching，加快对低bitwidths传统的硬件平台的神经网络执行，而不需要再培训或重新校准量化推理算法。 PrecisionBatching分解的神经网络到单独的bitlayers和使用快速1位操作，同时保持全精度激活积累他们。 PrecisionBatching不仅有利于在低bitwidths（<8个比特）量化推理，而不需要再培训 重新校准，而且1）使传统的硬件平台中的量化的更精细粒度（例如能力实现推理的加速比：1-16位执行）和2）允许精度和加速折衷在运行时由bitlayers数量暴露于累加作为一个可调参数。跨各种应用程序（mnist，语言建模，自然语言的推论）和神经网络结构（全连接，rnn，lstm），precisionbatching产量端至端的超过8倍的加速上的<1％的误差范围内的gpu完整的精密基准，超越传统的8位在同一容错量化推理超过1.5倍，2倍。< font>
</8个比特）量化推理，而不需要再培训></font></div>


<hr>
<div id="paper120"> <b>120. Convolutional Sparse Support Estimator Network (CSEN) From energy  efficient support estimation to learning-aided Compressive Sensing</b>  <a href="https://arxiv.org/pdf/2003.00768" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title120" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Yamac%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehmet Yamac</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ahishali%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mete Ahishali</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kiranyaz%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Serkan Kiranyaz</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Gabbouj%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Moncef Gabbouj</a><br>
<font size="3">
Abstract: Support estimation (SE) of a sparse signal refers to finding the location indices of the non-zero elements in a sparse representation. Most of the traditional approaches dealing with SE problem are iterative algorithms based on greedy methods or optimization techniques. Indeed, a vast majority of them use sparse signal recovery techniques to obtain support sets instead of directly mapping the non-zero locations from denser measurements (e.g., Compressively Sensed Measurements). This study proposes a novel approach for learning such a mapping from a training set. To accomplish this objective, the Convolutional Support Estimator Networks (CSENs), each with a compact configuration, are designed. The proposed CSEN can be a crucial tool for the following scenarios: (i) Real-time and low-cost support estimation can be applied in any mobile and low-power edge device for anomaly localization, simultaneous face recognition, etc. (ii) CSEN's output can directly be used as "prior information" which improves the performance of sparse signal recovery algorithms. The results over the benchmark datasets show that state-of-the-art performance levels can be achieved by the proposed approach with a significantly reduced computational complexity. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：稀疏信号的支持估计（SE）是指查找非零元素的位置索引中的稀疏表示。大多数处理SE问题的传统方法是基于贪婪方法或优化技术迭代算法。事实上，绝大多数的使用稀疏信号恢复技术来获得，而不是直接从映射更密集的测量（例如，压缩感测的测量值）中的非零位置的支持台。这项研究提出了从训练集学习这种映射的新方法。为了实现这个目标，卷积支持估计网络（CSENS），每个具有一个紧凑的结构，进行了设计。所提出的CSEN可以是用于在以下情况下的一个重要工具：（ⅰ）实时和低成本支持估计可以在任何移动和低功率边缘装置被应用于异常定位，同时人脸识别等。（ⅱ） CSEN的输出可以直接被用作这改善的稀疏信号恢复算法的性能“先验信息”。相对于基准数据集上的结果表明，国家的最先进的性能水平可以通过与显著降低计算复杂性所提出的方法来实现。</font>
</div>


<hr>
<div id="paper121"> <b>121. MVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation</b>  <a href="https://arxiv.org/pdf/2003.00667" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title121" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chanc%C3%A1n%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marvin Chancán</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milford%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Milford</a><br>
<font size="3">
Abstract: Autonomous navigation emerges from both motion and local visual perception in real-world environments. However, most successful robotic motion estimation methods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual place recognition-VPR) are often separately used for mapping and localization tasks. Conversely, recent reinforcement learning (RL) based methods for visual navigation rely on the quality of GPS data reception, which may not be reliable when directly using it as ground truth across multiple, month-spaced traversals in large environments. In this paper, we propose a novel motion and visual perception approach, dubbed MVP, that unifies these two sensor modalities for large-scale, target-driven navigation tasks. Our MVP-based method can learn faster, and is more accurate and robust to both extreme environmental changes and poor GPS data than corresponding vision-only navigation methods. MVP temporally incorporates compact image representations, obtained using VPR, with optimized motion estimation data, including but not limited to those from VO or optimized radar odometry (RO), to efficiently learn self-supervised navigation policies via RL. We evaluate our method on two large real-world datasets, Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast, night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall, summer) conditions using the new CityLearn framework; an interactive environment for efficiently training navigation agents. Our experimental results, on traversals of the Oxford RobotCar dataset with no GPS data, show that MVP can achieve 53% and 93% navigation success rate using VO and RO, respectively, compared to 7% for a vision-only method. We additionally report a trade-off between the RL success rate and the motion estimation precision. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自主导航从运动和当地的视觉感知真实世界的环境中出现。然而，最成功的机器人的运动估计方法（例如VO，SLAM，SFM）和视觉系统（例如CNN，视觉识别地点-VPR）通常分别用于标测和定位的任务。相反，对于可视化导航最近的强化学习（RL）的方法依赖于GPS数据接收的质量，在直接使用它作为跨多个地面实况，在大环境月份间距遍历这可能是不可靠的。在本文中，我们提出了一个新颖的运动和视觉感知的方法，被称为MVP，它统一为大规模，目标驱动导航任务这两个传感器的方式。我们基于MVP-方法可以学得更快，更精确和稳健两种极端的环境变化和相应的比仅视觉导航方法GPS数据不佳。 MVP时间上集成紧凑的图像表示，使用VPR获得，具有优化的运动估计的数据，包括但优化雷达测距法（RO）不限于那些从VO或，为了有效地学习经由RL自监督导航政策。我们评估我们的两个大现实世界的数据集，牛津Robotcar和诺德兰铁路，方法在一定范围的天气（如阴天，夜晚，雪，太阳，雨，云）和季节性（如冬，春，秋，夏季）条件使用新的CityLearn框架;为有效地训练导航代理一个互动的环境。我们的实验结果，在牛津RobotCar数据集没有GPS数据的遍历，表明MVP可以达到53％，并使用分别VO和RO，93％导航成功率，相比于仅视觉方法7％。我们还报告RL成功率和运动估计精度之间的权衡。</font>
</div>


<hr>
<div id="paper122"> <b>122. PSF--NET: A Non-parametric Point Spread Function Model for Ground Based  Optical Telescopes</b>  <a href="https://arxiv.org/pdf/2003.00615" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title122" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Jia%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Jia</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Wu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuebo Wu</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Huang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Huang</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Cai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bojun Cai</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Cai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongmei Cai</a><br>
<font size="3">
Abstract: Ground based optical telescopes are seriously affected by atmospheric turbulence induced aberrations. Understanding properties of these aberrations is important both for instruments design and image restoration methods development. Because the point spread function can reflect performance of the whole optic system, it is appropriate to use the point spread function to describe atmospheric turbulence induced aberrations. Assuming point spread functions induced by the atmospheric turbulence with the same profile belong to the same manifold space, we propose a non-parametric point spread function - PSF-NET. The PSF-NET has a cycle convolutional neural network structure and is a statistical representation of the manifold space of PSFs induced by the atmospheric turbulence with the same profile. Testing the PSF-NET with simulated and real observation data, we find that a well trained PSF--NET can restore any short exposure images blurred by atmospheric turbulence with the same profile. Besides, we further use the impulse response of the PSF-NET, which can be viewed as the statistical mean PSF, to analyze interpretation properties of the PSF-NET. We find that variations of statistical mean PSFs are caused by variations of the atmospheric turbulence profile: as the difference of the atmospheric turbulence profile increases, the difference between statistical mean PSFs also increases. The PSF-NET proposed in this paper provides a new way to analyze atmospheric turbulence induced aberrations, which would be benefit to develop new observation methods for ground based optical telescopes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于地面的光学望远镜是严重影响大气湍流引起的畸变。了解这些畸变的性质既是仪器的设计和图像复原方法发展具有重要意义。因为点扩散函数可以反映整个光学系统的性能，这是适合使用的点扩展函数来描述大气湍流诱发像差。假设用属于同一歧管空间相同的外形大气湍流引起的点扩散函数，我们提出了一种非参数的点扩散函数 -  PSF-NET。的PSF-NET具有循环卷积神经网络的结构，并通过用相同的配置文件中的大气湍流诱发的PSF的歧管空间的统计表示。测试PSF-NET与模拟和实际观测数据，我们发现，一个训练有素的PSF  -  NET可以恢复任何的短曝光图像模糊用相同的轮廓大气湍流。此外，我们还使用PSF-NET，这可以被看作是统计平均值PSF的脉冲响应，分析PSF-NET的解释性质。我们发现统计平均的PSF的变化是由大气湍流剖面的变化引起：由于大气湍流廓线的差增大，统计平均的PSF也增加之间的差额。本文提出的PSF-NET提供了一种新的方法来分析大气湍流引起的畸变，这将是受益于开发基于地面的光学望远镜新的观测方法。</font>
</div>


<hr>
<div id="paper123"> <b>123. 3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via  Coupled Feature Selection</b>  <a href="https://arxiv.org/pdf/2003.00535" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title123" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingang Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyang Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lili Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongkai Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiamao Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Zhang</a><br>
<font size="3">
Abstract: We propose a novel fast and robust 3D point clouds segmentation framework via coupled feature selection, named 3DCFS, that jointly performs semantic and instance segmentation. Inspired by the human scene perception process, we design a novel coupled feature selection module, named CFSM, that adaptively selects and fuses the reciprocal semantic and instance features from two tasks in a coupled manner. To further boost the performance of the instance segmentation task in our 3DCFS, we investigate a loss function that helps the model learn to balance the magnitudes of the output embedding dimensions during training, which makes calculating the Euclidean distance more reliable and enhances the generalizability of the model. Extensive experiments demonstrate that our 3DCFS outperforms state-of-the-art methods on benchmark datasets in terms of accuracy, speed and computational cost. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种快速，经由耦合特征选择强大的3D点云分割的框架，名为3DCFS，即共同执行语义和实例分割。由人的场景的感知过程的启发，我们设计了新型的耦合特征选择模块，命名CFSM，自适应地选择和融合的倒数语义和例如从在配对方式两个任务功能。为了进一步提升我们的3DCFS实例分割任务的性能，我们研究了损失函数，帮助模型学会平衡输出培训中嵌入的尺寸，这使得计算的欧氏距离更可靠，更增强了普遍性的大小模型。大量的实验证明在精度，速度和计算成本方面对标准数据集，我们3DCFS性能优于国家的最先进的方法。</font>
</div>


<hr>
<div id="paper124"> <b>124. Dimensionality reduction to maximize prediction generalization  capability</b>  <a href="https://arxiv.org/pdf/2003.00470" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title124" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/stat?searchtype=author&query=Isomura%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takuya Isomura</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Toyoizumi%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taro Toyoizumi</a><br>
<font size="3">
Abstract: This work develops an analytically solvable unsupervised learning scheme that extracts the most informative components for predicting future inputs, termed predictive principal component analysis (PredPCA). Our scheme can effectively remove unpredictable observation noise and globally minimize the test prediction error. Mathematical analyses demonstrate that, with sufficiently high-dimensional observations that are generated by a linear or nonlinear system, PredPCA can identify the optimal hidden state representation, true system parameters, and true hidden state dimensionality, with a global convergence guarantee. We demonstrate the performance of PredPCA by using sequential visual inputs comprising hand-digits, rotating 3D objects, and natural scenes. It reliably and accurately estimates distinct hidden states and predicts future outcomes of previously unseen test input data, even in the presence of considerable observation noise. The simple model structure and low computational cost of PredPCA make it highly desirable as a learning scheme for biological neural networks and neuromorphic chips. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：该作品的开发解析解无监督学习方案，其提取用于预测未来的输入的最有信息的部件，被称为预测主成分分析（PredPCA）。我们的方案可以有效地去除不可预知的观测噪声和全球最小化测试预测误差。数学分析表明，与由一个线性或非线性系统中产生足够高的维观察，PredPCA可以识别最佳隐藏状态表示，真正的系统参数，和真正的隐藏状态维数，具有全局收敛保证。我们证明PredPCA的通过使用连续的视觉输入包括手工数字，旋转的3D对象，以及自然场景的性能。它可靠和精确地估计不同隐藏状态和预测以前看不见的测试输入数据的未来的结果，即使在相当大的观测噪声的存在。简单的模型结构和PredPCA的低计算成本使作为生物神经网络和神经形态芯片学习计划是非常可取的。</font>
</div>


<hr>
<div id="paper125"> <b>125. Environment-agnostic Multitask Learning for Natural Language Grounded  Navigation</b>  <a href="https://arxiv.org/pdf/2003.00443" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title125" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vihan Jain</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ie%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eugene Ie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y" target="_blank" rel="noopener" style="color:#0000EE;">William Yang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kozareva%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zornitsa Kozareva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ravi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sujith Ravi</a><br>
<font size="3">
Abstract: Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalized navigation model from two novel perspectives: (1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks; (2) we propose to learn environment-agnostic representations for the navigation policy that are invariant among the environments seen during training, thus generalizing better on unseen environments. Extensive experiments show that our navigation model trained using environment-agnostic multitask learning significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing a new state-of-the-art for the NDH task. The code for training the navigation model using environment-agnostic multitask learning is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的研究工作能够在照片般逼真的环境中，例如，以下的自然语言指令或对话框的自然语言接地导航研究。但是，现有的方法往往在看到环境过度拟合训练数据，并未能在以前看不见的环境下推广好。为了关闭可见和不可见的环境之间的间隙中，我们的目标是在学习从两个新颖的观点广义导航模型：（1）我们介绍可以同时在视觉语言导航（VLN）和导航无缝训练多任务导航模型从对话历史（NDH）的任务，从更丰富的自然语言指导的利益和整个任务有效地传递知识; （2）我们提出学习的导航策略，是在训练中看到的环境中不变的环境无关的交涉，从而对看不见的环境中更好的推广。大量的实验表明，我们的导航模式使用环境无关的多任务显著学习培训的减少可见和不可见的环境之间的性能差距，并通过16％的VLN优于对看不见的环境基线（成功率相对度量）和120％（目标的进展情况）在NDH，建立一个新的国家的最先进的NDH任务。训练使用环境无关的多任务学习导航模型的代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper126"> <b>126. Why is the Mahalanobis Distance Effective for Anomaly Detection?</b>  <a href="https://arxiv.org/pdf/2003.00402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title126" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/stat?searchtype=author&query=Kamoi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryo Kamoi</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Kobayashi%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kei Kobayashi</a><br>
<font size="3">
Abstract: The Mahalanobis distance-based confidence score, a recently proposed anomaly detection method for pre-trained neural classifiers, achieves state-of-the-art performance on both out-of-distribution and adversarial example detection. This work analyzes why this method exhibits such strong performance while imposing an implausible assumption; namely, that class conditional distributions of intermediate features have tied covariance. We reveal that the reason for its effectiveness has been misunderstood. Although this method scores the prediction confidence for the original classification task, our analysis suggests that information critical for classification task does not contribute to state-of-the-art performance on anomaly detection. To support this hypothesis, we demonstrate that a simpler confidence score that does not use class information is as effective as the original method in most cases. Moreover, our experiments show that the confidence scores can exhibit different behavior on other frameworks such as metric learning models, and their detection performance is sensitive to model architecture choice. These findings provide insight into the behavior of neural classifiers when provided with anomalous inputs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于马氏距离的置信度得分，用于预训练神经分类最近提出的异常检测方法，实现了两个外的分布和对抗例如检测状态的最先进的性能。这项工作分析为什么这个方法表现而堂堂一个难以置信的假设，这种强劲的表现;即，中间特征该类条件分布有捆绑协方差。我们揭示了其有效性的原因被误解了。这种方法虽然成绩为原分类任务的预测有信心，我们的分析表明了分类任务至关重要的信息，不利于国家的最先进的性能上异常检测。为了支持这一假说，我们证明了不使用类信息进行简单的置信度得分有效，因为在大多数情况下，原来的方法。此外，我们的实验表明，信心分数可以表现出对其他框架不同的行为，如度量学习模型，他们的检测性能是模型架构的选择敏感。当有异常的输入提供的这些发现提供了洞察神经分类的行为。</font>
</div>


<hr>
<div id="paper127"> <b>127. Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI  Components by Deep Learning</b>  <a href="https://arxiv.org/pdf/2003.00380" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title127" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jieshan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunyang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenchang Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiwei Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liming Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoqiang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinshui Wang</a><br>
<font size="3">
Abstract: According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android apps. Most of these issues are caused by developers' lack of awareness and knowledge in considering the minority. And even if developers want to add the labels to UI components, they may not come up with concise and clear description as most of them are of no visual issues. To overcome these challenges, we develop a deep-learning based model, called LabelDroid, to automatically predict the labels of image-based buttons by learning from large-scale commercial apps in Google Play. The experimental results show that our model can make accurate predictions and the generated labels are of higher quality than that from real Android developers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：根据世界卫生组织（WHO），据估计，约130十亿人生活在某种形式的视力障碍的全球范围内，其中36万盲人。由于他们的残疾，从事这些少数族裔融入社会是一个具有挑战性的问题。最近的智能手机的崛起提供了通过使盲人用户对信息和服务的方便访问了了解世界的一个新的解决方案。有视力障碍的用户可以通过嵌入在移动操作系统的屏幕阅读器读取每个屏幕的应用中的内容，并用手势来与手机互动。然而，使用屏幕阅读器的前提是开发商必须自然语言标签添加到基于图像的组件时，他们正在开发的应用程序。不幸的是，超过77级％的应用程序都缺少标签的问题，根据我们的10408个Android应用分析。大多数的这些问题是由开发者缺乏考虑少数人的认识和知识造成的。而且，即使开发商想将标签添加到UI组件，它们可能无法拿出简洁清晰的描述，因为大多数人都没有直观的问题。为了克服这些挑战，我们开发了一个深刻的学习基于模型，称为LabelDroid，通过在谷歌玩大型的商业应用程序学习自动预测基于图像的按钮的标签。实验结果表明，我们的模型可以做出准确的预测和生成的标签比从真正的Android开发人员具有更高的质量。</font>
</div>


<hr>
<div id="paper128"> <b>128. Understanding the Intrinsic Robustness of Image Distributions using  Conditional Generative Models</b>  <a href="https://arxiv.org/pdf/2003.00378" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title128" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinghui Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Quanquan Gu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Evans%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Evans</a><br>
<font size="3">
Abstract: Starting with Gilmer et al. (2018), several works have demonstrated the inevitability of adversarial examples based on different assumptions about the underlying input probability space. It remains unclear, however, whether these results apply to natural image distributions. In this work, we assume the underlying data distribution is captured by some conditional generative model, and prove intrinsic robustness bounds for a general class of classifiers, which solves an open problem in Fawzi et al. (2018). Building upon the state-of-the-art conditional generative models, we study the intrinsic robustness of two common image benchmarks under $\ell_2$ perturbations, and show the existence of a large gap between the robustness limits implied by our theory and the adversarial robustness achieved by current state-of-the-art robust models. Code for all our experiments is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：吉尔默等人开始。 （2018），几工作已经证明基于关于底层输入概率空间中的不同的假设对抗性例子的必然性。目前还不清楚，但是，这些结果是否适用于自然的图像分布。在这项工作中，我们假设底层的数据分布是通过一些有条件的生成模型捕获，并证明了通用类的分类，这在法齐等人解决了一个公开问题的内在稳健发展。 （2018）。在国家的最先进的条件生成模型的基础上，我们研究了两种常见的图像基准固有的稳健性在$ \ ell_2 $扰动，并显示我们的理论和对抗所隐含的鲁棒性限制之间有很大的差距的存在通过稳健的国家的最先进的电流可靠的模型来实现。代号为我们所有的实验可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper129"> <b>129. An Evaluation of Knowledge Graph Embeddings for Autonomous Driving Data:  Experience and Practice</b>  <a href="https://arxiv.org/pdf/2003.00344" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title129" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wickramarachchi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruwan Wickramarachchi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Henson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cory Henson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sheth%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amit Sheth</a><br>
<font size="3">
Abstract: The autonomous driving (AD) industry is exploring the use of knowledge graphs (KGs) to manage the vast amount of heterogeneous data generated from vehicular sensors. The various types of equipped sensors include video, LIDAR and RADAR. Scene understanding is an important topic in AD which requires consideration of various aspects of a scene, such as detected objects, events, time and location. Recent work on knowledge graph embeddings (KGEs) - an approach that facilitates neuro-symbolic fusion - has shown to improve the predictive performance of machine learning models. With the expectation that neuro-symbolic fusion through KGEs will improve scene understanding, this research explores the generation and evaluation of KGEs for autonomous driving data. We also present an investigation of the relationship between the level of informational detail in a KG and the quality of its derivative embeddings. By systematically evaluating KGEs along four dimensions -- i.e. quality metrics, KG informational detail, algorithms, and datasets -- we show that (1) higher levels of informational detail in KGs lead to higher quality embeddings, (2) type and relation semantics are better captured by the semantic transitional distance-based TransE algorithm, and (3) some metrics, such as coherence measure, may not be suitable for intrinsically evaluating KGEs in this domain. Additionally, we also present an (early) investigation of the usefulness of KGEs for two use-cases in the AD domain. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用自主行驶（AD）行业正在探索使用知识图（KGS）的管理与车辆传感器产生的异构数据的大量。各种类型配备传感器包括视频，LIDAR和雷达。场景理解是AD的一个重要课题，需要考虑的一个场景的各个方面，如检测到物体，事件，时间和地点。一种方法有利于神经象征性的融合 -   - 已经表明，以改善机器学习模型的预测性能知识图嵌入（KGEs）最近的工作。并期望通过KGEs神经象征性的融合将提高现场了解，该研究探讨KGEs用于自主驾驶数据的生成和评估。我们还提出信息的细节在KG水平及其衍生的嵌入的质量之间的关系进行调查。通过系统地评估沿四个维度KGEs  - 即质量度量，KG信息细节，算法，和数据集 - 我们表明，（1）更高的水平在幼稚园信息的细节导致更高质量的嵌入，（2）式和关系的语义是由语义过渡基于距离的TRANSE算法，以及（3）一些度量，诸如相干性测量更好捕获，可能不适合于本领域固有评估KGEs。此外，我们还提出KGEs的有用的（早期）调查了两个使用情况的AD域。</font>
</div>


<hr>
<div id="paper130"> <b>130. An estimation-based method to segment PET images</b>  <a href="https://arxiv.org/pdf/2003.00317" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title130" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/physics?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziping Liu</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Laforest%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Laforest</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Mhlanga%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joyce Mhlanga</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Moon%2C+H+S" target="_blank" rel="noopener" style="color:#0000EE;">Hae Sol Moon</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Fraum%2C+T+J" target="_blank" rel="noopener" style="color:#0000EE;">Tyler J. Fraum</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Itani%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Malak Itani</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Mintz%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aaron Mintz</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Dehdashti%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Farrokh Dehdashti</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Siegel%2C+B+A" target="_blank" rel="noopener" style="color:#0000EE;">Barry A. Siegel</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Jha%2C+A+K" target="_blank" rel="noopener" style="color:#0000EE;">Abhinav K. Jha</a><br>
<font size="3">
Abstract: Tumor segmentation in oncological PET images is challenging, a major reason being the partial-volume effects that arise from low system resolution and a finite pixel size. The latter results in pixels containing more than one region, also referred to as tissue-fraction effects. Conventional classification-based segmentation approaches are inherently limited in accounting for the tissue-fraction effects. To address this limitation, we pose the segmentation task as an estimation problem. We propose a Bayesian method that estimates the posterior mean of the tumorfraction area within each pixel and uses these estimates to define the segmented tumor boundary. The method was implemented using an autoencoder. Quantitative evaluation of the method was performed using realistic simulation studies conducted in the context of segmenting the primary tumor in PET images of patients with lung cancer. For these studies, a framework was developed to generate clinically realistic simulated PET images. Realism of these images was quantitatively confirmed using a two-alternative-forced-choice study by six trained readers with expertise in reading PET scans. The evaluation studies demonstrated that the proposed segmentation method was accurate, significantly outperformed widely used conventional methods on the tasks of tumor segmentation and estimation of tumor-fraction areas, was relatively insensitive to partial-volume effects, and reliably estimated the ground-truth tumor boundaries. Further, these results were obtained across different clinical-scanner configurations. This proof-of-concept study demonstrates the efficacy of an estimation-based approach to PET segmentation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在肿瘤PET图像肿瘤分割是具有挑战性，一个主要的原因是，从较低的系统的分辨率和有限像素尺寸出现的部分卷影响。在包含多于一个的区域中的像素后者的效果，也被称为组织分数的影响。常规的基于分类的分割方法在占组织馏分效果固有地受到限制。为了解决这个限制，我们提出分割任务作为估计的问题。我们建议，估计每个像素内的tumorfraction区域的后均值，并使用这些估计来定义分割肿瘤边界的贝叶斯方法。方法，使用自动编码器来实现。使用分割患者PET图像的原发性肿瘤为肺癌的上下文中进行逼真的模拟研究中执行的方法的定量评价。对于这些研究，一个框架的开发是为了产生临床逼真的模拟PET图像。这些图像的现实使用由具有专业知识的培训6名读者两另类强迫选择学习阅读PET扫描定量证实。证明所提出的分割方法准确的评价研究，显著跑赢广泛用于对肿瘤分割和肿瘤部分区域的估计的任务的常规方法，是相对不敏感的部分卷效果，可靠地估计地面实况肿瘤边界。此外，在不同的临床扫描器配置，得到了这些结果。验证的概念这项研究表明，基于估计的方法来PET细分的功效。</font>
</div>


<hr>
<div id="paper131"> <b>131. Unsupervised Dictionary Learning for Anomaly Detection</b>  <a href="https://arxiv.org/pdf/2003.00293" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title131" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Irofti%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Irofti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=B%C4%83ltoiu%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andra Băltoiu</a><br>
<font size="3">
Abstract: We investigate the possibilities of employing dictionary learning to address the requirements of most anomaly detection applications, such as absence of supervision, online formulations, low false positive rates. We present new results of our recent semi-supervised online algorithm, TODDLeR, on a anti-money laundering application. We also introduce a novel unsupervised method of using the performance of the learning algorithm as indication of the nature of the samples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们研究使用的词典学习，解决最异常检测的应用，如缺乏监管，网上的配方，低误报率的要求的可能性。我们提出我们最近的半监督在线算法，蹒跚学步的新成果，在反洗钱中的应用。我们还介绍了使用学习算法作为样品的性质的指示的性能的新颖的无监督方法。</font>
</div>


<hr>
<div id="paper132"> <b>132. Image Hashing by Minimizing Independent Relaxed Wasserstein Distance</b>  <a href="https://arxiv.org/pdf/2003.00134" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title132" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Doan%2C+K+D" target="_blank" rel="noopener" style="color:#0000EE;">Khoa D. Doan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kimiyaie%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amir Kimiyaie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manchanda%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Saurav Manchanda</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reddy%2C+C+K" target="_blank" rel="noopener" style="color:#0000EE;">Chandan K. Reddy</a><br>
<font size="3">
Abstract: Image hashing is a fundamental problem in the computer vision domain with various challenges, primarily, in terms of efficiency and effectiveness. Existing hashing methods lack a principled characterization of the goodness of the hash codes and a principled approach to learn the discrete hash functions that are being optimized in the continuous space. Adversarial autoencoders are shown to be able to implicitly learn a robust hash function that generates hash codes which are balanced and have low-quantization error. However, the existing adversarial autoencoders for hashing are too inefficient to be employed for large-scale image retrieval applications because of the minmax optimization procedure. In this paper, we propose an Independent Relaxed Wasserstein Autoencoder, which presents a novel, efficient hashing method that can implicitly learn the optimal hash function by directly training the adversarial autoencoder without any discriminator/critic. Our method is an order-of-magnitude more efficient and has a much lower sample complexity than the Optimal Transport formulation of the Wasserstein distance. The proposed method outperforms the current state-of-the-art image hashing methods for the retrieval task on several prominent image collections. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图像哈希是各种挑战，主要是，在效率和有效性方面的计算机视觉领域的一个基本问题。现有散列方法缺乏哈希码的善良的原则性表征和一个原则性方法来了解在该连续空间被优化离散哈希函数。对抗性自动编码的证明是能够隐含了解其产生的平衡，并具有低量化误差的散列码强大的哈希函数。然而，对于散列现有的对抗性自动编码的效率太低被用于因为极小极大优化程序的大型图像检索的应用。在本文中，我们提出了一个独立的宽松瓦瑟斯坦自动编码器，它提出了一个新颖的，可以通过直接训练对抗性自动编码器没有任何鉴别/评论家隐学习的最佳散列函数高效散列方法。我们的方法是命令的数量级更有效的，并且具有比瓦瑟斯坦距离的最佳运输制剂低得多的样品的复杂性。该方法优于用于检索任务上几个著名的图像集合当前国家的最先进的图像哈希方法。</font>
</div>


<hr>
<div id="paper133"> <b>133. Inexpensive surface electromyography sleeve with consistent electrode  placement enables dexterous and stable prosthetic control through deep  learning</b>  <a href="https://arxiv.org/pdf/2003.00070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title133" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=George%2C+J+A" target="_blank" rel="noopener" style="color:#0000EE;">Jacob A. George</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neibling%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Neibling</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Paskett%2C+M+D" target="_blank" rel="noopener" style="color:#0000EE;">Michael D. Paskett</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+G+A" target="_blank" rel="noopener" style="color:#0000EE;">Gregory A. Clark</a><br>
<font size="3">
Abstract: The dexterity of conventional myoelectric prostheses is limited in part by the small datasets used to train the control algorithms. Variations in surface electrode positioning make it difficult to collect consistent data and to estimate motor intent reliably over time. To address these challenges, we developed an inexpensive, easy-to-don sleeve that can record robust and repeatable surface electromyography from 32 embedded monopolar electrodes. Embedded grommets are used to consistently align the sleeve with natural skin markings (e.g., moles, freckles, scars). The sleeve can be manufactured in a few hours for less than $60. Data from seven intact participants show the sleeve provides a signal-to-noise ratio of 14, a don-time under 11 seconds, and sub-centimeter precision for electrode placement. Furthermore, in a case study with one intact participant, we use the sleeve to demonstrate that neural networks can provide simultaneous and proportional control of six degrees of freedom, even 263 days after initial algorithm training. We also highlight that consistent recordings, accumulated over time to establish a large dataset, significantly improve dexterity. These results suggest that deep learning with a 74-layer neural network can substantially improve the dexterity and stability of myoelectric prosthetic control, and that deep-learning techniques can be readily instantiated and further validated through inexpensive sleeves/sockets with consistent recording locations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：常规肌电假肢的灵巧部分地由用于训练的控制算法的小的数据集的限制。在表面电极定位的变化使得很难收集一致的数据，并估算电动机意图可靠地随着时间的推移。为了应对这些挑战，我们开发了一种廉价的，易于穿上套可从32个嵌入式单极电极记录强大的和可重复的表面肌电。嵌入垫圈用于一致地对准天然皮肤标记（例如，痣，雀斑，疤痕）的套筒。套筒可以在几个小时内被用于制造低于$ 60来自七个完整的参与者的数据显示该套筒提供一个信噪比的14，低于11秒的DON-时间，和亚厘米精度的电极放置。此外，在一个完整的参与者的案例研究，我们使用套筒证明神经网络可以提供六个自由度，在最初的算法训练甚至263天同时和比例控制。我们还强调，一致的录音，日积月累建立了庞大的数据集，显著提高灵活性。这些结果表明用一个74层的神经网络深学习可以显着提高肌电假体控制的灵巧性和稳定性，而且深学习技术可以容易地实例化，并通过与相一致的记录的位置廉价的套筒/套筒进一步验证。</font>
</div>


<hr>
<div id="paper134"> <b>134. SeismiQB -- a novel framework for deep learning with seismic data</b>  <a href="https://arxiv.org/pdf/2001.06416" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title134" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/physics?searchtype=author&query=Koryagin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Koryagin</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Khudorozhkov%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roman Khudorozhkov</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Tsimfer%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergey Tsimfer</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Mylzenova%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Darima Mylzenova</a><br>
<font size="3">
Abstract: In recent years, Deep Neural Networks were successfully adopted in numerous domains to solve various image-related tasks, ranging from simple classification to fine borders annotation. Naturally, many researches proposed to use it to solve geological problems. Unfortunately, many of the seismic processing tools were developed years before the era of machine learning, including the most popular SEG-Y data format for storing seismic cubes. Its slow loading speed heavily hampers experimentation speed, which is essential for getting acceptable results. Worse yet, there is no widely-used format for storing surfaces inside the volume (for example, seismic horizons). To address these problems, we've developed an open-sourced Python framework with emphasis on working with neural networks, that provides convenient tools for (i) fast loading seismic cubes in multiple data formats and converting between them, (ii) generating crops of desired shape and augmenting them with various transformations, and (iii) pairing cube data with labeled horizons or other types of geobodies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，深层神经网络在众多领域进行了成功采用，解决各种图像相关的任务，从简单的分类细边框标注。当然，许多研究建议用它来解决地质问题。不幸的是，很多的地震数据处理工具，机器学习的时代年前被开发，其中包括用于存储地震立方体最流行的SEG-Y数据格式。它的加载速度慢的速度很大程度上阻碍了实验速度，这是获得可接受的结果至关重要。更糟糕的是，不存在用于存储表面的体积内（例如，地震层位）广泛使用的格式。为了解决这些问题，我们开发了与神经网络的工作重点是一个开源的Python框架，它提供便捷的工具，（我）快装于多种数据格式的地震立方体和它们之间的转换，（ii）产生的作物所需的形状，并用各种变换增强它们，以及（iii）用标记的视野或其它类型的地质体的配对立方体数据。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-03-03</title>
    <url>/2020/03/03/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-03/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Gated Mechanism for Attention Based Multimodal Sentiment Analysis <a href="https://arxiv.org/pdf/2003.01043" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Identification of primary and collateral tracks in stuttered speech <a href="https://arxiv.org/pdf/2003.01018" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Multi-View Learning for Vision-and-Language Navigation <a href="https://arxiv.org/pdf/2003.00857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> PhoBERT: Pre-trained language models for Vietnamese <a href="https://arxiv.org/pdf/2003.00744" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Style Example-Guided Text Generation using Generative Adversarial  Transformers <a href="https://arxiv.org/pdf/2003.00674" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Learning from Easy to Complex: Adaptive Multi-curricula Learning for  Neural Dialogue Generation <a href="https://arxiv.org/pdf/2003.00639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> StructSum: Incorporating Latent and Explicit Sentence Dependencies for  Single Document Summarization <a href="https://arxiv.org/pdf/2003.00576" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Clinical Text Summarization with Syntax-Based Negation and Semantic  Concept Identification <a href="https://arxiv.org/pdf/2003.00353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Voice trigger detection from LVCSR hypothesis lattices using  bidirectional lattice recurrent neural networks <a href="https://arxiv.org/pdf/2003.00304" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Depth-Adaptive Graph Recurrent Network for Text Classification <a href="https://arxiv.org/pdf/2003.00166" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> AraBERT: Transformer-based Model for Arabic Language Understanding <a href="https://arxiv.org/pdf/2003.00104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> The STEM-ECR Dataset: Grounding Scientific Entity References in STEM  Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources <a href="https://arxiv.org/pdf/2003.01006" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Pathological speech detection using x-vector embeddings <a href="https://arxiv.org/pdf/2003.00864" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Long Short-Term Sample Distillation <a href="https://arxiv.org/pdf/2003.00739" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Environment-agnostic Multitask Learning for Natural Language Grounded  Navigation <a href="https://arxiv.org/pdf/2003.00443" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> What Emotions Make One or Five Stars? Understanding Ratings of Online  Product Reviews by Sentiment Analysis and XAI <a href="https://arxiv.org/pdf/2003.00201" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Gated Mechanism for Attention Based Multimodal Sentiment Analysis</b>  <a href="https://arxiv.org/pdf/2003.01043" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ayush Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vepa%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jithendra Vepa</a><br>
<font size="3">
Abstract: Multimodal sentiment analysis has recently gained popularity because of its relevance to social media posts, customer service calls and video blogs. In this paper, we address three aspects of multimodal sentiment analysis; 1. Cross modal interaction learning, i.e. how multiple modalities contribute to the sentiment, 2. Learning long-term dependencies in multimodal interactions and 3. Fusion of unimodal and cross modal cues. Out of these three, we find that learning cross modal interactions is beneficial for this problem. We perform experiments on two benchmark datasets, CMU Multimodal Opinion level Sentiment Intensity (CMU-MOSI) and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Our approach on both these tasks yields accuracies of 83.9% and 81.1% respectively, which is 1.6% and 1.34% absolute improvement over current state-of-the-art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多模态的情感分析最近获得，因为其相关的社交媒体帖子，客户服务电话和视频博客的人气。在本文中，我们要解决多模态的情感分析的三个方面; 1.交叉模态相互作用的学习，即模式如何将多个向情绪，2.多模式交互学习长期依赖性和3的单峰和交叉模态线索的融合。在这些三，我们发现，学习跨模态相互作用，对于这个问题是有益的。我们在两个基准数据集进行实验，CMU多式联运意见级别情绪强度（CMU-MOSI）和CMU多式联运意见和情绪情感强度（CMU-MOSEI）语料库。我们在这两个任务分别做法产生的83​​.9％和81.1％的精度，这是1.6％和1.34％的绝对改进过电流状态的最先进的。</font>
</div>


<hr>
<div id="paper2"> <b>2. Identification of primary and collateral tracks in stuttered speech</b>  <a href="https://arxiv.org/pdf/2003.01018" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Riad%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rachid Riad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bachoud-L%C3%A9vi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anne-Catherine Bachoud-Lévi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rudzicz%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Frank Rudzicz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dupoux%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emmanuel Dupoux</a><br>
<font size="3">
Abstract: Disfluent speech has been previously addressed from two main perspectives: the clinical perspective focusing on diagnostic, and the Natural Language Processing (NLP) perspective aiming at modeling these events and detect them for downstream tasks. In addition, previous works often used different metrics depending on whether the input features are text or speech, making it difficult to compare the different contributions. Here, we introduce a new evaluation framework for disfluency detection inspired by the clinical and NLP perspective together with the theory of performance from \cite{clark1996using} which distinguishes between primary and collateral tracks. We introduce a novel forced-aligned disfluency dataset from a corpus of semi-directed interviews, and present baseline results directly comparing the performance of text-based features (word and span information) and speech-based (acoustic-prosodic information). Finally, we introduce new audio features inspired by the word-based span features. We show experimentally that using these features outperformed the baselines for speech-based predictions on the present dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：不流利的发言已经从以前的两个主要观点解决：临床的角度侧重于诊断，和自然语言处理（NLP）的角度，旨在模拟这些事件并检测它们对下游任务。此外，以前的作品中经常使用这取决于输入的功能是否是文本或语音，因此很难比较不同的贡献不同的指标。在这里，我们介绍通过临床和NLP的角度与性能从理论的启发在一起不流利检测新的评估框架\ {引用} clark1996using小学和抵押轨道之间用以区别。我们引入新的强制对齐不流利的数据集从半定向采访的语料库，和现在的基准结果直接比较的基于文本的功能（字和跨度信息）和基于语音（声韵律信息）的性能。最后，我们通过引入基于单词的跨功能激发了新的音频功能。我们展示实验上使用这些功能胜过对本数据集基于语音的预测基线。</font>
</div>


<hr>
<div id="paper3"> <b>3. Multi-View Learning for Vision-and-Language Navigation</b>  <a href="https://arxiv.org/pdf/2003.00857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiaolin Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiujun Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunyuan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bisk%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yonatan Bisk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhifang Sui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yejin Choi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N+A" target="_blank" rel="noopener" style="color:#0000EE;">Noah A. Smith</a><br>
<font size="3">
Abstract: Learning to navigate in a visual environment following natural language instructions is a challenging task because natural language instructions are highly variable, ambiguous, and under-specified. In this paper, we present a novel training paradigm, Learn from EveryOne (LEO), which leverages multiple instructions (as different views) for the same trajectory to resolve language ambiguity and improve generalization. By sharing parameters across instructions, our approach learns more effectively from limited training data and generalizes better in unseen environments. On the recent Room-to-Room (R2R) benchmark dataset, LEO achieves 16% improvement (absolute) over a greedy agent as the base agent (25.3% $\rightarrow$ 41.4%) in Success Rate weighted by Path Length (SPL). Further, LEO is complementary to most existing models for vision-and-language navigation, allowing for easy integration with the existing techniques, leading to LEO+, which creates the new state of the art, pushing the R2R benchmark to 62% (9% absolute improvement). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：学习在一个可视化的环境中导航以下的自然语言指令是一项具有挑战性的任务，因为自然语言指令是高度可变的，暧昧的，并在指定的。在本文中，我们提出了一个新颖的培训模式，从每个人（LEO），它采用多种指令（如不同的看法）对同一轨迹决心语言歧义学习和提高泛化。通过从有限的训练数据，概括了在看不见的环境更好更有效地跨越指令共享参数，我们的方法可以学习。在最近的房间到房间（R2R）基准数据集，LEO达到16％的改善（绝对值）在贪婪剂为基剂（25.3％$ \ RIGHTARROW $ 41.4％）成功率的路径长度（SPL）加权。此外，LEO是为视觉和语言导航大多数现有车型的补充，允许与现有技术易于集成，导致LEO +，这创造了新的艺术状态，推R2R基准，以62％（9％绝对改善）。</font>
</div>


<hr>
<div id="paper4"> <b>4. PhoBERT: Pre-trained language models for Vietnamese</b>  <a href="https://arxiv.org/pdf/2003.00744" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+D+Q" target="_blank" rel="noopener" style="color:#0000EE;">Dat Quoc Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+A+T" target="_blank" rel="noopener" style="color:#0000EE;">Anh Tuan Nguyen</a><br>
<font size="3">
Abstract: We present PhoBERT with two versions of "base" and "large"--the first public large-scale monolingual language models pre-trained for Vietnamese. We show that PhoBERT improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT is released at: this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前PhoBERT与“基地”和“大”的两个版本 - 第一次公开大规模的单语语言模型预训练越南。我们发现，PhoBERT提高了国家的最先进的多种具体的越南NLP任务，包括部分词性标注，命名实体识别和自然语言推理。我们发布PhoBERT，以方便未来的研究和越南NLP下游应用。此HTTPS URL：我们PhoBERT在发布</font>
</div>


<hr>
<div id="paper5"> <b>5. Style Example-Guided Text Generation using Generative Adversarial  Transformers</b>  <a href="https://arxiv.org/pdf/2003.00674" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kuo-Hao Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Shoeybi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Yu Liu</a><br>
<font size="3">
Abstract: We introduce a language generative model framework for generating a styled paragraph based on a context sentence and a style reference example. The framework consists of a style encoder and a texts decoder. The style encoder extracts a style code from the reference example, and the text decoder generates texts based on the style code and the context. We propose a novel objective function to train our framework. We also investigate different network design choices. We conduct extensive experimental validation with comparison to strong baselines to validate the effectiveness of the proposed framework using a newly collected dataset with diverse text styles. Both code and dataset will be released upon publication. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用于基于上下文的句子和样式参考示例风格的段落介绍语言生成模型框架。该框架包括一个风格编码器和解码器的文本中。样式编码器提取从参考例的样式码，并且解码器基于样式代码和上下文文本的文本。我们提出了一个新的目标函数来训练我们的框架。我们还研究了不同的网络设计选择。我们进行了广泛的实验验证与比较强的基线，以验证使用与不同的文本样式新收集的数据集所提出的框架的有效性。代码和数据集将出版时被释放。</font>
</div>


<hr>
<div id="paper6"> <b>6. Learning from Easy to Complex: Adaptive Multi-curricula Learning for  Neural Dialogue Generation</b>  <a href="https://arxiv.org/pdf/2003.00639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hengyi Cai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongshen Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yonghao Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaofang Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yangxi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongsheng Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dawei Yin</a><br>
<font size="3">
Abstract: Current state-of-the-art neural dialogue systems are mainly data-driven and are trained on human-generated responses. However, due to the subjectivity and open-ended nature of human conversations, the complexity of training dialogues varies greatly. The noise and uneven complexity of query-response pairs impede the learning efficiency and effects of the neural dialogue generation models. What is more, so far, there are no unified dialogue complexity measurements, and the dialogue complexity embodies multiple aspects of attributes---specificity, repetitiveness, relevance, etc. Inspired by human behaviors of learning to converse, where children learn from easy dialogues to complex ones and dynamically adjust their learning progress, in this paper, we first analyze five dialogue attributes to measure the dialogue complexity in multiple perspectives on three publicly available corpora. Then, we propose an adaptive multi-curricula learning framework to schedule a committee of the organized curricula. The framework is established upon the reinforcement learning paradigm, which automatically chooses different curricula at the evolving learning process according to the learning status of the neural dialogue generation model. Extensive experiments conducted on five state-of-the-art models demonstrate its learning efficiency and effectiveness with respect to 13 automatic evaluation metrics and human judgments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当前国家的最先进的神经对话系统主要是数据驱动和对人体产生反应的训练。然而，由于主观性和人类对话的开放性质，训练对话的复杂程度差别很大。噪声和不均匀的查询响应对的复杂性阻碍了学习效率和神经对话代车型的影响。更重要的是，到目前为止，还没有统一的对话复杂的测量和属性的对话复杂性体现多个方面---特异性，重复性，相关性等，通过学习，相反，人类的行为，让孩子从简单的对话，启发学习对复杂的，动态调整自己的学习进度，在本文中，我们首先分析了5个对话属性来衡量的三个公开可用的语料中多角度的对话复杂性。然后，我们提出了一种自适应多的课程学习框架安排有组织的课程组成的委员会。该框架是在强化学习模式，根据神经对话代模型的学习状态在不断变化的学习过程中自动选择不同的课程成立。在五个国家的最先进的模型进行了广泛的实验证明其学习效率和效力相对于13个自动评价指标和人的判断。</font>
</div>


<hr>
<div id="paper7"> <b>7. StructSum: Incorporating Latent and Explicit Sentence Dependencies for  Single Document Summarization</b>  <a href="https://arxiv.org/pdf/2003.00576" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Balachandran%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vidhisha Balachandran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pagnoni%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Artidoro Pagnoni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Jay Yoon Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rajagopal%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dheeraj Rajagopal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carbonell%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaime Carbonell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tsvetkov%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulia Tsvetkov</a><br>
<font size="3">
Abstract: Traditional preneural approaches to single document summarization relied on modeling the intermediate structure of a document before generating the summary. In contrast, the current state of the art neural summarization models do not preserve any intermediate structure, resorting to encoding the document as a sequence of tokens. The goal of this work is two-fold: to improve the quality of generated summaries and to learn interpretable document representations for summarization. To this end, we propose incorporating latent and explicit sentence dependencies into single-document summarization models. We use structure-aware encoders to induce latent sentence relations, and inject explicit coreferring mention graph across sentences to incorporate explicit structure. On the CNN/DM dataset, our model outperforms standard baselines and provides intermediate latent structures for analysis. We present an extensive analysis of our summaries and show that modeling document structure reduces copying long sequences and incorporates richer content from the source document while maintaining comparable summary lengths and an increased degree of abstraction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：传统preneural方法单个文档文摘依赖于产生该摘要之前建模文档的中间结构。与此相反，在本领域的神经总结模型的当前状态不保留任何中间结构，诉诸于编码文档作为标记序列。这项工作的目的是双重的：改善产生摘要的质量和学习解释文档表示进行汇总。为此，我们建议结合潜在的和明确的句子的依赖，进入单文档自动文摘模型。我们使用结构感知编码器，诱导潜在的句子关系，和整个句子注入明确提及coreferring图形纳入明确的结构。在CNN / DM数据集，我们的模型优于标准的基准，并提供中间潜在结构进行分析。我们提出我们的概要的一个广泛的分析和显示，建模文件结构减少复制长序列和同时保持相当摘要长度和抽象的增加的程度并入从源文档更丰富的内容。</font>
</div>


<hr>
<div id="paper8"> <b>8. Clinical Text Summarization with Syntax-Based Negation and Semantic  Concept Identification</b>  <a href="https://arxiv.org/pdf/2003.00353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Hung Weng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu-An Chung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tong%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Schrasing Tong</a><br>
<font size="3">
Abstract: In the era of clinical information explosion, a good strategy for clinical text summarization is helpful to improve the clinical workflow. The ideal summarization strategy can preserve important information in the informative but less organized, ill-structured clinical narrative texts. Instead of using pure statistical learning approaches, which are difficult to interpret and explain, we utilized knowledge of computational linguistics with human experts-curated biomedical knowledge base to achieve the interpretable and meaningful clinical text summarization. Our research objective is to use the biomedical ontology with semantic information, and take the advantage from the language hierarchical structure, the constituency tree, in order to identify the correct clinical concepts and the corresponding negation information, which is critical for summarizing clinical concepts from narrative text. We achieved the clinically acceptable performance for both negation detection and concept identification, and the clinical concepts with common negated patterns can be identified and negated by the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在临床信息爆炸的时代，为临床文本摘要一个很好的策略，有利于提高临床工作流程。理想的汇总策略可以保存在言之有物，但组织化程度较低，结构不良的临床叙事文本的重要信息。而是采用纯粹的统计学习方法，这是很难理解和解释，我们利用与人类专家策划的生物医学知识基础计算语言学的知识来实现​​可解释和有意义的临床文摘。我们的研究目标是利用语义信息，生物医学本体，并采取从语言层次结构，选区树的优势，以确定正确的临床概念和相应的否定信息，这是总结从叙事临床概念的关键文本。我们实现了两个否定检测和概念识别临床上可接受的性能，并与共同求反模式的临床概念可以被识别并且通过所提出的方法否定。</font>
</div>


<hr>
<div id="paper9"> <b>9. Voice trigger detection from LVCSR hypothesis lattices using  bidirectional lattice recurrent neural networks</b>  <a href="https://arxiv.org/pdf/2003.00304" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jeon%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Woojay Jeon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leo Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mason%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Henry Mason</a><br>
<font size="3">
Abstract: We propose a method to reduce false voice triggers of a speech-enabled personal assistant by post-processing the hypothesis lattice of a server-side large-vocabulary continuous speech recognizer (LVCSR) via a neural network. We first discuss how an estimate of the posterior probability of the trigger phrase can be obtained from the hypothesis lattice using known techniques to perform detection, then investigate a statistical model that processes the lattice in a more explicitly data-driven, discriminative manner. We propose using a Bidirectional Lattice Recurrent Neural Network (LatticeRNN) for the task, and show that it can significantly improve detection accuracy over using the 1-best result or the posterior. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议通过后处理通过神经网络的服务器端的大词汇量连续语音识别（LVCSR）的假设晶格以减少语音功能的个人助理的虚假语音触发的方法。我们首先讨论如何触发短语的后验概率的估计可以从假设晶格使用已知的技术进行检测而获得，然后调查其处理所述晶格在一个更明确地数据驱动，判别方式的统计模型。我们建议使用双向格递归神经网络（LatticeRNN）的任务，并表明，它可以显著提高检测精度比使用1最佳结果或后。</font>
</div>


<hr>
<div id="paper10"> <b>10. Depth-Adaptive Graph Recurrent Network for Text Classification</b>  <a href="https://arxiv.org/pdf/2003.00166" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yijin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fandong Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yufeng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinan Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Zhou</a><br>
<font size="3">
Abstract: The Sentence-State LSTM (S-LSTM) is a powerful and high efficient graph recurrent network, which views words as nodes and performs layer-wise recurrent steps between them simultaneously. Despite its successes on text representations, the S-LSTM still suffers from two drawbacks. Firstly, given a sentence, certain words are usually more ambiguous than others, and thus more computation steps need to be taken for these difficult words and vice versa. However, the S-LSTM takes fixed computation steps for all words, irrespective of their hardness. The secondary one comes from the lack of sequential information (e.g., word order) that is inherently important for natural language. In this paper, we try to address these issues and propose a depth-adaptive mechanism for the S-LSTM, which allows the model to learn how many computational steps to conduct for different words as required. In addition, we integrate an extra RNN layer to inject sequential information, which also serves as an input feature for the decision of adaptive depths. Results on the classic text classification task (24 datasets in various sizes and domains) show that our model brings significant improvements against the conventional S-LSTM and other high-performance models (e.g., the Transformer), meanwhile achieving a good accuracy-speed trade off. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：句子态LSTM（S-LSTM）是一个功能强大的，高效率的曲线图递归网络，哪些视图字作为节点和进行逐层复发它们之间同时的步骤。尽管其对文本表示成功的S-LSTM还是来自两个缺点。首先，给定一个句子中的某些词通常比其他人更暧昧的，因而更多的计算步骤，需要采取这些困难的话，反之亦然。然而，S-LSTM需要固定的所有字计算步骤，不论其硬度。二次一个来自缺乏顺序的信息（例如，单词顺序）是用于自然语言固有地重要。在本文中，我们试图解决这些问题，并提出了S-LSTM，这使得该模型了解有多少的计算步骤，为不同的单词根据需要进行深度的自适应机制。此外，我们还集成了一个额外的RNN层注入顺序信息，其也作为自适应深度的决定的输入功能。对经典文本分类任务结果（在各种尺寸和域24集）表明我们的模型带来了对传统的S-LSTM和其他高性能车型显著的改善（例如，变压器），同时达到良好的精度，速度贸易关闭。</font>
</div>


<hr>
<div id="paper11"> <b>11. AraBERT: Transformer-based Model for Arabic Language Understanding</b>  <a href="https://arxiv.org/pdf/2003.00104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Antoun%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wissam Antoun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baly%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fady Baly</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hajj%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hazem Hajj</a><br>
<font size="3">
Abstract: The Arabic language is a morphologically rich and complex language with relatively little resources and a less explored syntax compared to English. Given these limitations, tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models proved to have a very efficient understanding of languages, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. We then compare the performance of AraBERT with multilingual BERT provided by Google and other state-of-the-art approaches. The results of the conducted experiments show that the newly developed AraBERT achieved state-of-the-art results on most tested tasks. The pretrained araBERT models are publicly available on hoping to encourage research and applications for Arabic NLP. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：阿拉伯语是一个形态丰富而复杂的语言以相对较少的资源，相比英语不太探讨语法。鉴于这些局限性，比如情感分析（SA），命名实体识别（NER）和问答（QA）的任务，已被证明是非常具有挑战性的解决。近年来，随着变压器的浪涌基于模型，特定语言的BERT基础的模式被证明具有的语言非常有效的理解，提供了一个非常大的语料库他们预先训练。这种模式能够为大部分NLP任务中设定了新的标准和实现国家的最先进的成果。在本文中，我们预先训练BERT专为追求达到相同的成功，BERT没有为英语的阿拉伯语。然后，我们比较AraBERT的多语种BERT性能由谷歌和国家的最先进的其他方法提供。在所进行的实验结果表明，新开发的AraBERT实现大多数测试任务的国家的最先进的成果。预训练araBERT模型上希望鼓励研究和应用阿拉伯语NLP公开。</font>
</div>


<hr>
<div id="paper12"> <b>12. The STEM-ECR Dataset: Grounding Scientific Entity References in STEM  Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources</b>  <a href="https://arxiv.org/pdf/2003.01006" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=D%27Souza%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jennifer D'Souza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hoppe%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anett Hoppe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brack%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arthur Brack</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jaradeh%2C+M+Y" target="_blank" rel="noopener" style="color:#0000EE;">Mohamad Yaser Jaradeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Auer%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sören Auer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ewerth%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ralph Ewerth</a><br>
<font size="3">
Abstract: We introduce the STEM (Science, Technology, Engineering, and Medicine) Dataset for Scientific Entity Extraction, Classification, and Resolution, version 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to provide a benchmark for the evaluation of scientific entity extraction, classification, and resolution tasks in a domain-independent fashion. It comprises abstracts in 10 STEM disciplines that were found to be the most prolific ones on a major publishing platform. We describe the creation of such a multidisciplinary corpus and highlight the obtained findings in terms of the following features: 1) a generic conceptual formalism for scientific entities in a multidisciplinary scientific context; 2) the feasibility of the domain-independent human annotation of scientific entities under such a generic formalism; 3) a performance benchmark obtainable for automatic extraction of multidisciplinary scientific entities using BERT-based neural models; 4) a delineated 3-step entity resolution procedure for human annotation of the scientific entities via encyclopedic entity linking and lexicographic word sense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic links and lexicographic senses for our entities. Our findings cumulatively indicate that human annotation and automatic learning of multidisciplinary scientific concepts as well as their semantic disambiguation in a wide-ranging setting as STEM is reasonable. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍了STEM（科学，技术，工程和医学）数据集科研实体提取，分类和分辨率，1.0版（STEM-ECR V1.0）。干-ECR V1.0数据集已经发展提供科学的实体提取，分类和解决的任务在域无关的方式评价的基准。它包括在被认为是一个主要的发布平台上最多产的那些10个STEM学科摘要。我们描述了这样一个跨学科文集的创作，并强调所获得的结果在以下功能方面：1）在一个多学科的科学背景的科学机构的一般概念上的形式主义; 2）这样的通用的形式主义下科学实体的域无关的人注释的可行性; 3）用于使用基于BERT神经模型的多学科科学实体的自动提取可获得的性能基准; 4）用于经由百科全书实体链接和字典词义消歧科学实体的人注释的划定3步实体解决过程; 5）Babelfy的人评价返回百科全书式的联系，并为我们的实体词典的感觉。我们的研究结果表明累计在广泛的环境，人的注释和多学科的科学概念，以及它们的语义歧义自动学习作为STEM是合理的。</font>
</div>


<hr>
<div id="paper13"> <b>13. Pathological speech detection using x-vector embeddings</b>  <a href="https://arxiv.org/pdf/2003.00864" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Botelho%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Catarina Botelho</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Teixeira%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Francisco Teixeira</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Rolland%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Rolland</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Abad%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alberto Abad</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Trancoso%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Isabel Trancoso</a><br>
<font size="3">
Abstract: The potential of speech as a non-invasive biomarker to assess a speaker's health has been repeatedly supported by the results of multiple works, for both physical and psychological conditions. Traditional systems for speech-based disease classification have focused on carefully designed knowledge-based features. However, these features may not represent the disease's full symptomatology, and may even overlook its more subtle manifestations. This has prompted researchers to move in the direction of general speaker representations that inherently model symptoms, such as Gaussian Supervectors, i-vectors and, x-vectors. In this work, we focus on the latter, to assess their applicability as a general feature extraction method to the detection of Parkinson's disease (PD) and obstructive sleep apnea (OSA). We test our approach against knowledge-based features and i-vectors, and report results for two European Portuguese corpora, for OSA and PD, as well as for an additional Spanish corpus for PD. Both x-vector and i-vector models were trained with an out-of-domain European Portuguese corpus. Our results show that x-vectors are able to perform better than knowledge-based features in same-language corpora. Moreover, while x-vectors performed similarly to i-vectors in matched conditions, they significantly outperform them when domain-mismatch occurs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语音作为一种非侵入性的生物标志物来评估扬声器健康的潜力已经被多个工程的结果被一再支持，为物理和心理状况。对于基于语音的疾病分类传统的系统都集中在精心设计的以知识为基础的特征。但是，这些功能可能无法代表疾病的症状全面，甚至可以忽略其更微妙的表现。这促使研究人员一般扬声器表示的方向上移动固有地模型的症状，如高斯超向量，I-矢量和，X-载体。在这项工作中，我们侧重于后者，以评估它们的适用性作为一般的特征提取方法到检测帕金森氏病（PD）和阻塞性睡眠呼吸暂停（OSA）的。我们测试我们对以知识为基础的功能和i-载体，以及报告结果的方法有两个欧洲葡萄牙语语料库，对OSA和PD，以及为PD额外西班牙语料库。两种x矢量和i-矢量模型与一个彻头彻尾的域欧洲葡萄牙语语料库培训。我们的研究结果表明，X-载体能比同语料知识为基础的功能，以更好的表现。此外，虽然X-矢量执行类似于在匹配条件的i-载体，它们显著优于它们时域失配发生。</font>
</div>


<hr>
<div id="paper14"> <b>14. Long Short-Term Sample Distillation</b>  <a href="https://arxiv.org/pdf/2003.00739" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zujie Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongping Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yafang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Melo%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerard de Melo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangzhuang Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaxing Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolong Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Qi</a><br>
<font size="3">
Abstract: In the past decade, there has been substantial progress at training increasingly deep neural networks. Recent advances within the teacher--student training paradigm have established that information about past training updates show promise as a source of guidance during subsequent training steps. Based on this notion, in this paper, we propose Long Short-Term Sample Distillation, a novel training policy that simultaneously leverages multiple phases of the previous training process to guide the later training updates to a neural network, while efficiently proceeding in just one single generation pass. With Long Short-Term Sample Distillation, the supervision signal for each sample is decomposed into two parts: a long-term signal and a short-term one. The long-term teacher draws on snapshots from several epochs ago in order to provide steadfast guidance and to guarantee teacher--student differences, while the short-term one yields more up-to-date cues with the goal of enabling higher-quality updates. Moreover, the teachers for each sample are unique, such that, overall, the model learns from a very diverse set of teachers. Comprehensive experimental results across a range of vision and NLP tasks demonstrate the effectiveness of this new training method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在过去的十年里，在训练越来越深层神经网络已取得实质性进展。教师中的最新进展 - 学生培养模式已经建立了关于过去的培训更新的信息显示承诺为指导的在随后的训练步骤的来源。基于这个概念，在本文中，我们提出了长短期样品蒸馏，一种新颖的培训政策，即同时利用了以前的培训过程中的多个阶段，以指导以后的训练更新神经网络，而只是一个单一有效地进行代传。随着长短期样品蒸馏，每个样品的监管信号被分解为两个部分：一个长期的信号和短期的一个。长期的教师借鉴了几个时代的快照前，以提供坚定的指导和保障教师 - 学生的差异，而短期收益率一个更先进的最新线索有，可实现更高质量的更新的目标。此外，教师对每个样品是唯一的，这样，总体而言，从一个非常多样化的教师模型获悉。在一系列的视觉和NLP任务的综合实验结果表明，这种新的训练方法的有效性。</font>
</div>


<hr>
<div id="paper15"> <b>15. Environment-agnostic Multitask Learning for Natural Language Grounded  Navigation</b>  <a href="https://arxiv.org/pdf/2003.00443" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vihan Jain</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ie%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eugene Ie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y" target="_blank" rel="noopener" style="color:#0000EE;">William Yang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kozareva%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zornitsa Kozareva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ravi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sujith Ravi</a><br>
<font size="3">
Abstract: Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalized navigation model from two novel perspectives: (1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks; (2) we propose to learn environment-agnostic representations for the navigation policy that are invariant among the environments seen during training, thus generalizing better on unseen environments. Extensive experiments show that our navigation model trained using environment-agnostic multitask learning significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing a new state-of-the-art for the NDH task. The code for training the navigation model using environment-agnostic multitask learning is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的研究工作能够在照片般逼真的环境中，例如，以下的自然语言指令或对话框的自然语言接地导航研究。但是，现有的方法往往在看到环境过度拟合训练数据，并未能在以前看不见的环境下推广好。为了关闭可见和不可见的环境之间的间隙中，我们的目标是在学习从两个新颖的观点广义导航模型：（1）我们介绍可以同时在视觉语言导航（VLN）和导航无缝训练多任务导航模型从对话历史（NDH）的任务，从更丰富的自然语言指导的利益和整个任务有效地传递知识; （2）我们提出学习的导航策略，是在训练中看到的环境中不变的环境无关的交涉，从而对看不见的环境中更好的推广。大量的实验表明，我们的导航模式使用环境无关的多任务显著学习培训的减少可见和不可见的环境之间的性能差距，并通过16％的VLN优于对看不见的环境基线（成功率相对度量）和120％（目标的进展情况）在NDH，建立一个新的国家的最先进的NDH任务。训练使用环境无关的多任务学习导航模型的代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper16"> <b>16. What Emotions Make One or Five Stars? Understanding Ratings of Online  Product Reviews by Sentiment Analysis and XAI</b>  <a href="https://arxiv.org/pdf/2003.00201" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=So%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaehan So</a><br>
<font size="3">
Abstract: When people buy products online, they primarily base their decisions on the recommendations of others given in online reviews. The current work analyzed these online reviews by sentiment analysis and used the extracted sentiments as features to predict the product ratings by several machine learning algorithms. These predictions were disentangled by various meth-ods of explainable AI (XAI) to understand whether the model showed any bias during prediction. Study 1 benchmarked these algorithms (knn, support vector machines, random forests, gradient boosting machines, XGBoost) and identified random forests and XGBoost as best algorithms for predicting the product ratings. In Study 2, the analysis of global feature importance identified the sentiment joy and the emotional valence negative as most predictive features. Two XAI visualization methods, local feature attributions and partial dependency plots, revealed several incorrect prediction mechanisms on the instance-level. Performing the benchmarking as classification, Study 3 identified a high no-information rate of 64.4% that indicated high class imbalance as underlying reason for the identified problems. In conclusion, good performance by machine learning algorithms must be taken with caution because the dataset, as encountered in this work, could be biased towards certain predictions. This work demonstrates how XAI methods reveal such prediction bias. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当人们购买产品线上，他们主要是立足于在网上评论给别人的建议，他们的决定。目前的工作分析由情感分析这些网上的评论和使用提取的情绪作为特征由几个机器学习算法来预测产品的评级。这些预测是由可解释AI（XAI）的各种甲基-ODS解开理解模型是否表明预测过程中的任何偏差。研究1基准这些算法（KNN，支持向量机，随机森林，梯度升压机，XGBoost）并确定随机森林和XGBoost作为用于预测产品评分最好的算法。在研究2中，全局特征重要性分析鉴定的情绪快乐和情绪价否定的，因为大多数的预测功能。两个XAI可视化方法，局部特征归属和部分依赖图，透露了关于实例级别的几个不正确的预测机制。执行基准分类，研究3中鉴定的64.4％的高无信息速率指示高类不平衡作为根本原因的确定的问题。总之，通过机器学习算法良好的性能，必须谨慎考虑，因为数据集，在这个工作中遇到的，可能对某些预测偏差。这项工作表明XAI方法是如何揭示这样的预测偏差。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-03-02</title>
    <url>/2020/03/02/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-03-02/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Applying Tensor Decomposition to image for Robustness against  Adversarial Attack <a href="https://arxiv.org/pdf/2002.12913" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> A Multi-Hypothesis Classification Approach to Color Constancy <a href="https://arxiv.org/pdf/2002.12896" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Sketch-to-Art: Synthesizing Stylized Art Images From Sketches <a href="https://arxiv.org/pdf/2002.12888" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Infrared and 3D skeleton feature fusion for RGB-D action recognition <a href="https://arxiv.org/pdf/2002.12886" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Indoor Scene Recognition in 3D <a href="https://arxiv.org/pdf/2002.12819" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to  Adversarial Examples <a href="https://arxiv.org/pdf/2002.12749" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Predicting Sharp and Accurate Occlusion Boundaries in Monocular Depth  Estimation Using Displacement Fields <a href="https://arxiv.org/pdf/2002.12730" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> KeypointNet: A Large-scale 3D Keypoint Dataset Aggregated from Numerous  Human Annotations <a href="https://arxiv.org/pdf/2002.12687" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> A Spatiotemporal Volumetric Interpolation Network for 4D Dynamic Medical  Image <a href="https://arxiv.org/pdf/2002.12680" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Inverse Graphics GAN: Learning to Generate 3D Shapes from Unstructured  2D Data <a href="https://arxiv.org/pdf/2002.12674" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> A U-Net Based Discriminator for Generative Adversarial Networks <a href="https://arxiv.org/pdf/2002.12655" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> 4D Association Graph for Realtime Multi-person Motion Capture Using  Multiple Video Cameras <a href="https://arxiv.org/pdf/2002.12625" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment <a href="https://arxiv.org/pdf/2002.12623" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> SCALE-Net: Scalable Vehicle Trajectory Prediction Network under Random  Number of Interacting Vehicles via Edge-enhanced Graph Convolutional Neural  Network <a href="https://arxiv.org/pdf/2002.12609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Exploring and Distilling Cross-Modal Information for Image Captioning <a href="https://arxiv.org/pdf/2002.12585" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Neural Inheritance Relation Guided One-Shot Layer Assignment Search <a href="https://arxiv.org/pdf/2002.12580" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> MANet: Multimodal Attention Network based Point- View fusion for 3D  Shape Recognition <a href="https://arxiv.org/pdf/2002.12573" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Hand-Priming in Object Localization for Assistive Egocentric Vision <a href="https://arxiv.org/pdf/2002.12557" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Automated classification of stems and leaves of potted plants based on  point cloud data <a href="https://arxiv.org/pdf/2002.12536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> A Video Analysis Method on Wanfang Dataset via Deep Neural Network <a href="https://arxiv.org/pdf/2002.12535" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Detecting and Recovering Adversarial Examples: An Input Sensitivity  Guided Method <a href="https://arxiv.org/pdf/2002.12527" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Utilizing Network Properties to Detect Erroneous Inputs <a href="https://arxiv.org/pdf/2002.12520" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> DGST : Discriminator Guided Scene Text detector <a href="https://arxiv.org/pdf/2002.12509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Detecting Patch Adversarial Attacks with Image Residuals <a href="https://arxiv.org/pdf/2002.12504" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Road Curb Detection and Localization with Monocular Forward-view Vehicle  Camera <a href="https://arxiv.org/pdf/2002.12492" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Cross-modality Person re-identification with Shared-Specific Feature  Transfer <a href="https://arxiv.org/pdf/2002.12489" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Improving Learning Effectiveness For Object Detection and Classification  in Cluttered Backgrounds <a href="https://arxiv.org/pdf/2002.12467" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Target Detection, Tracking and Avoidance System for Low-cost UAVs using  AI-Based Approaches <a href="https://arxiv.org/pdf/2002.12461" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> TGGLines: A Robust Topological Graph Guided Line Segment Detector for  Low Quality Binary Images <a href="https://arxiv.org/pdf/2002.12428" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> MNN: A Universal and Efficient Inference Engine <a href="https://arxiv.org/pdf/2002.12418" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Learning in the Frequency Domain <a href="https://arxiv.org/pdf/2002.12416" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> SilhoNet-Fisheye: Adaptation of A ROI Based Object Pose Estimation  Network to Monocular Fisheye Images <a href="https://arxiv.org/pdf/2002.12415" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Brain-Inspired Model for Incremental Learning Using a Few Examples <a href="https://arxiv.org/pdf/2002.12411" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Affinity guided Geometric Semi-Supervised Metric Learning <a href="https://arxiv.org/pdf/2002.12394" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Joint 2D-3D Breast Cancer Classification <a href="https://arxiv.org/pdf/2002.12392" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Review: Noise and artifact reduction for MRI using deep learning <a href="https://arxiv.org/pdf/2002.12889" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Neural Network Segmentation of Interstitial Fibrosis, Tubular Atrophy,  and Glomerulosclerosis in Renal Biopsies <a href="https://arxiv.org/pdf/2002.12868" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> HOTCAKE: Higher Order Tucker Articulated Kernels for Deeper CNN  Compression <a href="https://arxiv.org/pdf/2002.12663" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> An Efficient Method of Training Small Models for Regression Problems  with Knowledge Distillation <a href="https://arxiv.org/pdf/2002.12597" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Regional Registration of Whole Slide Image Stacks Containing Highly  Deformed Artefacts <a href="https://arxiv.org/pdf/2002.12588" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Class-Specific Blind Deconvolutional Phase Retrieval Under a Generative  Prior <a href="https://arxiv.org/pdf/2002.12578" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> RSANet: Recurrent Slice-wise Attention Network for Multiple Sclerosis  Lesion Segmentation <a href="https://arxiv.org/pdf/2002.12470" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> LEEP: A New Measure to Evaluate Transferability of Learned  Representations <a href="https://arxiv.org/pdf/2002.12462" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Is the Meta-Learning Idea Able to Improve the Generalization of Deep  Neural Networks on the Standard Supervised Learning? <a href="https://arxiv.org/pdf/2002.12455" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Provable Robust Learning Based on Transformation-Specific Smoothing <a href="https://arxiv.org/pdf/2002.12398" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> NeurIPS 2019 Disentanglement Challenge: Improved Disentanglement through  Learned Aggregation of Convolutional Feature Maps <a href="https://arxiv.org/pdf/2002.12356" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Applying Tensor Decomposition to image for Robustness against  Adversarial Attack</b>  <a href="https://arxiv.org/pdf/2002.12913" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seungju Cho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jun%2C+T+J" target="_blank" rel="noopener" style="color:#0000EE;">Tae Joon Jun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingu Kang</a><br>
<font size="3">
Abstract: Nowadays the deep learning technology is growing faster and shows dramatic performance in computer vision areas. However, it turns out a deep learning based model is highly vulnerable to some small perturbation called an adversarial attack. It can easily fool the deep learning model by adding small perturbations. On the other hand, tensor decomposition method widely uses for compressing the tensor data, including data matrix, image, etc. In this paper, we suggest combining tensor decomposition for defending the model against adversarial example. We verify this idea is simple and effective to resist adversarial attack. In addition, this method rarely degrades the original performance of clean data. We experiment on MNIST, CIFAR10 and ImageNet data and show our method robust on state-of-the-art attack methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：如今，深度学习技术的增长速度，并显示在计算机视觉领域的戏剧表演。然而，事实证明了深刻的学习基于模型非常容易受到一些所谓的敌对攻击小的扰动。它可以很容易地通过添加少量的扰动愚弄深刻的学习模式。在另一方面，张量分解方法广泛用于压缩张量数据，包括数据矩阵，图像等在本文中，我们建议组合张量分解为防御敌对示例的模型使用。我们验证这种想法是简单而有效的抵御敌对攻击。此外，这种方法很少会降低清洁数据的原始性能。我们尝试对MNIST，CIFAR10和ImageNet数据并显示我们的方法在国家的最先进的攻击方法的鲁棒性。</font>
</div>


<hr>
<div id="paper2"> <b>2. A Multi-Hypothesis Classification Approach to Color Constancy</b>  <a href="https://arxiv.org/pdf/2002.12896" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hernandez-Juarez%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Hernandez-Juarez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Parisot%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sarah Parisot</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Busam%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin Busam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leonardis%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ales Leonardis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Slabaugh%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gregory Slabaugh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McDonagh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steven McDonagh</a><br>
<font size="3">
Abstract: Contemporary approaches frame the color constancy problem as learning camera specific illuminant mappings. While high accuracy can be achieved on camera specific data, these models depend on camera spectral sensitivity and typically exhibit poor generalisation to new devices. Additionally, regression methods produce point estimates that do not explicitly account for potential ambiguities among plausible illuminant solutions, due to the ill-posed nature of the problem. We propose a Bayesian framework that naturally handles color constancy ambiguity via a multi-hypothesis strategy. Firstly, we select a set of candidate scene illuminants in a data-driven fashion and apply them to a target image to generate of set of corrected images. Secondly, we estimate, for each corrected image, the likelihood of the light source being achromatic using a camera-agnostic CNN. Finally, our method explicitly learns a final illumination estimate from the generated posterior probability distribution. Our likelihood estimator learns to answer a camera-agnostic question and thus enables effective multi-camera training by disentangling illuminant estimation from the supervised learning task. We extensively evaluate our proposed approach and additionally set a benchmark for novel sensor generalisation without re-training. Our method provides state-of-the-art accuracy on multiple public datasets (up to 11% median angular error improvement) while maintaining real-time execution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现代方法框架颜色恒常问题作为学习的摄像机特定照明的映射。虽然可以在摄像机特定数据来实现高精确度，这些模型依赖于照相机的光谱灵敏度和通常表现出概括差到新设备。此外，回归方法产生的点估计没有明确解释合理光源解决方案中潜在的含糊之处，由于问题的病态性质。我们提出了一个贝叶斯框架自然处理通过多假设战略颜色恒常歧义。首先，我们选择在一个数据驱动的方式一组候选场景光源，并将其应用到目标图像生成组校正图像的。其次，我们估计，对于每个校正图像，光源的可能性是消色差的使用照相机不可知CNN。最后，我们的方法明确地得知从所生成的后验概率分布的最终照明估计。我们似然估计学会回答相机不可知的问题，从而从被动学习任务解开光源估测能够实现有效的多摄像机培训。我们广泛地评估我们提出的方法，另外设置无需重新培训新型传感器泛化的基准。我们的方法提供了先进的最先进的精度在多个公共数据集（高达11％中值角度误差的改善），同时维持实时执行。</font>
</div>


<hr>
<div id="paper3"> <b>3. Sketch-to-Art: Synthesizing Stylized Art Images From Sketches</b>  <a href="https://arxiv.org/pdf/2002.12888" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bingchen Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kunpeng Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Elgammal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmed Elgammal</a><br>
<font size="3">
Abstract: We propose a new approach for synthesizing fully detailed art-stylized images from sketches. Given a sketch, with no semantic tagging, and a reference image of a specific style, the model can synthesize meaningful details with colors and textures. The model consists of three modules designed explicitly for better artistic style capturing and generation. Based on a GAN framework, a dual-masked mechanism is introduced to enforce the content constraints (from the sketch), and a feature-map transformation technique is developed to strengthen the style consistency (to the reference image). Finally, an inverse procedure of instance-normalization is proposed to disentangle the style and content information, therefore yields better synthesis performance. Experiments demonstrate a significant qualitative and quantitative boost over baselines based on previous state-of-the-art techniques, adopted for the proposed process. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了一个新的方法从草图合成充分详细的艺术程式化的图像。给定一个草图，没有语义标签，以及特定的样式参考图像，模型可以合成颜色和纹理细节有意义。该模型由三个模块组成明确设计为更好的艺术风格捕捉和生成。根据一个GAN框架，双掩蔽机构被引入以执行（从草图）的含量的限制，和特征映射变换技术开发，加强式一致性（参考图像）。最后，例如归一化的逆过程，提出解开的样式和内容的信息，因此产生更好的合成性能。实验证明了基于以前的国家的最先进的技术，所提出的过程中采用基准的显著定性和定量的推动作用。</font>
</div>


<hr>
<div id="paper4"> <b>4. Infrared and 3D skeleton feature fusion for RGB-D action recognition</b>  <a href="https://arxiv.org/pdf/2002.12886" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Boissiere%2C+A+M" target="_blank" rel="noopener" style="color:#0000EE;">Alban Main de Boissiere</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Noumeir%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rita Noumeir</a><br>
<font size="3">
Abstract: A challenge of skeleton-based action recognition is the difficulty to classify actions with similar motions and object-related actions. Visual clues from other streams help in that regard. RGB data are sensible to illumination conditions, thus unusable in the dark. To alleviate this issue and still benefit from a visual stream, we propose a modular network (FUSION) combining skeleton and infrared data. A 2D convolutional neural network (CNN) is used as a pose module to extract features from skeleton data. A 3D CNN is used as an infrared module to extract visual cues from videos. Both feature vectors are then concatenated and exploited conjointly using a multilayer perceptron (MLP). Skeleton data also condition the infrared videos, providing a crop around the performing subjects and thus virtually focusing the attention of the infrared module. Ablation studies show that using pre-trained networks on other large scale datasets as our modules and data augmentation yield considerable improvements on the action classification accuracy. The strong contribution of our cropping strategy is also demonstrated. We evaluate our method on the NTU RGB+D dataset, the largest dataset for human action recognition from depth cameras, and report state-of-the-art performances. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于骨架动作识别的一个挑战是难以用类似的动作和对象相关行为分类行为。从其他流的视觉线索帮助在此方面。 RGB数据是明智的照明条件下，从而在黑暗中无法使用。为了缓解这一问题，并继续从视觉流中受益，我们建议结合骨架和红外数据模块化网络（融合）。二维卷积神经网络（CNN）被用作姿态模块提取从骨架数据的功能。一种3D CNN用作红外线模块，用于从视频中提取视觉线索。然后这两个特征向量被连接起来并共同地使用多层感知器（MLP）利用。骨架数据还调节所述红外线视频，提供围绕所述执行对象作物，因此几乎聚焦红外模块的注意。消融的研究表明，使用上其他大型数据集作为我们的行动分类准确模块和数据扩充产量相当大的改善预先训练网络。我们的作物战略的巨大贡献，也展示。我们评估我们对NTU RGB + d数据集的方法，从深度相机人类动作识别，和国家的最先进的报告性能上最大的数据集。</font>
</div>


<hr>
<div id="paper5"> <b>5. Indoor Scene Recognition in 3D</b>  <a href="https://arxiv.org/pdf/2002.12819" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shengyu Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Usvyatsov%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mikhail Usvyatsov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schindler%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Konrad Schindler</a><br>
<font size="3">
Abstract: Recognising in what type of environment one is located is an important perception task. For instance, for a robot operating in indoors it is helpful to be aware whether it is in a kitchen, a hallway or a bedroom. Existing approaches attempt to classify the scene based on 2D images or 2.5D range images. Here, we study scene recognition from 3D point cloud (or voxel) data, and show that it greatly outperforms methods based on 2D birds-eye views. Moreover, we advocate multi-task learning as a way of improving scene recognition, building on the fact that the scene type is highly correlated with the objects in the scene, and therefore with its semantic segmentation into different object classes. In a series of ablation studies, we show that successful scene recognition is not just the recognition of individual objects unique to some scene type (such as a bathtub), but depends on several different cues, including coarse 3D geometry, colour, and the (implicit) distribution of object categories. Moreover, we demonstrate that surprisingly sparse 3D data is sufficient to classify indoor scenes with good accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：认识到什么样的环境一类位于是一个重要的感知任务。例如，对于在室内机器人操作是有用知道它是否是在厨房，走廊或卧室。现有方法试图基于2D图像或2.5D的范围内图像的场景进行分类。在这里，我们研究从三维点云（或体素）数据场景识别，并表明它大大优于基于二维鸟瞰视图的方法。此外，我们提倡多任务学习作为提高场景识别的方式，建立在事实的场景类型高度与场景中的物体与它的语义分割成不同的对象类相关的，因此。在一系列消融研究，我们发现成功的场景识别是不是唯一的某些场景类型（如浴缸）单个对象的只是承认，但取决于几个不同的线索，包括粗大的三维几何结构，色彩，和（对象类别的隐含的）分布。此外，我们证明了令人惊讶的稀疏的3D数据足以与良好的精度的室内场景进行分类。</font>
</div>


<hr>
<div id="paper6"> <b>6. Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to  Adversarial Examples</b>  <a href="https://arxiv.org/pdf/2002.12749" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Neekhara%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paarth Neekhara</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hussain%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shehzeen Hussain</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jere%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Malhar Jere</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koushanfar%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Farinaz Koushanfar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julian McAuley</a><br>
<font size="3">
Abstract: Recent advances in video manipulation techniques have made the generation of fake videos more accessible than ever before. Manipulated videos can fuel disinformation and reduce trust in media. Therefore detection of fake videos has garnered immense interest in academia and industry. Recently developed Deepfake detection methods rely on deep neural networks (DNNs) to distinguish AI-generated fake videos from real videos. In this work, we demonstrate that it is possible to bypass such detectors by adversarially modifying fake videos synthesized using existing Deepfake generation methods. We further demonstrate that our adversarial perturbations are robust to image and video compression codecs, making them a real-world threat. We present pipelines in both white-box and black-box attack scenarios that can fool DNN based Deepfake detectors into classifying fake videos as real. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在视频处理技术的最新进展使得假视频的一代比以往任何时候都更容易。操作视频可以推动造谣和减少媒体的信任。因此，假视频检测已经获得学术界和工业界兴趣盎然。最近开发Deepfake检测方法依赖于深层神经网络（DNNs）从实际视频区分AI-产生的假视频。在这项工作中，我们表明，有可能旁路这种检测器通过修改adversarially使用现有Deepfake代方法合成假视频。我们进一步证明，我们的对抗扰动是稳健的图像和视频压缩编解码，使之成为一个真正的世界的威胁。我们在白盒和黑盒攻击场景都可以骗过基于DNN Deepfake探测器为假视频归类为真正存在管道。</font>
</div>


<hr>
<div id="paper7"> <b>7. Predicting Sharp and Accurate Occlusion Boundaries in Monocular Depth  Estimation Using Displacement Fields</b>  <a href="https://arxiv.org/pdf/2002.12730" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramamonjisoa%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Ramamonjisoa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuming Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lepetit%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vincent Lepetit</a><br>
<font size="3">
Abstract: Current methods for depth map prediction from monocular images tend to predict smooth, poorly localized contours for the occlusion boundaries in the input image. This is unfortunate as occlusion boundaries are important cues to recognize objects, and as we show, may lead to a way to discover new objects from scene reconstruction. To improve predicted depth maps, recent methods rely on various forms of filtering or predict an additive residual depth map to refine a first estimate. We instead learn to predict, given a depth map predicted by some reconstruction method, a 2D displacement field able to re-sample pixels around the occlusion boundaries into sharper reconstructions. Our method can be applied to the output of any depth estimation method, in an end-to-end trainable fashion. For evaluation, we manually annotated the occlusion boundaries in all the images in the test split of popular NYUv2-Depth dataset. We show that our approach improves the localization of occlusion boundaries for all state-of-the-art monocular depth estimation methods that we could evaluate, without degrading the depth accuracy for the rest of the images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用于从单目图像的深度图的预测的当前方法趋向于预测平滑，对于输入图像中的遮挡边界局部不佳轮廓。这是不幸的，因为遮挡边界识别物体的重要线索，并为大家展示，可能导致的方式来发现从现场重建的新对象。为了提高预测深度图，最近的方法依赖于各种形式的滤波的或预测的添加剂残留的深度图来细化的第一估计。我们代替学习预测，给定深度图由一些重建方法，二维位移字段能够围绕闭塞边界重采样像素到更清晰的重建预测。我们的方法可以应用于任何深度估计方法的输出，在一个端部 - 端可训练方式。对于评估中，我们手动注释在流行NYUv2深入数据集的测试分裂的所有图像的遮挡边界。我们表明，我们的方法提高了遮挡边界的国家的最先进的全单眼深度估计方法，我们可以评估定位，而不会降低深度精度的图像的其余部分。</font>
</div>


<hr>
<div id="paper8"> <b>8. KeypointNet: A Large-scale 3D Keypoint Dataset Aggregated from Numerous  Human Annotations</b>  <a href="https://arxiv.org/pdf/2002.12687" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=You%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang You</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lou%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yujing Lou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengkun Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhoujun Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangwei Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lizhuang Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cewu Lu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weiming Wang</a><br>
<font size="3">
Abstract: Detecting 3D objects keypoints is of great interest to the areas of both graphics and computer vision. There have been several 2D and 3D keypoint datasets aiming to address this problem in a data-driven way. These datasets, however, either lack scalability or bring ambiguity to the definition of keypoints. Therefore, we present KeypointNet: the first large-scale and diverse 3D keypoint dataset that contains 83,060 keypoints and 8,329 3D models from 16 object categories, by leveraging numerous human annotations. To handle the inconsistency between annotations from different people, we propose a novel method to aggregate these keypoints automatically, through minimization of a fidelity loss. Finally, ten state-of-the-art methods are benchmarked on our proposed dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：检测3D对象的关键点是非常感兴趣的图形和计算机视觉领域的。已经有一些2D和3D数据集关键点在目标数据驱动的方式来解决这个问题。这些数据集，但是，无论是缺乏扩展或带来歧义，以关键点的定义。因此，我们提出KeypointNet：第一个大规模和多样化的3D关键点的数据集，其中包含83060个关键点和16级对象的类别8329个的3D模型，通过利用众多人的注释。为了保证从不同的人注释之间的不一致，我们提出来自动收集这些关键点，通过逼真的损失最小化的新方法。最后，国家的最先进的十个方法是基准上我们提出的数据集。</font>
</div>


<hr>
<div id="paper9"> <b>9. A Spatiotemporal Volumetric Interpolation Network for 4D Dynamic Medical  Image</b>  <a href="https://arxiv.org/pdf/2002.12680" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuyu Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Bi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ahn%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Euijoon Ahn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dagan Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinman Kim</a><br>
<font size="3">
Abstract: Dynamic medical imaging is usually limited in application due to the large radiation doses and longer image scanning and reconstruction times. Existing methods attempt to reduce the dynamic sequence by interpolating the volumes between the acquired image volumes. However, these methods are limited to either 2D images and/or are unable to support large variations in the motion between the image volume sequences. In this paper, we present a spatiotemporal volumetric interpolation network (SVIN) designed for 4D dynamic medical images. SVIN introduces dual networks: first is the spatiotemporal motion network that leverages the 3D convolutional neural network (CNN) for unsupervised parametric volumetric registration to derive spatiotemporal motion field from two-image volumes; the second is the sequential volumetric interpolation network, which uses the derived motion field to interpolate image volumes, together with a new regression-based module to characterize the periodic motion cycles in functional organ structures. We also introduce an adaptive multi-scale architecture to capture the volumetric large anatomy motions. Experimental results demonstrated that our SVIN outperformed state-of-the-art temporal medical interpolation methods and natural video interpolation methods that have been extended to support volumetric images. Our ablation study further exemplified that our motion network was able to better represent the large functional motion compared with the state-of-the-art unsupervised medical registration methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：动态医学成像通常在应用上不限由于大的辐射剂量和更长的图像扫描和重建时间。现有方法试图通过内插获得的图像体积之间的体积减少动态序列。然而，这些方法被限制于任一2D图像和/或不能支持在图像体积序列之间的运动大的变化。在本文中，我们提出了设计用于4D动态医学图像时空体积内插网络（SVIN）。 SVIN介绍双网：第一是，利用用于从两个图像体积的无监督参数体积登记以导出时空运动场的三维卷积神经网络（CNN）的时空运动网络;第二个是连续的体积内插网络，它使用导出的运动场进行内插图像体积，用新的基于回归的模块一起在功能器官结构来表征的周期性的运动周期。我们还引入了自适应多级架构捕捉体积大解剖运动。实验结果表明，我们的SVIN优于国家的最先进的医疗时间插值方法，并已扩展为支持立体图像自然的视频插值方法。我们的研究消融进一步举例说明，与国家的最先进的无监督医疗登记的方法相比提供了运动网络能够更好地表示大的功能运动。</font>
</div>


<hr>
<div id="paper10"> <b>10. Inverse Graphics GAN: Learning to Generate 3D Shapes from Unstructured  2D Data</b>  <a href="https://arxiv.org/pdf/2002.12674" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lunz%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastian Lunz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingzhen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fitzgibbon%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Fitzgibbon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kushman%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nate Kushman</a><br>
<font size="3">
Abstract: Recent work has shown the ability to learn generative models for 3D shapes from only unstructured 2D images. However, training such models requires differentiating through the rasterization step of the rendering process, therefore past work has focused on developing bespoke rendering models which smooth over this non-differentiable process in various ways. Such models are thus unable to take advantage of the photo-realistic, fully featured, industrial renderers built by the gaming and graphics industry. In this paper we introduce the first scalable training technique for 3D generative models from 2D data which utilizes an off-the-shelf non-differentiable renderer. To account for the non-differentiability, we introduce a proxy neural renderer to match the output of the non-differentiable renderer. We further propose discriminator output matching to ensure that the neural renderer learns to smooth over the rasterization appropriately. We evaluate our model on images rendered from our generated 3D shapes, and show that our model can consistently learn to generate better shapes than existing models when trained with exclusively unstructured 2D images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的工作表明仅从非结构化2D图像为学习3D图形生成模型的能力。然而，训练这些模型需要经过渲染处理的光栅化步骤区分，因此过去的工作主要集中在定制呈现模型，其顺利过以各种方式本次非微过程开发。这样的模型，因此无从拍摄照片般逼真的，功能齐全，产业由游戏和图形行业的内置渲染器的优势。在本文中，我们介绍了3D生成模型，从它利用一个现成的，现成的非微渲染2D数据的第一个可扩展的训练技巧。为了考虑非微分，我们引入了代理神经渲染到非微渲染器的输出相匹配。我们进一步提出了鉴别输出匹配，以保证神经渲染学会光栅化适当的平滑过度。我们评估我们从我们生成的3D图形渲染图像模型，并表明我们的模型可以持续学习产生比现有机型更好的形状，当完全非结构化2D图像训练。</font>
</div>


<hr>
<div id="paper11"> <b>11. A U-Net Based Discriminator for Generative Adversarial Networks</b>  <a href="https://arxiv.org/pdf/2002.12655" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sch%C3%B6nfeld%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Edgar Schönfeld</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schiele%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernt Schiele</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khoreva%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Khoreva</a><br>
<font size="3">
Abstract: Among the major remaining challenges for generative adversarial networks (GANs) is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images. To target this issue we propose an alternative U-Net based discriminator architecture, borrowing the insights from the segmentation literature. The proposed U-Net based architecture allows to provide detailed per-pixel feedback to the generator while maintaining the global coherence of synthesized images, by providing the global image feedback as well. Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. This improves the U-Net discriminator training, further enhancing the quality of generated samples. The novel discriminator improves over the state of the art in terms of the standard distribution and image quality metrics, enabling the generator to synthesize images with varying structure, appearance and levels of detail, maintaining global and local realism. Compared to the BigGAN baseline, we achieve an average improvement of 2.7 FID points across FFHQ, CelebA, and the newly introduced COCO-Animals dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在对生成对抗网络（甘斯）主要存在的挑战是合成与物体形状的全局和局部连贯的图像能力和纹理的真实图像难以区分。要针对这个问题，我们提出了一个替代的基于掌中鉴别架构，借用分割文学的见解。所提出的U形网基础结构允许提供详细的每像素反馈至发生器，同时保持合成图像的全局一致性，通过提供全局图像反馈为好。由鉴别器的每个像素响应授权，我们进一步提出了基于CutMix数据增强每个像素的一致性正则化技术，鼓励在U-Net的鉴别器更专注于真假图像之间的语义和结构的变化。这改善了U形网鉴别训练，从而进一步提高生成的样本的质量。新颖的鉴别器改进了现有技术的状态在标准分布和图像质量度量而言，使发电机以合成图像具有不同的结构，外观和细节水平，保持全局和局部真实感。相比BigGAN基线，我们实现了2.7 FID个点来FFHQ，CelebA平均改善，以及新推出的COCO-动物数据集。</font>
</div>


<hr>
<div id="paper12"> <b>12. 4D Association Graph for Realtime Multi-person Motion Capture Using  Multiple Video Cameras</b>  <a href="https://arxiv.org/pdf/2002.12625" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxiang Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=An%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang An</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yebin Liu</a><br>
<font size="3">
Abstract: This paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime online motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection, and achieves high-quality online pose reconstruction quality. The proposed method outperforms the state-of-the-art method quantitatively without using high-level appearance information. We also contribute a multiview video dataset synchronized with a marker-based motion capture system for scientific evaluation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文使用有助于多视点视频输入的新颖实时多人动作捕捉算法。由于每个视图重闭塞，多视点的图像和多个时间帧进行联合优化是必不可少的，它会弹出实时效率的基本挑战。为此，对于第一次，我们统一按次解析，跨视图匹配，和时间跟踪到一个单一的优化框架，即，4D关联图，每个尺寸（图像空间，视点和时间）可以治疗同样，同时。为了有效地解决了4D关联图，我们进一步促进4D肢体的想法捆绑解析基于启发式搜索，然后用肢体束通过提出一个包Kruskal算法组装。我们的方法能够在使用上有5人现场5台摄像机30fps的运行实时在线动作捕捉系统。从统一解析受益，匹配和跟踪的限制，我们的方法是健壮的嘈杂检测，实现高品质的在线姿势重建质量。所提出的方法定量地优于国家的最先进的方法，而无需使用高级别外观的信息。我们也有助于与科学评价一个基于标记的运动捕捉系统同步的多视点视频数据集。</font>
</div>


<hr>
<div id="paper13"> <b>13. MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment</b>  <a href="https://arxiv.org/pdf/2002.12623" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bernard%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Bernard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Suri%2C+Z+K" target="_blank" rel="noopener" style="color:#0000EE;">Zeeshan Khan Suri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Theobalt%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Theobalt</a><br>
<font size="3">
Abstract: We present a convex mixed-integer programming formulation for non-rigid shape matching. To this end, we propose a novel shape deformation model based on an efficient low-dimensional discrete model, so that finding a globally optimal solution is tractable in (most) practical cases. Our approach combines several favourable properties: it is independent of the initialisation, it is much more efficient to solve to global optimality compared to analogous quadratic assignment problem formulations, and it is highly flexible in terms of the variants of matching problems it can handle. Experimentally we demonstrate that our approach outperforms existing methods for sparse shape matching, that it can be used for initialising dense shape matching methods, and we showcase its flexibility on several examples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出一个凸混合整数规划制剂非刚性形状匹配。为此，提出了一种基于有效率的低维离散模型的新颖形状的变形模型，以便找到一个全局最优解是在（大多数）实际情况下容易处理。我们的方法结合了几个有利的性质：它是独立的初始化的，它是更有效的解决全球最优相比于类似的二次分配问题的配方，它是相匹配的问题，它可以处理的变型方面非常灵活。实验我们证明了稀疏的形状匹配，我们的方法比现有的方法，它可用于初始化密集的形状匹配方法，我们展示的几个例子它的灵活性。</font>
</div>


<hr>
<div id="paper14"> <b>14. SCALE-Net: Scalable Vehicle Trajectory Prediction Network under Random  Number of Interacting Vehicles via Edge-enhanced Graph Convolutional Neural  Network</b>  <a href="https://arxiv.org/pdf/2002.12609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jeon%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyeongseok Jeon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junwon Choi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kum%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongsuk Kum</a><br>
<font size="3">
Abstract: Predicting the future trajectory of surrounding vehicles in a randomly varying traffic level is one of the most challenging problems in developing an autonomous vehicle. Since there is no pre-defined number of interacting vehicles participate in, the prediction network has to be scalable with respect to the vehicle number in order to guarantee the consistency in terms of both accuracy and computational load. In this paper, the first fully scalable trajectory prediction network, SCALE-Net, is proposed that can ensure both higher prediction performance and consistent computational load regardless of the number of surrounding vehicles. The SCALE-Net employs the Edge-enhance Graph Convolutional Neural Network (EGCN) for the inter-vehicular interaction embedding network. Since the proposed EGCN is inherently scalable with respect to the graph node (an agent in this study), the model can be operated independently from the total number of vehicles considered. We evaluated the scalability of the SCALE-Net on the publically available NGSIM datasets by comparing variations on computation time and prediction accuracy per single driving scene with respect to the varying vehicle number. The experimental test shows that both computation time and prediction performance of the SCALE-Net consistently outperform those of previous models regardless of the level of traffic complexities. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预测在一个随机变化的业务水平周围车辆的未来轨迹，这是发展中国家的自主车型中最具挑战性的问题之一。由于存在相互作用车辆参与没有预先定义的数量，预测网络必须是可扩展的，以便保证在两个精度和计算负荷方面的一致性对于车辆数​​。在本文中，第一个完全可伸缩的轨迹预测网络，规模型网，提出了能够保证双方更高的预测性能和一致的计算负载，无论周围车辆的数量。尺度网采用边沿增强图形卷积神经网络（EGCN）用于车车间交互嵌入网络。由于所提出的EGCN是相对于所述图形节点（在该研究中的试剂）可伸缩本质，该模型可以被独立地选自考虑车辆的总数操作。我们通过对于变化的车辆数量比较计算上的时间和每一个驾驶场景的预测精度变化评价上的公开可用的数据集NGSIM尺度网络的可扩展性。经实验测试表明，无论计算时间和放大网络的预测性能始终优于那些以前的型号无论交通复杂的水平。</font>
</div>


<hr>
<div id="paper15"> <b>15. Exploring and Distilling Cross-Modal Information for Image Captioning</b>  <a href="https://arxiv.org/pdf/2002.12585" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fenglin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuancheng Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanxin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Lei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xu Sun</a><br>
<font size="3">
Abstract: Recently, attention-based encoder-decoder models have been used extensively in image captioning. Yet there is still great difficulty for the current methods to achieve deep image understanding. In this work, we argue that such understanding requires visual attention to correlated image regions and semantic attention to coherent attributes of interest. To perform effective attention, we explore image captioning from a cross-modal perspective and propose the Global-and-Local Information Exploring-and-Distilling approach that explores and distills the source information in vision and language. It globally provides the aspect vector, a spatial and relational representation of images based on caption contexts, through the extraction of salient region groupings and attribute collocations, and locally extracts the fine-grained regions and attributes in reference to the aspect vector for word selection. Our fully-attentive model achieves a CIDEr score of 129.3 in offline COCO evaluation on the COCO testing set with remarkable efficiency in terms of accuracy, speed, and parameter budget. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，注意基于编码器的解码器模型已被广泛应用在影像字幕使用。然而，仍然有很大的难度当前方法来实现深图像理解。在这项工作中，我们认为，这样的理解，需要视觉注意相关的图像区域和语义重视利益一致的属性。要进行有效的关注，我们从跨模态的角度探讨图像字幕，并提出了全局和局部信息的探索和 - 蒸馏的方法，探索和提炼在视觉和语言的源信息。它全局提供方面向量，基于字幕上下文图像的空间和关系表示，通过显着区域分组和属性搭配的提取，并在本地提取细粒度区域和属性参考用于字选择纵横向量。我们全面周到的模型实现了129.3对的COCO测试集效率惊人离线COCO评价苹果酒得分在精度，速度和参数预算方面。</font>
</div>


<hr>
<div id="paper16"> <b>16. Neural Inheritance Relation Guided One-Shot Layer Assignment Search</b>  <a href="https://arxiv.org/pdf/2002.12580" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rang Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weijie Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Di Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiliang Pu</a><br>
<font size="3">
Abstract: Layer assignment is seldom picked out as an independent research topic in neural architecture search. In this paper, for the first time, we systematically investigate the impact of different layer assignments to the network performance by building an architecture dataset of layer assignment on CIFAR-100. Through analyzing this dataset, we discover a neural inheritance relation among the networks with different layer assignments, that is, the optimal layer assignments for deeper networks always inherit from those for shallow networks. Inspired by this neural inheritance relation, we propose an efficient one-shot layer assignment search approach via inherited sampling. Specifically, the optimal layer assignment searched in the shallow network can be provided as a strong sampling priori to train and search the deeper ones in supernet, which extremely reduces the network search space. Comprehensive experiments carried out on CIFAR-100 illustrate the efficiency of our proposed method. Our search results are strongly consistent with the optimal ones directly selected from the architecture dataset. To further confirm the generalization of our proposed method, we also conduct experiments on Tiny-ImageNet and ImageNet. Our searched results are remarkably superior to the handcrafted ones under the unchanged computational budgets. The neural inheritance relation discovered in this paper can provide insights to the universal neural architecture search. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：层分配很少挑选出来作为神经结构搜索一个独立的研究课题。在本文中，为我们首次系统地对CIFAR-100构建层分配的结构数据集研究了不同层分配给网络性能的影响。通过分析此数据集，我们发现不同的层分配网络中神经继承关系，也就是更深层次的网络优化层分配总是从那些浅薄的网络继承。这个神经继承关系的启发，我们提出通过继承采样的高效一次性层分配搜索方法。具体地，最佳层分配搜索的浅网络中可以提供作为强采样先验训练和查询的那些更深在超网，这极大地降低了网络的搜索空间。综合实验上CIFAR-100说明我们提出的方法的有效性进行。我们的搜索结果是从架构的数据集直接选择最优者强一致。为了进一步证实我们提出的方法的推广，我们也进行上微小的-ImageNet和ImageNet实验。我们的搜索结果明显优于下的不变计算预算手工制作的。在本文中发现的神经继承关系可以提供见解通用神经结构的搜索。</font>
</div>


<hr>
<div id="paper17"> <b>17. MANet: Multimodal Attention Network based Point- View fusion for 3D  Shape Recognition</b>  <a href="https://arxiv.org/pdf/2002.12573" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaxin Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jichao Jiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tangkun Zhang</a><br>
<font size="3">
Abstract: 3D shape recognition has attracted more and more attention as a task of 3D vision research. The proliferation of 3D data encourages various deep learning methods based on 3D data. Now there have been many deep learning models based on point-cloud data or multi-view data alone. However, in the era of big data, integrating data of two different modals to obtain a unified 3D shape descriptor is bound to improve the recognition accuracy. Therefore, this paper proposes a fusion network based on multimodal attention mechanism for 3D shape recognition. Considering the limitations of multi-view data, we introduce a soft attention scheme, which can use the global point-cloud features to filter the multi-view features, and then realize the effective fusion of the two features. More specifically, we obtain the enhanced multi-view features by mining the contribution of each multi-view image to the overall shape recognition, and then fuse the point-cloud features and the enhanced multi-view features to obtain a more discriminative 3D shape descriptor. We have performed relevant experiments on the ModelNet40 dataset, and experimental results verify the effectiveness of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：三维形状识别吸引了越来越多的关注，因为3D视觉研究的任务。三维数据的增殖鼓励基于三维数据的各种深刻的学习方法。现在已经出现了基于单独的点云数据或者多视图数据许多深的学习模式。然而，在大数据，集成了两个不同模态的数据以获得一个统一的3D形状描述符的时代必将提高识别的准确率。因此，本文提出了一种基于多注意机制的3D形状识别的融合网络。考虑到多视图数据的限制，我们引入了一个柔软的关注方案，它可以使用全局的点云功能来过滤多视图功能，进而实现两个特征的有效融合。更具体地说，我们获得增强多视图设有通过挖掘每个多视点图像的贡献的总体形状识别，然后保险丝的点群的特征和增强多视图设有以获得更有辨别力的3D形状描述符。我们已经在ModelNet40数据集进行相关的实验，实验结果验证了该方法的有效性。</font>
</div>


<hr>
<div id="paper18"> <b>18. Hand-Priming in Object Localization for Assistive Egocentric Vision</b>  <a href="https://arxiv.org/pdf/2002.12557" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kyungjun Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shrivastava%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhinav Shrivastava</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kacorri%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hernisa Kacorri</a><br>
<font size="3">
Abstract: Egocentric vision holds great promises for increasing access to visual information and improving the quality of life for people with visual impairments, with object recognition being one of the daily challenges for this population. While we strive to improve recognition performance, it remains difficult to identify which object is of interest to the user; the object may not even be included in the frame due to challenges in camera aiming without visual feedback. Also, gaze information, commonly used to infer the area of interest in egocentric vision, is often not dependable. However, blind users often tend to include their hand either interacting with the object that they wish to recognize or simply placing it in proximity for better camera aiming. We propose localization models that leverage the presence of the hand as the contextual information for priming the center area of the object of interest. In our approach, hand segmentation is fed to either the entire localization network or its last convolutional layers. Using egocentric datasets from sighted and blind individuals, we show that the hand-priming achieves higher precision than other approaches, such as fine-tuning, multi-class, and multi-task learning, which also encode hand-object interactions in localization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自我中心的愿景适用于增加获取视觉信息和提高生活为视障人士的质量，以目标识别是对这个人群的日常挑战一个很大的承诺。虽然我们努力提高识别性能，但它仍然很难确定哪些对象是用户感兴趣的;对象甚至可能不被包括在帧由于没有瞄准视觉反馈在相机的挑战。此外，注视信息，常用来推断在自我中心的视觉感兴趣的区域，往往是不可靠的。然而，盲人用户往往倾向于包括他们的手，他们希望确认或简单地将其放置在接近更好的摄像头瞄准的对象要么交互。我们建议本地化模式，利用手的存在作为引发的关注对象的中心区域的上下文信息。在我们的方法中，手分割被馈送到或者是整个网络的本地化或它的最后卷积层。使用来自短视和盲目的个人以自我为中心的数据集，我们证明了手吸实现比其他方法，如微调，多类，多任务学习，在本地化这也编码手工对象交互更高的精度。</font>
</div>


<hr>
<div id="paper19"> <b>19. Automated classification of stems and leaves of potted plants based on  point cloud data</b>  <a href="https://arxiv.org/pdf/2002.12536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zichu Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qing Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huiru Wang</a><br>
<font size="3">
Abstract: The accurate classification of plant organs is a key step in monitoring the growing status and physiology of plants. A classification method was proposed to classify the leaves and stems of potted plants automatically based on the point cloud data of the plants, which is a nondestructive acquisition. The leaf point training samples were automatically extracted by using the three-dimensional convex hull algorithm, while stem point training samples were extracted by using the point density of a two-dimensional projection. The two training sets were used to classify all the points into leaf points and stem points by utilizing the support vector machine (SVM) algorithm. The proposed method was tested by using the point cloud data of three potted plants and compared with two other methods, which showed that the proposed method can classify leaf and stem points accurately and efficiently. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：植物器官的准确分类是监测植物的生长状况和生理的关键一步。提出了一种分类方法的叶子分类并自动根据植物的点群数据，这是一个非破坏性的采集盆栽植物茎。叶点训练样本被自动通过使用三维凸包算法提取，而茎点训练样本通过使用二维投影的点密度萃取。使用两个训练集到所有的点分类为叶点并利用支持向量机（SVM）算法茎点。所提出的方法是通过使用三个盆栽植物的点云数据进行测试，并与其他两种方法，这表明，该方法可以分类叶和准确且高效地干百分点。</font>
</div>


<hr>
<div id="paper20"> <b>20. A Video Analysis Method on Wanfang Dataset via Deep Neural Network</b>  <a href="https://arxiv.org/pdf/2002.12535" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinlong Kang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaxiang Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heng Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoting Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Guo</a><br>
<font size="3">
Abstract: The topic of object detection has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as small object, compact and dense or highly overlapping object. Existing methods can detect multiple objects wonderfully, but because of the slight changes between frames, the detection effect of the model will become unstable, the detection results may result in dropping or increasing the object. In the pedestrian flow detection task, such phenomenon can not accurately calculate the flow. To solve this problem, in this paper, we describe the new function for real-time multi-object detection in sports competition and pedestrians flow detection in public based on deep learning. Our work is to extract a video clip and solve this frame of clips efficiently. More specfically, our algorithm includes two stages: judge method and optimization method. The judge can set a maximum threshold for better results under the model, the threshold value corresponds to the upper limit of the algorithm with better detection results. The optimization method to solve detection jitter problem. Because of the occurrence of frame hopping in the video, and it will result in the generation of video fragments discontinuity. We use optimization algorithm to get the key value, and then the detection result value of index is replaced by key value to stabilize the change of detection result sequence. Based on the proposed algorithm, we adopt wanfang sports competition dataset as the main test dataset and our own test dataset for YOLOv3-Abnormal Number Version(YOLOv3-ANV), which is 5.4% average improvement compared with existing methods. Also, video above the threshold value can be obtained for further analysis. Spontaneously, our work also can used for pedestrians flow detection and pedestrian alarm tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目标检测的主题已经在很大程度上最近有所改善，尤其是卷积神经网络的发展。然而，仍存在很多挑战的情况下，如小物体，结构紧凑且致密的或高度重叠的对象。现有方法可以检测奇妙多个对象，但由于帧之间的细微变化，模型的检测效果将变得不稳定，检测结果可能导致丢弃或增加的对象。在行人流检测任务，这样的现象不能准确地计算流量。为了解决这个问题，在本文中，我们描述了实时的多目标检测的新功能在体育比赛和行人基于深度学习的公共流程检测。我们的工作是提取视频剪辑，并有效地解决剪辑这个框架。更specfically，我们的算法包括两个阶段：法官法和优化方法。判断可以为更好的结果的最大阈值的模型下，所述阈值对应于与更好的检测结果的算法的上限值。最优化方法解决检测抖动的问题。因为帧的发生在视频跳频，并且它将导致视频片段不连续的产生。我们采用优化算法来获取键值，然后指标的检测结果值是通过键值所取代，以稳定检测结果序列的变化。基于算法上，我们采用万芳体育竞赛数据集作为主要的测试数据集和我们自己的测试数据集YOLOv3，数目异常版本（YOLOv3-ANV），这是与现有的方法相比，5.4％的平均改善。此外，可以进行进一步的分析而获得的视频的阈值以上。自然，我们的工作也可用于行人流量检测和行人报警任务。</font>
</div>


<hr>
<div id="paper21"> <b>21. Detecting and Recovering Adversarial Examples: An Input Sensitivity  Guided Method</b>  <a href="https://arxiv.org/pdf/2002.12527" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingxuan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingyuan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yufan Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuchang Zhou</a><br>
<font size="3">
Abstract: Deep neural networks undergo rapid development and achieve notable success in various tasks, including many security concerned scenarios. However, a considerable amount of works have proved its vulnerability in adversaries. To address this problem, we propose a Guided Robust and Efficient Defensive Model GRED integrating detection and recovery processes together. From the lens of the properties of gradient distribution of adversarial examples, our model detects malicious inputs effectively, as well as recovering the ground-truth label with high accuracy. Compared with commonly used adversarial training methods, our model is more efficient and outperforms state-of-the-art adversarial trained models by a large margin up to 99% on MNIST, 89 % on CIFAR-10 and 87% on ImageNet subsets. When exclusively compared with previous adversarial detection methods, the detector of GRED is robust under all threat settings with a detection rate of over 95% against most of the attacks. It is also demonstrated by empirical assessment that our model could increase attacking cost significantly resulting in either unacceptable time consuming or human perceptible image distortions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络进行快速的发展，实现在不同的任务，其中包括许多安全相关的场景显着的成功。然而，相当数量的作品已经证明了其在对手的漏洞。为了解决这个问题，我们提出了一个指导鲁棒高效防守型GRED整合检测和恢复过程在一起。从对抗性例子梯度分布的特性的透镜，我们的模型检测有效恶意输入，以及回收具有高精度的地面实况标签。与常用的对抗训练方法相比，我们的模型更有效，优于国家的最先进的对抗性训练的模型通过对ImageNet亚大比分高达99％的MNIST，89％的CIFAR-10和87％。当与前面的对抗性的检测方法相比，专用，GRED的探测器正在以超过95％对大多数的攻击检测率都威胁设置强劲。它也被实证评估，我们的模型可以提高攻击成本显著导致无论是不可接受的耗时或人类感知的图像失真证明。</font>
</div>


<hr>
<div id="paper22"> <b>22. Utilizing Network Properties to Detect Erroneous Inputs</b>  <a href="https://arxiv.org/pdf/2002.12520" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gorbett%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matt Gorbett</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Blanchard%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nathaniel Blanchard</a><br>
<font size="3">
Abstract: Neural networks are vulnerable to a wide range of erroneous inputs such as adversarial, corrupted, out-of-distribution, and misclassified examples. In this work, we train a linear SVM classifier to detect these four types of erroneous data using hidden and softmax feature vectors of pre-trained neural networks. Our results indicate that these faulty data types generally exhibit linearly separable activation properties from correct examples, giving us the ability to reject bad inputs with no extra training or overhead. We experimentally validate our findings across a diverse range of datasets, domains, pre-trained models, and adversarial attacks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经网络是容易受到广泛的错误输入，诸如对抗性，损坏，外的分布，和错误分类的例子。在这项工作中，我们训练的线性SVM分类检测这四种类型的使用预训练神经网络的隐藏和SOFTMAX特征向量错误数据。我们的研究结果表明，这些错误的数据类型通常从正确的实例展现线性可分活化性能，让我们拒绝，没有额外的培训或开销坏投入的能力。我们通过实验验证在一组不同的数据集的范围，域，预先训练模型，并对抗攻击我们的研究结果。</font>
</div>


<hr>
<div id="paper23"> <b>23. DGST : Discriminator Guided Scene Text detector</b>  <a href="https://arxiv.org/pdf/2002.12509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinyuan Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanna Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baihua Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cunzhao Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fuxi Jia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunheng Wang</a><br>
<font size="3">
Abstract: Scene text detection task has attracted considerable attention in computer vision because of its wide application. In recent years, many researchers have introduced methods of semantic segmentation into the task of scene text detection, and achieved promising results. This paper proposes a detector framework based on the conditional generative adversarial networks to improve the segmentation effect of scene text detection, called DGST (Discriminator Guided Scene Text detector). Instead of binary text score maps generated by some existing semantic segmentation based methods, we generate a multi-scale soft text score map with more information to represent the text position more reasonably, and solve the problem of text pixel adhesion in the process of text extraction. Experiments on standard datasets demonstrate that the proposed DGST brings noticeable gain and outperforms state-of-the-art methods. Specifically, it achieves an F-measure of 87% on ICDAR 2015 dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：场景文本检测任务已经吸引了，因为它的广泛应用计算机视觉相当大的关注。近年来，许多研究人员介绍了语义分割的方法为现场文字检测的任务，并取得了可喜的成果。本文提出了一种基于条件生成对抗性的网络，以提高现场文本检测的分割效果的检测框架，称为DGST（鉴别制导场景文字检测器）。代替二进制文本得分映射由一些现有的语义分割为基础的方法产生的，我们生成多尺度软文本得分图与更多的信息，以表示文本位置更加合理，并解决文字像素密合性的问题，在文本提取的过程中。在标准数据集的实验表明，该DGST带来了明显的增益和优于国家的最先进的方法。具体而言，实现了87％的ICDAR 2015的数据集的F值。</font>
</div>


<hr>
<div id="paper24"> <b>24. Detecting Patch Adversarial Attacks with Image Residuals</b>  <a href="https://arxiv.org/pdf/2002.12504" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Arvinte%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marius Arvinte</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tewfik%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmed Tewfik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vishwanath%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sriram Vishwanath</a><br>
<font size="3">
Abstract: We introduce an adversarial sample detection algorithm based on image residuals, specifically designed to guard against patch-based attacks. The image residual is obtained as the difference between an input image and a denoised version of it, and a discriminator is trained to distinguish between clean and adversarial samples. More precisely, we use a wavelet domain algorithm for denoising images and demonstrate that the obtained residuals act as a digital fingerprint for adversarial attacks. To emulate the limitations of a physical adversary, we evaluate the performance of our approach against localized (patch-based) adversarial attacks, including in settings where the adversary has complete knowledge about the detection scheme. Our results show that the proposed detection method generalizes to previously unseen, stronger attacks and that it is able to reduce the success rate (conversely, increase the computational effort) of an adaptive attacker. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍了基于图像残差对抗性的样品检测算法，专门设计用于防范基于补丁的攻击。的图像残留被作为输入图像和它的去噪版本之间的差而获得，和鉴别器被训练清洁和对抗性样品之间进行区分。更确切地说，我们使用了基于小波域的图像去噪，并证明所获得的残差充当对抗攻击的数字指纹。为了模拟物理对手的限制，我们评估我们对本地化（基于补丁）敌对攻击，包括在设置里的对手有关于检测方案完整的知识方法的性能。我们的研究结果表明，该检测方法推广到以前看不到的，更强的攻击，它能够降低成功率（反过来，增加计算工作量）自适应攻击者。</font>
</div>


<hr>
<div id="paper25"> <b>25. Road Curb Detection and Localization with Monocular Forward-view Vehicle  Camera</b>  <a href="https://arxiv.org/pdf/2002.12492" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Panev%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stanislav Panev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vicente%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Francisco Vicente</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=De+la+Torre%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fernando De la Torre</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prinet%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Véronique Prinet</a><br>
<font size="3">
Abstract: We propose a robust method for estimating road curb 3D parameters (size, location, orientation) using a calibrated monocular camera equipped with a fisheye lens. Automatic curb detection and localization is particularly important in the context of Advanced Driver Assistance System (ADAS), i.e. to prevent possible collision and damage of the vehicle's bumper during perpendicular and diagonal parking maneuvers. Combining 3D geometric reasoning with advanced vision-based detection methods, our approach is able to estimate the vehicle to curb distance in real time with mean accuracy of more than 90%, as well as its orientation, height and depth. Our approach consists of two distinct components - curb detection in each individual video frame and temporal analysis. The first part comprises of sophisticated curb edges extraction and parametrized 3D curb template fitting. Using a few assumptions regarding the real world geometry, we can thus retrieve the curb's height and its relative position w.r.t. the moving vehicle on which the camera is mounted. Support Vector Machine (SVM) classifier fed with Histograms of Oriented Gradients (HOG) is used for appearance-based filtering out outliers. In the second part, the detected curb regions are tracked in the temporal domain, so as to perform a second pass of false positives rejection. We have validated our approach on a newly collected database of 11 videos under different conditions. We have used point-wise LIDAR measurements and manual exhaustive labels as a ground truth. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出用于估计道路路边使用配备有鱼眼透镜校准的单眼照相机的3D参数（大小，位置，方向）一个稳健的方法。自动路边检测和定位是在高级驾驶辅助系统（ADAS），即上下文，以防止在垂直和对角线停车操作可能的碰撞和车辆的保险杠的损伤特别重要。结合3D几何推理与先进的基于视觉的检测方法，我们的做法是能够以超过90％的平均准确度，以及它的方向，高度和深度估计车辆路边距离的实时性。我们的方法包括两个不同的部件 - 在每个单独的视频帧和时间分析路边检测。复杂的路缘的所述第一部分包括边缘提取和参数化3D路边模板装配件。使用关于现实世界的几何几个假设，我们可以由此获取路边的高度和其相对位置w.r.t.其上安装摄像机的移动车辆。支持向量机（SVM）以方向梯度直方图（HOG）分类器供给的用于外观基础的滤出异常值。在第二部分中，检测到的路边区域跟踪在时间域中，以便执行误报排斥的第二遍。我们已经验证了不同条件下的11个视频一个新收集的数据库上我们的做法。我们已经使用了逐点激光雷达测量结果和手动详尽的标签，作为地面实况。</font>
</div>


<hr>
<div id="paper26"> <b>26. Cross-modality Person re-identification with Shared-Specific Feature  Transfer</b>  <a href="https://arxiv.org/pdf/2002.12489" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Lu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianzhu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baopu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Chu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nenghai Yu</a><br>
<font size="3">
Abstract: Cross-modality person re-identification (cm-ReID) is a challenging but key technology for intelligent video analysis. Existing works mainly focus on learning common representation by embedding different modalities into a same feature space. However, only learning the common characteristics means great information loss, lowering the upper bound of feature distinctiveness. In this paper, we tackle the above limitation by proposing a novel cross-modality shared-specific feature transfer algorithm (termed cm-SSFT) to explore the potential of both the modality-shared information and the modality-specific characteristics to boost the re-identification performance. We model the affinities of different modality samples according to the shared features and then transfer both shared and specific features among and across modalities. We also propose a complementary feature learning strategy including modality adaption, project adversarial learning and reconstruction enhancement to learn discriminative and complementary shared and specific features of each modality, respectively. The entire cm-SSFT algorithm can be trained in an end-to-end manner. We conducted comprehensive experiments to validate the superiority of the overall algorithm and the effectiveness of each component. The proposed algorithm significantly outperforms state-of-the-arts by 22.5% and 19.3% mAP on the two mainstream benchmark datasets SYSU-MM01 and RegDB, respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨模态的人重新鉴定（CM-REID）是智能视频分析一个具有挑战性的，但关键技术。现有工程主要集中在通过嵌入不同的方式划分到同一特征空间学习共同表示。然而，只有学习的共同特点意味着巨大的信息损失，降低上限特征的显着性。在本文中，我们通过提出一种新颖的跨模态解决上述限制共享特定功能转移算法（称为CM-SSFT）探讨的形态共享信息和所述模态具体特性二者的电位以刺激重新识别性能。我们根据共享特征的不同模态样品的亲和力模型，然后之间以及跨模态传输共享和特定功能。我们还提出了一个补充特征的学习策略，包括形态适应，项目对抗性学习和重建增强学习辨别和互补共享，并分别各模式的特定功能。整个厘米-SSFT算法可以在端至端的方式来训练。我们进行了全面的实验来验证整个算法的优越性和每个组件的有效性。 22.5％和两大主流标准数据集分别中山大学-MM01和RegDB，19.3％映像该算法显著优于国家的最艺术。</font>
</div>


<hr>
<div id="paper27"> <b>27. Improving Learning Effectiveness For Object Detection and Classification  in Cluttered Backgrounds</b>  <a href="https://arxiv.org/pdf/2002.12467" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Varatharasan%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vinorth Varatharasan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyo-Sang Shin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tsourdos%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonios Tsourdos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Colosimo%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nick Colosimo</a><br>
<font size="3">
Abstract: Usually, Neural Networks models are trained with a large dataset of images in homogeneous backgrounds. The issue is that the performance of the network models trained could be significantly degraded in a complex and heterogeneous environment. To mitigate the issue, this paper develops a framework that permits to autonomously generate a training dataset in heterogeneous cluttered backgrounds. It is clear that the learning effectiveness of the proposed framework should be improved in complex and heterogeneous environments, compared with the ones with the typical dataset. In our framework, a state-of-the-art image segmentation technique called DeepLab is used to extract objects of interest from a picture and Chroma-key technique is then used to merge the extracted objects of interest into specific heterogeneous backgrounds. The performance of the proposed framework is investigated through empirical tests and compared with that of the model trained with the COCO dataset. The results show that the proposed framework outperforms the model compared. This implies that the learning effectiveness of the framework developed is superior to the models with the typical dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通常情况下，神经网络模型在均匀背景的大型数据集的图像的训练。问题是，训练网络模型的性能，可以在复杂的异构环境中显著下降。为了缓解这一问题，本文建立了一个框架，允许自主生成异构杂乱背景的训练数据集。很显然，所提出的框架的学习成效应该在复杂的异构环境得到改善，与典型的数据集进行了对比。在我们的框架，称为DeepLab一个国家的最先进的图像分割技术从图像中使用与感兴趣提取的对象，然后色度键技术用于所提取的感兴趣对象合并成特定的异构背景。拟议框架的性能是通过实证检验调查，并与与COCO数据集训练模式的比较。结果表明，所提出的框架相比优于模型。这意味着开发框架的学习成效优于与典型的数据集模型。</font>
</div>


<hr>
<div id="paper28"> <b>28. Target Detection, Tracking and Avoidance System for Low-cost UAVs using  AI-Based Approaches</b>  <a href="https://arxiv.org/pdf/2002.12461" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Varatharasan%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vinorth Varatharasan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rao%2C+A+S+S" target="_blank" rel="noopener" style="color:#0000EE;">Alice Shuang Shuang Rao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Toutounji%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Toutounji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Ju-Hyeon Hong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyo-Sang Shin</a><br>
<font size="3">
Abstract: An onboard target detection, tracking and avoidance system has been developed in this paper, for low-cost UAV flight controllers using AI-Based approaches. The aim of the proposed system is that an ally UAV can either avoid or track an unexpected enemy UAV with a net to protect itself. In this point of view, a simple and robust target detection, tracking and avoidance system is designed. Two open-source tools were used for the aim: a state-of-the-art object detection technique called SSD and an API for MAVLink compatible systems called MAVSDK. The MAVSDK performs velocity control when a UAV is detected so that the manoeuvre is done simply and efficiently. The proposed system was verified with Software in the loop (SITL) and Hardware in the loop (HITL) simulators. The simplicity of this algorithm makes it innovative, and therefore it should be used in future applications needing robust performances with low-cost hardware such as delivery drone applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机载目标探测，跟踪和回避制度已在本文中被开发，利用基于人工智能的方法低成本无人机飞行控制器。所提出的系统的目的是盟友无人机既可避免或跟踪意想不到的敌人UAV用网来保护自己。在这个角度来看，简单的和鲁棒的目标检测，跟踪和规避系统的设计。两个开源工具被用于目的是：所谓的SSD的状态的最先进的对象检测技术和所谓MAVSDK为MAVLink兼容系统的API。当检测到UAV使得操纵简单地和高效地完成MAVSDK执行速度控制。所提出的系统在环（SITL）和硬件在环（HITL）模拟器使用软件验证。该算法的简单性使得它的创新，因此它应该在未来的应用需要强大的性能与低成本的硬件，如交付无人机应用。</font>
</div>


<hr>
<div id="paper29"> <b>29. TGGLines: A Robust Topological Graph Guided Line Segment Detector for  Low Quality Binary Images</b>  <a href="https://arxiv.org/pdf/2002.12428" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liping Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Potts%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Catherine Potts</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Asari%2C+V+K" target="_blank" rel="noopener" style="color:#0000EE;">Vijayan K. Asari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oyen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diane Oyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wohlberg%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brendt Wohlberg</a><br>
<font size="3">
Abstract: Line segment detection is an essential task in computer vision and image analysis, as it is the critical foundation for advanced tasks such as shape modeling and road lane line detection for autonomous driving. We present a robust topological graph guided approach for line segment detection in low quality binary images (hence, we call it TGGLines). Due to the graph-guided approach, TGGLines not only detects line segments, but also organizes the segments with a line segment connectivity graph, which means the topological relationships (e.g., intersection, an isolated line segment) of the detected line segments are captured and stored; whereas other line detectors only retain a collection of loose line segments. Our empirical results show that the TGGLines detector visually and quantitatively outperforms state-of-the-art line segment detection methods. In addition, our TGGLines approach has the following two competitive advantages: (1) our method only requires one parameter and it is adaptive, whereas almost all other line segment detection methods require multiple (non-adaptive) parameters, and (2) the line segments detected by TGGLines are organized by a line segment connectivity graph. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：线段检测是计算机视觉和图像分析的一项重要任务，因为它是高级任务，如形状建模和道路的车道线检测自动驾驶的重要基础。我们提出了一个坚固的拓扑图被引导为线段检测低质量二进制图像（因此，我们称它为TGGLines）的方法。由于该图的引导方法，TGGLines不仅检测线段，但还组织用线段连接图，这意味着拓扑关系的段（例如，交叉点，分离的线段）的检测到的线段的被捕获并存储;而其它线检测器只保留松散线段的集合。我们的经验表明，在视觉上TGGLines和定量检测器性能优于国家的最先进的线段检测方法。此外，我们的TGGLines方法有以下两种竞争优势：（1）我们的方法只需要一个参数，它是自适应的，而几乎所有其他线段检测方法需要多个（非自适应）参数，以及（2）线通过TGGLines检测段通过线段连接图组织。</font>
</div>


<hr>
<div id="paper30"> <b>30. MNN: A Universal and Efficient Inference Engine</b>  <a href="https://arxiv.org/pdf/2002.12418" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaotang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiliu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqi Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lichuan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Zou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yafeng Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zongyang Cui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Cai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianhang Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengfei Lv</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhihua Wu</a><br>
<font size="3">
Abstract: Deploying deep learning models on mobile devices draws more and more attention recently. However, designing an efficient inference engine on devices is under the great challenges of model compatibility, device diversity, and resource limitation. To deal with these challenges, we propose Mobile Neural Network (MNN), a universal and efficient inference engine tailored to mobile applications. In this paper, the contributions of MNN include: (1) presenting a mechanism called pre-inference that manages to conduct runtime optimization; (2)deliveringthorough kernel optimization on operators to achieve optimal computation performance; (3) introducing backend abstraction module which enables hybrid scheduling and keeps the engine lightweight. Extensive benchmark experiments demonstrate that MNN performs favorably against other popular lightweight deep learning frameworks. MNN is available to public at: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在移动设备上部署深度学习模型吸引越来越多的关注最近。然而，在设备设计一个有效的推理引擎是在模型的兼容性，设备的多样性和资源限制的巨大挑战。为了应对这些挑战，我们提出移动神经网络（MNN），针对移动应用的通用，高效的推理引擎。在本文中，MNN的贡献包括：（1）提出的机制称为预推论，即设法进行运行时优化; （2）deliveringthorough内核优化运营商，以实现最佳的运算效能; （3）将后端抽象模块使混合调度，并保持发动机轻量化。广泛的基准测试实验表明毫不逊色与其他流行的轻量级深度学习框架，MNN执行。 MNN是提供给公立：此HTTPS URL。</font>
</div>


<hr>
<div id="paper31"> <b>31. Learning in the Frequency Domain</b>  <a href="https://arxiv.org/pdf/2002.12416" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghai Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhao Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yen-kuang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fengbo Ren</a><br>
<font size="3">
Abstract: Deep neural networks have achieved remarkable success in computer vision tasks. Existing neural networks mainly operate in the spatial domain with fixed input sizes. For practical applications, images are usually large and have to be downsampled to the predetermined input size of neural networks. Even though the downsampling operations reduce computation and the required communication bandwidth, it removes both redundant and salient information obliviously, which results in accuracy degradation. Inspired by digital signal processing theories, we analyze the spectral bias from the frequency perspective and propose a learning-based frequency selection method to identify the trivial frequency components which can be removed without accuracy loss. The proposed method of learning in the frequency domain leverages identical structures of the well-known neural networks, such as ResNet-50, MobileNetV2, and Mask R-CNN, while accepting the frequency-domain information as the input. Experiment results show that learning in the frequency domain with static channel selection can achieve higher accuracy than the conventional spatial downsampling approach and meanwhile further reduce the input data size. Specifically for ImageNet classification with the same input size, the proposed method achieves 1.41% and 0.66% top-1 accuracy improvements on ResNet-50 and MobileNetV2, respectively. Even with half input size, the proposed method still improves the top-1 accuracy on ResNet-50 by 1%. In addition, we observe a 0.8% average precision improvement on Mask R-CNN for instance segmentation on the COCO dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络已实现了计算机视觉任务显着成效。现有神经网络主要操作与固定输入大小的空间域。对于实际应用，图像通常是大而不得不被下采样到神经网络的预定的输入大小。即使下采样操作减少计算量和所需的通信带宽，它东北角消除冗余和重要资料，这会导致精度下降。通过数字信号处理的理论启发，我们分析来自频率透视光谱偏压，提出了一种基于学习的频率选择方法，以确定可以在不损失精确度被去除的琐碎的频率分量。在频域中的学习所提出的方法利用了公知的神经网络的相同的结构，如RESNET-50，MobileNetV2，和掩码R-CNN，在接受频域信息作为输入。实验结果表明，在使用静态频道选择频域学习能够比传统的空间下采样方法达到更高的精度，同时进一步减小输入数据的大小。具体地，对于具有相同输入大小ImageNet分类，所提出的方法分别达到上RESNET-50和MobileNetV2 1.41％和0.66％顶部-1的精度的改进，。即使半输入大小，所提出的方法仍然改进了-50 RESNET 1％顶1的精度。此外，我们观察面膜R-CNN例如分割的COCO数据集0.8％的平均精度的提高。</font>
</div>


<hr>
<div id="paper32"> <b>32. SilhoNet-Fisheye: Adaptation of A ROI Based Object Pose Estimation  Network to Monocular Fisheye Images</b>  <a href="https://arxiv.org/pdf/2002.12415" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Billings%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gideon Billings</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Johnson-Roberson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Johnson-Roberson</a><br>
<font size="3">
Abstract: There has been much recent interest in deep learning methods for monocular image based object pose estimation. While object pose estimation is an important problem for autonomous robot interaction with the physical world, and the application space for monocular-based methods is expansive, there has been little work on applying these methods with fisheye imaging systems. Also, little exists in the way of annotated fisheye image datasets on which these methods can be developed and tested. The research landscape is even more sparse for object detection methods applied in the underwater domain, fisheye image based or otherwise. In this work, we present a novel framework for adapting a ROI-based 6D object pose estimation method to work on full fisheye images. The method incorporates the gnomic projection of regions of interest from an intermediate spherical image representation to correct for the fisheye distortions. Further, we contribute a fisheye image dataset, called UWHandles, collected in natural underwater environments, with 6D object pose and 2D bounding box annotations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近增加了一些深刻的学习方法单眼基于图像的目标姿态估计的极大兴趣。当物体姿态估计是与物理世界的自主机器人互动的一个重要问题，以及基于单眼的方法应用空间广阔的是，一直在运用这些方法与鱼眼镜头成像系统一点的工作。此外，很少存在于可以在其上开发和测试这些方法标注的鱼眼图像数据集的方式。研究环境为对象的检测方法更稀疏在水下域中应用，鱼眼图像基于或以其它方式。在这项工作中，我们提出了一个适应的基于ROI的6D对象姿态估计方法工作在全鱼眼图像的新框架。该方法包含的感兴趣区域的格言投影从中间球形图像表示以校正鱼眼失真。此外，我们的贡献鱼眼图像数据集，称为UWHandles，在天然环境中的水下收集，6D对象姿势和2D边界框注释。</font>
</div>


<hr>
<div id="paper33"> <b>33. Brain-Inspired Model for Incremental Learning Using a Few Examples</b>  <a href="https://arxiv.org/pdf/2002.12411" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ayub%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Ayub</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wagner%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alan Wagner</a><br>
<font size="3">
Abstract: Incremental learning attempts to develop a classifier which learns continuously from a stream of data segregated into different classes. Deep learning approaches suffer from catastrophic forgetting when learning classes incrementally. We propose a novel approach to incremental learning inspired by the concept learning model of the hippocampus that represents each image class as centroids and does not suffer from catastrophic forgetting. Classification of a test image is accomplished using the distance of the test image to the n closest centroids. We further demonstrate that our approach can incrementally learn from only a few examples per class. Evaluations of our approach on three class-incremental learning benchmarks: Caltech-101, CUBS-200-2011 and CIFAR-100 for incremental and few-shot incremental learning depict state-of-the-art results in terms of classification accuracy over all learned classes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：增量学习尝试开发一种从数据流中不断学习的分类分成不同的类别。学习班的时候逐步深学习方法灾难性遗忘痛苦。我们提出了一个新的方法来通过表示每个图像类的质心，不因灾难性遗忘遭受海马的概念学习模式的启发增量学习。测试图像的分类是使用于n最接近矩心的测试图像的距离来完成的。我们进一步证明我们的方法可以逐步从每类只举几个例子学习。我们的方法的三个类增量学习的基准评估：加州理工学院-101，CUBS-200-2011，并在分类准确性方面CIFAR-100为国家的最先进的增量和一些次增量学习描绘结果在所有学类。</font>
</div>


<hr>
<div id="paper34"> <b>34. Affinity guided Geometric Semi-Supervised Metric Learning</b>  <a href="https://arxiv.org/pdf/2002.12394" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dutta%2C+U+K" target="_blank" rel="noopener" style="color:#0000EE;">Ujjal Kr Dutta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Harandi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehrtash Harandi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sekhar%2C+C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chellu Chandra Sekhar</a><br>
<font size="3">
Abstract: In this paper, we address the semi-supervised metric learning problem, where we learn a distance metric using very few labeled examples, and additionally available unlabeled data. To address the limitations of existing semi-supervised approaches, we integrate some of the best practices across metric learning, to achieve the state-of-the-art in the semi-supervised setting. In particular, we make use of a graph-based approach to propagate the affinities or similarities among the limited labeled pairs to the unlabeled data. Considering the neighborhood of an example, we take into account the propagated affinities to mine triplet constraints. An angular loss is imposed on these triplets to learn a metric. Additionally, we impose orthogonality on the parameters of the learned embedding to avoid a model collapse. In contrast to existing approaches, we propose a stochastic approach that scales well to large-scale datasets. We outperform various semi-supervised metric learning approaches on a number of benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们解决了半监督度量学习问题，在这里我们学到了距离度量使用很少的标识样本，并另外提供标签数据。为了解决现有的半监督方法的局限性，我们整合了一些跨度量学习的最佳实践，实现了国家的最先进的半监督设置。特别是，我们利用一种基于图形的方法来有限标记对之间的亲和度或相似性传播至未标记的数据。考虑一个例子的附近，我们考虑到传播的亲和力矿三重约束。角损失强加于这些三胞胎学习的度量。此外，我们强加正交上了解到嵌入避免了模型崩溃的参数。相较于现有的方法，我们提出了一个随机的办法，很好地进行扩展的大型数据集。我们跑赢多项标准数据集的各种半监督度量学习方法。</font>
</div>


<hr>
<div id="paper35"> <b>35. Joint 2D-3D Breast Cancer Classification</b>  <a href="https://arxiv.org/pdf/2002.12392" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gongbo Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoqin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Blanton%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hunter Blanton</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Salem%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tawfiq Salem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jacobs%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nathan Jacobs</a><br>
<font size="3">
Abstract: Breast cancer is the malignant tumor that causes the highest number of cancer deaths in females. Digital mammograms (DM or 2D mammogram) and digital breast tomosynthesis (DBT or 3D mammogram) are the two types of mammography imagery that are used in clinical practice for breast cancer detection and diagnosis. Radiologists usually read both imaging modalities in combination; however, existing computer-aided diagnosis tools are designed using only one imaging modality. Inspired by clinical practice, we propose an innovative convolutional neural network (CNN) architecture for breast cancer classification, which uses both 2D and 3D mammograms, simultaneously. Our experiment shows that the proposed method significantly improves the performance of breast cancer classification. By assembling three CNN classifiers, the proposed model achieves 0.97 AUC, which is 34.72% higher than the methods using only one imaging modality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：乳腺癌是恶性肿瘤导致女性癌症死亡的人数最多。数字乳房X线照片（DM或2D乳房X线照片）和数字乳房断层合成（DBT或3D乳房X线照片）是两种类型的乳房X射线摄影图像，其将在临床实践中用于乳腺癌检测和诊断。放射科医生通常在读两者结合成像模态;然而，现有的计算机辅助诊断工具只使用一个成像模态设计。通过临床实践的启发，我们提出了乳腺癌的分类，它采用2D和3D乳房X光检查，同时创新的卷积神经网络（CNN）架构。我们的实验表明，该方法显著提高乳腺癌分类的性能。通过组装3个CNN分类器，所提出的模型达到0.97 AUC，其比仅使用一个成像模态的方法高34.72％。</font>
</div>


<hr>
<div id="paper36"> <b>36. Review: Noise and artifact reduction for MRI using deep learning</b>  <a href="https://arxiv.org/pdf/2002.12889" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Tamada%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daiki Tamada</a><br>
<font size="3">
Abstract: For several years, numerous attempts have been made to reduce noise and artifacts in MRI. Although there have been many successful methods to address these problems, practical implementation for clinical images is still challenging because of its complicated mechanism. Recently, deep learning received considerable attention, emerging as a machine learning approach in delivering robust MR image processing. The purpose here is therefore to explore further and review noise and artifact reduction using deep learning for MRI. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：几年来，已进行了多次尝试，以噪音和伪像减少MRI。虽然已经有解决这些问题的许多成功的方法，为临床影像实际执行仍是由于其复杂的机制挑战。近日，深学习受到了相当的关注，逐渐成为提供强大MR图像处理的机器学习方法。因此，这里的目的是进一步探讨和审查噪声和假象减少使用深度学习的MRI检查。</font>
</div>


<hr>
<div id="paper37"> <b>37. Neural Network Segmentation of Interstitial Fibrosis, Tubular Atrophy,  and Glomerulosclerosis in Renal Biopsies</b>  <a href="https://arxiv.org/pdf/2002.12868" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Ginley%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brandon Ginley</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Jen%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kuang-Yu Jen</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Rosenberg%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avi Rosenberg</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Yen%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Felicia Yen</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Jain%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanjay Jain</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Fogo%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Agnes Fogo</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Sarder%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pinaki Sarder</a><br>
<font size="3">
Abstract: Glomerulosclerosis, interstitial fibrosis, and tubular atrophy (IFTA) are histologic indicators of irrecoverable kidney injury. In standard clinical practice, the renal pathologist visually assesses, under the microscope, the percentage of sclerotic glomeruli and the percentage of renal cortical involvement by IFTA. Estimation of IFTA is a subjective process due to a varied spectrum and definition of morphological manifestations. Modern artificial intelligence and computer vision algorithms have the ability to reduce inter-observer variability through rigorous quantitation. In this work, we apply convolutional neural networks for the segmentation of glomerulosclerosis and IFTA in periodic acid-Schiff stained renal biopsies. The convolutional network approach achieves high performance in intra-institutional holdout data, and achieves moderate performance in inter-intuitional holdout data, which the network had never seen in training. The convolutional approach demonstrated interesting properties, such as learning to predict regions better than the provided ground truth as well as developing its own conceptualization of segmental sclerosis. Subsequent estimations of IFTA and glomerulosclerosis percentages showed high correlation with ground truth. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：肾小球硬化，间质纤维化和管萎缩（IFTA）是不可恢复的肾损伤的组织学指标。在标准的临床实践中，肾病理学家视觉评估，在显微镜下，肾小球硬化的百分比和肾皮质参与由IFTA的百分比。 IFTA的估计是一个主观过程中由于形态表现的多样化的光谱和定义。现代人工智能和计算机视觉算法必须经过严格的定量分析，以减少国际观察员变异的能力。在这项工作中，我们采用卷积神经网络的肾小球硬化和IFTA的高碘酸 - 希夫分割染色的肾活检。卷积网络的方式实现了在机构内维持数据的高性能，并实现跨直观维持数据，其中网络从未在训练中看到的性能适中。卷积方法证明有趣的特性，如学习到比所提供的地面实况更好地预测地区以及开发自己的节段性硬化的概念化。 IFTA和肾小球硬化百分比的后续估计显示，与地面实情很高的相关性。</font>
</div>


<hr>
<div id="paper38"> <b>38. HOTCAKE: Higher Order Tucker Articulated Kernels for Deeper CNN  Compression</b>  <a href="https://arxiv.org/pdf/2002.12663" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ching-Yun Ko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuolun He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cong Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chesi%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Graziano Chesi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wong%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ngai Wong</a><br>
<font size="3">
Abstract: The emerging edge computing has promoted immense interests in compacting a neural network without sacrificing much accuracy. In this regard, low-rank tensor decomposition constitutes a powerful tool to compress convolutional neural networks (CNNs) by decomposing the 4-way kernel tensor into multi-stage smaller ones. Building on top of Tucker-2 decomposition, we propose a generalized Higher Order Tucker Articulated Kernels (HOTCAKE) scheme comprising four steps: input channel decomposition, guided Tucker rank selection, higher order Tucker decomposition and fine-tuning. By subjecting each CONV layer to HOTCAKE, a highly compressed CNN model with graceful accuracy trade-off is obtained. Experiments show HOTCAKE can compress even pre-compressed models and produce state-of-the-art lightweight networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：新兴的边缘计算压实神经网络在不牺牲太多的精确性促进了巨大的利益。在这方面，低秩张量分解构成通过分解四向内核张量为多级较小的一个有力的工具来压缩卷积神经网络（细胞神经网络）。上塔克-2分解的顶部的基础上，提出了一种广义高阶塔克铰接式内核（HOTCAKE）方案，其包括四个步骤：输入信道分解，导塔克秩选择，高阶塔克分解和微调。通过对每个CONV层HOTCAKE，获得了具有优美的精度的折衷高度压缩CNN模型。实验表明HOTCAKE可以压缩甚至预压缩模式和国家的最先进的生产轻量级网络。</font>
</div>


<hr>
<div id="paper39"> <b>39. An Efficient Method of Training Small Models for Regression Problems  with Knowledge Distillation</b>  <a href="https://arxiv.org/pdf/2002.12597" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Takamoto%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Makoto Takamoto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morishita%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yusuke Morishita</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Imaoka%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hitoshi Imaoka</a><br>
<font size="3">
Abstract: Compressing deep neural network (DNN) models becomes a very important and necessary technique for real-world applications, such as deploying those models on mobile devices. Knowledge distillation is one of the most popular methods for model compression, and many studies have been made on developing this technique. However, those studies mainly focused on classification problems, and very few attempts have been made on regression problems, although there are many application of DNNs on regression problems. In this paper, we propose a new formalism of knowledge distillation for regression problems. First, we propose a new loss function, teacher outlier rejection loss, which rejects outliers in training samples using teacher model predictions. Second, we consider a multi-task network with two outputs: one estimates training labels which is in general contaminated by noisy labels; And the other estimates teacher model's output which is expected to modify the noise labels following the memorization effects. By considering the multi-task network, training of the feature extraction of student models becomes more effective, and it allows us to obtain a better student model than one trained from scratch. We performed comprehensive evaluation with one simple toy model: sinusoidal function, and two open datasets: MPIIGaze, and Multi-PIE. Our results show consistent improvement in accuracy regardless of the annotation error level in the datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：压缩深层神经网络（DNN）模型成为现实世界的应用，比如在移动设备上部署这些模型一个非常重要和必要的技术。知识蒸馏是对模型压缩最常用的方法之一，许多研究已取得开发这一技术。然而，这些研究主要集中在分类问题，很少尝试已经对回归问题，虽然有DNNs对回归问题的许多应用程序。在本文中，我们提出了知识蒸馏的回归问题提供了新的形式主义。首先，我们提出了一个新的损失函数，老师异常值拒绝的损失，这将拒绝使用教师模型预测训练样本中的异常值。其次，我们考虑两个输出的多任务网络：一个估计训练的标签，其是在一般嘈杂的标签污染;并预计将修改后的记忆效果的噪音标签的其他估计老师模型的输出。通过考虑多任务的网络，培养学生机型的特征提取变得更加有效，它使我们能够获得一个更好的学生模型比一个从头开始培训。正弦函数，和两个开放数据集：MPIIGaze和Multi-PIE我们一个简单的玩具模型进行综合评价。我们的研究结果显示，准确度持续改善，无论在数据集注释错误级别。</font>
</div>


<hr>
<div id="paper40"> <b>40. Regional Registration of Whole Slide Image Stacks Containing Highly  Deformed Artefacts</b>  <a href="https://arxiv.org/pdf/2002.12588" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Paknezhad%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahsa Paknezhad</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Loh%2C+S+Y+M" target="_blank" rel="noopener" style="color:#0000EE;">Sheng Yang Michael Loh</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Choudhury%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yukti Choudhury</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Koh%2C+V+K+C" target="_blank" rel="noopener" style="color:#0000EE;">Valerie Koh Cui Koh</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Yong%2C+T+K" target="_blank" rel="noopener" style="color:#0000EE;">TimothyTay Kwang Yong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tan%2C+H+S" target="_blank" rel="noopener" style="color:#0000EE;">Hui Shan Tan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kanesvaran%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ravindran Kanesvaran</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tan%2C+P+H" target="_blank" rel="noopener" style="color:#0000EE;">Puay Hoon Tan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Peng%2C+J+Y+S" target="_blank" rel="noopener" style="color:#0000EE;">John Yuen Shyi Peng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Yu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weimiao Yu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tan%2C+Y+B" target="_blank" rel="noopener" style="color:#0000EE;">Yongcheng Benjamin Tan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Loy%2C+Y+Z" target="_blank" rel="noopener" style="color:#0000EE;">Yong Zhen Loy</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Min-Han Tan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lee%2C+H+K" target="_blank" rel="noopener" style="color:#0000EE;">Hwee Kuan Lee</a><br>
<font size="3">
Abstract: Motivation: High resolution 2D whole slide imaging provides rich information about the tissue structure. This information can be a lot richer if these 2D images can be stacked into a 3D tissue volume. A 3D analysis, however, requires accurate reconstruction of the tissue volume from the 2D image stack. This task is not trivial due to the distortions that each individual tissue slice experiences while cutting and mounting the tissue on the glass slide. Performing registration for the whole tissue slices may be adversely affected by the deformed tissue regions. Consequently, regional registration is found to be more effective. In this paper, we propose an accurate and robust regional registration algorithm for whole slide images which incrementally focuses registration on the area around the region of interest. Results: Using mean similarity index as the metric, the proposed algorithm (mean $\pm$ std: $0.84 \pm 0.11$) followed by a fine registration algorithm ($0.86 \pm 0.08$) outperformed the state-of-the-art linear whole tissue registration algorithm ($0.74 \pm 0.19$) and the regional version of this algorithm ($0.81 \pm 0.15$). The proposed algorithm also outperforms the state-of-the-art nonlinear registration algorithm (original : $0.82 \pm 0.12$, regional : $0.77 \pm 0.22$) for whole slide images and a recently proposed patch-based registration algorithm (patch size 256: $0.79 \pm 0.16$ , patch size 512: $0.77 \pm 0.16$) for medical images. Availability: The C++ implementation code is available online at the github repository: this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：动机：高分辨率2D整个幻灯片成像提供了关于组织结构的丰富信息。该信息可以有很多更丰富的，如果这些2D图像可以被堆叠成三维组织体积。一种3D分析，但是，需要从2D图像堆栈中的组织体积的精确重建。此任务不是每个单独的组织切片的经验而切割和安装在载玻片组织中的失真琐碎所致。对于整个组织切片执行注册可以通过变形的组织区域会受到不利影响。因此，区域登记被发现是更有效的。在本文中，我们提出了整个幻灯片图像的精确和稳健的区块登记管理算法逐步专注于感兴趣的区域周围的区域注册。结果：使用平均相似性指数为指标，所提出的算法（平均$ \下午$ STD：$ 0.84 \下午0.11 $），接着是精细对准算法（$ 0.86 \下午0.08 $）优于国家的最先进的线性整个组织的注册算法（$ 0.74 \ 0.19下午$），该算法（$ 0.81 \ 0.15下午$）的区域版本。该算法也优于状态的最先进的非线性配准算法为整个幻灯片图像（原：$ 0.77 \下午0.22 $：$ 0.82 \下午0.12 $，区域）和最近提出基于块拼贴的登记算法（贴片尺寸256 ：$ 0.79 \ 0.16下午$，补丁大小为512：$ 0.77 \ 0.16下午$）医学图像。可用性：C ++实现代码是在GitHub的库可在线：此HTTPS URL</font>
</div>


<hr>
<div id="paper41"> <b>41. Class-Specific Blind Deconvolutional Phase Retrieval Under a Generative  Prior</b>  <a href="https://arxiv.org/pdf/2002.12578" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Shamshad%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fahad Shamshad</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ahmed%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Ahmed</a><br>
<font size="3">
Abstract: In this paper, we consider the highly ill-posed problem of jointly recovering two real-valued signals from the phaseless measurements of their circular convolution. The problem arises in various imaging modalities such as Fourier ptychography, X-ray crystallography, and in visible light communication. We propose to solve this inverse problem using alternating gradient descent algorithm under two pretrained deep generative networks as priors; one is trained on sharp images and the other on blur kernels. The proposed recovery algorithm strives to find a sharp image and a blur kernel in the range of the respective pre-generators that \textit{best} explain the forward measurement model. In doing so, we are able to reconstruct quality image estimates. Moreover, the numerics show that the proposed approach performs well on the challenging measurement models that reflect the physically realizable imaging systems and is also robust to noise </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们考虑的共同恢复从他们的循环卷积的无相位测量两个实值信号的高度病态问题。该问题出现在各种成像模态，诸如傅立叶ptychography，X射线晶体学，和在可见光通信。我们建议使用交流梯度下降算法下的两个预训练的深生成网络作为先验来解决这个反问题;一个是在清晰的图像，而另一个上模糊内核训练。所提出的恢复算法努力找到一个清晰的图像，并在各自的预生成器，\ textit {最好}解释的前向测量模型的范围内的模糊核。在此过程中，我们能够重建图像质量的估计。此外，NUMERICS表明，所提出的方法以及执行上反映物理上可实现的成像系统和具有挑战性的计量模型也是鲁棒的噪声</font>
</div>


<hr>
<div id="paper42"> <b>42. RSANet: Recurrent Slice-wise Attention Network for Multiple Sclerosis  Lesion Segmentation</b>  <a href="https://arxiv.org/pdf/2002.12470" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinwei Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qihao Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kim%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeremy Kim</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shun Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Gauthier%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Susan A. Gauthier</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Spincemaille%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pascal Spincemaille</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Nguyen%2C+T+D" target="_blank" rel="noopener" style="color:#0000EE;">Thanh D. Nguyen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sabuncu%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Mert R. Sabuncu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Wang</a><br>
<font size="3">
Abstract: Brain lesion volume measured on T2 weighted MRI images is a clinically important disease marker in multiple sclerosis (MS). Manual delineation of MS lesions is a time-consuming and highly operator-dependent task, which is influenced by lesion size, shape and conspicuity. Recently, automated lesion segmentation algorithms based on deep neural networks have been developed with promising results. In this paper, we propose a novel recurrent slice-wise attention network (RSANet), which models 3D MRI images as sequences of slices and captures long-range dependencies through a recurrent manner to utilize contextual information of MS lesions. Experiments on a dataset with 43 patients show that the proposed method outperforms the state-of-the-art approaches. Our implementation is available online at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：T2加权的MRI图像上测量的脑损伤体积在多发性硬化症（MS）的临床上重要的疾病标志物。 MS病变的手动圈定是一个耗时且高度依赖于操作者的任务，这是由损伤尺寸，形状和醒目的影响。最近，基于深层神经网络自动的肿瘤分割算法已经开发了可喜的成果。在本文中，我们提出了一种新颖的复发逐个切片注意网络（RSANet），该模型的三维MRI图像作为切片和捕获远距离依赖性序列通过反复地利用MS病变的上下文信息。与43个例的数据集实验结果表明，所提出的方法优于状态的最先进的方法。我们的实施可在网上该HTTPS URL。</font>
</div>


<hr>
<div id="paper43"> <b>43. LEEP: A New Measure to Evaluate Transferability of Learned  Representations</b>  <a href="https://arxiv.org/pdf/2002.12462" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+C+V" target="_blank" rel="noopener" style="color:#0000EE;">Cuong V. Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hassner%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tal Hassner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Archambeau%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cedric Archambeau</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Seeger%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Seeger</a><br>
<font size="3">
Abstract: We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们推出了新的措施，以评估通过分类学表示的转让。我们的措施，预计登录经验预测（LEEP），简单且易于计算：给定的训练了源数据集的分类时，只需要通过该分类一旦运行目标数据集。从理论上分析LEEP的性质和经验证明其有效性。我们的分析表明，LEEP可以预测的两种传输和元转让学习方法的性能和收敛速度，即使是小型或不均衡数据。此外，LEEP性能优于最近提出转让的措施，如消极条件熵和H得分。值得注意的是，从ImageNet转移到CIFAR100时，LEEP可以实现高达30％的改进相比，在与实际的转印精度的相关性方面的最佳方法竞争。</font>
</div>


<hr>
<div id="paper44"> <b>44. Is the Meta-Learning Idea Able to Improve the Generalization of Deep  Neural Networks on the Standard Supervised Learning?</b>  <a href="https://arxiv.org/pdf/2002.12455" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongfei Zhang</a><br>
<font size="3">
Abstract: Substantial efforts have been made on improving the generalization abilities of deep neural networks (DNNs) in order to obtain better performances without introducing more parameters. On the other hand, meta-learning approaches exhibit powerful generalization on new tasks in few-shot learning. Intuitively, few-shot learning is more challenging than the standard supervised learning as each target class only has a very few or no training samples. The natural question that arises is whether the meta-learning idea can be used for improving the generalization of DNNs on the standard supervised learning. In this paper, we propose a novel meta-learning based training procedure (MLTP) for DNNs and demonstrate that the meta-learning idea can indeed improve the generalization abilities of DNNs. MLTP simulates the meta-training process by considering a batch of training samples as a task. The key idea is that the gradient descent step for improving the current task performance should also improve a new task performance, which is ignored by the current standard procedure for training neural networks. MLTP also benefits from all the existing training techniques such as dropout, weight decay, and batch normalization. We evaluate MLTP by training a variety of small and large neural networks on three benchmark datasets, i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet. The experimental results show a consistently improved generalization performance on all the DNNs with different sizes, which verifies the promise of MLTP and demonstrates that the meta-learning idea is indeed able to improve the generalization of DNNs on the standard supervised learning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大量已作出努力上，以获得更好的性能，而不会引入更多的参数提高深层神经网络（DNNs）的泛化能力。在另一方面，元学习方法表现在几个次学习新任务提供了强大的概括。直观地说，很少拍学习超过标准监督学习困难，因为每个目标类仅有极少数或没有训练样本。这产生自然的问题是元学习的想法是否可以用于提高对标准的监督学习DNNs的推广。在本文中，我们提出了DNNs一种新颖元学习基础的培训过程（MLTP），并证明了元学习的想法的确可以提高DNNs的泛化能力。 MLTP模拟考虑了一批训练样本的任务元的培训过程。其核心思想是，对于提高当前任务的性能梯度下降步骤也应提高新任务的性能，这是由当前的标准程序训练神经网络忽略。 MLTP也从所有现有的训练技术，如辍学，重腐烂，批标准化的好处。我们通过对三个标准数据集，即CIFAR-10，CIFAR-100和微型ImageNet培养了各种大大小小的神经网络的评价MLTP。实验结果表明，不同尺寸的所有DNNs一个不断改善的泛化性能，从而验证MLTP的承诺，并证明了元学习理念确实能够提高DNNs的泛化标准监督学习。</font>
</div>


<hr>
<div id="paper45"> <b>45. Provable Robust Learning Based on Transformation-Specific Smoothing</b>  <a href="https://arxiv.org/pdf/2002.12398" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title45" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linyi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weber%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maurice Weber</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojun Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rimanic%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luka Rimanic</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ce Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Li</a><br>
<font size="3">
Abstract: As machine learning systems become pervasive, safeguarding their security is critical. Recent work has demonstrated that motivated adversaries could manipulate the test data to mislead ML systems to make arbitrary mistakes. So far, most research has focused on providing provable robustness guarantees for a specific $\ell_p$ norm bounded adversarial perturbation. However, in practice there are more adversarial transformations that are realistic and of semantic meaning, requiring to be analyzed and ideally certified. In this paper we aim to provide {\em a unified framework for certifying ML model robustness against general adversarial transformations}. First, we leverage the function smoothing strategy to certify robustness against a series of adversarial transformations such as rotation, translation, Gaussian blur, etc. We then provide sufficient conditions and strategies for certifying certain transformations. For instance, we propose a novel sampling based interpolation approach with the estimated Lipschitz upper bound to certify the robustness against rotation transformation. In addition, we theoretically optimize the smoothing strategies for certifying the robustness of ML models against different transformations. For instance, we show that smoothing by sampling from exponential distribution provides tighter robustness bound than Gaussian. We also prove two generalization gaps for the proposed framework to understand its theoretic barrier. Extensive experiments show that our proposed unified framework significantly outperforms the state-of-the-art certified robustness approaches on several datasets including ImageNet. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着机器学习系统变得热闹起来，维护他们的安全是至关重要的。最近的研究表明，积极的敌人可以操纵的测试数据来误导ML系统进行任意的错误。到目前为止，大多数研究都集中于提供可证明的鲁棒性担保特定$ \ $ ell_p范数有界对抗扰动。然而，在实践中还有更具对抗性的转换是现实的和语义，需要加以分析和理想的认证。在本文中，我们的目标是为{证明对一般的对抗转变ML模型的鲁棒性\他们一个统一的框架}提供。首先，我们利用函数平滑策略，以证明可以有效抵抗一系列对抗性变换，如旋转，平移，高斯模糊等。然后我们提供相关培训转化的充分条件和策略。举例来说，我们提出与李氏估计一个新的基于采样插补方法上必然要证明对旋转变换的鲁棒性。此外，我们从理论上优化平滑策略，证明针对不同的变换ML车型的稳健性。举例来说，我们表明，从指数分布抽样平滑提供势必比高斯更严格的鲁棒性。我们还证明了2个泛化空白，为拟议的框架，以了解它的理论障碍。大量的实验表明，我们提出的统一框架显著优于国家的最先进的认证办法的鲁棒性的几个数据集，包括ImageNet。</font>
</div>


<hr>
<div id="paper46"> <b>46. NeurIPS 2019 Disentanglement Challenge: Improved Disentanglement through  Learned Aggregation of Convolutional Feature Maps</b>  <a href="https://arxiv.org/pdf/2002.12356" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title46" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Seitzer%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maximilian Seitzer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Foltyn%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andreas Foltyn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kemeth%2C+F+P" target="_blank" rel="noopener" style="color:#0000EE;">Felix P. Kemeth</a><br>
<font size="3">
Abstract: This report to our stage 2 submission to the NeurIPS 2019 disentanglement challenge presents a simple image preprocessing method for learning disentangled latent factors. We propose to train a variational autoencoder on regionally aggregated feature maps obtained from networks pretrained on the ImageNet database, utilizing the implicit inductive bias contained in those features for disentanglement. This bias can be further enhanced by explicitly fine-tuning the feature maps on auxiliary tasks useful for the challenge, such as angle, position estimation, or color classification. Our approach achieved the 2nd place in stage 2 of the challenge. Code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：该报告对我们的第2阶段提交NeurIPS 2019解开挑战礼物学习解开潜在因素，一个简单的图像预处理方法。我们建议对训练区域聚集特征的变的自动编码映射从预训练的ImageNet数据库在网络上获得的，利用载于解开这些特征的隐含归纳偏置。该偏置可以通过显式微调的特征上的挑战有用，如角度，位置估计，或色分类辅助任务，映射来进一步增强。我们的方法在挑战的阶段2取得了第二名。代码可在此HTTPS URL。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-03-02</title>
    <url>/2020/03/02/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-03-02/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Do all Roads Lead to Rome? Understanding the Role of Initialization in  Iterative Back-Translation <a href="https://arxiv.org/pdf/2002.12867" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Metaphoric Paraphrase Generation <a href="https://arxiv.org/pdf/2002.12854" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> UniLMv2: Pseudo-Masked Language Models for Unified Language Model  Pre-Training <a href="https://arxiv.org/pdf/2002.12804" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Automatic Section Recognition in Obituaries <a href="https://arxiv.org/pdf/2002.12699" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Comparison of Speech Representations for Automatic Quality Estimation in  Multi-Speaker Text-to-Speech Synthesis <a href="https://arxiv.org/pdf/2002.12645" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural  Language Processing <a href="https://arxiv.org/pdf/2002.12620" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> DC-BERT: Decoupling Question and Document for Efficient Contextual  Encoding <a href="https://arxiv.org/pdf/2002.12591" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Modeling Future Cost for Neural Machine Translation <a href="https://arxiv.org/pdf/2002.12558" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Robust Unsupervised Neural Machine Translation with Adversarial Training <a href="https://arxiv.org/pdf/2002.12549" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> UKARA 1.0 Challenge Track 1: Automatic Short-Answer Scoring in Bahasa  Indonesia <a href="https://arxiv.org/pdf/2002.12540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Temporal Convolutional Attention-based Network For Sequence Modeling <a href="https://arxiv.org/pdf/2002.12530" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Optimizing Memory-Access Patterns for Deep Learning Accelerators <a href="https://arxiv.org/pdf/2002.12798" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> RP-DNN: A Tweet level propagation context based deep neural networks for  early rumor detection in Social Media <a href="https://arxiv.org/pdf/2002.12683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> A multi-layer approach to disinformation detection on Twitter <a href="https://arxiv.org/pdf/2002.12612" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Exploring and Distilling Cross-Modal Information for Image Captioning <a href="https://arxiv.org/pdf/2002.12585" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Learning Directly from Grammar Compressed Text <a href="https://arxiv.org/pdf/2002.12570" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Comment Ranking Diversification in Forum Discussions <a href="https://arxiv.org/pdf/2002.12457" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Do all Roads Lead to Rome? Understanding the Role of Initialization in  Iterative Back-Translation</b>  <a href="https://arxiv.org/pdf/2002.12867" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Artetxe%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mikel Artetxe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Labaka%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gorka Labaka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Casas%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Noe Casas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agirre%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eneko Agirre</a><br>
<font size="3">
Abstract: Back-translation provides a simple yet effective approach to exploit monolingual corpora in Neural Machine Translation (NMT). Its iterative variant, where two opposite NMT models are jointly trained by alternately using a synthetic parallel corpus generated by the reverse model, plays a central role in unsupervised machine translation. In order to start producing sound translations and provide a meaningful training signal to each other, existing approaches rely on either a separate machine translation system to warm up the iterative procedure, or some form of pre-training to initialize the weights of the model. In this paper, we analyze the role that such initialization plays in iterative back-translation. Is the behavior of the final system heavily dependent on it? Or does iterative back-translation converge to a similar solution given any reasonable initialization? Through a series of empirical experiments over a diverse set of warmup systems, we show that, although the quality of the initial system does affect final performance, its effect is relatively small, as iterative back-translation has a strong tendency to convergence to a similar solution. As such, the margin of improvement left for the initialization method is narrow, suggesting that future research should focus more on improving the iterative mechanism itself. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：回译提供了一个简单而有效的方法来利用单语语料库在神经机器翻译（NMT）。其迭代变型，其中两个相对的NMT模型共同通过交替地使用由反向模型产生的合成平行语料库的训练，起着无监督机器翻译中心作用。为了开始产生声音翻译和彼此提供有意义的训练信号，现有的方法依赖任一单独的机器翻译系统上预热迭代过程，或某种形式的预训练初始化模型的权重。在本文中，我们分析的作用，在反复的回译这样的初始化播放。在最终系统的行为很大程度上依赖于它？抑或反复回译收敛于给定的任何合理的初始化类似的解决方案？通过一系列在一组不同的预热系统的实证实验，我们发现，虽然初始系统的质量不会影响最终的性能，它的影响相对较小，因为反复的回译强烈倾向于收敛到类似解。因此，改善留给初始化方法的范围窄，表明将来的研究应该更加注重提高迭代机制本身。</font>
</div>


<hr>
<div id="paper2"> <b>2. Metaphoric Paraphrase Generation</b>  <a href="https://arxiv.org/pdf/2002.12854" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Stowe%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Stowe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ribeiro%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leonardo Ribeiro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Iryna Gurevych</a><br>
<font size="3">
Abstract: This work describes the task of metaphoric paraphrase generation, in which we are given a literal sentence and are charged with generating a metaphoric paraphrase. We propose two different models for this task: a lexical replacement baseline and a novel sequence to sequence model, 'metaphor masking', that generates free metaphoric paraphrases. We use crowdsourcing to evaluate our results, as well as developing an automatic metric for evaluating metaphoric paraphrases. We show that while the lexical replacement baseline is capable of producing accurate paraphrases, they often lack metaphoricity, while our metaphor masking model excels in generating metaphoric sentences while performing nearly as well with regard to fluency and paraphrase quality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：该作品描述了隐喻的释义产生的，任务中，我们给出了一个文字句子，并负责生成一个隐喻的释义。我们提出了两种不同的模式完成这个任务：一个词汇替换基线和新的序列序列模型，“比喻掩蔽”，能产生游离隐喻的释义。我们利用众包来评估我们的成果，以及开发用于评估隐喻的释义自动度量。我们发现，虽然词汇替换基线是能够产生准确的释义中，他们往往缺乏隐喻性，而我们的隐喻产生隐喻的句子，而对于流畅性和复述质量进行近也掩盖模型过人之处。</font>
</div>


<hr>
<div id="paper3"> <b>3. UniLMv2: Pseudo-Masked Language Models for Unified Language Model  Pre-Training</b>  <a href="https://arxiv.org/pdf/2002.12804" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hangbo Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenhui Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaodong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Piao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Songhao Piao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hon%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hsiao-Wuen Hon</a><br>
<font size="3">
Abstract: We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议预先训练一个统一的语言模型两个autoencoding和使用一种新的训练过程部分自回归语言建模任务，被称为伪掩盖语言模型（PMLM）。给定一个输入文本与屏蔽令牌，我们依靠传统的口罩通过autoencoding学习损坏的标记和上下文之间的相互关系，以及伪面具通过部分自回归模型学习掩盖跨度之间的内部关系。精心设计的嵌入位置和自我关注口罩，上下文编码被重新使用，以避免重复计算。此外，用于autoencoding传统面具提供全局隐蔽的信息，以使所有位置的嵌入是部分自回归语言模型访问。此外，这两个任务前训练一个统一的语言模型为双向编码器和序列对序列解码器，分别。我们的实验表明，统一的语言模型预先训练使用PMLM实现跨越几个广泛使用的基准测试就广泛的自然语言理解和生成任务新的国家的最先进的成果。</font>
</div>


<hr>
<div id="paper4"> <b>4. Automatic Section Recognition in Obituaries</b>  <a href="https://arxiv.org/pdf/2002.12699" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabbatino%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Valentino Sabbatino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bostan%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laura Bostan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Klinger%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roman Klinger</a><br>
<font size="3">
Abstract: Obituaries contain information about people's values across times and cultures, which makes them a useful resource for exploring cultural history. They are typically structured similarly, with sections corresponding to Personal Information, Biographical Sketch, Characteristics, Family, Gratitude, Tribute, Funeral Information and Other aspects of the person. To make this information available for further studies, we propose a statistical model which recognizes these sections. To achieve that, we collect a corpus of 20058 English obituaries from TheDaily Item, this http URL and The London Free Press. The evaluation of our annotation guidelines with three annotators on 1008 obituaries shows a substantial agreement of Fleiss k = 0.87. Formulated as an automatic segmentation task, a convolutional neural network outperforms bag-of-words and embedding-based BiLSTMs and BiLSTM-CRFs with a micro F1 = 0.81. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：讣告包含关于人的跨越时代和文化价值，这使得它们为探索文化史上的一个有用的资源信息。它们通常结构相似，对应于个人信息，传记素描，特点，家庭，感恩，致敬，殡葬信息和人的其他方面的部分。为了使可用于进一步研究这些信息，我们提出一种识别这些部分的统计模型。为了实现这一目标，我们将收集的20058个英文讣告从TheDaily项目语料库，这个HTTP URL和伦敦出版自由。我们的注释准则对1008个讣告显示弗雷斯K = 0.87的实质性协议3个注释的评价。配制为自动分割任务，卷积神经网络性能优于袋的词和嵌入基于BiLSTMs和具有微F1 = 0.81 BiLSTM-的CRF。</font>
</div>


<hr>
<div id="paper5"> <b>5. Comparison of Speech Representations for Automatic Quality Estimation in  Multi-Speaker Text-to-Speech Synthesis</b>  <a href="https://arxiv.org/pdf/2002.12645" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jennifer Williams</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rownicka%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joanna Rownicka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oplustil%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pilar Oplustil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=King%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon King</a><br>
<font size="3">
Abstract: We aim to characterize how different speakers contribute to the perceived output quality of multi-speaker Text-to-Speech (TTS) synthesis. We automatically rate the quality of TTS using a neural network (NN) trained on human mean opinion score (MOS) ratings. First, we train and evaluate our NN model on 13 different TTS and voice conversion (VC) systems from the ASVSpoof 2019 Logical Access (LA) Dataset. Since it is not known how best to represent speech for this task, we compare 8 different representations alongside MOSNet frame-based features. Our representations include image-based spectrogram features and x-vector embeddings that explicitly model different types of noise such as T60 reverberation time. Our NN predicts MOS with a high correlation to human judgments. We report prediction correlation and error. A key finding is the quality achieved for certain speakers seems consistent, regardless of the TTS or VC system. It is widely accepted that some speakers give higher quality than others for building a TTS system: our method provides an automatic way to identify such speakers. Finally, to see if our quality prediction models generalize, we predict quality scores for synthetic speech using a separate multi-speaker TTS system that was trained on LibriTTS data, and conduct our own MOS listening test to compare human ratings with our NN predictions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们的目标是表征音箱如何不同的贡献多扬声器文本到语音转换（TTS）合成的感知输出质量。我们利用受过训练的人平均意见得分（MOS）评级神经网络（NN）自动评分TTS的质量。首先，我们训练和评估从ASVSpoof 2019逻辑访问（LA）数据集13个不同的TTS语音转换（VC）系统，我们的神经网络模型。因为它不知道如何最好地表示语音完成这个任务，我们比较靠MOSNet基于帧的功能8级不同的表示。我们表示包括基于图像的频谱特性和X-矢量的嵌入不同类型的噪音清晰的模型，如T60混响时间。我们NN预测MOS具有较高的相关性，以人的判断。我们报告预测的相关性和误差。一个关键发现是肯定的扬声器达到的质量似乎一致，无论TTS或VC系统。它已被广泛接受，一些发言者给予比其他人更高的质量为建设TTS系统：我们的方法提供了一种自动的方式来识别这些扬声器。最后，来看看我们的质量预测模型推广，我们预测使用已在LibriTTS数据训练一个独立的多扬声器系统，TTS合成语音质量得分，并进行我们自己的MOS听力测试对人体评级我们预测NN比较。</font>
</div>


<hr>
<div id="paper6"> <b>6. TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural  Language Processing</b>  <a href="https://arxiv.org/pdf/2002.12620" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqing Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiming Cui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhipeng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Che%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wanxiang Che</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Ting Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shijin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoping Hu</a><br>
<font size="3">
Abstract: In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing. It works with different neural network models and supports various kinds of tasks, such as text classification, reading comprehension, sequence labeling. TextBrewer provides a simple and uniform workflow that enables quick setup of distillation experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configuration, we achieve results that are comparable with or even higher than the state-of-the-art performance. Our toolkit is available through: this http URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们介绍TextBrewer，一个开放源代码的知识蒸馏工具箱专为自然语言处理。它的工作原理与不同的神经网络模型，并支持各种任务，如文本分类，阅读理解，序列标注。 TextBrewer提供了一种简单且均匀的工作流程，使具有高度灵活的配置的蒸馏实验快速设置。它提供了一组预定义的蒸馏方法，可以用自定义代码进行扩展。作为一个案例研究中，我们使用TextBrewer提炼BERT几个典型的NLP任务。通过简单的配置，就可以实现的结果与相当或高于国家的最先进的性能更高。我们的工具包可以通过：此http网址</font>
</div>


<hr>
<div id="paper7"> <b>7. DC-BERT: Decoupling Question and Document for Efficient Contextual  Encoding</b>  <a href="https://arxiv.org/pdf/2002.12591" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuyu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Ping Nie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiubo Geng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramamurthy%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arun Ramamurthy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Le Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daxin Jiang</a><br>
<font size="3">
Abstract: Recent studies on open-domain question answering have achieved prominent performance improvement using pre-trained language models such as BERT. State-of-the-art approaches typically follow the "retrieve and read" pipeline and employ BERT-based reranker to filter retrieved documents before feeding them into the reader module. The BERT retriever takes as input the concatenation of question and each retrieved document. Despite the success of these approaches in terms of QA accuracy, due to the concatenation, they can barely handle high-throughput of incoming questions each with a large collection of retrieved documents. To address the efficiency problem, we propose DC-BERT, a decoupled contextual encoding framework that has dual BERT models: an online BERT which encodes the question only once, and an offline BERT which pre-encodes all the documents and caches their encodings. On SQuAD Open and Natural Questions Open datasets, DC-BERT achieves 10x speedup on document retrieval, while retaining most (about 98%) of the QA performance compared to state-of-the-art approaches for open-domain question answering. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：开放域问答最近的研究使用预训练的语言模型来实现显着的性能改进，如BERT。国家的最先进的方法通常遵循“检索和阅读”管道和采用基于BERT-reranker进行筛选检索文档它们送入读卡模块之前。所述BERT检索作为输入问题的级联和每个检索的文档。尽管这些成功的QA准确性方面接近，由于级联，他们可以勉强应付高吞吐量的每一个大集合检索文档的传入的问题。为了解决效率问题，我们提出了DC-BERT，一个去耦上下文编码框架，它具有双重BERT模式：在线BERT编码的问题只有一次，和离线BERT预编码的所有文件和缓存的编码。在队内开阔自然的问题打开数据集，DC-BERT实现对文档检索10倍速度提升，同时相对于国家的最先进的开放域问答接近QA性能的保留大部分（约98％）。</font>
</div>


<hr>
<div id="paper8"> <b>8. Modeling Future Cost for Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.12558" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaoqun Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kehai Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masao Utiyama</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eiichiro Sumita</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Conghui Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tiejun Zhao</a><br>
<font size="3">
Abstract: Existing neural machine translation (NMT) systems utilize sequence-to-sequence neural networks to generate target translation word by word, and then make the generated word at each time-step and the counterpart in the references as consistent as possible. However, the trained translation model tends to focus on ensuring the accuracy of the generated target word at the current time-step and does not consider its future cost which means the expected cost of generating the subsequent target translation (i.e., the next target word). To respond to this issue, we propose a simple and effective method to model the future cost of each target word for NMT systems. In detail, a time-dependent future cost is estimated based on the current generated target word and its contextual information to boost the training of the NMT model. Furthermore, the learned future context representation at the current time-step is used to help the generation of the next target word in the decoding. Experimental results on three widely-used translation datasets, including the WMT14 German-to-English, WMT14 English-to-French, and WMT17 Chinese-to-English, show that the proposed approach achieves significant improvements over strong Transformer-based NMT baseline. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有神经机器翻译（NMT）系统利用序列到序列神经网络通过字，以生成目标翻译单词，然后使在每个时间步骤中的参考文献中所产生的字和对应尽可能一致。然而，训练的翻译模型往往把重点放在确保在当前时间步生成的目标词的准确性，也没有考虑，这意味着产生后续的目标翻译的预期成本的未来成本（即，下一个目标字） 。为了应对这个问题，我们提出了一个简单有效的方法，为NMT系统的每个目标词的未来成本模型。具体而言，与时间相关的未来费用是根据所产生的电流目标词和它的上下文信息来提振NMT模型的训练估计。此外，在当前时间步学习未来上下文表示是用来帮助解码中的下一个目标词的产生。三个广泛使用的翻译数据集，包括WMT14德国到英国，WMT14英语到法语，WMT17中国到英语，结果表明，该方法实现了强大的基于变压器的NMT基线显著改善实验结果。</font>
</div>


<hr>
<div id="paper9"> <b>9. Robust Unsupervised Neural Machine Translation with Adversarial Training</b>  <a href="https://arxiv.org/pdf/2002.12549" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haipeng Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kehai Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Utiyama%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masao Utiyama</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sumita%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eiichiro Sumita</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tiejun Zhao</a><br>
<font size="3">
Abstract: Unsupervised neural machine translation (UNMT) has recently attracted great interest in the machine translation community, achieving only slightly worse results than supervised neural machine translation. However, in real-world scenarios, there usually exists minor noise in the input sentence and the neural translation system is sensitive to the small perturbations in the input, leading to poor performance. In this paper, we first define two types of noises and empirically show the effect of these noisy data on UNMT performance. Moreover, we propose adversarial training methods to improve the robustness of UNMT in the noisy scenario. To the best of our knowledge, this paper is the first work to explore the robustness of UNMT. Experimental results on several language pairs show that our proposed methods substantially outperform conventional UNMT systems in the noisy scenario. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督神经机器翻译（UNMT）最近吸引了机器翻译界的极大兴趣，达到仅比监管神经机器翻译略差的结果。然而，在现实情况中，通常存在于输入句子轻微的噪声和神经翻译系统是在输入的小扰动敏感，导致业绩不佳。在本文中，我们首先定义两种类型的噪声和经验显示在UNMT性能这些噪声数据的影响。此外，我们提出了对抗性训练方法，提高UNMT的稳健性在喧嚣的场景。据我们所知，这是本文探讨UNMT的鲁棒性的第一部作品。在几个语言对实验结果表明，该方法显着优于在喧闹的场景传统UNMT系统。</font>
</div>


<hr>
<div id="paper10"> <b>10. UKARA 1.0 Challenge Track 1: Automatic Short-Answer Scoring in Bahasa  Indonesia</b>  <a href="https://arxiv.org/pdf/2002.12540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Septiandri%2C+A+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Akbar Septiandri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Winatmoko%2C+Y+A" target="_blank" rel="noopener" style="color:#0000EE;">Yosef Ardhito Winatmoko</a><br>
<font size="3">
Abstract: We describe our third-place solution to the UKARA 1.0 challenge on automated essay scoring. The task consists of a binary classification problem on two datasets | answers from two different questions. We ended up using two different models for the two datasets. For task A, we applied a random forest algorithm on features extracted using unigram with latent semantic analysis (LSA). On the other hand, for task B, we only used logistic regression on TF-IDF features. Our model results in F1 score of 0.812. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们描述了我们的第三位解决方案的自动化作文得分UKARA 1.0挑战。该任务由一个二元分类问题的两个数据集|从两个不同的问题的答案。我们结束了使用两种不同的模式在两个数据集。对于任务A，我们应用使用的单字组与潜在语义分析（LSA）提取的特征随机森林算法。在另一方面，任务B，我们只用在TF-IDF功能回归。我们在F1模型结果得分为0.812。</font>
</div>


<hr>
<div id="paper11"> <b>11. Temporal Convolutional Attention-based Network For Sequence Modeling</b>  <a href="https://arxiv.org/pdf/2002.12530" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongyan Hao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yudi Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furao Shen</a><br>
<font size="3">
Abstract: With the development of feed-forward models, the default model for sequence modeling has gradually evolved to replace recurrent networks. Many powerful feed-forward models based on convolutional networks and attention mechanism were proposed and show more potential to handle sequence modeling tasks. We wonder that is there an architecture that can not only achieve an approximate substitution of recurrent network, but also absorb the advantages of feed-forward models. So we propose an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts shallow layer's important information and transfers to deep layers. We improve the state-of-the-art results of bpc/perplexity to 26.92 on word-level PTB, 1.043 on character-level PTB, and 6.66 on WikiText-2. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着前馈机型的发展，为序列建模的默认模式已经逐渐演变为经常更换网络。基于卷积网络和注意机制的许多功能强大的前馈模型，提出并表现出更多的潜力来处理序列建模任务。我们不知道这是有一个架构，不仅可以实现循环网络的近似替代，但也吸收前馈机型的优点。因此，我们提出了一个试探性的架构称之为基于注意时态卷积网络（TCAN）相结合的时间卷积网络和注意机制。 TCAN包括两个部分，一个是颞注意（TA），其捕获的序列内的相关特征，另一种是增强的残留（ER），其提取浅层的重要信息，并转移到深层。我们提高wikitext的-2 BPC /困惑的上字级PTB，1.043的字符级PTB，和6.66的国家的最先进成果26.92。</font>
</div>


<hr>
<div id="paper12"> <b>12. Optimizing Memory-Access Patterns for Deep Learning Accelerators</b>  <a href="https://arxiv.org/pdf/2002.12798" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongbin Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sejong Oh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huiqing Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Briggs%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Preston Briggs</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gai%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiading Gai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Animesh Jain</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yizhi Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Heaton%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rich Heaton</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Randy Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yida Wang</a><br>
<font size="3">
Abstract: Deep learning (DL) workloads are moving towards accelerators for faster processing and lower cost. Modern DL accelerators are good at handling the large-scale multiply-accumulate operations that dominate DL workloads; however, it is challenging to make full use of the compute power of an accelerator since the data must be properly staged in a software-managed scratchpad memory. Failing to do so can result in significant performance loss. This paper proposes a systematic approach which leverages the polyhedral model to analyze all operators of a DL model together to minimize the number of memory accesses. Experiments show that our approach can substantially reduce the impact of memory accesses required by common neural-network models on a homegrown AWS machine-learning inference chip named Inferentia, which is available through Amazon EC2 Inf1 instances. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习（DL），工作负载加速迈向移动进行更快的处理和更低的成本。现代DL加速器是善于处理大型乘法累加运算主宰DL工作量;但是，它是具有挑战性的充分利用油门的计算能力，因为数据必须是正确的软件管理暂存器上演。如果不这样做可能会导致显著的性能损失。本文提出了一种利用多面体模型来分析DL模型在一起的所有运营商，以尽量减少内存访问次数的系统方法。实验结果表明，我们的方法可以大大降低内存存取影响，通过对自主开发的常见的神经网络模型所需的AWS命名Inferentia机器学习的推理片，这是可以通过Amazon EC2的INF1实例。</font>
</div>


<hr>
<div id="paper13"> <b>13. RP-DNN: A Tweet level propagation context based deep neural networks for  early rumor detection in Social Media</b>  <a href="https://arxiv.org/pdf/2002.12683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sooji Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xingyi Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ciravegna%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fabio Ciravegna</a><br>
<font size="3">
Abstract: Early rumor detection (ERD) on social media platform is very challenging when limited, incomplete and noisy information is available. Most of the existing methods have largely worked on event-level detection that requires the collection of posts relevant to a specific event and relied only on user-generated content. They are not appropriate to detect rumor sources in the very early stages, before an event unfolds and becomes widespread. In this paper, we address the task of ERD at the message level. We present a novel hybrid neural network architecture, which combines a task-specific character-based bidirectional language model and stacked Long Short-Term Memory (LSTM) networks to represent textual contents and social-temporal contexts of input source tweets, for modelling propagation patterns of rumors in the early stages of their development. We apply multi-layered attention models to jointly learn attentive context embeddings over multiple context inputs. Our experiments employ a stringent leave-one-out cross-validation (LOO-CV) evaluation setup on seven publicly available real-life rumor event data sets. Our models achieve state-of-the-art(SoA) performance for detecting unseen rumors on large augmented data which covers more than 12 events and 2,967 rumors. An ablation study is conducted to understand the relative contribution of each component of our proposed model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当有限的，不完整的社交媒体平台的早期传言检测（ERD）是非常具有挑战性和嘈杂的信息是可用的。大多数现有的方法大都工作在事件等级检测，需要相关的特定事件的帖子收集和只在用户生成内容的依据。他们并不适合检测非常早期的谣言来源，事件都呈现出前，成为普遍。在本文中，我们解决在消息级别的ERD的任务。我们提出了一个新颖的混合神经网络体系结构，它结合了一个基于字符任务专用双向语言模型和堆叠长短期存储器（LSTM）网络来表示文本内容和输入源鸣叫的社会时空上下文，用于建模传播模式在其发展的早期阶段的传言。我们采用多层关注车型，共同学习了多个方面的投入周到的背景下的嵌入。我们的实验采用了严格的留一交叉验证（LOO-CV）评估设置七个公开提供真实的谣言事件数据。我们的模型实现状态的最先进的（SOA）表现为在其上覆盖超过12个事件和2967个传言大增强数据检测看不见传言。消融研究以了解我们提出的模型中各组分的相对贡献。</font>
</div>


<hr>
<div id="paper14"> <b>14. A multi-layer approach to disinformation detection on Twitter</b>  <a href="https://arxiv.org/pdf/2002.12612" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pierri%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Francesco Pierri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Piccardi%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlo Piccardi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ceri%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefano Ceri</a><br>
<font size="3">
Abstract: We tackle the problem of classifying news articles pertaining to disinformation vs mainstream news by solely inspecting their diffusion mechanisms on Twitter. Our technique is inherently simple compared to existing text-based approaches, as it allows to by-pass the multiple levels of complexity which are found in news content (e.g. grammar, syntax, style). We employ a multi-layer representation of Twitter diffusion networks, and we compute for each layer a set of global network features which quantify different aspects of the sharing process. Experimental results with two large-scale datasets, corresponding to diffusion cascades of news shared respectively in the United States and Italy, show that a simple Logistic Regression model is able to classify disinformation vs mainstream networks with high accuracy (AUROC up to 94%), also when considering the political bias of different sources in the classification task. We also highlight differences in the sharing patterns of the two news domains which appear to be country-independent. We believe that our network-based approach provides useful insights which pave the way to the future development of a system to detect misleading and harmful information spreading on social media. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们通过处理仅仅检查在Twitter上的扩散机制有关造谣VS主流新闻新闻文章进行分类的问题。相比于现有的基于文本的方法我们的技术本质上是简单的，因为它允许绕过这些新闻内容中发现的复杂的多层次（如语法，句法，风格）。我们使用Twitter的扩散网络的多层表示，我们计算每个层的一套全球网络功能，其量化的共享过程的不同方面。有两个大型数据集的实验结果，对应的消息扩散级联在美国和意大利分别共享，表明一个简单的逻辑回归模型能够进行分类造谣VS高精度主流网络（AUROC高达94％），还考虑在分类任务不同来源的政治偏见的时候。我们还强调在这似乎是国家独立的两个新闻领域的交流模式的差异。我们相信，我们的基于网络的方法提供了铺平了道路系统的未来发展，以检测误导和有害信息在社会化媒体传播有益的见解。</font>
</div>


<hr>
<div id="paper15"> <b>15. Exploring and Distilling Cross-Modal Information for Image Captioning</b>  <a href="https://arxiv.org/pdf/2002.12585" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fenglin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuancheng Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanxin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Lei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xu Sun</a><br>
<font size="3">
Abstract: Recently, attention-based encoder-decoder models have been used extensively in image captioning. Yet there is still great difficulty for the current methods to achieve deep image understanding. In this work, we argue that such understanding requires visual attention to correlated image regions and semantic attention to coherent attributes of interest. To perform effective attention, we explore image captioning from a cross-modal perspective and propose the Global-and-Local Information Exploring-and-Distilling approach that explores and distills the source information in vision and language. It globally provides the aspect vector, a spatial and relational representation of images based on caption contexts, through the extraction of salient region groupings and attribute collocations, and locally extracts the fine-grained regions and attributes in reference to the aspect vector for word selection. Our fully-attentive model achieves a CIDEr score of 129.3 in offline COCO evaluation on the COCO testing set with remarkable efficiency in terms of accuracy, speed, and parameter budget. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，注意基于编码器的解码器模型已被广泛应用在影像字幕使用。然而，仍然有很大的难度当前方法来实现深图像理解。在这项工作中，我们认为，这样的理解，需要视觉注意相关的图像区域和语义重视利益一致的属性。要进行有效的关注，我们从跨模态的角度探讨图像字幕，并提出了全局和局部信息的探索和 - 蒸馏的方法，探索和提炼在视觉和语言的源信息。它全局提供方面向量，基于字幕上下文图像的空间和关系表示，通过显着区域分组和属性搭配的提取，并在本地提取细粒度区域和属性参考用于字选择纵横向量。我们全面周到的模型实现了129.3对的COCO测试集效率惊人离线COCO评价苹果酒得分在精度，速度和参数预算方面。</font>
</div>


<hr>
<div id="paper16"> <b>16. Learning Directly from Grammar Compressed Text</b>  <a href="https://arxiv.org/pdf/2002.12570" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/stat?searchtype=author&query=Sasaki%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoichi Sasaki</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Akimoto%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kosuke Akimoto</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Maehara%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takanori Maehara</a><br>
<font size="3">
Abstract: Neural networks using numerous text data have been successfully applied to a variety of tasks. While massive text data is usually compressed using techniques such as grammar compression, almost all of the previous machine learning methods assume already decompressed sequence data as their input. In this paper, we propose a method to directly apply neural sequence models to text data compressed with grammar compression algorithms without decompression. To encode the unique symbols that appear in compression rules, we introduce composer modules to incrementally encode the symbols into vector representations. Through experiments on real datasets, we empirically showed that the proposal model can achieve both memory and computational efficiency while maintaining moderate performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用大量的文字资料神经网络已经成功地应用于各种任务。虽然大量的文本数据使用的技术，如语法压缩通常被压缩，几乎所有的以前的机器学习方法假设已经解压缩序列数据作为其输入。在本文中，我们建议的神经序列模型直接适用于与语法的压缩算法压缩不减压的文本数据的方法。编码显示在压缩规则独特的符号，我们介绍作曲家模块的符号递增编码为向量表示。通过对真实数据集实验中，我们经验表明，建议模型可以实现内存和计算效率，同时保持适度的性能。</font>
</div>


<hr>
<div id="paper17"> <b>17. Comment Ranking Diversification in Forum Discussions</b>  <a href="https://arxiv.org/pdf/2002.12457" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Northcutt%2C+C+G" target="_blank" rel="noopener" style="color:#0000EE;">Curtis G. Northcutt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leon%2C+K+A" target="_blank" rel="noopener" style="color:#0000EE;">Kimberly A. Leon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naichun Chen</a><br>
<font size="3">
Abstract: Viewing consumption of discussion forums with hundreds or more comments depends on ranking because most users only view top-ranked comments. When comments are ranked by an ordered score (e.g. number of replies or up-votes) without adjusting for semantic similarity of near-ranked comments, top-ranked comments are more likely to emphasize the majority opinion and incur redundancy. In this paper, we propose a top K comment diversification re-ranking model using Maximal Marginal Relevance (MMR) and evaluate its impact in three categories: (1) semantic diversity, (2) inclusion of the semantics of lower-ranked comments, and (3) redundancy, within the context of a HarvardX course discussion forum. We conducted a double-blind, small-scale evaluation experiment requiring subjects to select between the top 5 comments of a diversified ranking and a baseline ranking ordered by score. For three subjects, across 100 trials, subjects selected the diversified (75% score, 25% diversification) ranking as significantly (1) more diverse, (2) more inclusive, and (3) less redundant. Within each category, inter-rater reliability showed moderate consistency, with typical Cohen-Kappa scores near 0.2. Our findings suggest that our model improves (1) diversification, (2) inclusion, and (3) redundancy, among top K ranked comments in online discussion forums. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：查看论坛的消耗与数百个或多个注释取决于排名，因为大多数用户只能查看排名第一的意见。当评论通过一个有序的得分没有调整的近位列评论语义相似的排名（如回复或向上的票数），排名第一的评论更可能强调多数人的意见，并招致冗余。在本文中，我们提出了用最大边缘相关（MMR）一前K评论多样化重排序模型，并评估其在三类影响：（1）语义的多样性，（2）列入排名较低的意见语义，并（3）冗余，一个HarvardX当然讨论论坛的范围内。我们进行了一项双盲，小规模的评价实验，要求受试者顶部的5条评论多样化的排名和基准之间进行选择按分数排名排序。对于三个科，跨越100次试验，选择受试者多样化（75％的分数，25％多样化）作为显著（1）排名更加多样化，（2）更包容，和（3）较少的冗余。在每个类别中，评估者间可靠性显示中度的一致性，用近0.2典型科恩-卡帕分数。我们的研究结果表明，我们的模型提高了（1）多元化，（2）包容，和（3）冗余，跻身K的在线论坛排名意见。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-28</title>
    <url>/2020/02/28/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-28/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Measures to Evaluate Generative Adversarial Networks Based on Direct  Analysis of Generated Images <a href="https://arxiv.org/pdf/2002.12345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Visual Camera Re-Localization from RGB and RGB-D Images Using DSAC <a href="https://arxiv.org/pdf/2002.12324" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Semantically-Guided Representation Learning for Self-Supervised  Monocular Depth <a href="https://arxiv.org/pdf/2002.12319" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> 2D Convolutional Neural Networks for 3D Digital Breast Tomosynthesis  Classification <a href="https://arxiv.org/pdf/2002.12314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Blurry Video Frame Interpolation <a href="https://arxiv.org/pdf/2002.12259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> The Mertens Unrolled Network (MU-Net): A High Dynamic Range Fusion  Neural Network for Through the Windshield Driver Recognition <a href="https://arxiv.org/pdf/2002.12257" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> ZoomCount: A Zooming Mechanism for Crowd Counting in Static Images <a href="https://arxiv.org/pdf/2002.12256" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Learning Representations by Predicting Bags of Visual Words <a href="https://arxiv.org/pdf/2002.12247" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Meta-Transfer Learning for Zero-Shot Super-Resolution <a href="https://arxiv.org/pdf/2002.12213" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Total3DUnderstanding: Joint Layout, Object Pose and Mesh Reconstruction  for Indoor Scenes from a Single Image <a href="https://arxiv.org/pdf/2002.12212" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Visual Commonsense R-CNN <a href="https://arxiv.org/pdf/2002.12204" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> University-1652: A Multi-view Multi-source Benchmark for Drone-based  Geo-localization <a href="https://arxiv.org/pdf/2002.12186" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Evolving Losses for Unsupervised Video Representation Learning <a href="https://arxiv.org/pdf/2002.12177" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Domain Decluttering: Simplifying Images to Mitigate Synthetic-Real  Domain Shift and Improve Depth Estimation <a href="https://arxiv.org/pdf/2002.12114" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Deep Slow Motion Video Reconstruction with Hybrid Imaging System <a href="https://arxiv.org/pdf/2002.12106" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> The Data Representativeness Criterion: Predicting the Performance of  Supervised Classification Based on Data Set Similarity <a href="https://arxiv.org/pdf/2002.12105" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Action Quality Assessment using Siamese Network-Based Deep Metric  Learning <a href="https://arxiv.org/pdf/2002.12096" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Reducing Geographic Performance Differential for Face Recognition <a href="https://arxiv.org/pdf/2002.12093" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> XSepConv: Extremely Separated Convolution <a href="https://arxiv.org/pdf/2002.12046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Attention-guided Chained Context Aggregation for Semantic Segmentation <a href="https://arxiv.org/pdf/2002.12041" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Multiple Discrimination and Pairwise CNN for View-based 3D Object  Retrieval <a href="https://arxiv.org/pdf/2002.11977" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Unbiased Scene Graph Generation from Biased Training <a href="https://arxiv.org/pdf/2002.11949" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Features for Ground Texture Based Localization -- A Survey <a href="https://arxiv.org/pdf/2002.11948" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Weakly supervised discriminative feature learning with state information  for person identification <a href="https://arxiv.org/pdf/2002.11939" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Auto-Encoding Twin-Bottleneck Hashing <a href="https://arxiv.org/pdf/2002.11930" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural  Network for Human Trajectory Prediction <a href="https://arxiv.org/pdf/2002.11927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Set-Constrained Viterbi for Set-Supervised Action Segmentation <a href="https://arxiv.org/pdf/2002.11925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference <a href="https://arxiv.org/pdf/2002.11921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Unshuffling Data for Improved Generalization <a href="https://arxiv.org/pdf/2002.11894" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Hierarchical Memory Decoding for Video Captioning <a href="https://arxiv.org/pdf/2002.11886" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Defense-PointNet: Protecting PointNet Against Adversarial Attacks <a href="https://arxiv.org/pdf/2002.11881" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> GATCluster: Self-Supervised Gaussian-Attention Network for Image  Clustering <a href="https://arxiv.org/pdf/2002.11863" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Towards Universal Representation Learning for Deep Face Recognition <a href="https://arxiv.org/pdf/2002.11841" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Joint Unsupervised Learning of Optical Flow and Egomotion with Bi-Level  Optimization <a href="https://arxiv.org/pdf/2002.11826" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Learning to Shade Hand-drawn Sketches <a href="https://arxiv.org/pdf/2002.11812" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> On Leveraging Pretrained GANs for Limited-Data Generation <a href="https://arxiv.org/pdf/2002.11810" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Rethinking the Hyperparameters for Fine-tuning <a href="https://arxiv.org/pdf/2002.11770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Hallucinative Topological Memory for Zero-Shot Visual Planning <a href="https://arxiv.org/pdf/2002.12336" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Coronary Wall Segmentation in CCTA Scans via a Hybrid Net with Contours  Regularization <a href="https://arxiv.org/pdf/2002.12263" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Opportunities of a Machine Learning-based Decision Support System for  Stroke Rehabilitation Assessment <a href="https://arxiv.org/pdf/2002.12261" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Optimization of Graph Total Variation via Active-Set-based Combinatorial  Reconditioning <a href="https://arxiv.org/pdf/2002.12236" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Multi-source Domain Adaptation in the Deep Learning Era: A Systematic  Survey <a href="https://arxiv.org/pdf/2002.12169" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Infinitely Wide Graph Convolutional Networks: Semi-supervised Learning  via Gaussian Processes <a href="https://arxiv.org/pdf/2002.12168" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> A Comprehensive Approach to Unsupervised Embedding Learning based on AND  Algorithm <a href="https://arxiv.org/pdf/2002.12158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Multi-Cycle-Consistent Adversarial Networks for CT Image Denoising <a href="https://arxiv.org/pdf/2002.12130" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> Two-stage breast mass detection and segmentation system towards  automated high-resolution full mammogram analysis <a href="https://arxiv.org/pdf/2002.12079" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> Understanding and Enhancing Mixed Sample Data Augmentation <a href="https://arxiv.org/pdf/2002.12047" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> Transductive Few-shot Learning with Meta-Learned Confidence <a href="https://arxiv.org/pdf/2002.12017" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Face Verification Using 60~GHz 802.11 waveforms <a href="https://arxiv.org/pdf/2002.11965" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> Supervised Dimensionality Reduction and Visualization using  Centroid-encoder <a href="https://arxiv.org/pdf/2002.11934" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> Segmentation-based Method combined with Dynamic Programming for Brain  Midline Delineation <a href="https://arxiv.org/pdf/2002.11918" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> Max-Affine Spline Insights into Deep Generative Networks <a href="https://arxiv.org/pdf/2002.11912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> A Proto-Object Based Dynamic Visual Saliency Model with an FPGA  Implementation <a href="https://arxiv.org/pdf/2002.11898" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> Gradient Boosted Flows <a href="https://arxiv.org/pdf/2002.11896" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> BBAND Index: A No-Reference Banding Artifact Predictor <a href="https://arxiv.org/pdf/2002.11891" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> Kernel Bi-Linear Modeling for Reconstructing Data on Manifolds: The  Dynamic-MRI Case <a href="https://arxiv.org/pdf/2002.11885" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Comparison of Multi-Class and Binary Classification Machine Learning  Models in Identifying Strong Gravitational Lenses <a href="https://arxiv.org/pdf/2002.11849" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
<div id="title58">
<b>58.</b> Analysis of diversity-accuracy tradeoff in image captioning <a href="https://arxiv.org/pdf/2002.11848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper58" style="color:#0000EE;">摘要</a><br></div>
<div id="title59">
<b>59.</b> Improving Robustness of Deep-Learning-Based Image Reconstruction <a href="https://arxiv.org/pdf/2002.11821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper59" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Measures to Evaluate Generative Adversarial Networks Based on Direct  Analysis of Generated Images</b>  <a href="https://arxiv.org/pdf/2002.12345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuyue Guan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Loew%2C+M+H" target="_blank" rel="noopener" style="color:#0000EE;">Murray H. Loew</a><br>
<font size="3">
Abstract: The Generative Adversarial Network (GAN) is a state-of-the-art technique in the field of deep learning. A number of recent papers address the theory and applications of GANs in various fields of image processing. Fewer studies, however, have directly evaluated GAN outputs. Those that have been conducted focused on using classification performance (e.g., Inception Score) and statistical metrics (e.g., Fréchet Inception Distance). , Here, we consider a fundamental way to evaluate GANs by directly analyzing the images they generate, instead of using them as inputs to other classifiers. We characterize the performance of a GAN as an image generator according to three aspects: 1) Creativity: non-duplication of the real images. 2) Inheritance: generated images should have the same style, which retains key features of the real images. 3) Diversity: generated images are different from each other. A GAN should not generate a few different images repeatedly. Based on the three aspects of ideal GANs, we have designed two measures: Creativity-Inheritance-Diversity (CID) index and Likeness Score (LS) to evaluate GAN performance, and have applied them to evaluate three typical GANs. We compared our proposed measures with three commonly used GAN evaluation methods: Inception Score (IS), Fréchet Inception Distance (FID) and 1-Nearest Neighbor classifier (1NNC). In addition, we discuss how these evaluations could help us deepen our understanding of GANs and improve their performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：剖成对抗性网络（GAN）是国家的最先进的技术中深学习的领域。最近的一些论文解决的理论和图像处理的各个领域甘斯的应用。研究较少，但是，已经直接评估甘输出。那些已被进行了集中在使用分类的性能（例如，启得分）和统计指标（例如，Fréchet可启距离）。 ，在这里，我们考虑通过直接分析其产生的图像，而不是利用他们作为输入到其他分类评价甘斯的根本途径。 1）创意：实像的非重复我们按照三个方面表征GAN中表现为图像发生器。 2）继承：生成的图像应具有相同的风格，保留了真实图像的主要特征。 3）分集：生成的图像彼此不同。阿甘不应该重复产生一些不同的图像。基于理想甘斯的三个方面，我们设计了两项措施：创造性继承，多样性（CID）指数和相像度得分（LS），以评估GAN性能，并已申请他们评估三个典型甘斯。我们比较了我们提出的措施有三个常用GAN评价方法：盗梦空间得分（IS），Fréchet可盗梦空间距离（FID）和1近邻分类（1NNC）。此外，我们将讨论如何这些评价可以帮助我们加深对甘斯的认识，提高其性能。</font>
</div>


<hr>
<div id="paper2"> <b>2. Visual Camera Re-Localization from RGB and RGB-D Images Using DSAC</b>  <a href="https://arxiv.org/pdf/2002.12324" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Brachmann%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Brachmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rother%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carsten Rother</a><br>
<font size="3">
Abstract: We describe a learning-based system that estimates the camera position and orientation from a single input image relative to a known environment. The system is flexible w.r.t. the amount of information available at test and at training time, catering to different applications. Input images can be RGB-D or RGB, and a 3D model of the environment can be utilized for training but is not necessary. In the minimal case, our system requires only RGB images and ground truth poses at training time, and it requires only a single RGB image at test time. The framework consists of a deep neural network and fully differentiable pose optimization. The neural network predicts so called scene coordinates, i.e. dense correspondences between the input image and 3D scene space of the environment. The pose optimization implements robust fitting of pose parameters using differentiable RANSAC (DSAC) to facilitate end-to-end training. The system, an extension of DSAC++ and referred to as DSAC*, achieves state-of-the-art accuracy an various public datasets for RGB-based re-localization, and competitive accuracy for RGB-D based re-localization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们描述了一个基于学习的系统，估计从单一相对于已知环境中的输入图像的摄像机的位置和方向。该系统是灵活的w.r.t.信息可在测试和培训时间，迎合不同的应用程序的数量。输入图像可以是RGB-d或RGB，并且可以被用于训练的环境的3D模型，但不是必需的。在最小的情况下，我们的系统只需要RGB图像和地面实况姿势在训练时间，并在测试时只需要一个RGB图像。该框架包含了深刻的神经网络，完全区分的姿态优化。该神经网络预测所谓的场景坐标，输入图像和环境的3D场景空间之间即致密的对应关系。使用微RANSAC（DSAC）姿态参数的姿势优化工具强大的配件，方便终端到终端的培训。该系统中，DSAC ++的扩展并且被称为DSAC *，达到状态的最先进的精度的各种公共数据集为基于RGB的重新定位，和竞争精度RGB-d基于重新定位。</font>
</div>


<hr>
<div id="paper3"> <b>3. Semantically-Guided Representation Learning for Self-Supervised  Monocular Depth</b>  <a href="https://arxiv.org/pdf/2002.12319" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guizilini%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vitor Guizilini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ambrus%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rares Ambrus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gaidon%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrien Gaidon</a><br>
<font size="3">
Abstract: Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task approach, we propose a new architecture leveraging fixed pretrained semantic segmentation networks to guide self-supervised representation learning via pixel-adaptive convolutions. Furthermore, we propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Our method improves upon the state of the art for self-supervised monocular depth prediction over all pixels, fine-grained details, and per semantic categories. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自监督学习正显示出单眼深度估计巨大潜力，运用几何学作为监管的唯一来源。深度网络确实能够学习了涉及通过隐含利用类别层次的模式视觉外观的3D性能表示。在这项工作中，我们研究如何更直接地利用这个语义结构来引导几何表示学习，自我监督的政权剩下的一段时间。相反，使用语义标签，并在多任务方式代理的损失，我们提出了一个新的架构，利用固定预训练的语义分割网络，通过像素自适应的卷积引导自我监督表示学习。此外，我们提出了两个阶段的训练过程通过重新采样来克服动态对象的公共语义偏差。我们的方法改进了现有技术的用于自监督单眼深度预测在所有的像素，细粒度细节，状态和每个语义类别。</font>
</div>


<hr>
<div id="paper4"> <b>4. 2D Convolutional Neural Networks for 3D Digital Breast Tomosynthesis  Classification</b>  <a href="https://arxiv.org/pdf/2002.12314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoqin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Blanton%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hunter Blanton</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gongbo Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jacobs%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nathan Jacobs</a><br>
<font size="3">
Abstract: Automated methods for breast cancer detection have focused on 2D mammography and have largely ignored 3D digital breast tomosynthesis (DBT), which is frequently used in clinical practice. The two key challenges in developing automated methods for DBT classification are handling the variable number of slices and retaining slice-to-slice changes. We propose a novel deep 2D convolutional neural network (CNN) architecture for DBT classification that simultaneously overcomes both challenges. Our approach operates on the full volume, regardless of the number of slices, and allows the use of pre-trained 2D CNNs for feature extraction, which is important given the limited amount of annotated training data. In an extensive evaluation on a real-world clinical dataset, our approach achieves 0.854 auROC, which is 28.80% higher than approaches based on 3D CNNs. We also find that these improvements are stable across a range of model configurations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用于检测乳腺癌的自动化方法集中于2D乳房X射线摄影和在很大程度上忽略了3D数字乳房断层合成（DBT），这是经常在临床实践中使用。在制定DBT分类自动化方法的两个主要挑战是处理可变数目片和保持片对片变化。我们提出了一个新颖的深二维卷积神经网络（CNN）架构DBT分类，能同时克服了两个挑战。我们的方法全卷上运行，无论片的数量，并允许进行特征提取，给出注释的训练数据的量有限，这是非常重要的使用预训练的二维细胞神经网络的。在对现实世界的临床数据集进行广泛的评估，我们的方法达到0.854 AUROC，这比基于三维细胞神经网络方法提高28.80％。我们还发现，这些改进是通过一系列模型配置的稳定。</font>
</div>


<hr>
<div id="paper5"> <b>5. Blurry Video Frame Interpolation</b>  <a href="https://arxiv.org/pdf/2002.12259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wang Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenbo Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guangtao Zhai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Min%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiongkuo Min</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyong Gao</a><br>
<font size="3">
Abstract: Existing works reduce motion blur and up-convert frame rate through two separate ways, including frame deblurring and frame interpolation. However, few studies have approached the joint video enhancement problem, namely synthesizing high-frame-rate clear results from low-frame-rate blurry inputs. In this paper, we propose a blurry video frame interpolation method to reduce motion blur and up-convert frame rate simultaneously. Specifically, we develop a pyramid module to cyclically synthesize clear intermediate frames. The pyramid module features adjustable spatial receptive field and temporal scope, thus contributing to controllable computational complexity and restoration ability. Besides, we propose an inter-pyramid recurrent module to connect sequential models to exploit the temporal relationship. The pyramid module integrates a recurrent module, thus can iteratively synthesize temporally smooth results without significantly increasing the model size. Extensive experimental results demonstrate that our method performs favorably against state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有作品减少运动模糊和上变频帧率通过两个独立的方法，包括帧去模糊和帧插值。然而，很少有研究走近联合视频增强的问题，即从合成低帧率模糊输入的高帧率明确的结果。在本文中，我们提出了一个模糊的视频帧插值方法同时减少运动模糊和上变频帧率。具体而言，我们开发了一个金字塔模块周期性合成清晰的中间帧。金字塔模块设有可调的空间感受域和时间范围，从而促进可控的计算复杂度和恢复的能力。此外，我们提出了一个金字塔间反复模块连接顺序机型利用的时间关系。金字塔模块集成了一个经常性的模块，从而可以反复合成时间平滑的结果，而显著增加模型的大小。广泛的实验结果表明，我们的方法对国家的最先进的方法有利地进行。</font>
</div>


<hr>
<div id="paper6"> <b>6. The Mertens Unrolled Network (MU-Net): A High Dynamic Range Fusion  Neural Network for Through the Windshield Driver Recognition</b>  <a href="https://arxiv.org/pdf/2002.12257" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruby%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Max Ruby</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bolme%2C+D+S" target="_blank" rel="noopener" style="color:#0000EE;">David S. Bolme</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brogan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joel Brogan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cornett%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Cornett III</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Delgado%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baldemar Delgado</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jager%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gavin Jager</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christi Johnson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martinez-Mendoza%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jose Martinez-Mendoza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Santos-Villalobos%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hector Santos-Villalobos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Srinivas%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nisha Srinivas</a><br>
<font size="3">
Abstract: Face recognition of vehicle occupants through windshields in unconstrained environments poses a number of unique challenges ranging from glare, poor illumination, driver pose and motion blur. In this paper, we further develop the hardware and software components of a custom vehicle imaging system to better overcome these challenges. After the build out of a physical prototype system that performs High Dynamic Range (HDR) imaging, we collect a small dataset of through-windshield image captures of known drivers. We then re-formulate the classical Mertens-Kautz-Van Reeth HDR fusion algorithm as a pre-initialized neural network, which we name the Mertens Unrolled Network (MU-Net), for the purpose of fine-tuning the HDR output of through-windshield images. Reconstructed faces from this novel HDR method are then evaluated and compared against other traditional and experimental HDR methods in a pre-trained state-of-the-art (SOTA) facial recognition pipeline, verifying the efficacy of our approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：脸部识别车辆乘员通过挡风玻璃在不受约束的环境中，会产生若干从眩光，照明较差，驾驶员的姿势和运动模糊独特的挑战。在本文中，我们进一步开发自定义的车载成像系统，以更好地应对这些挑战的硬件和软件组件。构建出一个物理样机系统进行高动态范围（HDR）成像，我们收集已知驱动程序通过挡风玻璃图像捕获的小数据集之后。然后我们重新制订了经典梅尔滕斯-Kautz酒店面包车Reeth HDR融合算法作为预初始化神经网络，这是我们命名梅尔滕斯展开网络（MU-NET）进行微调通的HDR输出的目的挡风玻璃上的图像。然后从该新颖方法HDR重构面进行评估，并针对在一个预训练的状态的最先进的（SOTA）面部识别管道等传统和实验方法HDR，验证我们的方法的效力进行比较。</font>
</div>


<hr>
<div id="paper7"> <b>7. ZoomCount: A Zooming Mechanism for Crowd Counting in Static Images</b>  <a href="https://arxiv.org/pdf/2002.12256" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sajid%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Usman Sajid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sajid%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hasan Sajid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongcheng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guanghui Wang</a><br>
<font size="3">
Abstract: This paper proposes a novel approach for crowd counting in low to high density scenarios in static images. Current approaches cannot handle huge crowd diversity well and thus perform poorly in extreme cases, where the crowd density in different regions of an image is either too low or too high, leading to crowd underestimation or overestimation. The proposed solution is based on the observation that detecting and handling such extreme cases in a specialized way leads to better crowd estimation. Additionally, existing methods find it hard to differentiate between the actual crowd and the cluttered background regions, resulting in further count overestimation. To address these issues, we propose a simple yet effective modular approach, where an input image is first subdivided into fixed-size patches and then fed to a four-way classification module labeling each image patch as low, medium, high-dense or no-crowd. This module also provides a count for each label, which is then analyzed via a specifically devised novel decision module to decide whether the image belongs to any of the two extreme cases (very low or very high density) or a normal case. Images, specified as high- or low-density extreme or a normal case, pass through dedicated zooming or normal patch-making blocks respectively before routing to the regressor in the form of fixed-size patches for crowd estimate. Extensive experimental evaluations demonstrate that the proposed approach outperforms the state-of-the-art methods on four benchmarks under most of the evaluation criteria. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种用于人群中计数低密度到高密度的场景中的静态图像的新方法。目前的方法无法处理庞大的人群的多样性以及从而在极端情况下，如果在图像的不同区域人群密度太低或太高表现不佳，导致人群低估或高估。提出的解决方案是基于检测和专业的方式带来更好的人群估计处理这样极端的情况下观察。此外，现有的方法很难实际的人群和杂乱背景的区域区分，从而进一步计数高估。为了解决这些问题，我们提出了一个简单而有效的模块化的方法，其中，将输入图像首先被细分成固定大小的补丁，然后提供给四通分类模块标记每个图像补丁为低，中，高致密的或无-人群。该模块还提供了用于每个标签，然后将其通过一个专门设计新颖决定模块分析以决定图像是否属于任何的两种极端情况下（非常低或非常高的密度）或正常情况下的计数。图像，指定为高或低的密度极端或正常情况下，通过专用的变焦或正常膜片使得块分别在固定大小的补丁的用于人群估计的形式路由到回归之前。大量的实验评估表明，该方法比四个基准国家的最先进的方法，在大多数的评价标准。</font>
</div>


<hr>
<div id="paper8"> <b>8. Learning Representations by Predicting Bags of Visual Words</b>  <a href="https://arxiv.org/pdf/2002.12247" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gidaris%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Spyros Gidaris</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bursuc%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrei Bursuc</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Komodakis%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikos Komodakis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%C3%A9rez%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Pérez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cord%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthieu Cord</a><br>
<font size="3">
Abstract: Self-supervised representation learning targets to learn convnet-based image representations from unlabeled data. Inspired by the success of NLP methods in this area, in this work we propose a self-supervised approach based on spatially dense image descriptions that encode discrete visual concepts, here called visual words. To build such discrete representations, we quantize the feature maps of a first pre-trained self-supervised convnet, over a k-means based vocabulary. Then, as a self-supervised task, we train another convnet to predict the histogram of visual words of an image (i.e., its Bag-of-Words representation) given as input a perturbed version of that image. The proposed task forces the convnet to learn perturbation-invariant and context-aware image features, useful for downstream image understanding tasks. We extensively evaluate our method and demonstrate very strong empirical results, e.g., our pre-trained self-supervised representations transfer better on detection task and similarly on classification over classes "unseen" during pre-training, when compared to the supervised case. This also shows that the process of image discretization into visual words can provide the basis for very powerful self-supervised approaches in the image domain, thus allowing further connections to be made to related methods from the NLP domain that have been extremely successful so far. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自监督表示学习目标，以无标签的数据基于学习convnet形象表示。通过在这一领域NLP方法的成功的启发，在这项工作中，我们提出了一种基于空间密集图像描述的是编码独立视觉概念，这里称为视觉词的自我监督的做法。要建立这样的离散表示，我们量化第一预先训练自我监督convnet的特征图，基于词汇K均值。然后，作为自监督任务，我们培养另一个convnet来预测的图像的视觉词的直方图（即，其袋的字表示）作为输入提供一个扰动该图像的版本。所提出的工作队的convnet学习扰动不变和上下文感知的图像特征，为下游的图像理解任务非常有用。我们广泛地评估我们的方法，并表现出非常强的实证结果，例如，我们的预先训练自我监督的申述检测任务的分类过班“看不见”的过程中预先训练相比，监督的情况下更好地转移，同样，。这也说明，像离散化的过程变成视觉的话可以在图像领域非常强大的自我监督的方法提供依据，从而使进一步连接到从NLP领域相关的方法已经非常成功，到目前为止进行。</font>
</div>


<hr>
<div id="paper9"> <b>9. Meta-Transfer Learning for Zero-Shot Super-Resolution</b>  <a href="https://arxiv.org/pdf/2002.12213" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Soh%2C+J+W" target="_blank" rel="noopener" style="color:#0000EE;">Jae Woong Soh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sunwoo Cho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+N+I" target="_blank" rel="noopener" style="color:#0000EE;">Nam Ik Cho</a><br>
<font size="3">
Abstract: Convolutional neural networks (CNNs) have shown dramatic improvements in single image super-resolution (SISR) by using large-scale external samples. Despite their remarkable performance based on the external dataset, they cannot exploit internal information within a specific image. Another problem is that they are applicable only to the specific condition of data that they are supervised. For instance, the low-resolution (LR) image should be a "bicubic" downsampled noise-free image from a high-resolution (HR) one. To address both issues, zero-shot super-resolution (ZSSR) has been proposed for flexible internal learning. However, they require thousands of gradient updates, i.e., long inference time. In this paper, we present Meta-Transfer Learning for Zero-Shot Super-Resolution (MZSR), which leverages ZSSR. Precisely, it is based on finding a generic initial parameter that is suitable for internal learning. Thus, we can exploit both external and internal information, where one single gradient update can yield quite considerable results. (See Figure 1). With our method, the network can quickly adapt to a given image condition. In this respect, our method can be applied to a large spectrum of image conditions within a fast adaptation process. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）已经通过使用大规模的外部的样品示于单个图像超分辨率（SISR）的显着改善。尽管基于外部数据集的表现可圈可点，他们不能利用特定图像中的内部信息。另一个问题是，他们只对他们进行监督数据的具体情况都适用。例如，低分辨率（LR）图像应该是一个“双三次”从一个高分辨率（HR）一个下采样无噪声的图像。为了解决这两个问题，零次超级分辨率（ZSSR）已经提出了灵活的内部学习。然而，他们需要上千梯度更新，即长推理时间的。在本文中，我们目前元转让学习的零射门超分辨率（MZSR），它利用ZSSR。确切地说，它是基于寻找适合于内部学习一个通用的初始参数。因此，我们可以利用外部和内部信息，其中一个单一的梯度更新可以产生相当可观的效果。 （参见图1）。随着我们的方法中，网络能够快速适应给定的图像条件。在这方面，我们的方法可以快速的适应过程中被施加到较大的频谱的图像的条件。</font>
</div>


<hr>
<div id="paper10"> <b>10. Total3DUnderstanding: Joint Layout, Object Pose and Mesh Reconstruction  for Indoor Scenes from a Single Image</b>  <a href="https://arxiv.org/pdf/2002.12212" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinyu Nie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoguang Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shihui Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yujian Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Jun Zhang</a><br>
<font size="3">
Abstract: Semantic reconstruction of indoor scenes refers to both scene understanding and object reconstruction. Existing works either address one part of this problem or focus on independent objects. In this paper, we bridge the gap between understanding and reconstruction, and propose an end-to-end solution to jointly reconstruct room layout, object bounding boxes and meshes from a single image. Instead of separately resolving scene understanding and object reconstruction, our method builds upon a holistic scene context and proposes a coarse-to-fine hierarchy with three components: 1. room layout with camera pose; 2. 3D object bounding boxes; 3. object meshes. We argue that understanding the context of each component can assist the task of parsing the others, which enables joint understanding and reconstruction. The experiments on the SUN RGB-D and Pix3D datasets demonstrate that our method consistently outperforms existing methods in indoor layout estimation, 3D object detection and mesh reconstruction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：室内场景的语义重构指的是现场的理解和对象的重建。现有的作品无论是地址独立的对象这个问题，或者聚焦的一部分。在本文中，我们弥合理解和重建之间的间隙，并从单个图像提出的端至端的解决方案，共同重构房间布局，对象边界框和网格。而不是单独分辨场景理解和对象重建，我们的方法是建立在一个整体的场景上下文，并提出了由粗到细的层次结构具有三个组件：1.房间布局与照相机的姿态; 2. 3D对象的边界框; 3.对象网格。我们认为，了解每个组件的情况下可以帮助解析他人，使共同谅解和重建任务。在SUN RGB-d和Pix3D数据集上的实验结果表明，我们的方法始终优于在室内布局估计，立体物检测的现有方法和网状重建。</font>
</div>


<hr>
<div id="paper11"> <b>11. Visual Commonsense R-CNN</b>  <a href="https://arxiv.org/pdf/2002.12204" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianqiang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanwang Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianru Sun</a><br>
<font size="3">
Abstract: We present a novel unsupervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for high-level tasks such as captioning and VQA. Given a set of detected object regions in an image (e.g., using Faster R-CNN), like any other unsupervised feature learning methods (e.g., word2vec), the proxy training objective of VC R-CNN is to predict the contextual objects of a region. However, they are fundamentally different: the prediction of VC R-CNN is by using causal intervention: P(Y|do(X)), while others are by using the conventional likelihood: P(Y|X). This is also the core reason why VC R-CNN can learn "sense-making" knowledge like chair can be sat --- while not just "common" co-occurrences such as chair is likely to exist if table is observed. We extensively apply VC R-CNN features in prevailing models of three popular tasks: Image Captioning, VQA, and VCR, and observe consistent performance boosts across all the methods and tasks, achieving many new state-of-the-arts. Code and feature are available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提出一种新的无监督特征表示学习方法，基于区域的视觉常识卷积神经网络（VC R-CNN），以作为高层次的任务，例如字幕和VQA的改进的视觉区域编码器。给定一组检测到的对象区域的图像中（例如，使用快速R-CNN），就像任何其它无监督特征点学习方法（例如，word2vec），VC R-CNN的代理训练目标是预测的语境对象区域。然而，它们是根本不同的：VC R-CNN的预测是通过使用因果干预：P（Y |做（X）），而其它的是通过使用常规的似然度：P（Y | X）。这也是最核心的原因，为什么VC R-CNN可以学习“意义建构”像椅子学问可SAT ---而不仅仅是“普通”共同出现，如椅子是有可能，如果观察表存在。我们广泛采用VC R-CNN在现行的三种流行的任务不同型号的功能：图片字幕，VQA和VCR，并观察一致的性能提升在所有方法和任务，实现了许多新的国家的最艺术。代码和功能可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper12"> <b>12. University-1652: A Multi-view Multi-source Benchmark for Drone-based  Geo-localization</b>  <a href="https://arxiv.org/pdf/2002.12186" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhedong Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunchao Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Yang</a><br>
<font size="3">
Abstract: We consider the problem of cross-view geo-localization. The primary challenge of this task is to learn the robust feature against large viewpoint changes. Existing benchmarks can help, but are limited in the number of viewpoints. Image pairs, containing two viewpoints, e.g., satellite and ground, are usually provided, which may compromise the feature learning. Besides phone cameras and satellites, in this paper, we argue that drones could serve as the third platform to deal with the geo-localization problem. In contrast to the traditional ground-view images, drone-view images meet fewer obstacles, e.g., trees, and could provide a comprehensive view when flying around the target place. To verify the effectiveness of the drone platform, we introduce a new multi-view multi-source benchmark for drone-based geo-localization, named University-1652. University-1652 contains data from three platforms, i.e., synthetic drones, satellites and ground cameras of 1,652 university buildings around the world. To our knowledge, University-1652 is the first drone-based geo-localization dataset and enables two new tasks, i.e., drone-view target localization and drone navigation. As the name implies, drone-view target localization intends to predict the location of the target place via drone-view images. On the other hand, given a satellite-view query image, drone navigation is to drive the drone to the area of interest in the query. We use this dataset to analyze a variety of off-the-shelf CNN features and propose a strong CNN baseline on this challenging dataset. The experiments show that University-1652 helps the model to learn the viewpoint-invariant features and also has good generalization ability in the real-world scenario. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们认为交叉视角的地理定位的问题。这项任务的主要挑战是学习对大视角变化的强大的功能。现有的基准测试可以帮助，但在视点的数量是有限的。图像对，含有两个观点出发，例如，卫星和地面，通常设置，这可能损害功能学习。除了手机相机和卫星，在本文中，我们认为，无人驾驶飞机可以作为第三平台应对地理定位问题。相较于传统的地面观看的图像，无人机视图像满足更少的障碍，例如，树木和到处乱飞的目标位置时，可以提供一个全面的看法。为了验证无人机平台的有效性，我们引入了基于无人机的地理定位新的多视角多源基准，命名为大学-1652。大学-1652包含三个平台，即合成的无人驾驶飞机，卫星和1,652所大学在世界各地的建筑接地相机数据。据我们所知，大学-1652的第一个基于无人机的地理定位数据集并启用两个新的任务，即无人机视目标定位和无人机导航。顾名思义，无人机视目标定位打算通过无人机视图像预测目标地点的位置。在另一方面，在给定的卫星视图查询图像，无人驾驶飞机导航是驱动无人驾驶飞机的查询中的感兴趣的区域。我们用这个数据集来分析各种现成的，货架CNN特点，并提出在这一具有挑战性的数据集强大的CNN基线。实验结果表明，大学-1652帮助模型学习的观点不变特征，也有在真实的场景中良好的泛化能力。</font>
</div>


<hr>
<div id="paper13"> <b>13. Evolving Losses for Unsupervised Video Representation Learning</b>  <a href="https://arxiv.org/pdf/2002.12177" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Piergiovanni%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">AJ Piergiovanni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Angelova%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anelia Angelova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ryoo%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Michael S. Ryoo</a><br>
<font size="3">
Abstract: We present a new method to learn video representations from large-scale unlabeled video data. Ideally, this representation will be generic and transferable, directly usable for new tasks such as action recognition and zero or few-shot learning. We formulate unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are shared across different modalities via distillation. Further, we introduce the concept of loss function evolution by using an evolutionary search algorithm to automatically find optimal combination of loss functions capturing many (self-supervised) tasks and modalities. Thirdly, we propose an unsupervised representation evaluation metric using distribution matching to a large unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised constraint, which is not guided by any labeling, produces similar results to weakly-supervised, task-specific ones. The proposed unsupervised representation learning results in a single RGB network and outperforms previous methods. Notably, it is also more effective than several label-based methods (e.g., ImageNet), with the exception of large, fully labeled video datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出要学会从大规模未标记的视频数据的视频表示的新方法。理想情况下，表示将是通用的，可转让的，可直接使用新的任务，例如动作识别和零个或几个次学习。我们制定监督的代表学习的多模式，多任务学习问题，其中表示是跨不同模态通过蒸馏共享。此外，我们通过使用进化搜索算法自动查找捕获许多（自我监督）的任务和方式的损失功能的最佳组合，引入损失函数演变的概念。第三，我们提出了一种无监督的表现评价指标使用分布匹配的大型数据集未标记作为先验约束的基础上，齐普夫定律。这种无监督约束，这是没有任何标签的指导下，产生类似的结果，以弱监督，任务具体的。所提出的无监督表示学习在一个单一的RGB网络结果，远远超过前的方法。值得注意的是，它也比几个基于标签的方法（例如，ImageNet）更有效的，具有大，中，充分标记的视频数据集的异常。</font>
</div>


<hr>
<div id="paper14"> <b>14. Domain Decluttering: Simplifying Images to Mitigate Synthetic-Real  Domain Shift and Improve Depth Estimation</b>  <a href="https://arxiv.org/pdf/2002.12114" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunhan Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shu Kong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daeyun Shin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fowlkes%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Charless Fowlkes</a><br>
<font size="3">
Abstract: Leveraging synthetically rendered data offers great potential to improve monocular depth estimation, but closing the synthetic-real domain gap is a non-trivial and important task. While much recent work has focused on unsupervised domain adaptation, we consider a more realistic scenario where a large amount of synthetic training data is supplemented by a small set of real images with ground-truth. In this setting we find that existing domain translation approaches are difficult to train and offer little advantage over simple baselines that use a mix of real and synthetic data. A key failure mode is that real-world images contain novel objects and clutter not present in synthetic training. This high-level domain shift isn't handled by existing image translation models. Based on these observations, we develop an attentional module that learns to identify and remove (hard) out-of-domain regions in real images in order to improve depth prediction for a model trained primarily on synthetic data. We carry out extensive experiments to validate our attend-remove-complete approach (ARC) and find that it significantly outperforms state-of-the-art domain adaptation methods for depth prediction. Visualizing the removed regions provides interpretable insights into the synthetic-real domain gap. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：利用综合呈现的数据提供了巨大的潜力，提高单眼深度估计，但收盘合成实域差距是一个不平凡而重要的任务。虽然最近的许多工作集中在无监督的领域适应性，我们认为当大量合成训练数据是由一小部分与地面实况影像进行真实补充了更真实的场景。在这一背景下，我们发现，现有的域名翻译方法是很难培养，并在使用真实和合成数据的混合简单基线提供很少的优势。一个关键的故障模式是，现实世界图像包含新物体和杂波在合成训练不存在。这种高层次的域名转移不被现有的图像翻译模型处理。基于这些观察，我们开发一个所注意的模块，学习识别和删除（硬），以提高深度预测训练主要在合成数据的模型在实际图像外的结构域区域。我们开展了广泛的实验来验证我们的出席 - 删除 - 完整的方法（ARC），并发现它显著优于国家的最先进的领域适应性方法的深度预测。可视化去除区域提供可解释的分析上市公司合成真实域间隙。</font>
</div>


<hr>
<div id="paper15"> <b>15. Deep Slow Motion Video Reconstruction with Hybrid Imaging System</b>  <a href="https://arxiv.org/pdf/2002.12106" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Paliwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avinash Paliwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kalantari%2C+N+K" target="_blank" rel="noopener" style="color:#0000EE;">Nima Khademi Kalantari</a><br>
<font size="3">
Abstract: Slow motion videos are becoming increasingly popular, but capturing high-resolution videos at extremely high frame rates requires professional high-speed cameras. To mitigate this problem, current techniques increase the frame rate of standard videos through frame interpolation by assuming linear motion between the existing frames. While this assumption holds true for simple cases with small motion, in challenging cases the motion is usually complex and this assumption is no longer valid. Therefore, they typically produce results with unnatural motion in these challenging cases. In this paper, we address this problem using two video streams as the input; an auxiliary video with high frame rate and low spatial resolution, providing temporal information, in addition to the standard main video with low frame rate and high spatial resolution. We propose a two-stage deep learning system consisting of alignment and appearance estimation that reconstructs high resolution slow motion video from the hybrid video input. For alignment, we propose to use a set of pre-trained and trainable convolutional neural networks (CNNs) to compute the flows between the missing frame and the two existing frames of the main video by utilizing the content of the auxiliary video frames. We then warp the existing frames using the flows to produce a set of aligned frames. For appearance estimation, we propose to combine the aligned and auxiliary frames using a context and occlusion aware CNN. We train our model on a set of synthetically generated hybrid videos and show high-quality results on a wide range of test scenes. We further demonstrate the practicality of our approach by showing the performance of our system on two real dual camera setups with small baseline. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：慢动作视频正变得越来越流行，但在非常高帧率捕捉高清晰度视频需要专业的高速摄像机。为了缓解这个问题，目前的技术通过假设现有帧之间直线运动增加的标准视频帧速率通过帧插值。虽然这个假设成立的情况下，简单的小运动真正的，在具有挑战性的情况下，运动通常是复杂的，这种假设不再有效。因此，它们通常产生具有在这些挑战性的情况下不自然运动的结果。在本文中，我们要解决使用两个视频流作为输入这个问题;辅助视频具有高帧速率和低的空间分辨率，提供时间信息，除了具有低帧速率和高空间分辨率的标准主视频。我们建议由对齐和外观估计从混合视频输入重建高分辨率的慢动作视频的两级深学习系统。用于对准，我们建议使用一组预训练和可训练卷积神经网络（细胞神经网络）的计算丢失的帧，并且通过利用所述辅助视频帧的内容的主视频的两个现有帧之间的流动。然后，我们使用流以产生一组对齐的帧的翘曲现有帧。对于外观的估计，我们建议使用上下文的排列和辅助框架结合起来，闭塞知道CNN。我们培养的一套合成产生的混合视频我们的模型，并显示了广泛的测试场景的高品质的结果。我们通过显示对两个真正的双摄像头设置有小基线我们的系统的性能进一步证明了该方法的实用性。</font>
</div>


<hr>
<div id="paper16"> <b>16. The Data Representativeness Criterion: Predicting the Performance of  Supervised Classification Based on Data Set Similarity</b>  <a href="https://arxiv.org/pdf/2002.12105" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schat%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evelien Schat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=van+de+Schoot%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rens van de Schoot</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kouw%2C+W+M" target="_blank" rel="noopener" style="color:#0000EE;">Wouter M. Kouw</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Veen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duco Veen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mendrik%2C+A+M" target="_blank" rel="noopener" style="color:#0000EE;">Adriënne M. Mendrik</a><br>
<font size="3">
Abstract: In a broad range of fields it may be desirable to reuse a supervised classification algorithm and apply it to a new data set. However, generalization of such an algorithm and thus achieving a similar classification performance is only possible when the training data used to build the algorithm is similar to new unseen data one wishes to apply it to. It is often unknown in advance how an algorithm will perform on new unseen data, being a crucial reason for not deploying an algorithm at all. Therefore, tools are needed to measure the similarity of data sets. In this paper, we propose the Data Representativeness Criterion (DRC) to determine how representative a training data set is of a new unseen data set. We present a proof of principle, to see whether the DRC can quantify the similarity of data sets and whether the DRC relates to the performance of a supervised classification algorithm. We compared a number of magnetic resonance imaging (MRI) data sets, ranging from subtle to severe difference is acquisition parameters. Results indicate that, based on the similarity of data sets, the DRC is able to give an indication as to when the performance of a supervised classifier decreases. The strictness of the DRC can be set by the user, depending on what one considers to be an acceptable underperformance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在广泛领域的可能希望重新使用受监督的分类算法，并将其应用到一个新的数据集。然而，这样的算法，并由此实现了类似的分类性能的概括是唯一可能当用于构建算法训练数据类似，新的看不见的数据的一个愿望将其应用到。它往往是未知的提前的算法将如何在新的看不见的数据执行，是在所有未部署的算法的重要原因。因此，需要工具来衡量数据集的相似性。在本文中，我们提出了数据典型性标准（DRC）来确定训练数据集是一个新的看不见的数据集的代表性如何。我们提出一个原则的证明，看是否DRC可以量化的数据集，以及是否在刚果（金）涉及监督分类算法的性能相似。我们比较了一些磁共振成像（MRI）的数据集，从细微的严重差是采集参数。结果表明，基于对数据集的相似性，则DRC是能够给出的指示时有监督的分类器的性能下降到。所述DRC的严格性可以由用户，这取决于一个认为是一个可接受的表现不佳来设置。</font>
</div>


<hr>
<div id="paper17"> <b>17. Action Quality Assessment using Siamese Network-Based Deep Metric  Learning</b>  <a href="https://arxiv.org/pdf/2002.12096" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiteshi Jain</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Harit%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gaurav Harit</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avinash Sharma</a><br>
<font size="3">
Abstract: Automated vision-based score estimation models can be used as an alternate opinion to avoid judgment bias. In the past works the score estimation models were learned by regressing the video representations to the ground truth score provided by the judges. However such regression-based solutions lack interpretability in terms of giving reasons for the awarded score. One solution to make the scores more explicable is to compare the given action video with a reference video. This would capture the temporal variations w.r.t. the reference video and map those variations to the final score. In this work, we propose a new action scoring system as a two-phase system: (1) A Deep Metric Learning Module that learns similarity between any two action videos based on their ground truth scores given by the judges; (2) A Score Estimation Module that uses the first module to find the resemblance of a video to a reference video in order to give the assessment score. The proposed scoring model has been tested for Olympics Diving and Gymnastic vaults and the model outperforms the existing state-of-the-art scoring models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于视觉的自动计分估计模型可以用作备用的意见，以避免判断的偏见。在过去的作品中得分推测模型，通过回归的视频表示由法官提供的地面实况比分教训。然而这种基于回归的解决方案给予了奖励分数的原因方面缺乏可解释性。一种解决方案，使分数更可解释为给定的动作视频与参考视频进行比较。这将捕捉时间变化w.r.t.参考视频和映射这些变化的最终得分。在这项工作中，我们提出了一个新的动作评分系统为两相体系：（1）深度量学习模块，基于由法官给予他们的地面实况比分任意两个动作视频之间的相似度获悉; （2）分数估计模块使用第一模块来找到一个视频参考视频的相似，为了给评估得分。建议的计分模型已经过测试，奥运跳水和体操拱顶和模型优于现有的国家的最先进的评分模型。</font>
</div>


<hr>
<div id="paper18"> <b>18. Reducing Geographic Performance Differential for Face Recognition</b>  <a href="https://arxiv.org/pdf/2002.12093" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bruveris%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martins Bruveris</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gietema%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jochem Gietema</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mortazavian%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pouria Mortazavian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mahadevan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohan Mahadevan</a><br>
<font size="3">
Abstract: As face recognition algorithms become more accurate and get deployed more widely, it becomes increasingly important to ensure that the algorithms work equally well for everyone. We study the geographic performance differentials-differences in false acceptance and false rejection rates across different countries-when comparing selfies against photos from ID documents. We show how to mitigate geographic performance differentials using sampling strategies despite large imbalances in the dataset. Using vanilla domain adaptation strategies to fine-tune a face recognition CNN on domain-specific doc-selfie data improves the performance of the model on such data, but, in the presence of imbalanced training data, also significantly increases the demographic bias. We then show how to mitigate this effect by employing sampling strategies to balance the training procedure. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着人脸识别算法变得更准确，得到更广泛的应用，它变得越来越重要，以确保该算法同样适合每一个人。比较自拍免受身份证件照片时的国家，我们研究的地理表现在错误接受和错误拒绝率在不同的差异，差异。我们将展示如何使用抽样策略，尽管在数据集中存在很大的不平衡，以减轻地理性能差异。使用香草域适应战略微调人脸识别CNN对特定领域的DOC-自拍的数据提高了对这些数据的模型的性能，但是，在不平衡的训练数据的情况下，也显著增加的人口偏差。然后，我们将展示如何通过采用抽样方法，以平衡训练过程，以减轻这种影响。</font>
</div>


<hr>
<div id="paper19"> <b>19. XSepConv: Extremely Separated Convolution</b>  <a href="https://arxiv.org/pdf/2002.12046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiarong Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zongqing Lu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing-Hao Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingmin Liao</a><br>
<font size="3">
Abstract: Depthwise convolution has gradually become an indispensable operation for modern efficient neural networks and larger kernel sizes ($\ge5$) have been applied to it recently. In this paper, we propose a novel extremely separated convolutional block (XSepConv), which fuses spatially separable convolutions into depthwise convolution to further reduce both the computational cost and parameter size of large kernels. Furthermore, an extra $2\times2$ depthwise convolution coupled with improved symmetric padding strategy is employed to compensate for the side effect brought by spatially separable convolutions. XSepConv is designed to be an efficient alternative to vanilla depthwise convolution with large kernel sizes. To verify this, we use XSepConv for the state-of-the-art architecture MobileNetV3-Small and carry out extensive experiments on four highly competitive benchmark datasets (CIFAR-10, CIFAR-100, SVHN and Tiny-ImageNet) to demonstrate that XSepConv can indeed strike a better trade-off between accuracy and efficiency. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在深度上卷积逐渐成为近来已应用到它的现代高效的神经网络和更大尺寸的内核（$ \ $ GE5）不可缺少的操作。在本文中，我们提出了一种新颖的分离极其卷积块（XSepConv），其融合空间可分离卷积成深度方向卷积，以进一步减少两者的计算成本和大内核的参数大小。此外，再加上改进的对称填充策略一个额外的$ 2 \ times2 $深度方向卷积被用来补偿由空间可分离卷积所带来的副作用。 XSepConv被设计成一个有效的替代香草深度方向卷积大内核尺寸。为了验证这一点，我们使用XSepConv的国家的最先进的架构MobileNetV3  - 小型和开展4个高度竞争的基准数据集大量的实验（CIFAR-10，CIFAR-100，SVHN和微小-ImageNet）证明XSepConv确实可以取得一个更好的权衡精度和效率之间。</font>
</div>


<hr>
<div id="paper20"> <b>20. Attention-guided Chained Context Aggregation for Semantic Segmentation</b>  <a href="https://arxiv.org/pdf/2002.12041" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Quan Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fagui Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zhang</a><br>
<font size="3">
Abstract: Recent breakthroughs in semantic segmentation methods based on Fully Convolutional Networks (FCNs) have aroused great research interest. One of the critical issues is how to aggregate multi-scale contextual information effectively to obtain reliable results. To address this problem, we propose a novel paradigm called the Chained Context Aggregation Module (CAM). CAM gains features of various spatial scales through chain-connected ladder-style information flows. The features are then guided by Flow Guidance Connections to interact and fuse in a two-stage process, which we refer to as pre-fusion and re-fusion. We further adopt attention models in CAM to productively recombine and select those fused features to refine performance. Based on these developments, we construct the Chained Context Aggregation Network (CANet), which employs a two-step decoder to recover precise spatial details of prediction maps. We conduct extensive experiments on three challenging datasets, including Pascal VOC 2012, CamVid and SUN-RGBD. Results evidence that our CANet achieves state-of-the-art performance. Codes will be available on the publication of this paper. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在语义分割方法的最新突破，基于全卷积网络（FCNs）已经引起了极大的研究兴趣。其中一个关键问题是如何有效聚合多尺度的上下文信息，以获得可靠的结果。为了解决这个问题，我们提出了一个叫做链式语境聚集模块（CAM）的新范式。通过链连接的阶梯式的信息的各种空间尺度的CAM增益特性流动。该功能，然后通过流诱导连接进行互动和保险丝两阶段的过程，我们称之为预融合和重新融合的指导。我们进一步采取关注车型在CAM富有成效复合，选择那些融合功能来优化性能。基于这些发展，我们构建了已链接的上下文聚合网络（卡内安），其采用两个步骤的解码器，以恢复预测图的精确的空间细节。我们在三个有挑战性的数据集，包括帕斯卡尔VOC 2012，CamVid和SUN-RGBD进行了广泛的实验。结果证明，我们的卡内实现国家的最先进的性能。代码将在今年的论文发表上。</font>
</div>


<hr>
<div id="paper21"> <b>21. Multiple Discrimination and Pairwise CNN for View-based 3D Object  Retrieval</b>  <a href="https://arxiv.org/pdf/2002.11977" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Z. Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+K+X" target="_blank" rel="noopener" style="color:#0000EE;">K.X Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wan%2C+S+H" target="_blank" rel="noopener" style="color:#0000EE;">S.H Wan</a><br>
<font size="3">
Abstract: With the rapid development and wide application of computer, camera device, network and hardware technology, 3D object (or model) retrieval has attracted widespread attention and it has become a hot research topic in the computer vision domain. Deep learning features already available in 3D object retrieval have been proven to be better than the retrieval performance of hand-crafted features. However, most existing networks do not take into account the impact of multi-view image selection on network training, and the use of contrastive loss alone only forcing the same-class samples to be as close as possible. In this work, a novel solution named Multi-view Discrimination and Pairwise CNN (MDPCNN) for 3D object retrieval is proposed to tackle these issues. It can simultaneously input of multiple batches and multiple views by adding the Slice layer and the Concat layer. Furthermore, a highly discriminative network is obtained by training samples that are not easy to be classified by clustering. Lastly, we deploy the contrastive-center loss and contrastive loss as the optimization objective that has better intra-class compactness and inter-class separability. Large-scale experiments show that the proposed MDPCNN can achieve a significant performance over the state-of-the-art algorithms in 3D object retrieval. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着国民经济的快速发展和广泛应用计算机，照相机设备，网络和硬件技术，3D对象（或模型）检索已引起广泛关注，并已成为计算机视觉领域的一个研究热点。检索已被证明优于手工制作的功能检索性能深度学习功能在3D对象已经可用。然而，大多数现有网络没有考虑到多视角图像选择对网络训练的影响，并运用对比损失的单独只迫使同类别样本是尽可能接近。在这项工作中，命名为多视图歧视和成对CNN（MDPCNN）用于3D对象检索一个新的解决方案，提出了解决这些问题。它可以多批次，通过将片层和所述的毗连层的多个视图的同时输入。此外，高辨别网络通过训练未易于被聚类而分类的样品获得。最后，我们部署对比中心的损失和对比损失有更好的类内致密性和类间可分性的优化目标。大规模实验表明，该MDPCNN可以实现对国家的最先进的算法，在3D对象检索显著的性能。</font>
</div>


<hr>
<div id="paper22"> <b>22. Unbiased Scene Graph Generation from Biased Training</b>  <a href="https://arxiv.org/pdf/2002.11949" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaihua Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulei Niu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianqiang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaxin Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanwang Zhang</a><br>
<font size="3">
Abstract: Today's scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, e.g., collapsing diverse human walk on/ sit on/lay on beach into human on beach. Given such SGG, the down-stream tasks such as VQA can hardly infer better scene structures than merely a bag of objects. However, debiasing in SGG is not trivial because traditional debiasing methods cannot distinguish between the good and bad bias, e.g., good context prior (e.g., person read book rather than eat) and bad long-tailed bias (e.g., behind/in front of collapsed to near). In this paper, we present a novel SGG framework based on causal inference but not the conventional likelihood. We first build a causal graph for SGG, and perform traditional biased training with the graph. Then, we propose to draw the counterfactual causality from the trained graph to infer the effect from the bad bias, which should be removed. In particular, we use Total Direct Effect as the proposed final predicate score for unbiased SGG. Note that our framework is agnostic to any SGG model and thus can be widely applied in the community who seeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit on the SGG benchmark Visual Genome and several prevailing models, we observed significant improvements over the previous state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：今天的场景图代（SGG）的任务是从实际还有差距，主要是由于严格的训练偏差，例如，上/坐/在沙滩上躺进人上海滩倒塌的多样化人类的步行路程。鉴于这种SGG中，下游的任务，如VQA几乎不能推断不仅仅是对象的袋子更好的现场搭建。然而，在SGG消除直流偏压是不平凡的，因为传统的去除偏差的方法不能在好的和坏的偏见，如区分，良好的语境优先（例如，人读的书，而不是吃）和坏长尾偏置（例如，暂列前/晕倒附近）。在本文中，我们提出基于因果推断的新颖SGG框架而不是常规可能性。我们首先建立SGG因果关系图，并执行与图形传统偏见的训练。然后，我们建议从训练的图形绘制反因果关系来推断从坏的偏见，它应该被删除的效果。特别是，我们用总的直接影响作为拟议最后断言分数公正SGG。请注意，我们的框架是不可知的任何SGG模型，因此可以在谁寻求公正的预测社会得到广泛应用。通过使用对SGG基准视觉基因组和几个流行的模型所提出的场景图诊断工具，我们观察到以前的状态的最先进的方法显著的改善。</font>
</div>


<hr>
<div id="paper23"> <b>23. Features for Ground Texture Based Localization -- A Survey</b>  <a href="https://arxiv.org/pdf/2002.11948" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schmid%2C+J+F" target="_blank" rel="noopener" style="color:#0000EE;">Jan Fabian Schmid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Simon%2C+S+F" target="_blank" rel="noopener" style="color:#0000EE;">Stephan F. Simon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mester%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rudolf Mester</a><br>
<font size="3">
Abstract: Ground texture based vehicle localization using feature-based methods is a promising approach to achieve infrastructure-free high-accuracy localization. In this paper, we provide the first extensive evaluation of available feature extraction methods for this task, using separately taken image pairs as well as synthetic transformations. We identify AKAZE, SURF and CenSurE as best performing keypoint detectors, and find pairings of CenSurE with the ORB, BRIEF and LATCH feature descriptors to achieve greatest success rates for incremental localization, while SIFT stands out when considering severe synthetic transformations as they might occur during absolute localization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：采用基于特征的方法基于车辆定位地面纹理是一种很有前途的方法来实现无基础设施的高精度定位。在本文中，我们提供的可用的特征提取方法在第一广泛的评估执行此任务，使用分别取出的图像对以及合成转化。我们确定AKAZE，SURF和谴责为表现最好的关键点检测器，并找到谴责的配对与ORB，BRIEF和LATCH特征描述符，以实现增量本土化最大的成功率，而SIFT脱颖而出考虑到严峻的合成转换时的过程中可能发生，他们绝对定位。</font>
</div>


<hr>
<div id="paper24"> <b>24. Weakly supervised discriminative feature learning with state information  for person identification</b>  <a href="https://arxiv.org/pdf/2002.11939" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hong-Xing Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Shi Zheng</a><br>
<font size="3">
Abstract: Unsupervised learning of identity-discriminative visual feature is appealing in real-world tasks where manual labelling is costly. However, the images of an identity can be visually discrepant when images are taken under different states, e.g. different camera views and poses. This visual discrepancy leads to great difficulty in unsupervised discriminative learning. Fortunately, in real-world tasks we could often know the states without human annotation, e.g. we can easily have the camera view labels in person re-identification and facial pose labels in face recognition. In this work we propose utilizing the state information as weak supervision to address the visual discrepancy caused by different states. We formulate a simple pseudo label model and utilize the state information in an attempt to refine the assigned pseudo labels by the weakly supervised decision boundary rectification and weakly supervised feature drift regularization. We evaluate our model on unsupervised person re-identification and pose-invariant face recognition. Despite the simplicity of our method, it could outperform the state-of-the-art results on Duke-reID, MultiPIE and CFP datasets with a standard ResNet-50 backbone. We also find our model could perform comparably with the standard supervised fine-tuning results on the three datasets. Code is available at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：身份辨别视觉特征的无监督学习在现实世界的任务是有吸引力，其中手工贴标是昂贵的。然而，身份的图像可以是在视觉上有差异的图像时不同状态下采取，例如不同的摄像机视图和姿势。这种视觉差异导致了很大的难度在无人监督的判别学习。幸运的是，在现实世界的任务，我们往往可以知道状态，而无需人的注释，例如我们可以很容易地在人重新识别和人脸识别的面部姿态标签摄影机视图标签。在这项工作中我们提出利用国家信息监管不力，以解决因不同状态下的视觉差异。我们制定一个简单的伪标签模型，并利用企图由弱监督的决策边界整顿和弱监督功能漂移正规化细化分配伪标签的状态信息。我们评估我们的无监督人再次鉴定和姿态不变的人脸识别模型。尽管我们的方法的简单性，它可​​以超越在杜克 - 里德，MultiPIE和CFP数据集的国家的最先进的结果与标准RESNET-50骨干。我们还发现我们的模型可以用在三个数据集标准监督微调效果相当执行。代码可在此HTTPS URL</font>
</div>


<hr>
<div id="paper25"> <b>25. Auto-Encoding Twin-Bottleneck Hashing</b>  <a href="https://arxiv.org/pdf/2002.11930" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuming Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaxin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengyang Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fan Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fumin Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Ling Shao</a><br>
<font size="3">
Abstract: Conventional unsupervised hashing methods usually take advantage of similarity graphs, which are either pre-computed in the high-dimensional space or obtained from random anchor points. On the one hand, existing methods uncouple the procedures of hash function learning and graph construction. On the other hand, graphs empirically built upon original data could introduce biased prior knowledge of data relevance, leading to sub-optimal retrieval performance. In this paper, we tackle the above problems by proposing an efficient and adaptive code-driven graph, which is updated by decoding in the context of an auto-encoder. Specifically, we introduce into our framework twin bottlenecks (i.e., latent variables) that exchange crucial information collaboratively. One bottleneck (i.e., binary codes) conveys the high-level intrinsic data structure captured by the code-driven graph to the other (i.e., continuous variables for low-level detail information), which in turn propagates the updated network feedback for the encoder to learn more discriminative binary codes. The auto-encoding learning objective literally rewards the code-driven graph to learn an optimal encoder. Moreover, the proposed model can be simply optimized by gradient descent without violating the binary constraints. Experiments on benchmarked datasets clearly show the superiority of our framework over the state-of-the-art hashing methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：常规的无监督的散列方法通常利用相似的曲线图，其或者预先计算在高维空间中或从随机定位点获得的。在一方面，不耦合哈希函数的学习和施工图的过程现有的方法。在另一方面，图表经验在原始数据可能引入数据相关的偏先验知识构建，导致次优的检索性能。在本文中，我们处理通过提出一种有效的和适应性代码驱动图形，这是由在自动编码器的上下文中进行解码更新了上述问题。具体来说，我们引入了该协作交流的关键信息框架双瓶颈（即潜在变量）。一个瓶颈（即，二进制码）传送由代码驱动图形捕获到另一个高层次特性的数据结构（即，低级别的详细信息的信息连续变量），这反过来又传播更新后的网络反馈的编码器学习更有辨别力的二进制代码。自动编码学习目标字面上奖励代码驱动图形学会最佳的编码器。此外，所提出的模型可以简单地通过梯度下降而不违反约束的二进制优化。在基准测试数据集的实验清楚地表明我们对国家的最先进的哈希方法框架的优越性。</font>
</div>


<hr>
<div id="paper26"> <b>26. Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural  Network for Human Trajectory Prediction</b>  <a href="https://arxiv.org/pdf/2002.11927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohamed%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abduallah Mohamed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Qian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Elhoseiny%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohamed Elhoseiny</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Claudel%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Claudel</a><br>
<font size="3">
Abstract: Better machine understanding of pedestrian behaviors enables faster progress in modeling interactions between agents such as autonomous vehicles and humans. Pedestrian trajectories are not only influenced by the pedestrian itself but also by interaction with surrounding objects. Previous methods modeled these interactions by using a variety of aggregation methods that integrate different learned pedestrians states. We propose the Social Spatio-Temporal Graph Convolutional Neural Network (Social-STGCNN), which substitutes the need of aggregation methods by modeling the interactions as a graph. Our results show an improvement over the state of art by 20% on the Final Displacement Error (FDE) and an improvement on the Average Displacement Error (ADE) with 8.5 times less parameters and up to 48 times faster inference speed than previously reported methods. In addition, our model is data efficient, and exceeds previous state of the art on the ADE metric with only 20% of the training data. We propose a kernel function to embed the social interactions between pedestrians within the adjacency matrix. Through qualitative analysis, we show that our model inherited social behaviors that can be expected between pedestrians trajectories. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：行人行为的更好的机器理解实现了更快的建模代理商之间的相互作用，如自动驾驶汽车和人类进步。行人轨迹不仅行人本身，而且还通过与周围物体相互作用的影响。先前的方法通过使用各种集成了不同的教训行人状态汇总的方法模拟这些相互作用。我们建议社会时空图卷积神经网络（社会STGCNN），其替代通过模拟互动为图形的需要汇总的方法。我们的结果表明由20％的最终位移误差（FDE）和在平均位移误差（ADE）与8.5倍以下参数的改进超过现有技术状态的改进和更快的推理速度比以前报道的方法高达48倍。此外，我们的模型数据高效，超过了艺术上的ADE度量以前的状态，只有20％的训练数据。我们提出了一个内核函数嵌入邻接矩阵内的行人之间的社会互动。通过定性分析，我们表明，我们的模型继承，可以行人轨线之间可以预期的社会行为。</font>
</div>


<hr>
<div id="paper27"> <b>27. Set-Constrained Viterbi for Set-Supervised Action Segmentation</b>  <a href="https://arxiv.org/pdf/2002.11925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Todorovic%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sinisa Todorovic</a><br>
<font size="3">
Abstract: This paper is about weakly supervised action segmentation, where ground truth specifies only a set of actions present in a training video. This problem is more challenging than the standard weakly supervised setting where the temporal ordering of actions is provided. Prior work typically uses a classifier that independently labels video frames for generating the pseudo ground truth, and multiple instance learning for training the classifier. We extend this framework by specifying an HMM, which accounts for co-occurrences of action classes and their temporal lengths, and by explicitly training the HMM on a Viterbi-based loss. Our first contribution is the formulation of a new set-constrained Viterbi algorithm (SCV). Given a video, the SCV generates the MAP action segmentation that satisfies the ground truth. This prediction is used as a framewise pseudo ground truth in our HMM training. Our second contribution is a new regularization of learning by a n-pair loss that regularizes the feature affinity of training videos sharing the same action classes. Evaluation on action segmentation and alignment on the Breakfast, MPII Cooking2, Hollywood Extended datasets demonstrates our significant performance improvement for the two tasks over prior work. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文是关于弱监督作用分割，其中地面实况只指定一组动作呈现培训视频。这个问题比标准弱更具挑战性的监督在提供行动的时间顺序设置。以前的工作通常使用分类器独立标签的视频帧生成模拟接地真相，并多示例学习训练的分类。我们通过指定HMM，占操作类的共同出现和他们的时间长度扩展这一框架，并通过显式训练HMM在基于维特比损失。我们的第一个贡献是一套新的约束Viterbi算法（SCV）的制定。鉴于视频时，SCV产生映射动作分割一个满足地面实况。这种预测被用作我们的HMM训练逐帧模拟接地真理。我们的第二个贡献是通过规则化的培训视频共享相同的动作类的功能亲和力的正失去对学习的新正规化。行动细分和定位的早餐评价，MPII Cooking2，好莱坞扩展数据集显示了我们对于这两个任务对现有的工作显著的性能提升。</font>
</div>


<hr>
<div id="paper28"> <b>28. RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference</b>  <a href="https://arxiv.org/pdf/2002.11921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saha%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oindrila Saha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kusupati%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aditya Kusupati</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Simhadri%2C+H+V" target="_blank" rel="noopener" style="color:#0000EE;">Harsha Vardhan Simhadri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Varma%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manik Varma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prateek Jain</a><br>
<font size="3">
Abstract: Pooling operators are key components in most Convolutional Neural Networks (CNNs) as they serve to downsample images, aggregate feature information, and increase receptive field. However, standard pooling operators reduce the feature size gradually to avoid significant loss in information via gross aggregation. Consequently, CNN architectures tend to be deep, computationally expensive and challenging to deploy on RAM constrained devices. We introduce RNNPool, a novel pooling operator based on Recurrent Neural Networks (RNNs), that efficiently aggregate features over large patches of an image and rapidly downsamples its size. Our empirical evaluation indicates that an RNNPool layer(s) can effectively replace multiple blocks in a variety of architectures such as MobileNets (Sandler et al., 2018), DenseNet (Huang et al., 2017) and can be used for several vision tasks like image classification and face detection. That is, RNNPool can significantly decrease computational complexity and peak RAM usage for inference, while retaining comparable accuracy. Further, we use RNNPool to construct a novel real-time face detection method that achieves state-of-the-art MAP within computational budget afforded by a tiny Cortex M4 microcontroller with ~256 KB RAM. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：池运营商是最卷积神经网络（细胞神经网络），因为它们有助于下采样图像，骨料特征信息，并增加感受野的关键部件。然而，标准池运营商降低特征尺寸逐渐以避免通过总聚集信息显著损失。因此，CNN架构往往是深，计算昂贵的和具有挑战性的RAM上受限设备部署。我们引进RNNPool，基于回归神经网络（RNNs）一种新型的池操作，即在图像的大片有效地聚合功能和快速降频的大小。我们的经验评估表明一个RNNPool层（一个或多个）能够有效地替换在各种架构，诸如MobileNets的多个块（Sandler等人，2018），DenseNet（Huang等人，2017），并且可以用于多种视觉任务像图像分类和面部检测。也就是说，RNNPool可以显著减少对推理的计算复杂度和峰RAM的使用，同时保持相当的准确度。此外，我们使用RNNPool来构建实现通过一个微小的Cortex M4微控制器〜256 KB RAM，得到计算的预算之内的状态的最先进的MAP的新颖实时面部检测方法。</font>
</div>


<hr>
<div id="paper29"> <b>29. Unshuffling Data for Improved Generalization</b>  <a href="https://arxiv.org/pdf/2002.11894" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Teney%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Damien Teney</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Abbasnejad%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ehsan Abbasnejad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=van+den+Hengel%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anton van den Hengel</a><br>
<font size="3">
Abstract: The inability to generalize to test data outside the distribution of a training set is at the core of practical limits of machine learning. We show that mixing and shuffling training examples when training deep neural networks is not an optimal practice. On the opposite, partitioning the training data into non-i.i.d. subsets can guide the model to use reliable statistical patterns, while ignoring spurious correlations in the training data. We demonstrate multiple use cases where these subsets are built using unsupervised clustering, prior knowledge, or other meta-data available in existing datasets. The approach is supported by recent results on a causal view of generalization, it is simple to apply, and demonstrably improves generalization. Applied to the task of visual question answering, we obtain state-of-the-art performance on VQA-CP. We also show improvement over data augmentation using equivalent questions on GQA. Finally, we show a small improvement when training a model simultaneously on VQA v2 and Visual Genome, treating them as two distinct environments rather than one aggregated training set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：不能推广到测试数据外的训练集的分布是在机器学习的实际限制的核心。我们表明，混合和洗牌训练样本训练深层神经网络时，是不是最佳做法。在对面，分割训练数据到非i.i.d。子集可以引导使用可靠的统计模式的模型，而忽略了培训数据虚假相关。我们证明，其中这些子集使用无监督聚类，先验知识，或其他元数据在现有的数据集提供内置多个用例。这种方法是通过对泛化的因果鉴于最近结果的支持，这是简单的应用，且令人信服提高泛化。适用于视觉答疑的任务，我们获得VQA-CP国家的最先进的性能。我们还使用上相当于GQA问题表明改善了数据增强。最后，我们展示了一个小的改进上VQA v2和视觉基因组同时训练模型时，将它们视为两种截然不同的环境，而不是一个聚集的训练集。</font>
</div>


<hr>
<div id="paper30"> <b>30. Hierarchical Memory Decoding for Video Captioning</b>  <a href="https://arxiv.org/pdf/2002.11886" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aming Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yahong Han</a><br>
<font size="3">
Abstract: Recent advances of video captioning often employ a recurrent neural network (RNN) as the decoder. However, RNN is prone to diluting long-term information. Recent works have demonstrated memory network (MemNet) has the advantage of storing long-term information. However, as the decoder, it has not been well exploited for video captioning. The reason partially comes from the difficulty of sequence decoding with MemNet. Instead of the common practice, i.e., sequence decoding with RNN, in this paper, we devise a novel memory decoder for video captioning. Concretely, after obtaining representation of each frame through a pre-trained network, we first fuse the visual and lexical information. Then, at each time step, we construct a multi-layer MemNet-based decoder, i.e., in each layer, we employ a memory set to store previous information and an attention mechanism to select the information related to the current input. Thus, this decoder avoids the dilution of long-term information. And the multi-layer architecture is helpful for capturing dependencies between frames and word sequences. Experimental results show that even without the encoding network, our decoder still could obtain competitive performance and outperform the performance of RNN decoder. Furthermore, compared with one-layer RNN decoder, our decoder has fewer parameters. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：字幕经常视频的最新进展采用的解码器回归神经网络（RNN）。然而，RNN是容易稀释长期信息。最近的作品已经证明了记忆网络（MemNet）具有保持长期信息的优势。然而，作为解码器，它没有得到很好的利用了视频字幕。其原因部分源于序列解码的与MemNet难度。代替通常的做法，即，序列与RNN解码的，在本文中，我们设计一种用于视频字幕的新型存储器解码器。具体而言，通过预先训练的网络获得每个帧的表示之后，我们首先熔化​​视觉和词汇信息。然后，在每个时间步骤中，我们构建的多层MemNet基于解码器，即，在每一层中，我们采用的存储器组，以存储以前的信息和注意机制来选择与当前输入的信息。因此，该解码器可避免长期信息稀释。和多层体系结构是用于捕获帧和字序列之间的相关性有帮助。实验结果表明，即使没有编码的网络，我们的解码器仍能获得有竞争力的性能和超越RNN解码器的性能。此外，具有一层RNN解码器相比，我们的解码器具有较少的参数。</font>
</div>


<hr>
<div id="paper31"> <b>31. Defense-PointNet: Protecting PointNet Against Adversarial Attacks</b>  <a href="https://arxiv.org/pdf/2002.11881" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gongbo Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Salem%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tawfiq Salem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jacobs%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nathan Jacobs</a><br>
<font size="3">
Abstract: Despite remarkable performance across a broad range of tasks, neural networks have been shown to be vulnerable to adversarial attacks. Many works focus on adversarial attacks and defenses on 2D images, but few focus on 3D point clouds. In this paper, our goal is to enhance the adversarial robustness of PointNet, which is one of the most widely used models for 3D point clouds. We apply the fast gradient sign attack method (FGSM) on 3D point clouds and find that FGSM can be used to generate not only adversarial images but also adversarial point clouds. To minimize the vulnerability of PointNet to adversarial attacks, we propose Defense-PointNet. We compare our model with two baseline approaches and show that Defense-PointNet significantly improves the robustness of the network against adversarial samples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管在广泛的任务的表现可圈可点，神经网络已经被证明是脆弱的敌对攻击。许多作品专注于三维点云敌对攻击和2D图像的防御，但很少关注的焦点。在本文中，我们的目标是提升PointNet，这对3D点云中最广泛使用的模型之一的对抗鲁棒性。我们采用三维点云的快速倾斜的符号的攻击方法（FGSM），并发现FGSM可以用来生成不仅对抗性图像，而且对抗性的点云。为了尽量减少PointNet以对抗攻击的漏洞，我们建议防御PointNet。我们比较了两种基准模型方法和结果表明，防御PointNet显著改善了对于对抗样本网络的健壮性。</font>
</div>


<hr>
<div id="paper32"> <b>32. GATCluster: Self-Supervised Gaussian-Attention Network for Image  Clustering</b>  <a href="https://arxiv.org/pdf/2002.11863" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuang Niu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Ge Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jimin Liang</a><br>
<font size="3">
Abstract: Deep clustering has achieved state-of-the-art results via joint representation learning and clustering, but still has an inferior performance for the real scene images, e.g., those in ImageNet. With such images, deep clustering methods face several challenges, including extracting discriminative features, avoiding trivial solutions, capturing semantic information, and performing on large-size image datasets. To address these problems, here we propose a self-supervised attention network for image clustering (AttentionCluster). Rather than extracting intermediate features first and then performing the traditional clustering algorithm, AttentionCluster directly outputs semantic cluster labels that are more discriminative than intermediate features and does not need further post-processing. To train the AttentionCluster in a completely unsupervised manner, we design four learning tasks with the constraints of transformation invariance, separability maximization, entropy analysis, and attention mapping. Specifically, the transformation invariance and separability maximization tasks learn the relationships between sample pairs. The entropy analysis task aims to avoid trivial solutions. To capture the object-oriented semantics, we design a self-supervised attention mechanism that includes a parameterized attention module and a soft-attention loss. All the guiding signals for clustering are self-generated during the training process. Moreover, we develop a two-step learning algorithm that is training-friendly and memory-efficient for processing large-size images. Extensive experiments demonstrate the superiority of our proposed method in comparison with the state-of-the-art image clustering benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深集群已实现通过联合代表的学习和聚类国家的先进成果，但仍然有真实场景图像，例如，那些ImageNet的性能较差。有了这样的图像，深聚类方法面临着一些挑战，包括提取判别特征，避免琐碎的解决方案，捕捉语义信息，并进行大尺寸图像数据集。为了解决这些问题，我们在这里提出了一个自我监督的关注网络图像集群（AttentionCluster）。而不是第一提取中间特征，然后进行传统的聚类算法，AttentionCluster直接输出语义聚类的标签，比中间特征更有辨别力，并且不需要进一步的后处理。要培养在完全无人监管的方式AttentionCluster，我们大概设计四个学习任务与改造不变性，可分性最大化，熵分析，并注意映射的约束。具体而言，转型不变性和可分性最大化任务学习样本对之间的关​​系。熵分析任务目标，以避免琐碎的解决方案。为了捕捉面向对象的语义，我们设计了包括参数维护模块和软注意力丧失自我监督的注意机制。所有聚类的引导信号自身生成的训练过程。此外，我们开发了一个两步学习算法训练友好和内存高效处理大尺寸图像。大量的实验证明与国家的最先进的图像聚类基准比较我们提出的方法的优越性。</font>
</div>


<hr>
<div id="paper33"> <b>33. Towards Universal Representation Learning for Deep Face Recognition</b>  <a href="https://arxiv.org/pdf/2002.11841" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yichun Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sohn%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kihyuk Sohn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chandraker%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manmohan Chandraker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jain%2C+A+K" target="_blank" rel="noopener" style="color:#0000EE;">Anil K. Jain</a><br>
<font size="3">
Abstract: Recognizing wild faces is extremely hard as they appear with all kinds of variations. Traditional methods either train with specifically annotated variation data from target domains, or by introducing unlabeled target variation data to adapt from the training data. Instead, we propose a universal representation learning framework that can deal with larger variation unseen in the given training data without leveraging target domain knowledge. We firstly synthesize training data alongside some semantically meaningful variations, such as low resolution, occlusion and head pose. However, directly feeding the augmented data for training will not converge well as the newly introduced samples are mostly hard examples. We propose to split the feature embedding into multiple sub-embeddings, and associate different confidence values for each sub-embedding to smooth the training procedure. The sub-embeddings are further decorrelated by regularizing variation classification loss and variation adversarial loss on different partitions of them. Experiments show that our method achieves top performance on general face recognition datasets such as LFW and MegaFace, while significantly better on extreme benchmarks such as TinyFace and IJB-S. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：因为他们出现了各种变化的认识野生面是极其困难的。传统的方法或者与从目标域中特别注释的变化数据，或者通过引入未标记的目标变化数据火车从训练数据适应。相反，我们提出了一个普遍的代表性学习的框架，可以处理不利用目标领域知识较大的变化在给定的训练数据看不见。我们首先合成训练数据一起一些语义上有意义的变化，如低分辨率，遮挡和头部姿势。然而，直接喂养培训增强数据不收敛以及新近推出的样品大多是硬的例子。我们建议拆分功能嵌入到多个子的嵌入，以及不同的置信度值对每个子嵌入到平滑的训练过程相关联。子的嵌入是通过对它们的不同分区正规化变化分类损失和变化对抗性损失进一步去相关。实验表明，我们的方法实现对一般的面部识别的数据集，如LFW和菲斯顶级的性能，而在极端的基准，如TinyFace和IJB-S显著更好。</font>
</div>


<hr>
<div id="paper34"> <b>34. Joint Unsupervised Learning of Optical Flow and Egomotion with Bi-Level  Optimization</b>  <a href="https://arxiv.org/pdf/2002.11826" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shihao Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dylan Campbell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Miaomiao Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gould%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Gould</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hartley%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Hartley</a><br>
<font size="3">
Abstract: We address the problem of joint optical flow and camera motion estimation in rigid scenes by incorporating geometric constraints into an unsupervised deep learning framework. Unlike existing approaches which rely on brightness constancy and local smoothness for optical flow estimation, we exploit the global relationship between optical flow and camera motion using epipolar geometry. In particular, we formulate the prediction of optical flow and camera motion as a bi-level optimization problem, consisting of an upper-level problem to estimate the flow that conforms to the predicted camera motion, and a lower-level problem to estimate the camera motion given the predicted optical flow. We use implicit differentiation to enable back-propagation through the lower-level geometric optimization layer independent of its implementation, allowing end-to-end training of the network. With globally-enforced geometric constraints, we are able to improve the quality of the estimated optical flow in challenging scenarios and obtain better camera motion estimates compared to other unsupervised learning methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通过将几何约束成一种无监督的深度学习框架，解决刚性场面联合光流和摄像机运动估计的问题。不同于依靠恒定的亮度和光流估计本地平滑现有的方法，我们利用光流和使用极几何摄像机运动之间的全球合作关系。特别是，我们制定光流和摄像机运动为双电平优化问题的预测，由上级问题来估计符合预测的摄像机运动，和一个较低级别的问题来估计照相机的流动运动给定预测的光流。我们使用隐分化通过较低级别的几何优化层独立于其实现的，以使反向传播，从而允许端至端训练网络。在全球强制几何约束，我们能够提高在挑战场景估计光流的质量和获得更好的相机运动估计相比于其它无监督学习方法。</font>
</div>


<hr>
<div id="paper35"> <b>35. Learning to Shade Hand-drawn Sketches</b>  <a href="https://arxiv.org/pdf/2002.11812" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingyuan Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuoru Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bargteil%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adam Bargteil</a><br>
<font size="3">
Abstract: We present a fully automatic method to generate detailed and accurate artistic shadows from pairs of line drawing sketches and lighting directions. We also contribute a new dataset of one thousand examples of pairs of line drawings and shadows that are tagged with lighting directions. Remarkably, the generated shadows quickly communicate the underlying 3D structure of the sketched scene. Consequently, the shadows generated by our approach can be used directly or as an excellent starting point for artists. We demonstrate that the deep learning network we propose takes a hand-drawn sketch, builds a 3D model in latent space, and renders the resulting shadows. The generated shadows respect the hand-drawn lines and underlying 3D space and contain sophisticated and accurate details, such as self-shadowing effects. Moreover, the generated shadows contain artistic effects, such as rim lighting or halos appearing from back lighting, that would be achievable with traditional 3D rendering methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出一个全自动的方法来产生由双线条画草图和照明方向详细和精确的艺术阴影。我们也贡献的线条图和阴影对标记有灯光的方向一千例新的数据集。值得注意的是，所产生的阴影迅速传达绘制场景的底层3D结构。因此，由我们的方法所产生的阴影，可直接或作为一个很好的起点为艺术家使用。我们证明了深度学习网络，我们建议采用手绘草图，建立在潜在空间中的3D模型，并呈现所产生的阴影。所产生的阴影尊重手绘线和底层3D空间和包含精密准确的细节，诸如自阴影效应。此外，所产生的阴影包含艺术效果，如照明轮缘或从背后照明出现晕圈，这将是与传统的3D渲染方法实现的。</font>
</div>


<hr>
<div id="paper36"> <b>36. On Leveraging Pretrained GANs for Limited-Data Generation</b>  <a href="https://arxiv.org/pdf/2002.11810" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Miaoyun Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulai Cong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lawrence Carin</a><br>
<font size="3">
Abstract: Recent work has shown GANs can generate highly realistic images that are indistinguishable by human. Of particular interest here is the empirical observation that most generated images are not contained in training datasets, indicating potential generalization with GANs. That generalizability makes it appealing to exploit GANs to help applications with limited available data, e.g., augment training data to alleviate overfitting. To better facilitate training a GAN on limited data, we propose to leverage already-available GAN models pretrained on large-scale datasets (like ImageNet) to introduce additional common knowledge (which may not exist within the limited data) following the transfer learning idea. Specifically, exampled by natural image generation tasks, we reveal the fact that low-level filters (those close to observations) of both the generator and discriminator of pretrained GANs can be transferred to help the target limited-data generation. For better adaption of the transferred filters to the target domain, we introduce a new technique named adaptive filter modulation (AdaFM), which provides boosted performance over baseline methods. Unifying the transferred filters and the introduced techniques, we present our method and conduct extensive experiments to demonstrate its training efficiency and better performance on limited-data generation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的工作表明甘斯可以产生是由人类无法区分高度逼真的图像。特别令人感兴趣的是这里大多数生成的图像中不包含的训练数据，表明与甘斯潜在泛化的经验观察。这普遍性使它吸引与利用有限的可用数据，例如，扩充训练数据，以减轻过度拟合甘斯来帮助应用程序。为了更好地促进训练数据有限的GaN，我们建议利用预先训练的大型数据集（如ImageNet）引进后的转移学习理念额外的常识（这可能不是有限的数据中存在）已获得GAN模式。具体而言，通过自然的图像生成任务所示例中，我们揭示了一个事实，即在发电机和预训练甘斯的鉴别器两者的低级别过滤器（那些接近的观察结果）可以被转移到帮助目标限于数据生成。所转移的过滤器到目标域的更好的适应，我们引入了一个名为自适应滤波器调制（AdaFM）的新技术，它提供升压超过基线方法的性能。统一转移过滤器和引进技术，我们提出我们的方法，并进行了广泛的实验来证明其培训效率和有限的数据产生更好的性能。</font>
</div>


<hr>
<div id="paper37"> <b>37. Rethinking the Hyperparameters for Fine-tuning</b>  <a href="https://arxiv.org/pdf/2002.11770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaudhari%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pratik Chaudhari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Lam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ravichandran%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avinash Ravichandran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhotika%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rahul Bhotika</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soatto%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefano Soatto</a><br>
<font size="3">
Abstract: Fine-tuning from pre-trained ImageNet models has become the de-facto standard for various computer vision tasks. Current practices for fine-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for fine-tuning. Our findings are based on extensive empirical evaluation for fine-tuning on various transfer learning benchmarks. (1) While prior works have thoroughly investigated learning rate and batch size, momentum for fine-tuning is a relatively unexplored parameter. We find that the value of momentum also affects fine-tuning performance and connect it with previous theoretical findings. (2) Optimal hyperparameters for fine-tuning, in particular, the effective learning rate, are not only dataset dependent but also sensitive to the similarity between the source domain and target domain. This is in contrast to hyperparameters for training from scratch. (3) Reference-based regularization that keeps models close to the initial model does not necessarily apply for "dissimilar" datasets. Our findings challenge common practices of fine-tuning and encourages deep learning practitioners to rethink the hyperparameters for fine-tuning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从预先训练ImageNet模型微调已成为各种计算机视觉任务的事实上的标准。微调目前的做法通常涉及选择超参数的一个特设的选择，保证它们的固定通常用于从头训练值。本文重新审视微调设定超参数的几种常见的做法。我们的研究结果是基于对各种转移学习基准微调的广泛经验评估。 （1）虽然现有作品彻底调查学习率和批量大小，动量进行微调是一个相对未知参数。我们发现，动量的值也会影响微调性能，并将其与以前的理论成果进行连接。 （2）最适超参数进行微调，特别是有效的学习率，不仅是数据集依赖，但到源域和目标域之间的相似性也很敏感。这是相对于超参数从头开始训练。 （3），保持车型接近初始模型基于参考的正则不一定适用于“不同”的数据集。我们的研究结果挑战微调的通行做法，并鼓励深度学习从业者重新考虑进行微调的超参数。</font>
</div>


<hr>
<div id="paper38"> <b>38. Hallucinative Topological Memory for Zero-Shot Visual Planning</b>  <a href="https://arxiv.org/pdf/2002.12336" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kara Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kurutach%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thanard Kurutach</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tung%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christine Tung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Abbeel%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pieter Abbeel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tamar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aviv Tamar</a><br>
<font size="3">
Abstract: In visual planning (VP), an agent learns to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. Most previous works on VP approached the problem by planning in a learned latent space, resulting in low-quality visual plans, and difficult training algorithms. Here, instead, we propose a simple VP method that plans directly in image space and displays competitive performance. We build on the semi-parametric topological memory (SPTM) method: image samples are treated as nodes in a graph, the graph connectivity is learned from image sequence data, and planning can be performed using conventional graph search methods. We propose two modifications on SPTM. First, we train an energy-based graph connectivity function using contrastive predictive coding that admits stable training. Second, to allow zero-shot planning in new domains, we learn a conditional VAE model that generates images given a context of the domain, and use these hallucinated samples for building the connectivity graph and planning. We show that this simple approach significantly outperform the state-of-the-art VP methods, in terms of both plan interpretability and success rate when using the plan to guide a trajectory-following controller. Interestingly, our method can pick up non-trivial visual properties of objects, such as their geometry, and account for it in the plans. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉规划（VP），从一个动态系统的观测的药剂学会计划目标导向的行为离线获得，例如，从自监督机器人相互作用而获得的图像。在VP大多数以前的作品被规划在了解到潜在空间接近了问题，导致低品质的视觉计划，且难以训练算法。在这里，相反，我们提出了一个简单的方法副总裁直接在图像空间和显示竞争力的性能计划。我们建立在半参数拓扑存储器（SPTM）方法：图像样本被视为在图中的节点，该图的连通是由图像序列数据学习，并且可以使用常规的图搜索方法来进行规划。我们建议对SPTM两处修改。首先，我们训练用对比预测编码即承认稳定的训练基于能量的图形连接功能。其次，允许在新领域的零射门的规划中，我们了解到，生成的图像给出的域的范围内，并使用这些幻觉样本构建连通图和规划条件VAE模型。我们表明，这种简单的方法利用该计划来指导的轨迹，跟踪控制时显著优于国家的最先进的VP方法，在这两个计划解释性和成功率方面。有趣的是，我们的方法可以拿起对象的不平凡的视觉特性，如它们的几何形状，并在计划占了它。</font>
</div>


<hr>
<div id="paper39"> <b>39. Coronary Wall Segmentation in CCTA Scans via a Hybrid Net with Contours  Regularization</b>  <a href="https://arxiv.org/pdf/2002.12263" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Huang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaikai Huang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tejero-de-Pablos%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonio Tejero-de-Pablos</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Yamane%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiroaki Yamane</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kurose%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yusuke Kurose</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Iho%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junichi Iho</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tokunaga%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youji Tokunaga</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Horie%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Makoto Horie</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Nishizawa%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Keisuke Nishizawa</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hayashi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yusaku Hayashi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Koyama%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yasushi Koyama</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Harada%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tatsuya Harada</a><br>
<font size="3">
Abstract: Providing closed and well-connected boundaries of coronary artery is essential to assist cardiologists in the diagnosis of coronary artery disease (CAD). Recently, several deep learning-based methods have been proposed for boundary detection and segmentation in a medical image. However, when applied to coronary wall detection, they tend to produce disconnected and inaccurate boundaries. In this paper, we propose a novel boundary detection method for coronary arteries that focuses on the continuity and connectivity of the boundaries. In order to model the spatial continuity of consecutive images, our hybrid architecture takes a volume (i.e., a segment of the coronary artery) as input and detects the boundary of the target slice (i.e., the central slice of the segment). Then, to ensure closed boundaries, we propose a contour-constrained weighted Hausdorff distance loss. We evaluate our method on a dataset of 34 patients of coronary CT angiography scans with curved planar reconstruction (CCTA-CPR) of the arteries (i.e., cross-sections). Experiment results show that our method can produce smooth closed boundaries outperforming the state-of-the-art accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提供冠状动脉闭合和连接良好的边界是必不可少的，以协助心脏病专家在冠状动脉疾病（CAD）的诊断。最近，一些深基于学习的方法已经提出了一种在医学图像边界检测和分割。然而，当施加到冠状壁检测时，往往会产生断开和不准确的边界。在本文中，我们提出了冠状动脉侧重于边界的连续性和连接一个新的边界检测方法。为了连续图像的空间上的连续性进行建模，我们的混合体系结构需要的体积（即，冠状动脉的段）作为输入，并检测所述目标切片的边界（即，段的中心切片）。然后，以确保封闭的界限，我们提出了一个轮廓约束的加权Hausdorff距离的损失。我们评估对34例动脉（即，横截面）的曲面重建（CCTA-CPR）冠状动脉CT血管造影扫描的数据集我们的方法。实验结果表明，我们的方法可以产生平滑的闭合边界超越国家的最先进的精度。</font>
</div>


<hr>
<div id="paper40"> <b>40. Opportunities of a Machine Learning-based Decision Support System for  Stroke Rehabilitation Assessment</b>  <a href="https://arxiv.org/pdf/2002.12261" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+M+H" target="_blank" rel="noopener" style="color:#0000EE;">Min Hun Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Siewiorek%2C+D+P" target="_blank" rel="noopener" style="color:#0000EE;">Daniel P. Siewiorek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Smailagic%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asim Smailagic</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bernardino%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandre Bernardino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Badia%2C+S+B+i" target="_blank" rel="noopener" style="color:#0000EE;">Sergi Bermúdez i Badia</a><br>
<font size="3">
Abstract: Rehabilitation assessment is critical to determine an adequate intervention for a patient. However, the current practices of assessment mainly rely on therapist's experience, and assessment is infrequently executed due to the limited availability of a therapist. In this paper, we identified the needs of therapists to assess patient's functional abilities (e.g. alternative perspective on assessment with quantitative information on patient's exercise motions). As a result, we developed an intelligent decision support system that can identify salient features of assessment using reinforcement learning to assess the quality of motion and summarize patient specific analysis. We evaluated this system with seven therapists using the dataset from 15 patient performing three exercises. The evaluation demonstrates that our system is preferred over a traditional system without analysis while presenting more useful information and significantly increasing the agreement over therapists' evaluation from 0.6600 to 0.7108 F1-scores ($p <0.05$). we discuss the importance of presenting contextually relevant and salient information adaptation to develop a human machine collaborative decision making system. < font>
<br>
<font size="2" style="line-height:30px;">
摘要：康复评估是至关重要的，以确定病人足够的干预。然而，评估目前的做法主要依靠治疗师的经验，并评估经常因治疗的有限执行。在本文中，我们确定治疗师的需求来评估患者的功能能力（与定量评估信息例如替代的角度对患者的练习动作）。因此，我们开发了一个智能决策支持系统，可以使用强化学习，以评估运动的质量和总结病人具体分析鉴定评估的显着特征。我们评估该系统使用的数据集7名理疗师从15患者实施三个练习。评估表明，我们的系统优于传统的系统不加分析，同时提出更多有用的信息和显著增加了治疗师的评估协议，从0.6600到0.7108 F1-得分（$ P <0.05 $）。我们讨论呈现上下文相关的和显着的信息和适应开发的人机协同决策系统的重要性。< font>
</0.05></font></0.05$).></font></div>


<hr>
<div id="paper41"> <b>41. Optimization of Graph Total Variation via Active-Set-based Combinatorial  Reconditioning</b>  <a href="https://arxiv.org/pdf/2002.12236" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/math?searchtype=author&query=Ye%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenzhang Ye</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=M%C3%B6llenhoff%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Möllenhoff</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Wu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Wu</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Cremers%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Cremers</a><br>
<font size="3">
Abstract: Structured convex optimization on weighted graphs finds numerous applications in machine learning and computer vision. In this work, we propose a novel adaptive preconditioning strategy for proximal algorithms on this problem class. Our preconditioner is driven by a sharp analysis of the local linear convergence rate depending on the "active set" at the current iterate. We show that nested-forest decomposition of the inactive edges yields a guaranteed local linear convergence rate. Further, we propose a practical greedy heuristic which realizes such nested decompositions and show in several numerical experiments that our reconditioning strategy, when applied to proximal gradient or primal-dual hybrid gradient algorithm, achieves competitive performances. Our results suggest that local convergence analysis can serve as a guideline for selecting variable metrics in proximal algorithms. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：加权图形结构凸优化发现在机器学习和计算机视觉众多应用。在这项工作中，我们提出了这个问题类近算法一种自适应预处理策略。我们的预处理器通过根据所述“活动集”，在当前迭代局部线性收敛速度的一个尖锐的分析来驱动。我们展示了非活动边缘的该嵌套分解森林​​得到保证局部线性收敛速度。此外，我们提出了一个实用的启发式贪婪实现这样的嵌套分解，并显示在几个数值实验，我们的修复策略，当应用于近端梯度或原 - 对偶混合梯度算法，实现了有竞争力的表演。我们的研究结果表明，当地的收敛性分析可以作为近端算法选择变量指标的指导方针。</font>
</div>


<hr>
<div id="paper42"> <b>42. Multi-source Domain Adaptation in the Deep Learning Era: A Systematic  Survey</b>  <a href="https://arxiv.org/pdf/2002.12169" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sicheng Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reed%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Colorado Reed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kurt Keutzer</a><br>
<font size="3">
Abstract: In many practical applications, it is often difficult and expensive to obtain enough large-scale labeled data to train deep neural networks to their full capability. Therefore, transferring the learned knowledge from a separate, labeled source domain to an unlabeled or sparsely labeled target domain becomes an appealing alternative. However, direct transfer often results in significant performance decay due to domain shift. Domain adaptation (DA) addresses this problem by minimizing the impact of domain shift between the source and target domains. Multi-source domain adaptation (MDA) is a powerful extension in which the labeled data may be collected from multiple sources with different distributions. Due to the success of DA methods and the prevalence of multi-source data, MDA has attracted increasing attention in both academia and industry. In this survey, we define various MDA strategies and summarize available datasets for evaluation. We also compare modern MDA methods in the deep learning era, including latent space transformation and intermediate domain generation. Finally, we discuss future research directions for MDA. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在许多实际应用中，它往往是困难和昂贵的获得大规模足够的标记数据，以深层神经网络训练其全部的能力。因此，传送从一个单独的，标记的源域所学的知识到未标记的或标记的稀少目标域成为一个吸引人的替代方案。然而，直接转移往往导致显著的性能衰减因域转移。域的适应（DA）通过最小化源和目标域之间的域转移的影响解决了这个问题。多源域的适应（MDA）是一个强大的扩展，其中所述标记的数据可以从具有不同分布的多种来源收集。由于DA方法的成功和多源数据的流行，MDA已经引起了学术界和工业界越来越多的关注。在本次调查中，我们定义各种MDA战略和总结评估提供数据集。我们也比较深学习时代现代MDA方法，包括潜在空间转化和中间域的产生。最后，我们讨论了MDA未来的研究方向。</font>
</div>


<hr>
<div id="paper43"> <b>43. Infinitely Wide Graph Convolutional Networks: Semi-supervised Learning  via Gaussian Processes</b>  <a href="https://arxiv.org/pdf/2002.12168" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jilin Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianbing Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Ling Shao</a><br>
<font size="3">
Abstract: Graph convolutional neural networks~(GCNs) have recently demonstrated promising results on graph-based semi-supervised classification, but little work has been done to explore their theoretical properties. Recently, several deep neural networks, e.g., fully connected and convolutional neural networks, with infinite hidden units have been proved to be equivalent to Gaussian processes~(GPs). To exploit both the powerful representational capacity of GCNs and the great expressive power of GPs, we investigate similar properties of infinitely wide GCNs. More specifically, we propose a GP regression model via GCNs~(GPGC) for graph-based semi-supervised learning. In the process, we formulate the kernel matrix computation of GPGC in an iterative analytical form. Finally, we derive a conditional distribution for the labels of unobserved nodes based on the graph structure, labels for the observed nodes, and the feature matrix of all the nodes. We conduct extensive experiments to evaluate the semi-supervised classification performance of GPGC and demonstrate that it outperforms other state-of-the-art methods by a clear margin on all the datasets while being efficient. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图形卷积神经网络〜（GCNs）最近表明看好基于图的半监督分类结果，但很少工作已经完成，探讨其理论特性。最近，几个深神经网络，例如，全连接，并卷积神经网络，具有无限的隐藏单元已经被证明是相当于高斯过程〜（GPS）。为了利用GCNs的两个强大的代表能力和GP的巨大表现力，我们研究了无限宽GCNs的相似的特性。更具体地讲，我们提出了基于图的半监督学习通过GCNs〜（广东电网）一个GP回归模型。在这个过程中，我们制定的迭代分析形式GPGC的核矩阵计算。最后，我们推导出基于所述图结构未观察到的节点的标签条件分布，标签所观察到的结点，并且所有节点的特征矩阵。我们进行了广泛的实验，以评估GPGC的半监督分类性能，并证明它通过一个明确的保证金上的所有数据集优于其他国家的最先进的方法，同时有效。</font>
</div>


<hr>
<div id="paper44"> <b>44. A Comprehensive Approach to Unsupervised Embedding Learning based on AND  Algorithm</b>  <a href="https://arxiv.org/pdf/2002.12158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sungwon Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yizhan Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sungwon Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cha%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meeyoung Cha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheng-Te Li</a><br>
<font size="3">
Abstract: Unsupervised embedding learning aims to extract good representation from data without the need for any manual labels, which has been a critical challenge in many supervised learning tasks. This paper proposes a new unsupervised embedding approach, called Super-AND, which extends the current state-of-the-art model. Super-AND has its unique set of losses that can gather similar samples nearby within a low-density space while keeping invariant features intact against data augmentation. Super-AND outperforms all existing approaches and achieves an accuracy of 89.2% on the image classification task for CIFAR-10. We discuss the practical implications of this method in assisting semi-supervised tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督嵌入学习旨在从数据中提取良好的代表性，而无需任何手动标签，已在许多监督学习任务的关键挑战。本文提出了一种新的无监督的嵌入方法，称为超AND，它扩展了当前国家的最先进的模型。超有其独特的可附近的低密度空间内收集类似的样品，同时保持不变的特点对数据增强完好的损失。超级，优于所有现有的方法，并实现了89.2％的图像分类任务CIFAR-10的精确度。我们在帮助半监督任务中讨论该方法的实际影响。</font>
</div>


<hr>
<div id="paper45"> <b>45. Multi-Cycle-Consistent Adversarial Networks for CT Image Denoising</b>  <a href="https://arxiv.org/pdf/2002.12130" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title45" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinglan Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ding%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yukun Ding</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xiong%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinjun Xiong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Jia%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianjun Jia</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Huang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meiping Huang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhuang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Zhuang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xie%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bike Xie</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chun-Chen Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Shi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiyu Shi</a><br>
<font size="3">
Abstract: CT image denoising can be treated as an image-to-image translation task where the goal is to learn the transform between a source domain $X$ (noisy images) and a target domain $Y$ (clean images). Recently, cycle-consistent adversarial denoising network (CCADN) has achieved state-of-the-art results by enforcing cycle-consistent loss without the need of paired training data. Our detailed analysis of CCADN raises a number of interesting questions. For example, if the noise is large leading to significant difference between domain $X$ and domain $Y$, can we bridge $X$ and $Y$ with an intermediate domain $Z$ such that both the denoising process between $X$ and $Z$ and that between $Z$ and $Y$ are easier to learn? As such intermediate domains lead to multiple cycles, how do we best enforce cycle-consistency? Driven by these questions, we propose a multi-cycle-consistent adversarial network (MCCAN) that builds intermediate domains and enforces both local and global cycle-consistency. The global cycle-consistency couples all generators together to model the whole denoising process, while the local cycle-consistency imposes effective supervision on the process between adjacent domains. Experiments show that both local and global cycle-consistency are important for the success of MCCAN, which outperforms the state-of-the-art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：CT图像去噪可以被视为一个图像到图像的翻译任务，其目的是学习的源域$ X $（噪声图像）和目标域$ Y $（清洁图像）之间的转换。近日，周期一致对抗降噪网络（CCADN）已通过强制循环一致的损失，而无需配对训练数据的取得国家的先进成果。我们的详细CCADN的分析，提出了一些有趣的问题。例如，如果噪音过大导致域$ X $和域名$ Y $之间显著的差异，我们可以弥合$ X $和$ Y $与中间域$ Z $，使得$ X $之间的两个降噪处理和$ Z $和$之间Z $ $和Y $更容易学吗？因此中间领域导致多个周期，我们如何最好的执行周期的一致性？通过这些问题的推动下，我们建议建立中间域和强制执行局部和全局周期一致性的多周期一致对抗网络（MCCAN）。全球周期的一致性夫妇所有发电机一起建模的整个过程去噪，而当地周期的一致性强加给相邻域之间的过程有效监督。实验表明，无论是局部和全局周期的一致性是MCCAN的成功，这优于国家的最先进的重要。</font>
</div>


<hr>
<div id="paper46"> <b>46. Two-stage breast mass detection and segmentation system towards  automated high-resolution full mammogram analysis</b>  <a href="https://arxiv.org/pdf/2002.12079" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title46" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yutong Yan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Conze%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre-Henri Conze</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Quellec%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gwenolé Quellec</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lamard%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mathieu Lamard</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cochener%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Béatrice Cochener</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Coatrieux%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gouenou Coatrieux</a><br>
<font size="3">
Abstract: Mammography is the primary imaging modality used for early detection and diagnosis of breast cancer. Mammography analysis mainly refers to the extraction of regions of interest around tumors, followed by a segmentation step, which is essential to further classification of benign or malignant tumors. Breast masses are the most important findings among breast abnormalities. However, manual delineation of masses from native mammogram is a time consuming and error-prone task. An integrated computer-aided diagnosis system to assist radiologists in automatically detecting and segmenting breast masses is therefore in urgent need. We propose a fully-automated approach that guides accurate mass segmentation from full mammograms at high resolution through a detection stage. First, mass detection is performed by an efficient deep learning approach, You-Only-Look-Once, extended by integrating multi-scale predictions to improve automatic candidate selection. Second, a convolutional encoder-decoder network using nested and dense skip connections is employed to fine-delineate candidate masses. Unlike most previous studies based on segmentation from regions, our framework handles mass segmentation from native full mammograms without user intervention. Trained on INbreast and DDSM-CBIS public datasets, the pipeline achieves an overall average Dice of 80.44% on high-resolution INbreast test images, outperforming state-of-the-art methods. Our system shows promising accuracy as an automatic full-image mass segmentation system. The comprehensive evaluation provided for both detection and segmentation stages reveals strong robustness to the diversity of size, shape and appearance of breast masses, towards better computer-aided diagnosis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：乳腺摄影是用于早期检测和诊断乳腺癌的主成像模态。乳房X线照相分析主要是指肿瘤周围感兴趣区域的提取，随后进行分割步骤，这是良性或恶性肿瘤的进一步分类是必不可少的。乳腺肿块是乳腺异常中最重要的发现。然而，从天然乳房X光检查群众的手动圈定费时又容易出错的时间。因此，一个集成的计算机辅助诊断系统，以协助放射科医师在自动检测和分割乳腺肿块是迫切需要。我们提出了一个完全自动化的方法，从全乳房X线照片以高分辨率通过检测阶段引导精确质量的分割。首先，质量检测是通过一个有效的深层学习方法进行，你-ONLY-查询一次，通过集成多尺度预测，以提高自动选扩展。其次，使用嵌套和密集跳过连接的卷积编码器 - 解码器网络被用于精细描绘候选块。与基于分割从地区大多数以前的研究，我们从无需用户干预本地全乳房X线照片框架处理质量的分割。上训练INbreast和DDSM-CBIS公共数据集，所述管道实现了80.44％的总平均骰子高分辨率INbreast测试图像，表现优于国家的最先进的方法。我们的系统显示有前途的精度自动全图像质量分割系统。提供两种检测与分割阶段的综合评价表明的强稳健性的大小，形状和乳腺肿块的外观的多样性，争取更好的计算机辅助诊断。</font>
</div>


<hr>
<div id="paper47"> <b>47. Understanding and Enhancing Mixed Sample Data Augmentation</b>  <a href="https://arxiv.org/pdf/2002.12047" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title47" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Harris%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ethan Harris</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marcu%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonia Marcu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Painter%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Painter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niranjan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahesan Niranjan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pr%C3%BCgel-Bennett%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adam Prügel-Bennett</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hare%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathon Hare</a><br>
<font size="3">
Abstract: Mixed Sample Data Augmentation (MSDA) has received increasing attention in recent years, with many successful variants such as MixUp and CutMix. Following insight on the efficacy of CutMix in particular, we propose FMix, an MSDA that uses binary masks obtained by applying a threshold to low frequency images sampled from Fourier space. FMix improves performance over MixUp and CutMix for a number of state-of-the-art models across a range of data sets and problem settings. We go on to analyse MixUp, CutMix, and FMix from an information theoretic perspective, characterising learned models in terms of how they progressively compress the input with depth. Ultimately, our analyses allow us to decouple two complementary properties of augmentations, and present a unified framework for reasoning about MSDA. Code for all experiments is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：混合样品数据增强（MSDA）已获得越来越多的关注，近年来，有许多成功的变体，如查询股价和CutMix。继CutMix特别的功效见解，我们提出FMix，使用通过将阈值从傅立叶空间取样低频图像而获得的二进制掩模的MSDA。 FMix改善了查询股价和CutMix性能在一系列的数据集和问题设置了一些国家的最先进的车型。我们继续来分析查询股价，CutMix和FMix从信息理论的角度看，在他们如何逐步与深度压缩输入方面表征了解到的机型。最后，我们的分析让我们去耦增强系统的两个互补性，并提出一个统一的框架推理MSDA。代号为所有实验可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper48"> <b>48. Transductive Few-shot Learning with Meta-Learned Confidence</b>  <a href="https://arxiv.org/pdf/2002.12017" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title48" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kye%2C+S+M" target="_blank" rel="noopener" style="color:#0000EE;">Seong Min Kye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H+B" target="_blank" rel="noopener" style="color:#0000EE;">Hae Beom Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hoirin Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hwang%2C+S+J" target="_blank" rel="noopener" style="color:#0000EE;">Sung Ju Hwang</a><br>
<font size="3">
Abstract: We propose a novel transductive inference framework for metric-based meta-learning models, which updates the prototype of each class with the confidence-weighted average of all the support and query samples. However, a caveat here is that the model confidence may be unreliable, which could lead to incorrect prediction in the transductive setting. To tackle this issue, we further propose to meta-learn to assign correct confidence scores to unlabeled queries. Specifically, we meta-learn the parameters of the distance-metric, such that the model can improve its transductive inference performance on unseen tasks with the generated confidence scores. We also consider various types of uncertainties to further enhance the reliability of the meta-learned confidence. We combine our transductive meta-learning scheme, Meta-Confidence Transduction (MCT) with a novel dense classifier, Dense Feature Matching Network (DFMN), which performs both instance-level and feature-level classification without global average pooling and validate it on four benchmark datasets. Our model achieves state-of-the-art results on all datasets, outperforming existing state-of-the-art models by 11.11% and 7.68% on miniImageNet and tieredImageNet dataset respectively. Further qualitative analysis confirms that this impressive performance gain is indeed due to its ability to assign high confidence to instances with the correct labels. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了基于度量元模型的学习一种新的转导推理的框架，这将更新与信心，加权平均各界的支持和查询样本的每个类的原型。但是，这里需要注意的是，该模型的信心可能是不可靠的，这可能导致不正确的预测在直推设置。为了解决这个问题，我们更建议把元学习正确的置信度分数分配给未标记的查询。具体来说，我们META-学习距离度量的参数，该模型可以提高与所产生的置信分数看不见的任务及其转导推理性能。我们还考虑不同类型的不确定性进一步增强的元了解到信心的可靠性。我们有一个新的密集分类，密集特征匹配网络（DFMN），它没有全球平均池同时执行实例级和功能级分类结合我们的直推式元学习方案，荟萃信心转导（MCT），并验证其在四个基准数据集。我们的模型实现了对所有数据集的国家的最先进的成果，分别由11.11％和miniImageNet和tieredImageNet数据集的7.68％，表现优于现有的国家的最先进的车型。进一步的定性分析证实，这令人印象深刻的性能提升的确是由于其高可信度分配到实例与正确的标签的能力。</font>
</div>


<hr>
<div id="paper49"> <b>49. Face Verification Using 60~GHz 802.11 waveforms</b>  <a href="https://arxiv.org/pdf/2002.11965" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title49" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Hof%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eran Hof</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sanderovich%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amichai Sanderovich</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hemo%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evyatar Hemo</a><br>
<font size="3">
Abstract: Verification of an identity based on the human face radar signature in mmwave is studied. The chipset for 802.11ad/y networking that is cable of operating in a radar mode is used. A dataset with faces of 200 different persons was collected for the testing. Our preliminary study shows promising results for the application of autoencoder for the setup at hand. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于毫米波的中的人脸雷达信号的身份的验证研究。用于802.11ad的/ Y联网的芯片组，处于雷达模式中操作的电缆。与200对不同的人的面部的数据集收集时的测试。我们的初步研究显示有前途的自动编码器的应用手头的安装结果。</font>
</div>


<hr>
<div id="paper50"> <b>50. Supervised Dimensionality Reduction and Visualization using  Centroid-encoder</b>  <a href="https://arxiv.org/pdf/2002.11934" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title50" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomojit Ghosh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kirby%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Kirby</a><br>
<font size="3">
Abstract: Visualizing high-dimensional data is an essential task in Data Science and Machine Learning. The Centroid-Encoder (CE) method is similar to the autoencoder but incorporates label information to keep objects of a class close together in the reduced visualization space. CE exploits nonlinearity and labels to encode high variance in low dimensions while capturing the global structure of the data. We present a detailed analysis of the method using a wide variety of data sets and compare it with other supervised dimension reduction techniques, including NCA, nonlinear NCA, t-distributed NCA, t-distributed MCML, supervised UMAP, supervised PCA, Colored Maximum Variance Unfolding, supervised Isomap, Parametric Embedding, supervised Neighbor Retrieval Visualizer, and Multiple Relational Embedding. We empirically show that centroid-encoder outperforms most of these techniques. We also show that when the data variance is spread across multiple modalities, centroid-encoder extracts a significant amount of information from the data in low dimensional space. This key feature establishes its value to use it as a tool for data visualization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可视化高维数据是数据科学和机器学习的一项重要任务。质心编码器（CE）方法类似于自动编码器，但采用了标签信息的可视化降低的空间，以保持一类的对象靠近在一起。 CE利用非线性和标签编码在低维高方差同时捕捉的数据的全局结构。我们使用多种数据集的呈现的方法的详细的分析，并将其与其他监督尺寸减小技术，包括NCA，非线性NCA，叔分布式NCA，叔分布式MCML比较，监督UMAP，监督PCA，彩色最大方差展开，监督Isomap的CWME，参数嵌入，监督的邻居检索可视化，以及多个关系嵌入。我们经验表明，心编码器性能优于这些技术大部分。我们还表明，当数据变化是跨越多个方式传播，质心编码器在低维空间中的数据中提取信息的显著量。这主要特点确立其价值，用它作为数据可视化的工具。</font>
</div>


<hr>
<div id="paper51"> <b>51. Segmentation-based Method combined with Dynamic Programming for Brain  Midline Delineation</b>  <a href="https://arxiv.org/pdf/2002.11918" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title51" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shen Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kongming Liang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Pan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengwei Pan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ye%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuyang Ye</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiuli Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Yu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yizhou Yu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yizhou Wang</a><br>
<font size="3">
Abstract: The midline related pathological image features are crucial for evaluating the severity of brain compression caused by stroke or traumatic brain injury (TBI). The automated midline delineation not only improves the assessment and clinical decision making for patients with stroke symptoms or head trauma but also reduces the time of diagnosis. Nevertheless, most of the previous methods model the midline by localizing the anatomical points, which are hard to detect or even missing in severe cases. In this paper, we formulate the brain midline delineation as a segmentation task and propose a three-stage framework. The proposed framework firstly aligns an input CT image into the standard space. Then, the aligned image is processed by a midline detection network (MD-Net) integrated with the CoordConv Layer and Cascade AtrousCconv Module to obtain the probability map. Finally, we formulate the optimal midline selection as a pathfinding problem to solve the problem of the discontinuity of midline delineation. Experimental results show that our proposed framework can achieve superior performance on one in-house dataset and one public dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：中线相关的病理图像特征是评价由中风或创伤性脑损伤（TBI）脑受压的严重程度是至关重要的。自动化的中线划分，不仅提高了患者的中风症状或头部外伤的评估和临床决策，但也减少了诊断时间。然而，大多数以前的方法通过定位解剖点，这是很难发现，甚至在严重的情况下失踪中线建模。在本文中，我们制定了大脑中线划分为分段任务，并提出三阶段框架。所提出的框架首先对准输入CT图像为标准的空间。然后，将对准的图像通过与CoordConv层和级联AtrousCconv模块，以获得所述概率图一体的中线检测网络（MD-净）处理。最后，我们制定最佳的中线选择的路径发现问题，解决中线划分的不连续的问题。实验结果表明，该框架可以在一个内部数据集和一个公共数据集实现卓越的性能。</font>
</div>


<hr>
<div id="paper52"> <b>52. Max-Affine Spline Insights into Deep Generative Networks</b>  <a href="https://arxiv.org/pdf/2002.11912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title52" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/stat?searchtype=author&query=Balestriero%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Randall Balestriero</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Paris%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastien Paris</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Baraniuk%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Baraniuk</a><br>
<font size="3">
Abstract: We connect a large class of Generative Deep Networks (GDNs) with spline operators in order to derive their properties, limitations, and new opportunities. By characterizing the latent space partition, dimension and angularity of the generated manifold, we relate the manifold dimension and approximation error to the sample size. The manifold-per-region affine subspace defines a local coordinate basis; we provide necessary and sufficient conditions relating those basis vectors with disentanglement. We also derive the output probability density mapped onto the generated manifold in terms of the latent space density, which enables the computation of key statistics such as its Shannon entropy. This finding also enables the computation of the GDN likelihood, which provides a new mechanism for model comparison as well as providing a quality measure for (generated) samples under the learned distribution. We demonstrate how low entropy and/or multimodal distributions are not naturally modeled by DGNs and are a cause of training instabilities. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们与运营商花一大类剖成深层网络（GDNS）的连接，以获得它们的性质，限制和新的机遇。通过表征所生成的歧管的潜在空间分区，尺寸和角度，我们涉及歧管尺寸和近似误差的样本大小。该歧管每个区域的仿射子空间定义了一个局部坐标基础;我们提供关于与解开那些基本向量的充分必要条件。我们还导出映射到潜在空间密度，这使得关键统计数据的计算方面所产生的歧管中的输出概率密度，例如其香农熵。这一发现也使GDN可能性，这提供了模型比较，以及提供用于学习的分布下（生成）样品的质量测量的新机制的计算。我们演示了如何低熵和/或峰分布不被DGNs自然为蓝本，并训练不稳定性的原因。</font>
</div>


<hr>
<div id="paper53"> <b>53. A Proto-Object Based Dynamic Visual Saliency Model with an FPGA  Implementation</b>  <a href="https://arxiv.org/pdf/2002.11898" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title53" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Molin%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jamal Lottier Molin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thakur%2C+C+S" target="_blank" rel="noopener" style="color:#0000EE;">Chetan Singh Thakur</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Etienne-Cummings%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ralph Etienne-Cummings</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niebur%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ernst Niebur</a><br>
<font size="3">
Abstract: The ability to attend to salient regions of a visual scene is an innate and necessary preprocessing step for both biological and engineered systems performing high-level visual tasks (e.g. object detection, tracking, and classification). Computational efficiency, in regard to processing bandwidth and speed, is improved by only devoting computational resources to salient regions of the visual stimuli. In this paper, we first present a biologically-plausible, bottom-up, dynamic visual saliency model based on the notion of proto-objects. This is achieved by incorporating the temporal characteristics of the visual stimulus into the model, similarly to the manner in which early stages of the human visual system extracts temporal information. This model outperforms state-of-the-art dynamic visual saliency models in predicting human eye fixations on a commonly-used video dataset with associated eye tracking data. Secondly, for this model to have practical applications, it must be capable of performing its computations in real-time under lowpower, small-size, and lightweight constraints. To address this, we introduce a Field-Programmable Gate Array implementation of the model on an Opal Kelly 7350 Kintex-7 board. This novel hardware implementation allows for processing of up to 23.35 frames per second running on a 100 MHz clock -- better than 26x speedup from the software implementation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：能力参加到视觉场景的显着区域是先天和用于执行高层次视觉任务生物和工程系统必要的预处理步骤（例如物体检测，跟踪和分类）。计算效率，关于处理带宽和速度，只由投入计算资源来视觉刺激的显着区域的改善。在本文中，我们首先提出一个生物似是而非的，自下而上的，动态的视觉显着性基于原对象的概念模型。这是通过将视觉刺激的时间特征到模型，其中，所述人类视觉系统的早期阶段中提取的时间信息的方式同样地实现。此模型优于状态的最先进的动态视觉显着性模型中的具有相关联的眼睛跟踪数据通常使用的视频数据集预测人眼的注视。其次，这个模型有实际应用中，必须能够在低功耗，小尺寸，轻便约束实时进行的计算。为了解决这个问题，我们引入了模型的一个上蛋白石凯利7350的Kintex-7板现场可编程门阵列实现。这种新颖的硬件实现允许最多的处理每秒运行23.35帧上的100MHz的时钟 - 从软件实现优于26倍的加速。</font>
</div>


<hr>
<div id="paper54"> <b>54. Gradient Boosted Flows</b>  <a href="https://arxiv.org/pdf/2002.11896" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title54" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Giaquinto%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Giaquinto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arindam Banerjee</a><br>
<font size="3">
Abstract: Normalizing flows (NF) are a powerful framework for approximating posteriors. By mapping a simple base density through invertible transformations, flows provide an exact method of density evaluation and sampling. The trend in normalizing flow literature has been to devise deeper, more complex transformations to achieve greater flexibility. We propose an alternative: Gradient Boosted Flows (GBF) model a variational posterior by successively adding new NF components by gradient boosting so that each new NF component is fit to the residuals of the previously trained components. The GBF formulation results in a variational posterior that is a mixture model, whose flexibility increases as more components are added. Moreover, GBFs offer a wider, not deeper, approach that can be incorporated to improve the results of many existing NFs. We demonstrate the effectiveness of this technique for density estimation and, by coupling GBF with a variational autoencoder, generative modeling of images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：正火流（NF）是近似的后验一个强大的框架。通过可逆的转换映射的简单基密度，流动提供密度评估和取样的精确方法。在标准化流程文学的发展趋势是设计出更深刻，更复杂的转换，以实现更大的灵活性。我们提出了一个替代方案：通过梯度提升，使每个新NF组件装配到先前训练部件的残余先后加入新的NF成分梯度提振流（GBF）建模变后路。所述GBF制剂导致变后这是一个混合模型，其灵活性随着更多的组分加入。此外，GBFS提供更广泛，更深入的不是，可引入改善许多现有NFS的结果的方法。我们证明这种技术密度估计的有效性，并通过与变自动编码，图像的生成模型耦合GBF。</font>
</div>


<hr>
<div id="paper55"> <b>55. BBAND Index: A No-Reference Banding Artifact Predictor</b>  <a href="https://arxiv.org/pdf/2002.11891" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title55" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Tu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengzhong Tu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jessie Lin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yilin Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Adsumilli%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Balu Adsumilli</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bovik%2C+A+C" target="_blank" rel="noopener" style="color:#0000EE;">Alan C. Bovik</a><br>
<font size="3">
Abstract: Banding artifact, or false contouring, is a common video compression impairment that tends to appear on large flat regions in encoded videos. These staircase-shaped color bands can be very noticeable in high-definition videos. Here we study this artifact, and propose a new distortion-specific no-reference video quality model for predicting banding artifacts, called the Blind BANding Detector (BBAND index). BBAND is inspired by human visual models. The proposed detector can generate a pixel-wise banding visibility map and output a banding severity score at both the frame and video levels. Experimental results show that our proposed method outperforms state-of-the-art banding detection algorithms and delivers better consistency with subjective evaluations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：带状伪像，或伪轮廓，是一种常见的视频压缩功能障碍倾向于出现在编码的视频大平坦区域。这些楼梯形色带可在高清视频非常显着。我们在这里学习这件神器，并提出了新的扭曲专用无参考视频质量预测模型的带状伪影，称为盲绑扎检测器（BBAND指数）。 BBAND是由人类视觉模型的启发。该检测器可以产生一个逐像素条带可见度的地图和输出条带严重程度评分在框架和视频电平二者。实验结果表明，国家的最先进的我们的方法优于条纹检测算法，并提供与主观评价较好的一致性。</font>
</div>


<hr>
<div id="paper56"> <b>56. Kernel Bi-Linear Modeling for Reconstructing Data on Manifolds: The  Dynamic-MRI Case</b>  <a href="https://arxiv.org/pdf/2002.11885" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title56" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shetty%2C+G+N" target="_blank" rel="noopener" style="color:#0000EE;">Gaurav N.Shetty</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Slavakis%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Konstantinos Slavakis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nakarmi%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Ukash Nakarmi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scutari%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gesualdo Scutari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ying%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leslie Ying</a><br>
<font size="3">
Abstract: This paper establishes a kernel-based framework for reconstructing data on manifolds, tailored to fit the dynamic-(d)MRI-data recovery problem. The proposed methodology exploits simple tangent-space geometries of manifolds in reproducing kernel Hilbert spaces and follows classical kernel-approximation arguments to form the data-recovery task as a bi-linear inverse problem. Departing from mainstream approaches, the proposed methodology uses no training data, employs no graph Laplacian matrix to penalize the optimization task, uses no costly (kernel) pre-imaging step to map feature points back to the input space, and utilizes complex-valued kernel functions to account for k-space data. The framework is validated on synthetically generated dMRI data, where comparisons against state-of-the-art schemes highlight the rich potential of the proposed approach in data-recovery problems. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文建立了重建流形上的，量身定制，以适应dynamic-（d）数据的基于内核的架构MRI数据恢复问题。所提出的方法利用歧管的简单切空间的几何形状，再生核Hilbert空间，并遵循古典内核逼近参数形成数据恢复任务，双线性反问题。从主流的情况下接近，所提出的方法不使用训练数据，采用无图形拉普拉斯矩阵惩罚优化任务，不使用昂贵的（内核）预成像步骤来映射特征点反馈到输入端的空间，并利用复值的内核功能以考虑k空间数据。该框架被验证的合成产生DMRI数据，其中对国家的最先进的方案比较突出的数据恢复问题所提出的方法的丰富潜力。</font>
</div>


<hr>
<div id="paper57"> <b>57. Comparison of Multi-Class and Binary Classification Machine Learning  Models in Identifying Strong Gravitational Lenses</b>  <a href="https://arxiv.org/pdf/2002.11849" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title57" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Teimoorinia%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossen Teimoorinia</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Toyonaga%2C+R+D" target="_blank" rel="noopener" style="color:#0000EE;">Robert D. Toyonaga</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Fabbro%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastien Fabbro</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&query=Bottrell%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Connor Bottrell</a><br>
<font size="3">
Abstract: Typically, binary classification lens-finding schemes are used to discriminate between lens candidates and non-lenses. However, these models often suffer from substantial false-positive classifications. Such false positives frequently occur due to images containing objects such as crowded sources, galaxies with arms, and also images with a central source and smaller surrounding sources. Therefore, a model might confuse the stated circumstances with an Einstein ring. It has been proposed that by allowing such commonly misclassified image types to constitute their own classes, machine learning models will more easily be able to learn the difference between images that contain real lenses, and images that contain lens imposters. Using Hubble Space Telescope (HST) images, in the F814W filter, we compare the usage of binary and multi-class classification models applied to the lens finding task. From our findings, we conclude there is not a significant benefit to using the multi-class model over a binary model. We will also present the results of a simple lens search using a multi-class machine learning model, and potential new lens candidates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通常情况下，二元分类透镜调查方案被用于透镜候选人和非透镜之间判别。然而，这些模型经常遭受大量的假阳性分类。这种误报频繁发生由于诸如拥挤源，与武器的星系，并且还图像与中央源和更小的周边来源包含对象的图像。因此，一个模型可能与爱因斯坦环混淆规定的情况。有人提出通过允许这样的常见错误分类的图像类型，构成自己的类，机器学习模型将更加容易就能学会包含真正的镜头图像，以及包含镜头骗子图像之间的差异。利用哈勃太空望远镜（HST）的图像，在F814W过滤器，我们比较二进制和多级分类模型应用到镜头发现任务使用。从我们的调查结果，我们得出结论：没有使用多级车型在二元模型显著的好处。我们也将使用多级的机器学习模型，和潜在的新镜头的候选人提出一个简单的镜头搜索的结果。</font>
</div>


<hr>
<div id="paper58"> <b>58. Analysis of diversity-accuracy tradeoff in image captioning</b>  <a href="https://arxiv.org/pdf/2002.11848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title58" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruotian Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shakhnarovich%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gregory Shakhnarovich</a><br>
<font size="3">
Abstract: We investigate the effect of different model architectures, training objectives, hyperparameter settings and decoding procedures on the diversity of automatically generated image captions. Our results show that 1) simple decoding by naive sampling, coupled with low temperature is a competitive and fast method to produce diverse and accurate caption sets; 2) training with CIDEr-based reward using Reinforcement learning harms the diversity properties of the resulting generator, which cannot be mitigated by manipulating decoding parameters. In addition, we propose a new metric AllSPICE for evaluating both accuracy and diversity of a set of captions by a single value. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们调查不同的模型结构，培养目标，超参数设置和解码程序自动生成图片说明多样性的影响。我们的研究结果表明：1）由幼稚采样，加上低温简单的解码是产生不同的和准确的标题集的有竞争力的和快速的方法; 2）训练用强化学习危害所得的发生器，它不能由操纵解码参数来缓解的多样性基于属性苹果酒奖励。此外，我们提出了一个新的度量五香粉由单一值判断一组字幕的准确度和多样性。</font>
</div>


<hr>
<div id="paper59"> <b>59. Improving Robustness of Deep-Learning-Based Image Reconstruction</b>  <a href="https://arxiv.org/pdf/2002.11821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title59" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Raj%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankit Raj</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bresler%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoram Bresler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Li</a><br>
<font size="3">
Abstract: Deep-learning-based methods for different applications have been shown vulnerable to adversarial examples. These examples make deployment of such models in safety-critical tasks questionable. Use of deep neural networks as inverse problem solvers has generated much excitement for medical imaging including CT and MRI, but recently a similar vulnerability has also been demonstrated for these tasks. We show that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of the signal-space as in previous work. In this paper, we propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. We introduce an auxiliary network to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we show for a linear reconstruction scheme the min-max formulation results in a singular-value(s) filter regularized solution, which suppresses the effect of adversarial examples occurring because of ill-conditioning in the measurement matrix. We find that a linear network using the proposed min-max learning scheme indeed converges to the same solution. In addition, for non-linear Compressed Sensing (CS) reconstruction using deep networks, we show significant improvement in robustness using the proposed approach over other methods. We complement the theory by experiments for CS on two different datasets and evaluate the effect of increasing perturbations on trained networks. We find the behavior for ill-conditioned and well-conditioned measurement matrices to be qualitatively different. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：针对不同的应用为基础的深学习方法已被证明容易受到敌对的例子。这些例子都在安全关键任务，这种模式的部署怀疑。深层神经网络，逆解决问题的使用已经产生的医疗成像，包括CT和MRI太多的激动，但最近一个类似的漏洞也已经证实了这些任务。我们表明，这种逆问题的解决者，应该分析和研究在测量空间敌人的效果，而不是信号空间在以前的工作。在本文中，我们提出了修改终端到终端的深学习基于逆问题解决者的培训策略，以提高耐用性。我们引入一个辅助网络，以产生对抗的例子，这是在一个最小 - 最大制剂用于构建鲁棒的图像重建网络。从理论上讲，我们显示了一个线性重建方案的最小 - 最大制剂导致奇异值（一个或多个）过滤器正则化方案，其抑制由于在测量矩阵病态的发生对抗例子的效果。我们发现，使用线性网络所提出的最大最小学习方案确实收敛于相同的解决方案。另外，使用深层网络的非线性压缩感知（CS）重建，我们显示了使用其他方法相比该方法的稳健性显著的改善。我们通过实验对CS在两个不同的数据集相配套的理论和评估培训的网络增加扰动的影响。我们找到病态和状态良好的观测阵的行为是质的不同。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-28</title>
    <url>/2020/02/28/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-28/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Generating Followup Questions for Interpretable Multi-hop Question  Answering <a href="https://arxiv.org/pdf/2002.12344" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Few-shot Natural Language Generation for Task-Oriented Dialog <a href="https://arxiv.org/pdf/2002.12328" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> A Primer in BERTology: What we know about how BERT works <a href="https://arxiv.org/pdf/2002.12327" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Annotation of Emotion Carriers in Personal Narratives <a href="https://arxiv.org/pdf/2002.12196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Improving cross-lingual model transfer by chunking <a href="https://arxiv.org/pdf/2002.12097" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Binarized PMI Matrix: Bridging Word Embeddings and Hyperbolic Spaces <a href="https://arxiv.org/pdf/2002.12005" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Integrating Boundary Assembling into a DNN Framework for Named Entity  Recognition in Chinese Social Media Text <a href="https://arxiv.org/pdf/2002.11910" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue  Dataset <a href="https://arxiv.org/pdf/2002.11893" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Analysis of diversity-accuracy tradeoff in image captioning <a href="https://arxiv.org/pdf/2002.11848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Echo State Neural Machine Translation <a href="https://arxiv.org/pdf/2002.11847" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Universal Phone Recognition with a Multilingual Allophone System <a href="https://arxiv.org/pdf/2002.11800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Train Large, Then Compress: Rethinking Model Size for Efficient Training  and Inference of Transformers <a href="https://arxiv.org/pdf/2002.11794" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Towards Zero-shot Learning for Automatic Phonemic Transcription <a href="https://arxiv.org/pdf/2002.11781" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> SkinAugment: Auto-Encoding Speaker Conversions for Automatic Speech  Translation <a href="https://arxiv.org/pdf/2002.12231" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Attacking Neural Text Detectors <a href="https://arxiv.org/pdf/2002.11768" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Generating Followup Questions for Interpretable Multi-hop Question  Answering</b>  <a href="https://arxiv.org/pdf/2002.12344" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Malon%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christopher Malon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bing Bai</a><br>
<font size="3">
Abstract: We propose a framework for answering open domain multi-hop questions in which partial information is read and used to generate followup questions, to finally be answered by a pretrained single-hop answer extractor. This framework makes each hop interpretable, and makes the retrieval associated with later hops as flexible and specific as for the first hop. As a first instantiation of this framework, we train a pointer-generator network to predict followup questions based on the question and partial information. This provides a novel application of a neural question generation network, which is applied to give weak ground truth single-hop followup questions based on the final answers and their supporting facts. Learning to generate followup questions that select the relevant answer spans against downstream supporting facts, while avoiding distracting premises, poses an exciting semantic challenge for text generation. We present an evaluation using the two-hop bridge questions of HotpotQA. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了回答，其中的部分信息被读取并用来产生后续问题开域多跳的问题，最终由预训练的单跳的答案提取回答了一个框架。这种框架使每一跳解释，并与后来跳灵活和具体为第一跳相关的检索。作为这一框架的第一个实例，我们培养一个指针发电机网络基础上的问题，部分信息来预测后续问题。这提供了神经问题生成网络，并应用于给基础上，最终的答案及其配套事实弱地面实况单跳后续问题的一种新的应用程序。学习产生选择对下游配套事实相关答案跨度后续问题，同时避免分散注意力的前提下，提出了文本生成一个令人兴奋的语义挑战。我们目前使用HotpotQA的两跳桥问题的评估。</font>
</div>


<hr>
<div id="paper2"> <b>2. Few-shot Natural Language Generation for Task-Oriented Dialog</b>  <a href="https://arxiv.org/pdf/2002.12328" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baolin Peng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenguang Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunyuan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiujun Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinchao Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Gao</a><br>
<font size="3">
Abstract: As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical models typically relies on heavily annotated data, which is infeasible for new domains. Therefore, it is pivotal for an NLG system to generalize well with limited labelled data in real applications. To this end, we present FewShotWoz, the first NLG benchmark to simulate the few-shot learning setting in task-oriented dialog systems. Further, we develop the SC-GPT model. It is pre-trained on a large set of annotated NLG corpus to acquire the controllable generation ability, and fine-tuned with only a few domain-specific labels to adapt to new domains. Experiments on FewShotWoz and the large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly outperforms existing methods, measured by various automatic metrics and human evaluations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：作为面向任务的对话系统的重要组成部分，自然语言生成（NLG）模块将对话行为代表了一个语义形式到自然语言的响应。传统的基于模板或统计模型的成功通常依赖于大量标注的数据，这是不可行的新领域。因此，枢转用于NLG系统以在实际应用中的限制标记数据概括良好。为此，我们提出FewShotWoz，第一NLG基准，以模拟几拍学习面向任务的对话系统设置。此外，我们开发的SC-GPT模式。这是预先训练对大量注释NLG语料库的获取可控发电能力，并微调只有少数特定领域的标签，以适应新的领域。上FewShotWoz和大多域-WOZ数据集实验结果表明，所提出的SC-GPT显著优于现有方法，通过各种自动度量和人的评估测量。</font>
</div>


<hr>
<div id="paper3"> <b>3. A Primer in BERTology: What we know about how BERT works</b>  <a href="https://arxiv.org/pdf/2002.12327" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rogers%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Rogers</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kovaleva%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Olga Kovaleva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rumshisky%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Rumshisky</a><br>
<font size="3">
Abstract: Transformer-based models are now widely used in NLP, but we still do not understand a lot about their inner workings. This paper describes what is known to date about the famous BERT model (Devlin et al. 2019), synthesizing over 40 analysis studies. We also provide an overview of the proposed modifications to the model and its training regime. We then outline the directions for further research. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现在基于变压器的模型被广泛应用于NLP，但我们还是不明白了很多关于他们的内部运作。本文介绍了什么是迄今已知约著名的BERT模型（Devlin等。2019），合成了40分析研究。我们还提供了建议修改模型及其培训制度的概述。然后，我们概述了进一步的研究方向。</font>
</div>


<hr>
<div id="paper4"> <b>4. Annotation of Emotion Carriers in Personal Narratives</b>  <a href="https://arxiv.org/pdf/2002.12196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tammewar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aniruddha Tammewar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cervone%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessandra Cervone</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Messner%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eva-Maria Messner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Riccardi%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giuseppe Riccardi</a><br>
<font size="3">
Abstract: We are interested in the problem of understanding personal narratives (PN) spoken or written - recollections of facts, events, and thoughts. In PN, emotion carriers are the speech or text segments that best explain the emotional state of the user. Such segments may include entities, verb or noun phrases. Advanced automatic understanding of PNs requires not only the prediction of the user emotional state but also to identify which events (e.g. {\em the loss of relative} or {\em the visit of grandpa}) or people ( e.g. {\em the old group of high school mates}) carry the emotion manifested during the personal recollection. This work proposes and evaluates an annotation model for identifying emotion carriers in spoken personal narratives. Compared to other text genres such as news and microblogs, spoken PNs are particularly challenging because a narrative is usually unstructured, involving multiple sub-events and characters as well as thoughts and associated emotions perceived by the narrator. In this work, we experiment with annotating emotion carriers from speech transcriptions in the Ulm State-of-Mind in Speech (USoMS) corpus, a dataset of German PNs. We believe this resource could be used for experiments in the automatic extraction of emotion carriers from PN, a task that could provide further advancements in narrative understanding. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们感兴趣的是口头或书面的谅解个人叙述（PN）的问题 - 的事实，事件和想法的回忆。在PN，情感载体是语音或文本段最能说明用户的情绪状态。这样的段可以包括实体，动词或名词短语。期票先进的自动理解，不仅需要用户的情绪状态的预测还要找出哪些事件（例如{\ EM的相对损失}或{\ EM爷爷的访问}）或人（例如{\ EM老高中队友}）的组携带的个人记忆中表现出的情感。这项工作提出了评估和注释模型，用于识别口语个人叙述的情感载体。相对于其他类型的文字，如新闻，微博，口头的PN特别具有挑战性，因为叙述通常是非结构化的，涉及多个子事件和人物，以及思想和叙述者感知相关的情感。在这项工作中，我们实验在语音（USoMS）语料库，德国期票的数据集从语音转录标注情感运营商在乌尔姆州的头脑。我们相信，这种资源可以从PN情感载体的自动提取用于实验任务，可以在叙事理解提供进一步的进展。</font>
</div>


<hr>
<div id="paper5"> <b>5. Improving cross-lingual model transfer by chunking</b>  <a href="https://arxiv.org/pdf/2002.12097" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Das%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ayan Das</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sarkar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sudeshna Sarkar</a><br>
<font size="3">
Abstract: We present a shallow parser guided cross-lingual model transfer approach in order to address the syntactic differences between source and target languages more effectively. In this work, we assume the chunks or phrases in a sentence as transfer units in order to address the syntactic differences between the source and target languages arising due to the differences in ordering of words in the phrases and the ordering of phrases in a sentence separately. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为了更有效地解决源语言和目标语言的语法上的不同呈现出浅解析器引导跨语言模型传递方法。在这项工作中，我们假设在一个句子作为传输单位块或短语，以解决由于在一个句子里分别产生于在词组词和短语的顺序进行排序的不同源语言和目标语言之间的语法差异。</font>
</div>


<hr>
<div id="paper6"> <b>6. Binarized PMI Matrix: Bridging Word Embeddings and Hyperbolic Spaces</b>  <a href="https://arxiv.org/pdf/2002.12005" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Assylbekov%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenisbek Assylbekov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jangeldin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alibi Jangeldin</a><br>
<font size="3">
Abstract: We show analytically that removing sigmoid transformation in the SGNS objective does not harm the quality of word vectors significantly and at the same time is related to factorizing a binarized PMI matrix which, in turn, can be treated as an adjacency matrix of a certain graph. Empirically, such graph is a complex network, i.e. it has strong clustering and scale-free degree distribution, and is tightly connected with hyperbolic spaces. In short, we show the connection between static word embeddings and hyperbolic spaces through the binarized PMI matrix using analytical and empirical methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们证明分析，在SGNS目标去除乙状结肠转型并不显著伤害词矢量的质量，并在同一时间与因式分解，反过来，可以被视为一个特定的邻接矩阵二值化PMI矩阵图形。根据经验，这样的图是一个复杂的网络，即，它具有很强的聚类和无标度分布，并紧密地与双曲空间相连。总之，我们将展示通过使用分析和实证方法的二值化PMI矩阵静态字的嵌入和双曲空间之间的连接。</font>
</div>


<hr>
<div id="paper7"> <b>7. Integrating Boundary Assembling into a DNN Framework for Named Entity  Recognition in Chinese Social Media Text</b>  <a href="https://arxiv.org/pdf/2002.11910" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhaoheng Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Ping Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiang Zhou</a><br>
<font size="3">
Abstract: Named entity recognition is a challenging task in Natural Language Processing, especially for informal and noisy social media text. Chinese word boundaries are also entity boundaries, therefore, named entity recognition for Chinese text can benefit from word boundary detection, outputted by Chinese word segmentation. Yet Chinese word segmentation poses its own difficulty because it is influenced by several factors, e.g., segmentation criteria, employed algorithm, etc. Dealt improperly, it may generate a cascading failure to the quality of named entity recognition followed. In this paper we integrate a boundary assembling method with the state-of-the-art deep neural network model, and incorporate the updated word boundary information into a conditional random field model for named entity recognition. Our method shows a 2% absolute improvement over previous state-of-the-art results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：命名实体识别是自然语言处理一项艰巨的任务，尤其是非正式和嘈杂的社会化媒体的文字。中国字边界也是实体的边界，因此，对于中国的文字命名实体识别可以从字边界检测中受益，通过中国的分词输出。然而，中国的分词带来了自己的困难，因为它是由几个因素，例如，细分的标准，所采用的算法等影响肾阴虚不当，则可能产生级联故障命名实体识别的质量紧随其后。在本文中，我们结合与国家的最先进的深层神经网络模型的边界组装方法，并纳入更新字边界信息转化为命名实体识别条件随机场模型。我们的方法显示了状态的最先进的以前的结果2％的绝对改进。</font>
</div>


<hr>
<div id="paper8"> <b>8. CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue  Dataset</b>  <a href="https://arxiv.org/pdf/2002.11893" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaili Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyan Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minlie Huang</a><br>
<font size="3">
Abstract: To advance multi-domain (cross-domain) dialogue modeling as well as alleviate the shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides. About 60% of the dialogues have cross-domain user goals that favor inter-domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined task-oriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为了推进以及缓解中国面向任务的数据集的不足多域（跨域）对话建模，我们建议CrossWOZ，第一次大规模的中国跨域向导的盎司的面向任务的数据集。它包含6K对话会议和102K话语5个领域，包括酒店，餐厅，景点，地铁和出租车。此外，该语料库包含在用户和系统双方对话国和对话行为的丰富诠释。对话的约60％的有利于域间的依赖，并鼓励在谈话中跨多个域的自然过渡跨域用户的目标。我们还提供用户模拟器和一些标杆车型的流水线面向任务的对话系统，这将有助于研究人员比较和评价这个语料库他们的模型。大尺寸和CrossWOZ丰富的注解使它适合进行调查的各种跨域对话建模任务，如对话状态跟踪，政策学习，用户模拟等。</font>
</div>


<hr>
<div id="paper9"> <b>9. Analysis of diversity-accuracy tradeoff in image captioning</b>  <a href="https://arxiv.org/pdf/2002.11848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruotian Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shakhnarovich%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gregory Shakhnarovich</a><br>
<font size="3">
Abstract: We investigate the effect of different model architectures, training objectives, hyperparameter settings and decoding procedures on the diversity of automatically generated image captions. Our results show that 1) simple decoding by naive sampling, coupled with low temperature is a competitive and fast method to produce diverse and accurate caption sets; 2) training with CIDEr-based reward using Reinforcement learning harms the diversity properties of the resulting generator, which cannot be mitigated by manipulating decoding parameters. In addition, we propose a new metric AllSPICE for evaluating both accuracy and diversity of a set of captions by a single value. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们调查不同的模型结构，培养目标，超参数设置和解码程序自动生成图片说明多样性的影响。我们的研究结果表明：1）由幼稚采样，加上低温简单的解码是产生不同的和准确的标题集的有竞争力的和快速的方法; 2）训练用强化学习危害所得的发生器，它不能由操纵解码参数来缓解的多样性基于属性苹果酒奖励。此外，我们提出了一个新的度量五香粉由单一值判断一组字幕的准确度和多样性。</font>
</div>


<hr>
<div id="paper10"> <b>10. Echo State Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.11847" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankush Garg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Ge</a><br>
<font size="3">
Abstract: We present neural machine translation (NMT) models inspired by echo state network (ESN), named Echo State NMT (ESNMT), in which the encoder and decoder layer weights are randomly generated then fixed throughout training. We show that even with this extremely simple model construction and training procedure, ESNMT can already reach 70-80% quality of fully trainable baselines. We examine how spectral radius of the reservoir, a key quantity that characterizes the model, determines the model behavior. Our findings indicate that randomized networks can work well even for complicated sequence-to-sequence prediction NLP tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：回送状态网络（ESN），命名为回声状态NMT（ESNMT），其中，所述编码器和译码器层的权重是随机生成的，然后固定在整个训练启发我们本神经机器翻译（NMT）模型。我们发现，即使有这个非常简单的模型构建和训练过程，ESNMT已经可以达到充分训练的基准70-80％的质量。我们研究水库谱半径，表征模型中的核心数量，如何确定模型的行为。我们的研究结果表明，随机网络甚至可以为复杂的序列，以序列预测NLP任务工作。</font>
</div>


<hr>
<div id="paper11"> <b>11. Universal Phone Recognition with a Multilingual Allophone System</b>  <a href="https://arxiv.org/pdf/2002.11800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinjian Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dalmia%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siddharth Dalmia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juncheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Littell%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Littell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiali Yao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonios Anastasopoulos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mortensen%2C+D+R" target="_blank" rel="noopener" style="color:#0000EE;">David R. Mortensen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Graham Neubig</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W" target="_blank" rel="noopener" style="color:#0000EE;">Alan W Black</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Metze</a><br>
<font size="3">
Abstract: Multilingual models can improve language processing, particularly for low resource situations, by sharing parameters across languages. Multilingual acoustic models, however, generally ignore the difference between phonemes (sounds that can support lexical contrasts in a particular language) and their corresponding phones (the sounds that are actually spoken, which are language independent). This can lead to performance degradation when combining a variety of training languages, as identically annotated phonemes can actually correspond to several different underlying phonetic realizations. In this work, we propose a joint model of both language-independent phone and language-dependent phoneme distributions. In multilingual ASR experiments over 11 languages, we find that this model improves testing performance by 2% phoneme error rate absolute in low-resource conditions. Additionally, because we are explicitly modeling language-independent phones, we can build a (nearly-)universal phone recognizer that, when combined with the PHOIBLE large, manually curated database of phone inventories, can be customized into 2,000 language dependent recognizers. Experiments on two low-resourced indigenous languages, Inuktitut and Tusom, show that our recognizer achieves phone accuracy improvements of more than 17%, moving a step closer to speech recognition for all languages in the world. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多语言模型可以提高语言处理，特别是对低资源情况，针对不同的语言共享参数。语言声学模型，然而，通常忽略音素（声音，可以支持在一个特定的语言词汇对比）及其相应的手机（这实际上是讲的声音，这是独立的语言）之间的差异。结合各种培训语言的时候，因为相同的注解音素实际上可以对应多个不同的基础语音的实现这可能会导致性能下降。在这项工作中，我们提出了两种语言无关的电话和语言相关的音素分布的联合模型。在多语言ASR实验超过11种语言，我们发现，这个模型提高了2％音素误差率在低资源条件绝对性能测试。此外，因为我们明确地建模语言无关的电话，我们可以建立一个（nearly-）通用手机识别，当与手机库存的PHOIBLE大，人工监管的数据库相结合，可定制成2000语言相关的识别。两个资源不足地区的土著语言，因纽特语和Tusom实验，证明我们的识别器实现了超过17％的手机正确率提高，移动更近了一步语音识别所有的语言在世界上。</font>
</div>


<hr>
<div id="paper12"> <b>12. Train Large, Then Compress: Rethinking Model Size for Efficient Training  and Inference of Transformers</b>  <a href="https://arxiv.org/pdf/2002.11794" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuohan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wallace%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Wallace</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sheng Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keutzer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kurt Keutzer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dan Klein</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gonzalez%2C+J+E" target="_blank" rel="noopener" style="color:#0000EE;">Joseph E. Gonzalez</a><br>
<font size="3">
Abstract: Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由于硬件资源有限，培养深度学习模型的目的通常是为了最大限度地提高准确性受到的训练和推理的时间和内存的限制。我们研究模型的大小在此设置的影响，专注于变压器模型由计算限制NLP任务：自我监督的训练前和高资源机器翻译。我们首先表明，即使较小的变压器模型执行每次迭代更快，更广泛和更深入的模型显著更少的步骤收敛。此外，这种加速收敛通常赶不上使用更大型号的额外计算开销。因此，计算最有效的培训战略是后少量的迭代，以违反直觉训练非常大的模型，但停止。这就导致了一个明显的权衡的大型变压器模型的训练效率和小型变压器模型推理效率之间。然而，我们表明，大的模型更加坚固的压缩技术，如量化和修剪比小车型。因此，人们可以得到两全其美：严重压缩，大型模型实现比轻微压缩，小型号更高的精度。</font>
</div>


<hr>
<div id="paper13"> <b>13. Towards Zero-shot Learning for Automatic Phonemic Transcription</b>  <a href="https://arxiv.org/pdf/2002.11781" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinjian Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dalmia%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siddharth Dalmia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mortensen%2C+D+R" target="_blank" rel="noopener" style="color:#0000EE;">David R. Mortensen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juncheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W" target="_blank" rel="noopener" style="color:#0000EE;">Alan W Black</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Metze</a><br>
<font size="3">
Abstract: Automatic phonemic transcription tools are useful for low-resource language documentation. However, due to the lack of training sets, only a tiny fraction of languages have phonemic transcription tools. Fortunately, multilingual acoustic modeling provides a solution given limited audio training data. A more challenging problem is to build phonemic transcribers for languages with zero training data. The difficulty of this task is that phoneme inventories often differ between the training languages and the target language, making it infeasible to recognize unseen phonemes. In this work, we address this problem by adopting the idea of zero-shot learning. Our model is able to recognize unseen phonemes in the target language without any training data. In our model, we decompose phonemes into corresponding articulatory attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over articulatory attributes, and then compute phoneme distributions with a customized acoustic model. We evaluate our model by training it using 13 languages and testing it using 7 unseen languages. We find that it achieves 7.7% better phoneme error rate on average over a standard multilingual model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动音位标工具是资源少的语言的文档非常有用。然而，由于缺乏训练集，只有语言的一小部分有音位标音工具。幸运的是，语言声学建模提供给有限的音频训练数据的解决方案。一个更具挑战性的问题是建立音素誊写与零个的训练数据的语言。这个任务的难度是音素库存经常训练语言和目标语言之间的差异，使得它不可能认识到看不见的音素。在这项工作中，我们采用零射门学习的想法解决这个问题。我们的模型能够识别在目标语言音素看不见的，没有任何的训练数据。在我们的模型中，我们分解音素为元音辅音和成相应的关节等属性。而不是直接预测音素的，我们首先预测了关节的属性分布，然后计算音素分布与定制声学模型。我们用13种语言训练，并使用7种看不见的语言测试它评估我们的模型。我们发现它在一个标准的多语言模型达到平均7.7％更好的音素的错误率。</font>
</div>


<hr>
<div id="paper14"> <b>14. SkinAugment: Auto-Encoding Speaker Conversions for Automatic Speech  Translation</b>  <a href="https://arxiv.org/pdf/2002.12231" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=McCarthy%2C+A+D" target="_blank" rel="noopener" style="color:#0000EE;">Arya D. McCarthy</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Puzon%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liezl Puzon</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Pino%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juan Pino</a><br>
<font size="3">
Abstract: We propose autoencoding speaker conversion for training data augmentation in automatic speech translation. This technique directly transforms an audio sequence, resulting in audio synthesized to resemble another speaker's voice. Our method compares favorably to SpecAugment on English$\to$French and English$\to$Romanian automatic speech translation (AST) tasks as well as on a low-resource English automatic speech recognition (ASR) task. Further, in ablations, we show the benefits of both quantity and diversity in augmented data. Finally, we show that we can combine our approach with augmentation by machine-translated transcripts to obtain a competitive end-to-end AST model that outperforms a very strong cascade model on an English$\to$French AST task. Our method is sufficiently general that it can be applied to other speech generation and analysis tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出autoencoding扬声器转换为自动语音翻译训练数据增强。此技术直接变换的音频序列，从而导致合成类似于另一个说话者的声音的音频。我们的方法相比毫不逊色SpecAugment英语$ \至$法语和英语$ \达罗马尼亚自动语音翻译（AST）的任务，以及在低资源英语自动语音识别（ASR）的任务。此外，在消融，我们展示的数量和多样性的增强数据的好处。最后，我们表明，我们可以通过机器翻译的成绩单与增强结合我们的方法来获得优于上一个英语$ \至$法国AST任务非常强的级联模型有竞争力的终端到终端的AST模型。我们的方法是足够的一般，它可以应用到其他语音生成和分析任务。</font>
</div>


<hr>
<div id="paper15"> <b>15. Attacking Neural Text Detectors</b>  <a href="https://arxiv.org/pdf/2002.11768" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wolff%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Max Wolff</a><br>
<font size="3">
Abstract: Machine learning based language models have recently made significant progress, which introduces a danger to spread misinformation. To combat this potential danger, several methods have been proposed for detecting text written by these language models. This paper presents two classes of black-box attacks on these detectors, one which randomly replaces characters with homoglyphs, and the other a simple scheme to purposefully misspell words. The homoglyph and misspelling attacks decrease a popular neural text detector's recall on neural text from 97.44% to 0.26% and 22.68%, respectively. Results also indicate that the attacks are transferable to other neural text detectors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于机器学习的语言模型最近取得显著的进展，介绍以传播误传的危险。为了解决这个潜在的危险，几种方法已经被提出用于检测由这些语言模型的书面文字。本文呈现两类对这些检测器，其中一个与随机替换同形字字符黑盒攻击，而另一种简单的方案来有目的地拼错单词。在同形字拼写错误和攻击从97.44％减少对神经的文本流行的神经文本探测器的召回至0.26％和22.68％，分别。结果还表明，这些攻击是转移到其他神经文本探测器。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-27</title>
    <url>/2020/02/27/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-27/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Graphcore C2 Card performance for image-based deep learning application:  A Report <a href="https://arxiv.org/pdf/2002.11670" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Pedestrian Models for Autonomous Driving Part I: low level models, from  sensing to tracking <a href="https://arxiv.org/pdf/2002.11669" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in  Multi-output Classification Problems <a href="https://arxiv.org/pdf/2002.11644" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Dynamic Graph Correlation Learning for Disease Diagnosis with Incomplete  Labels <a href="https://arxiv.org/pdf/2002.11629" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video  Super-Resolution <a href="https://arxiv.org/pdf/2002.11616" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> ARMA Nets: Expanding Receptive Field for Dense Prediction <a href="https://arxiv.org/pdf/2002.11609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Automatically Searching for U-Net Image Translator Architecture <a href="https://arxiv.org/pdf/2002.11581" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Object Relational Graph with Teacher-Recommended Learning for Video  Captioning <a href="https://arxiv.org/pdf/2002.11566" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> CookGAN: Meal Image Synthesis from Ingredients <a href="https://arxiv.org/pdf/2002.11493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Learning a Directional Soft Lane Affordance Model for Road Scenes Using  Self-Supervision <a href="https://arxiv.org/pdf/2002.11477" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Towards Interpretable Semantic Segmentation via Gradient-weighted Class  Activation Mapping <a href="https://arxiv.org/pdf/2002.11434" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Efficient Semantic Video Segmentation with Per-frame Inference <a href="https://arxiv.org/pdf/2002.11433" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Deform-GAN:An Unsupervised Learning Model for Deformable Registration <a href="https://arxiv.org/pdf/2002.11430" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Performance Evaluation of Deep Generative Models for Generating  Hand-Written Character Images <a href="https://arxiv.org/pdf/2002.11424" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Disentangling Image Distortions in Deep Feature Space <a href="https://arxiv.org/pdf/2002.11409" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Force-Ultrasound Fusion:Bringing Spine Robotic-US to the Next "Level" <a href="https://arxiv.org/pdf/2002.11404" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Controllable Descendant Face Synthesis <a href="https://arxiv.org/pdf/2002.11376" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Adversarial Attack on Deep Product Quantization Network for Image  Retrieval <a href="https://arxiv.org/pdf/2002.11374" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> PuzzleNet: Scene Text Detection by Segment Context Graph Learning <a href="https://arxiv.org/pdf/2002.11371" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Unsupervised Temporal Video Segmentation as an Auxiliary Task for  Predicting the Remaining Surgery Duration <a href="https://arxiv.org/pdf/2002.11367" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Rethinking the Route Towards Weakly Supervised Object Localization <a href="https://arxiv.org/pdf/2002.11359" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Refined Gate: A Simple and Effective Gating Mechanism for Recurrent  Units <a href="https://arxiv.org/pdf/2002.11338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Self-supervised Image Enhancement Network: Training with Low Light  Images Only <a href="https://arxiv.org/pdf/2002.11300" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Generalized ODIN: Detecting Out-of-distribution Image without Learning  from Out-of-distribution Data <a href="https://arxiv.org/pdf/2002.11297" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Adversarial Ranking Attack and Defense <a href="https://arxiv.org/pdf/2002.11293" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Generalized Product Quantization Network for Semi-supervised Hashing <a href="https://arxiv.org/pdf/2002.11281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Learning Light Field Angular Super-Resolution via a Geometry-Aware  Network <a href="https://arxiv.org/pdf/2002.11263" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Multi-Attribute Guided Painting Generation <a href="https://arxiv.org/pdf/2002.11261" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Back to the Future: Joint Aware Temporal Deep Learning 3D Human Pose  Estimation <a href="https://arxiv.org/pdf/2002.11251" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Super-Resolving Commercial Satellite Imagery Using Realistic Training  Data <a href="https://arxiv.org/pdf/2002.11248" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Transfer Learning from Synthetic to Real-Noise Denoising with Adaptive  Instance Normalization <a href="https://arxiv.org/pdf/2002.11244" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Style Transfer for Light Field Photography <a href="https://arxiv.org/pdf/2002.11220" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Geometric Fusion via Joint Delay Embeddings <a href="https://arxiv.org/pdf/2002.11201" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Unsupervised Semantic Attribute Discovery and Control in Generative  Models <a href="https://arxiv.org/pdf/2002.11169" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> CLARA: Clinical Report Auto-completion <a href="https://arxiv.org/pdf/2002.11701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Inceptive Event Time-Surfaces for Object Classification Using  Neuromorphic Cameras <a href="https://arxiv.org/pdf/2002.11656" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Revisiting Ensembles in an Adversarial Context: Improving Natural  Accuracy <a href="https://arxiv.org/pdf/2002.11572" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Region of Interest Identification for Brain Tumors in Magnetic Resonance  Images <a href="https://arxiv.org/pdf/2002.11509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Unpaired Image Super-Resolution using Pseudo-Supervision <a href="https://arxiv.org/pdf/2002.11397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> CheXpedition: Investigating Generalization Challenges for Translation of  Chest X-Ray Algorithms to the Clinical Setting <a href="https://arxiv.org/pdf/2002.11379" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Invariance vs. Robustness of Neural Networks <a href="https://arxiv.org/pdf/2002.11318" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation  (VIN) <a href="https://arxiv.org/pdf/2002.11310" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Deep Learning and Statistical Models for Time-Critical Pedestrian  Behaviour Prediction <a href="https://arxiv.org/pdf/2002.11226" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> End-to-End Models for the Analysis of System 1 and System 2 Interactions  based on Eye-Tracking Data <a href="https://arxiv.org/pdf/2002.11192" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Graphcore C2 Card performance for image-based deep learning application:  A Report</b>  <a href="https://arxiv.org/pdf/2002.11670" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kacher%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ilyes Kacher</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Portaz%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maxime Portaz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Randrianarivo%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hicham Randrianarivo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peyronnet%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sylvain Peyronnet</a><br>
<font size="3">
Abstract: Recently, Graphcore has introduced an IPU Processor for accelerating machine learning applications. The architecture of the processor has been designed to achieve state of the art performance on current machine intelligence models for both training and inference. In this paper, we report on a benchmark in which we have evaluated the performance of IPU processors on deep neural networks for inference. We focus on deep vision models such as ResNeXt. We report the observed latency, throughput and energy efficiency. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，Graphcore推出了一款IPU处理器加速机器学习应用。处理器的架构已被设计为实现对现有机器智能机型的训练和推理的先进的性能。在本文中，我们在其中我们已经评估对推理深层神经网络IPU处理器性能的基准测试报告。我们专注于深远景等车型ResNeXt。我们报告所观察到的延迟，吞吐量和能效。</font>
</div>


<hr>
<div id="paper2"> <b>2. Pedestrian Models for Autonomous Driving Part I: low level models, from  sensing to tracking</b>  <a href="https://arxiv.org/pdf/2002.11669" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Camara%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fanta Camara</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bellotto%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicola Bellotto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cosar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Serhan Cosar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nathanael%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dimitris Nathanael</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Althoff%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Althoff</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingyuan Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruenz%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Johannes Ruenz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dietrich%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">André Dietrich</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fox%2C+C+W" target="_blank" rel="noopener" style="color:#0000EE;">Charles W. Fox</a><br>
<font size="3">
Abstract: Autonomous vehicles (AVs) must share space with human pedestrians, both in on-road cases such as cars at pedestrian crossings and off-road cases such as delivery vehicles navigating through crowds on high-streets. Unlike static and kinematic obstacles, pedestrians are active agents with complex, interactive motions. Planning AV actions in the presence of pedestrians thus requires modelling of their probable future behaviour as well as detection and tracking which enable such modelling. This narrative review article is Part I of a pair which together survey the current technology stack involved in this process, organising recent research into a hierarchical taxonomy ranging from low level image detection to high-level psychology models, from the perspective of an AV designer. This self-contained Part I covers the lower levels of this stack, from sensing, through detection and recognition, up to tracking of pedestrians. Technologies at these levels are found to be mature and available as foundations for use in higher level systems such as behaviour modelling, prediction and interaction control. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自主车（AVS）与人类的行人，无论是在路上的情况下，如在行人过路轿车和越野情况下，如送货车辆通过上高街道人群导航必须共享空间。不同于静态和动态的障碍物，行人都用复杂的，互动的运动活性剂。在行人的存在规划AV行动因此需要他们的未来很可能行为，以及检测和跟踪这使得这种造型的造型。这个故事的评论文章是一对一起调查涉及在此过程中，目前的技术堆栈，组织最近的研究层级分类从低级别图像检测到高层次的心理模型，从AV设计师的角度来看的第一部分。这种自足第一部分涵盖这个栈的较低水平，从检测，通过检测与识别，达到行人的跟踪。在这些水平技术被发现成熟和可作为基础，为在更高层次的系统，如行为建模，预测和控制的交互使用。</font>
</div>


<hr>
<div id="paper3"> <b>3. A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in  Multi-output Classification Problems</b>  <a href="https://arxiv.org/pdf/2002.11644" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Proen%C3%A7a%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hugo Proença</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yaghoubi%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ehsan Yaghoubi</a><br>
<font size="3">
Abstract: This paper describes one objective function for learning semantically coherent feature embeddings in multi-output classification problems, i.e., when the response variables have dimension higher than one. In particular, we consider the problems of identity retrieval and soft biometrics in visual surveillance environments, which have been attracting growing interests. Inspired by the triplet loss function, we propose a generalization of that concept: a quadruplet loss, that 1) defines a metric that analyzes the number of agreeing labels between pairs of elements; and 2) disregards the notion of anchor, replacing d(A1,A2) < d(A1,B) by d(A,B) < d(C,D) distance constraints, according to such perceived semantic similarity between the elements of each pair. Inherited from the triplet loss formulation, our proposal also privileges small distances between positive pairs, but also explicitly enforces that the distances between negative pairs directly correspond to their similarity in terms of the number of agreeing labels. This typically yields feature embeddings with a strong correspondence between the classes centroids and their semantic descriptions, i.e., where elements that share some of the labels are closer to each other in the destiny space than elements with fully disjoint classes membership. Also, in opposition to its triplet counterpart, the proposed loss is not particularly sensitive to the way learning pairs are mined, being agnostic with regard to demanding criteria for mining learning instances (such as the semi-hard pairs of triplet loss). Our experiments were carried out in four different datasets (BIODI, LFW, Megaface and PETA) and validate our assumptions, showing highly promising results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文描述了一个目标函数用于学习在多输出分类问题，即语义上相干的嵌入特征，当响应变量具有尺寸高于1。特别是，我们考虑的身份检索的视频监控环境，已吸引了越来越多的利益问题和柔软的生物识别技术。由三重态损耗函数的启发，我们建议概念的概括：四元损失，即1）定义的度量，其分析同意对元件之间的标签的数量;和2）忽略锚的概念，代替d（A1，A2）<d（a1，b）由d（a，b）<d（c，d）的距离的限制，根据的元件之间的这种感知语义相似度每一对。从三重损失配方继承，我们的建议也正对之间的权限小的距离，但也明确强制执行负对之间的距离直接对应于它们的相似性在同意标签的数量方面。这典型地产生特征的嵌入用的类质心和它们的语义描述，即，其中共享一些标签元件处于命运空间相互接近比具有完全不相交的类成员资格的元素之间的强的对应关系。此外，在反对它的三重对应，所提出的损失不学习对开采，是不可知对于要求苛刻的标准，挖掘学习情况（如半硬对三重损失）的方式特别敏感。我们的实验在四种不同的数据集（biodi，lfw，菲斯和peta）和验证我们的假设，显示出大有希望的结果进行。< font>
</d（a1，b）由d（a，b）<d（c，d）的距离的限制，根据的元件之间的这种感知语义相似度每一对。从三重损失配方继承，我们的建议也正对之间的权限小的距离，但也明确强制执行负对之间的距离直接对应于它们的相似性在同意标签的数量方面。这典型地产生特征的嵌入用的类质心和它们的语义描述，即，其中共享一些标签元件处于命运空间相互接近比具有完全不相交的类成员资格的元素之间的强的对应关系。此外，在反对它的三重对应，所提出的损失不学习对开采，是不可知对于要求苛刻的标准，挖掘学习情况（如半硬对三重损失）的方式特别敏感。我们的实验在四种不同的数据集（biodi，lfw，菲斯和peta）和验证我们的假设，显示出大有希望的结果进行。<></font></div>


<hr>
<div id="paper4"> <b>4. Dynamic Graph Correlation Learning for Disease Diagnosis with Incomplete  Labels</b>  <a href="https://arxiv.org/pdf/2002.11629" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daizong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuangjie Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pan Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zichuan Xu</a><br>
<font size="3">
Abstract: Disease diagnosis on chest X-ray images is a challenging multi-label classification task. Previous works generally classify the diseases independently on the input image without considering any correlation among diseases. However, such correlation actually exists, for example, Pleural Effusion is more likely to appear when Pneumothorax is present. In this work, we propose a Disease Diagnosis Graph Convolutional Network (DD-GCN) that presents a novel view of investigating the inter-dependency among different diseases by using a dynamic learnable adjacency matrix in graph structure to improve the diagnosis accuracy. To learn more natural and reliable correlation relationship, we feed each node with the image-level individual feature map corresponding to each type of disease. To our knowledge, our method is the first to build a graph over the feature maps with a dynamic adjacency matrix for correlation learning. To further deal with a practical issue of incomplete labels, DD-GCN also utilizes an adaptive loss and a curriculum learning strategy to train the model on incomplete labels. Experimental results on two popular chest X-ray (CXR) datasets show that our prediction accuracy outperforms state-of-the-arts, and the learned graph adjacency matrix establishes the correlation representations of different diseases, which is consistent with expert experience. In addition, we apply an ablation study to demonstrate the effectiveness of each component in DD-GCN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：胸部X射线图像的疾病诊断是一个具有挑战性的多标签分类任务。先前的工作通常独立地进行分类的疾病在输入图像上，而不考虑疾病中的任何相关性。然而，这样的相关性实际上存在，例如，胸腔积液是更可能出现时气胸存在。在这项工作中，我们提出了一种疾病诊断格拉夫卷积网络（DD-GCN）呈现在图结构使用动态可学习邻接矩阵来提高诊断精度调查不同的疾病之间的相互依存的新颖图。要了解更自然和可靠的相关性的关系，我们进料与对应于每种类型的疾病的图像级单独的特征图中的每个节点。据我们所知，我们的方法是第一个通过与相关学习动态邻接矩阵特征地图建立一个图表。为了进一步处理不完整的标签的实际问题，DD-GCN还采用了自适应损失和课程学习策略训练不完整的标签模型。两个流行的胸部X射线实验结果（CXR）数据集显示，我们的预测精度性能优于国家的最艺术，学习邻接矩阵建立不同的疾病，这与专家的经验相一致的相关陈述。此外，我们采用消融研究，以证明DD-GCN每个组件的有效性。</font>
</div>


<hr>
<div id="paper5"> <b>5. Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video  Super-Resolution</b>  <a href="https://arxiv.org/pdf/2002.11616" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyu Xiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yapeng Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulun Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Allebach%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Jan P. Allebach</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenliang Xu</a><br>
<font size="3">
Abstract: In this paper, we explore the space-time video super-resolution task, which aims to generate a high-resolution (HR) slow-motion video from a low frame rate (LFR), low-resolution (LR) video. A simple solution is to split it into two sub-tasks: video frame interpolation (VFI) and video super-resolution (VSR). However, temporal interpolation and spatial super-resolution are intra-related in this task. Two-stage methods cannot fully take advantage of the natural property. In addition, state-of-the-art VFI or VSR networks require a large frame-synthesis or reconstruction module for predicting high-quality video frames, which makes the two-stage methods have large model sizes and thus be time-consuming. To overcome the problems, we propose a one-stage space-time video super-resolution framework, which directly synthesizes an HR slow-motion video from an LFR, LR video. Rather than synthesizing missing LR video frames as VFI networks do, we firstly temporally interpolate LR frame features in missing LR video frames capturing local temporal contexts by the proposed feature temporal interpolation network. Then, we propose a deformable ConvLSTM to align and aggregate temporal information simultaneously for better leveraging global temporal contexts. Finally, a deep reconstruction network is adopted to predict HR slow-motion video frames. Extensive experiments on benchmark datasets demonstrate that the proposed method not only achieves better quantitative and qualitative performance but also is more than three times faster than recent two-stage state-of-the-art methods, e.g., DAIN+EDVR and DAIN+RBPN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们将探讨时空视频超分辨率任务，以生成高分辨率（HR）慢动作视频从低帧率（LFR），低分辨率（LR）的视频，其目的。一个简单的解决方案是将它分成两个子任务：视频帧内插（VFI）和视频超分辨率（VSR）。然而，时间插值和空间超分辨率进行内部相关这项工作。两级方法不能充分利用的自然属性的优势。此外，国家的最先进的VFI或VSR网络需要一个大的帧合成或重建模块，用于预测高品质的视频帧，这使得两阶段方法具有大的模型大小，因此是耗时的。为了克服这些问题，我们提出了一个阶段的时空视频超分辨率框架，直接从LFR，LR视频合成的HR慢动作视频。而不是合成失踪LR视频帧作为VFI网络做什么，我们在缺少所提出的功能时间内插网络捕获的局部时间背景LR视频帧首先时间内插LR框架功能。然后，我们同时提出了一个变形的ConvLSTM来调整和汇总时间信息为更好地利用全球环境的时间。最后，一个深重建网络采用预测HR慢动作视频帧。在基准数据集大量实验表明，该方法不仅实现更好的定量和定性的性能，而且比最近两阶段的国家的最先进的方法，例如，岱恩+ EDVR和岱恩+ RBPN快三倍以上。</font>
</div>


<hr>
<div id="paper6"> <b>6. ARMA Nets: Expanding Receptive Field for Dense Prediction</b>  <a href="https://arxiv.org/pdf/2002.11609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiahao Su</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiqi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furong Huang</a><br>
<font size="3">
Abstract: Global information is essential for dense prediction problems, whose goal is to compute a discrete or continuous label for each pixel in the images. Traditional convolutional layers in neural networks, originally designed for image classification, are restrictive in these problems since their receptive fields are limited by the filter size. In this work, we propose autoregressive moving-average (ARMA) layer, a novel module in neural networks to allow explicit dependencies of output neurons, which significantly expands the receptive field with minimal extra parameters. We show experimentally that the effective receptive field of neural networks with ARMA layers expands as autoregressive coefficients become larger. In addition, we demonstrate that neural networks with ARMA layers substantially improve the performance of challenging pixel-level video prediction tasks as our model enlarges the effective receptive field. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：全球信息密集的预测问题，其目标是计算离散或连续标签图像中的每个像素是至关重要的。在神经网络，最初设计用于图像分类传统的卷积的层，在这些问题限制，因为它们的感受域由过滤器尺寸的限制。在这项工作中，我们提出的自回归移动平均（ARMA）层，在神经网络的新的模块，以允许输出神经元，以最少的额外的参数，其显著扩展感受域的显式依赖性。我们实验表明，神经网络的ARMA层上的有效感受野扩大为自回归系数变大。此外，我们证明了用ARMA层神经网络大幅提高挑战像素级的视频预测的任务，我们的模型扩大有效感受野的表现。</font>
</div>


<hr>
<div id="paper7"> <b>7. Automatically Searching for U-Net Image Translator Architecture</b>  <a href="https://arxiv.org/pdf/2002.11581" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Shu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunhe Wang</a><br>
<font size="3">
Abstract: Image translators have been successfully applied to many important low level image processing tasks. However, classical network architecture of image translator like U-Net, is borrowed from other vision tasks like biomedical image segmentation. This straightforward adaptation may not be optimal and could cause redundancy in the network structure. In this paper, we propose an automatic architecture searching method for image translator. By utilizing evolutionary algorithm, we investigate a more efficient network architecture which costs less computation resources and achieves better performance than the original one. Extensive qualitative and quantitative experiments are conducted to demonstrate the effectiveness of the proposed method. Moreover, we transplant the searched network architecture to other datasets which are not involved in the architecture searching procedure. Efficiency of the searched architecture on these datasets further demonstrates the generalization of the method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图片翻译已成功应用到许多重要的低级别的图像处理任务。然而，像掌中图像翻译的经典网络架构，与其他视觉任务，例如生物医学图像分割借来的。这个简单的适应可能不是最优的，并可能在网络结构造成的冗余。在本文中，我们提出了图像翻译自动架构搜索方法。通过使用进化算法，我们研究了一个更高效的网络架构，它成本更低的计算资源，并实现比原来的更好的性能。广泛的定性和定量实验以证明了该方法的有效性。此外，我们移植了搜索网络架构，未涉及到的架构搜索过程中的其他数据集。对这些数据集的搜索架构的效率，进一步证明了该方法的推广。</font>
</div>


<hr>
<div id="paper8"> <b>8. Object Relational Graph with Teacher-Recommended Learning for Video  Captioning</b>  <a href="https://arxiv.org/pdf/2002.11566" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaya Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunfeng Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bing Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peijin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weiming Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zha%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengjun Zha</a><br>
<font size="3">
Abstract: Taking full advantage of the information from both vision and language is critical for the video captioning task. Existing models lack adequate visual representation due to the neglect of interaction between object, and sufficient training for content-related words due to long-tailed problems. In this paper, we propose a complete video captioning system including both a novel model and an effective training strategy. Specifically, we propose an object relational graph (ORG) based encoder, which captures more detailed interaction features to enrich visual representation. Meanwhile, we design a teacher-recommended learning (TRL) method to make full use of the successful external language model (ELM) to integrate the abundant linguistic knowledge into the caption model. The ELM generates more semantically similar word proposals which extend the ground-truth words used for training to deal with the long-tailed problem. Experimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the proposed ORG-TRL system achieves state-of-the-art performance. Extensive ablation studies and visualizations illustrate the effectiveness of our system. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在充分考虑来自视觉和语言的信息，充分利用是视频字幕任务的关键。现有车型缺乏足够的可视化表示，由于物体之间的相互作用的忽视，以及由于长尾问题内容有关的词汇足够的培训。在本文中，我们提出了一个完整的视频字幕系统，既包括新的模型和有效的培训战略。具体地，我们提出了一种对象关系图（ORG）基于编码器，其捕获更详细交互功能来丰富视觉表示。同时，我们设计了一个老师推荐的学习（TRL）方法，以充分利用外部成功的语言模型（ELM）与丰富的语言知识融入到标题的模式。榆树产生更多的语义延伸用于训练应对长尾问题的地面实况话相似字的建议。在三个基准实验评估：MSVD，MSR-VTT和VATEX表明了该ORG-TRL系统实现国家的最先进的性能。广泛切除研究和可视化说明我们系统的有效性。</font>
</div>


<hr>
<div id="paper9"> <b>9. CookGAN: Meal Image Synthesis from Ingredients</b>  <a href="https://arxiv.org/pdf/2002.11493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fangda Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guerrero%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ricardo Guerrero</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pavlovic%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vladimir Pavlovic</a><br>
<font size="3">
Abstract: In this work we propose a new computational framework, based on generative deep models, for synthesis of photo-realistic food meal images from textual list of its ingredients. Previous works on synthesis of images from text typically rely on pre-trained text models to extract text features, followed by generative neural networks (GAN) aimed to generate realistic images conditioned on the text features. These works mainly focus on generating spatially compact and well-defined categories of objects, such as birds or flowers, but meal images are significantly more complex, consisting of multiple ingredients whose appearance and spatial qualities are further modified by cooking methods. To generate real-like meal images from ingredients, we propose Cook Generative Adversarial Networks (CookGAN), CookGAN first builds an attention-based ingredients-image association model, which is then used to condition a generative neural network tasked with synthesizing meal images. Furthermore, a cycle-consistent constraint is added to further improve image quality and control appearance. Experiments show our model is able to generate meal images corresponding to the ingredients. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们提出了一个新的计算框架的基础上，生成深车型，从它的成份文本列表照片般逼真的食品餐图像的合成。从文字图像的合成之前的作品通常依赖于预先训练文本模式，以提取文本功能，其次是旨在生成条件上的文本特征逼真的图像生成神经网络（GAN）。这些作品主要集中在生成空间紧凑和良好定义的类别的对象，如鸟类或花，但餐图像显著更加复杂，包含多个成分，其外观和空间特性由烹调方法进一步修饰的。为了产生实质般从食材餐的图像，我们建议库克剖成对抗性网络（CookGAN），CookGAN首先构建一个基于注意机制的成分图像关联模型，然后将其用于调节与合成图像一顿负责一个生成的神经网络。此外，周期一致的约束被添加以进一步改善图像质量和控制外观。实验表明，我们的模型能够产生对应于成分餐的图像。</font>
</div>


<hr>
<div id="paper10"> <b>10. Learning a Directional Soft Lane Affordance Model for Road Scenes Using  Self-Supervision</b>  <a href="https://arxiv.org/pdf/2002.11477" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Karlsson%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robin Karlsson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sjoberg%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erik Sjoberg</a><br>
<font size="3">
Abstract: Humans navigate complex environments in an organized yet flexible manner, adapting to the context and implicit social rules. Understanding these naturally learned patterns of behavior is essential for applications such as autonomous vehicles. However, algorithmically defining these implicit rules of human behavior remains difficult. This work proposes a novel self-supervised method for training a probabilistic network model to estimate the regions humans are most likely to drive in as well as a multimodal representation of the inferred direction of travel at each point. The model is trained on individual human trajectories conditioned on a representation of the driving environment. The model is shown to successfully generalize to new road scenes, demonstrating potential for real-world application as a prior for socially acceptable driving behavior in challenging or ambiguous scenarios which are poorly handled by explicit traffic rules. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类导航有组织且灵活地处理复杂的环境，适应环境和隐含的社会规则。了解这些行为自然学习的模式是为应用程序，如自动驾驶汽车是必不可少的。然而，算法定义人类行为的这些潜规则依然困难。这项工作提出了训练概率网络模型来估计区域人类最有可能推动以及旅行的每一点的推断方向的多峰表示一种新的自我监督方法。该模型是对人类个体的轨迹有条件地驾驶环境的表示训练。该模型显示出成功推广到新的道路的场景，展现现实世界的应用潜力作为现有的在挑战社会可接受的驾驶行为或不明确的情况下这是很难用明确的交通规则处理。</font>
</div>


<hr>
<div id="paper11"> <b>11. Towards Interpretable Semantic Segmentation via Gradient-weighted Class  Activation Mapping</b>  <a href="https://arxiv.org/pdf/2002.11434" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Vinogradova%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kira Vinogradova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dibrov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandr Dibrov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Myers%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gene Myers</a><br>
<font size="3">
Abstract: Convolutional neural networks have become state-of-the-art in a wide range of image recognition tasks. The interpretation of their predictions, however, is an active area of research. Whereas various interpretation methods have been suggested for image classification, the interpretation of image segmentation still remains largely unexplored. To that end, we propose SEG-GRAD-CAM, a gradient-based method for interpreting semantic segmentation. Our method is an extension of the widely-used Grad-CAM method, applied locally to produce heatmaps showing the relevance of individual pixels for semantic segmentation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络已经成为国家的最先进的大范围的图像识别任务。他们的预言的解释，但是，是一个活跃的研究领域。虽然各种解释方法已经被建议用于图像分类，图像分割的解释仍然是很大的未开发。为此，我们提出了SEG-GRAD-CAM，解释语义分割基于梯度的方法。我们的方法是被广泛使用的梯度-CAM法的延伸，局部地施加以产生热图表示语义分割各个像素的相关性。</font>
</div>


<hr>
<div id="paper12"> <b>12. Efficient Semantic Video Segmentation with Per-frame Inference</b>  <a href="https://arxiv.org/pdf/2002.11433" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yifan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunhua Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changqian Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingdong Wang</a><br>
<font size="3">
Abstract: In semantic segmentation, most existing real-time deep models trained with each frame independently may produce inconsistent results for a video sequence. Advanced methods take into considerations the correlations in the video sequence, e.g., by propagating the results to the neighboring frames using optical flow, or extracting the frame representations with other frames, which may lead to inaccurate results or unbalanced latency. In this work, we process efficient semantic video segmentation in a per-frame fashion during the inference process. Different from previous per-frame models, we explicitly consider the temporal consistency among frames as extra constraints during the training process and embed the temporal consistency into the segmentation network. Therefore, in the inference process, we can process each frame independently with no latency, and improve the temporal consistency with no extra computational cost and post-processing. We employ compact models for real-time execution. To narrow the performance gap between compact models and large models, new knowledge distillation methods are designed. Our results outperform previous keyframe based methods with a better trade-off between the accuracy and the inference speed on popular benchmarks, including the Cityscapes and Camvid. The temporal consistency is also improved compared with corresponding baselines which are trained with each frame independently. Code is available at: this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在语义分割，每帧训练有素的大多数现有的实时深模型可独立地产生视频序列不一致的结果。先进的方法考虑到考虑的相关性的视频序列中，例如，通过使用光流中传播的结果，以所述相邻帧或提取与其它帧，这可能导致不准确的结果或不平衡延迟的帧表示。在这项工作中，我们处理在推理过程中每一帧的时尚高效的视频语义分割。从以前的每画幅机型不同的是，我们明确地考虑帧作为额外的约束之间的时间一致性在训练过程中，并嵌入时间一致性到分割网络。因此，在推理过程中，我们可以独立无延迟地处理每个帧，并改善与没有额外的计算成本和后期处理的时间一致性。我们采用了实时执行紧凑车型。为了缩小紧凑车型和大型模型之间的性能差距，新知识蒸馏法进行了设计。我们的研究结果优于具有更好的权衡准确性和流行的基准，包括城市景观和Camvid推理速度之间的一个关键帧为基础的方法。时间一致性与对应其与每个帧独立地训练的基线相比也有所提高。代码，请访问：此HTTPS URL</font>
</div>


<hr>
<div id="paper13"> <b>13. Deform-GAN:An Unsupervised Learning Model for Deformable Registration</b>  <a href="https://arxiv.org/pdf/2002.11430" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyue Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jian%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weijian Jian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shihting Yang</a><br>
<font size="3">
Abstract: Deformable registration is one of the most challenging task in the field of medical image analysis, especially for the alignment between different sequences and modalities. In this paper, a non-rigid registration method is proposed for 3D medical images leveraging unsupervised learning. To the best of our knowledge, this is the first attempt to introduce gradient loss into deep-learning-based registration. The proposed gradient loss is robust across sequences and modals for large deformation. Besides, adversarial learning approach is used to transfer multi-modal similarity to mono-modal similarity and improve the precision. Neither ground-truth nor manual labeling is required during training. We evaluated our network on a 3D brain registration task comprehensively. The experiments demonstrate that the proposed method can cope with the data which has non-functional intensity relations, noise and blur. Our approach outperforms other methods especially in accuracy and speed. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可变形配准是在医用图像分析领域中最具挑战性的任务之一，特别是对于不同的序列和方式之间的对准。在本文中，非刚性配准方法提出了一种用于三维医学图像利用无监督学习。据我们所知，这是引入梯度损失为基础的深学习登记的首次尝试。所提出的梯度损失是跨大变形序列和模态的鲁棒性。此外，对抗性的学习方法被用于多模态相似转移到单模态的相似性，提高了精度。无论是地面实况也不手工贴标训练过程中是必需的。我们综合评估我们在3D大脑登记工作网络。实验结果表明，所提出的方法可以与具有非功能性强度的关系，噪声和模糊数据对应。我们的方法优于其他方法，尤其是在精度和速度。</font>
</div>


<hr>
<div id="paper14"> <b>14. Performance Evaluation of Deep Generative Models for Generating  Hand-Written Character Images</b>  <a href="https://arxiv.org/pdf/2002.11424" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mondal%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tanmoy Mondal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Trang%2C+L+T+T" target="_blank" rel="noopener" style="color:#0000EE;">LE Thi Thuy Trang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Coustaty%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mickaël Coustaty</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ogier%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-Marc Ogier</a><br>
<font size="3">
Abstract: There have been many work in the literature on generation of various kinds of images such as Hand-Written characters (MNIST dataset), scene images (CIFAR-10 dataset), various objects images (ImageNet dataset), road signboard images (SVHN dataset) etc. Unfortunately, there have been very limited amount of work done in the domain of document image processing. Automatic image generation can lead to the enormous increase of labeled datasets with the help of only limited amount of labeled data. Various kinds of Deep generative models can be primarily divided into two categories. First category is auto-encoder (AE) and the second one is Generative Adversarial Networks (GANs). In this paper, we have evaluated various kinds of AE as well as GANs and have compared their performances on hand-written digits dataset (MNIST) and also on historical hand-written character dataset of Indonesian BALI language. Moreover, these generated characters are recognized by using character recognition tool for calculating the statistical performance of these generated characters with respect to original character images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：已经有上一代的各种图像，例如手写字符（MNIST数据集），场景图像（CIFAR-10数据集），各种物体的图像（ImageNet数据集）的文献很多，道路招牌图片（SVHN数据集）等。遗憾的是，一直在文档图像处理领域所做的工作的量非常有限。自动图像生成可导致标记数据集与标记的数据量只有有限的帮助下，大量增加。各种深生成模式主要可以分为两类。第一类是自动编码器（AE），第二个是剖成对抗性网络（甘斯）。在本文中，我们已经评估了各种自动曝光以及甘斯和比较了他们的手写数字数据集（MNIST）以及印尼巴厘语的历史手写字符集的演出。此外，这些生成的字符通过使用字符识别工具，用于相对于原始字符图像计算这些生成的字符的统计性能的认可。</font>
</div>


<hr>
<div id="paper15"> <b>15. Disentangling Image Distortions in Deep Feature Space</b>  <a href="https://arxiv.org/pdf/2002.11409" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bianco%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simone Bianco</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Celona%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luigi Celona</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Napoletano%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paolo Napoletano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schettini%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raimondo Schettini</a><br>
<font size="3">
Abstract: Previous literature suggests that perceptual similarity is an emergent property shared across deep visual representations. Experiments conducted on a dataset of human-judged image distortions have proven that deep features outperform, by a large margin, classic perceptual metrics. In this work we take a further step in the direction of a broader understanding of such property by analyzing the capability of deep visual representations to intrinsically characterize different types of image distortions. To this end, we firstly generate a number of synthetically distorted images by applying three mainstream distortion types to the LIVE database and then we analyze the features extracted by different layers of different Deep Network architectures. We observe that a dimension-reduced representation of the features extracted from a given layer permits to efficiently separate types of distortions in the feature space. Moreover, each network layer exhibits a different ability to separate between different types of distortions, and this ability varies according to the network architecture. As a further analysis, we evaluate the exploitation of features taken from the layer that better separates image distortions for: i) reduced-reference image quality assessment, and ii) distortion types and severity levels characterization on both single and multiple distortion databases. Results achieved on both tasks suggest that deep visual representations can be unsupervisedly employed to efficiently characterize various image distortions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：以前的文献表明，感知相似跨越深视觉表示共享的出射特性。就人类判断图像失真的数据集进行的实验已经证明，深特征胜过，由一大截，经典感性指标。在这项工作中，我们采取通过分析深视觉表示的能力固有地表征不同类型的图像失真的中的这种性质的更广泛的理解的方向上的进一步的步骤。为此，我们首先通过应用三种主流失真类型的LIVE数据库生成多个合成图像扭曲的，然后我们分析由不同深度的网络架构的不同层中提取的特征。我们观察到的一个降维表示从给定的层，以允许有效地分离类型在特征空间中的失真的提取的特征。此外，每个网络层显示出不同类型的失真之间分开不同的能力，而这种能力根据网络架构而变化。作为进一步的分析，我们评估从层采取特征开采更好中隔离的图像失真对于：1）减小参考图像质量评价，和ii）对单个和多个失真数据库失真类型和严重性级别表征。在这两项任务取得的成果表明，深视觉表现可以unsupervisedly采用有效表征各种图像失真。</font>
</div>


<hr>
<div id="paper16"> <b>16. Force-Ultrasound Fusion:Bringing Spine Robotic-US to the Next "Level"</b>  <a href="https://arxiv.org/pdf/2002.11404" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tirindelli%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maria Tirindelli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Victorova%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maria Victorova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Esteban%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Javier Esteban</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S+T" target="_blank" rel="noopener" style="color:#0000EE;">Seong Tae Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Navarro-Alarcon%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Navarro-Alarcon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Y+P" target="_blank" rel="noopener" style="color:#0000EE;">Yong Ping Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Navab%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nassir Navab</a><br>
<font size="3">
Abstract: Spine injections are commonly performed in several clinical procedures. The localization of the target vertebral level (i.e. the position of a vertebra in a spine) is typically done by back palpation or under X-ray guidance, yielding either higher chances of procedure failure or exposure to ionizing radiation. Preliminary studies have been conducted in the literature, suggesting that ultrasound imaging may be a precise and safe alternative to X-ray for spine level detection. However, ultrasound data are noisy and complicated to interpret. In this study, a robotic-ultrasound approach for automatic vertebral level detection is introduced. The method relies on the fusion of ultrasound and force data, thus providing both "tactile" and visual feedback during the procedure, which results in higher performances in presence of data corruption. A robotic arm automatically scans the volunteer's back along the spine by using force-ultrasound data to locate vertebral levels. The occurrences of vertebral levels are visible on the force trace as peaks, which are enhanced by properly controlling the force applied by the robot on the patient back. Ultrasound data are processed with a Deep Learning method to extract a 1D signal modelling the probabilities of having a vertebra at each location along the spine. Processed force and ultrasound data are fused using a 1D Convolutional Network to compute the location of the vertebral levels. The method is compared to pure image and pure force-based methods for vertebral level counting, showing improved performance. In particular, the fusion method is able to correctly classify 100% of the vertebral levels in the test set, while pure image and pure force-based method could only classify 80% and 90% vertebrae, respectively. The potential of the proposed method is evaluated in an exemplary simulated clinical application. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：脊椎注射在一些临床常用的程序进行。所述目标椎骨水平的定位（即，在脊柱椎骨的位置）通常是由背面触诊或透视指导下进行的，得到的过程失败或暴露于电离辐射或高机会。初步研究已在文献中进行的，这表明超声成像可以是X射线精确和安全的替代脊柱电平检测。然而，超声数据是嘈杂和复杂的解释。在这项研究中，自动椎体水平检测机器人，超声波的方法介绍。该方法依赖于超声和力数据的融合，从而该过程，这导致了数据损坏的存在更高的性能期间提供两个“触觉”和视觉反馈。机械臂自动扫描通过使用武力，超声数据来定位椎体水平沿脊柱志愿者的背部。椎骨水平的出现是在力轨迹作为峰，这是通过适当地控制由在患者背部的机器人施加的力增强的可见光。超声数据与深度学习方法处理，以提取一个1D信号建模具有沿脊柱的每个位置椎骨的概率。加工力和超声数据使用的是一维卷积网络融合到计算椎体水平的位置。该方法相比于纯图像和纯基于力的方法椎骨水平计数，示出改进的性能。特别地，该熔融法而纯图像和纯基于力的方法仅椎骨，分别能分类为80％和90％是能够正确地进行分类的测试集合中的椎骨水平的100％。所提出的方法的潜力在示例性模拟的临床应用进行评价。</font>
</div>


<hr>
<div id="paper17"> <b>17. Controllable Descendant Face Synthesis</b>  <a href="https://arxiv.org/pdf/2002.11376" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Le Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhilei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baoyuan Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanbo Fan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhifeng Li</a><br>
<font size="3">
Abstract: Kinship face synthesis is an interesting topic raised to answer questions like "what will your future children look like?". Published approaches to this topic are limited. Most of the existing methods train models for one-versus-one kin relation, which only consider one parent face and one child face by directly using an auto-encoder without any explicit control over the resemblance of the synthesized face to the parent face. In this paper, we propose a novel method for controllable descendant face synthesis, which models two-versus-one kin relation between two parent faces and one child face. Our model consists of an inheritance module and an attribute enhancement module, where the former is designed for accurate control over the resemblance between the synthesized face and parent faces, and the latter is designed for control over age and gender. As there is no large scale database with father-mother-child kinship annotation, we propose an effective strategy to train the model without using the ground truth descendant faces. No carefully designed image pairs are required for learning except only age and gender labels of training faces. We conduct comprehensive experimental evaluations on three public benchmark databases, which demonstrates encouraging results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面对血族合成提高到答案的问题，如一个有趣的话题“会有什么你未来的孩子是什么样子？”。发布时间的方法这个话题是有限的。大多数现有方法的训练对一个抗一个亲属关系，只有通过直接使用自动编码器来避免过度合成脸的相似父面对任何明确的控制考虑一个父母的脸和一个小孩的脸部模型。在本文中，我们提出了可控的后裔脸合成的新方法，该机型双与一两家母公司的面孔和一个孩子的脸之间的亲属关系。我们的模型由一个继承模块和属性增强模块，其中前者是专为在合成脸和家长面之间的相似之处精确控制的，而后者是专为在年龄和性别控制。由于没有与父亲的母子亲情注解没有大规模的数据库，我们建议训练模型，而不使用地面实况后裔脸的有效策略。没有精心设计的图像对所需要的训练之外的面孔只有年龄和性别标签学习。我们三个公共基准数据库进行全面的实验评估，这表明了令人鼓舞的结果。</font>
</div>


<hr>
<div id="paper18"> <b>18. Adversarial Attack on Deep Product Quantization Network for Image  Retrieval</b>  <a href="https://arxiv.org/pdf/2002.11374" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Dai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shutao Xia</a><br>
<font size="3">
Abstract: Deep product quantization network (DPQN) has recently received much attention in fast image retrieval tasks due to its efficiency of encoding high-dimensional visual features especially when dealing with large-scale datasets. Recent studies show that deep neural networks (DNNs) are vulnerable to input with small and maliciously designed perturbations (a.k.a., adversarial examples). This phenomenon raises the concern of security issues for DPQN in the testing/deploying stage as well. However, little effort has been devoted to investigating how adversarial examples affect DPQN. To this end, we propose product quantization adversarial generation (PQ-AG), a simple yet effective method to generate adversarial examples for product quantization based retrieval systems. PQ-AG aims to generate imperceptible adversarial perturbations for query images to form adversarial queries, whose nearest neighbors from a targeted product quantizaiton model are not semantically related to those from the original queries. Extensive experiments show that our PQ-AQ successfully creates adversarial examples to mislead targeted product quantization retrieval models. Besides, we found that our PQ-AG significantly degrades retrieval performance in both white-box and black-box settings. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深厚的产品量化网络（DPQN）最近已收到快速图像检索任务的关注，由于与大型数据集处理时，尤其是编码高维视觉特征的效率。最近的研究表明，深层神经网络（DNNs）很容易受到小和恶意设计的扰动（也叫做对抗的例子）输入。这种现象引起的安全问题为DPQN在测试的关注/部署阶段为好。然而，很少的努力，一直致力于研究对抗性的例子是如何影响DPQN。为此，我们提出了量化产品对抗代（PQ-AG），一个简单而产生的产品量化基于内容的检索系统对抗的例子有效的方法。 PQ-AG旨在生成查询图像潜移默化的对抗扰动，形成对抗性的查询，其从针对性的产品quantizaiton模型最近的邻居没有语义与那些从原来的查询。大量的实验表明，我们的PQ-AQ成功创建误导针对性的产品量化检索模型对抗性的例子。此外，我们发现，我们的PQ-AG显著降低在这两个白盒和黑盒设置检索性能。</font>
</div>


<hr>
<div id="paper19"> <b>19. PuzzleNet: Scene Text Detection by Segment Context Graph Learning</b>  <a href="https://arxiv.org/pdf/2002.11371" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antai Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deqiang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiqing Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Ren</a><br>
<font size="3">
Abstract: Recently, a series of decomposition-based scene text detection methods has achieved impressive progress by decomposing challenging text regions into pieces and linking them in a bottom-up manner. However, most of them merely focus on linking independent text pieces while the context information is underestimated. In the puzzle game, the solver often put pieces together in a logical way according to the contextual information of each piece, in order to arrive at the correct solution. Inspired by it, we propose a novel decomposition-based method, termed Puzzle Networks (PuzzleNet), to address the challenging scene text detection task in this work. PuzzleNet consists of the Segment Proposal Network (SPN) that predicts the candidate text segments fitting arbitrary shape of text region, and the two-branch Multiple-Similarity Graph Convolutional Network (MSGCN) that models both appearance and geometry correlations between each segment to its contextual ones. By building segments as context graphs, MSGCN effectively employs segment context to predict combinations of segments. Final detections of polygon shape are produced by merging segments according to the predicted combinations. Evaluations on three benchmark datasets, ICDAR15, MSRA-TD500 and SCUT-CTW1500, have demonstrated that our method can achieve better or comparable performance than current state-of-the-arts, which is beneficial from the exploitation of segment context graph. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，一系列基于分解场景文字检测方法已通过分解挑战文本区域成片，并以自下而上的方式将它们连接起来取得了令人瞩目的进展。然而，他们大多只注重，而上下文信息被低估连接独立的文本块。在益智游戏，求解器常常把拼在一起以逻辑方式根据每个块的上下文信息，以在正确的解决办法。通过它的启发，我们提出了一种新的基于分解方法被称为益智网络（PuzzleNet），以解决具有挑战性的场景文字检测任务了这项工作。 PuzzleNet由区段提议网络（SPN），预测候选文本段嵌合文本区域的任意形状，并且所述两分支多相似度图形卷积网络（MSGCN），每个段到其上下文之间模型外观和几何相关性那些。通过构建段作为上下文图形，MSGCN有效采用段上下文来预测段的组合。多边形形状的最终检测通过根据所预测的组合合并段产生。对三个标准数据集，ICDAR15，MSRA-TD500和华南理工大学，CTW1500，经过评估，证明了我们的方法可以达到更好或相当的性能比目前的状态的最艺术，它是从段背景图的开发是有益的。</font>
</div>


<hr>
<div id="paper20"> <b>20. Unsupervised Temporal Video Segmentation as an Auxiliary Task for  Predicting the Remaining Surgery Duration</b>  <a href="https://arxiv.org/pdf/2002.11367" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rivoir%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dominik Rivoir</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bodenstedt%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastian Bodenstedt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Bechtolsheim%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Felix von Bechtolsheim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Distler%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marius Distler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weitz%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jürgen Weitz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Speidel%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefanie Speidel</a><br>
<font size="3">
Abstract: Estimating the remaining surgery duration (RSD) during surgical procedures can be useful for OR planning and anesthesia dose estimation. With the recent success of deep learning-based methods in computer vision, several neural network approaches have been proposed for fully automatic RSD prediction based solely on visual data from the endoscopic camera. We investigate whether RSD prediction can be improved using unsupervised temporal video segmentation as an auxiliary learning task. As opposed to previous work, which presented supervised surgical phase recognition as auxiliary task, we avoid the need for manual annotations by proposing a similar but unsupervised learning objective which clusters video sequences into temporally coherent segments. In multiple experimental setups, results obtained by learning the auxiliary task are incorporated into a deep RSD model through feature extraction, pretraining or regularization. Further, we propose a novel loss function for RSD training which attempts to counteract unfavorable characteristics of the RSD ground truth. Using our unsupervised method as an auxiliary task for RSD training, we outperform other self-supervised methods and are comparable to the supervised state-of-the-art. Combined with the novel RSD loss, we slightly outperform the supervised approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在手术过程中估计剩余的手术时间（RSD）可用于或计划和麻醉剂量估算是有用的。随着近年来计算机视觉深基于学习的方法成功，几个神经网络的方法已经被提出了完全基于来自内窥镜摄像机的可视数据全自动RSD预测。我们调查是否RSD预测可以使用无监督时间视频分割作为辅助学习任务加以改进。相对于以前的工作，其中提出监督外科手术相识别作为辅助任务，我们避免了手工标注有必要通过提出一个类似的，但监督的学习目标，其集群视频序列为暂时相干段。在多个实验设置，通过学习辅助任务所获得的结果是通过特征提取，训练前或正则并入深RSD模型。此外，我们提出了RSD培训，试图抵消RSD地面实况的不利特征的新的损失函数。使用我们的无监督方法为RSD训练辅助任务，我们优于其他自我监督的方法和媲美监测状态的最先进的。以新颖的RSD损失相结合，我们略微跑赢监督办法。</font>
</div>


<hr>
<div id="paper21"> <b>21. Rethinking the Route Towards Weakly Supervised Object Localization</b>  <a href="https://arxiv.org/pdf/2002.11359" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen-Lin Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun-Hao Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianxin Wu</a><br>
<font size="3">
Abstract: Weakly supervised object localization (WSOL) aims to localize objects with only image-level labels. Previous methods often try to utilize feature maps and classification weights to localize objects using image level annotations indirectly. In this paper, we demonstrate that weakly supervised object localization should be divided into two parts: class-agnostic object localization and object classification. For class-agnostic object localization, we should use class-agnostic methods to generate noisy pseudo annotations and then perform bounding box regression on them without class labels. We propose the pseudo supervised object localization (PSOL) method as a new way to solve WSOL. Our PSOL models have good transferability across different datasets without fine-tuning. With generated pseudo bounding boxes, we achieve 58.00% localization accuracy on ImageNet and 74.74% localization accuracy on CUB-200, which have a large edge over previous models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：弱监督对象定位（WSOL）目标本地化，只有影像级标签的对象。以前的方法常常试图利用特征映射和分类的权重本地化使用图像级别注释对象是间接的。在本文中，我们证明了弱监督的目标定位应分为两个部分：类无关的目标定位和目标分类。对于类无关的目标定位，我们应该使用类无关的方法来产生嘈杂伪注释，然后无类标签对它们执行边界框回归。我们提出的伪监督对象定位（PSOL）的方法来解决WSOL了新的途径。我们PSOL车型有跨不同的数据集良好的转印没有微调。随着产生的伪边界框，就可以实现对ImageNet和74.74％，定位精度58.00％，定位精度在CUB-200，它拥有较以前的型号大的优势。</font>
</div>


<hr>
<div id="paper22"> <b>22. Refined Gate: A Simple and Effective Gating Mechanism for Recurrent  Units</b>  <a href="https://arxiv.org/pdf/2002.11338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhanzhan Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunlu Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingjian Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Qiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiliang Pu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Niu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a><br>
<font size="3">
Abstract: Recurrent neural network (RNN) has been widely studied in sequence learning tasks, while the mainstream models (e.g., LSTM and GRU) rely on the gating mechanism (in control of how information flows between hidden states). However, the vanilla gates in RNN (e.g. the input gate in LSTM) suffer from the problem of gate undertraining mainly due to the saturating activation functions, which may result in failures of learning gating roles and thus the weak performance. In this paper, we propose a new gating mechanism within general gated recurrent neural networks to handle this issue. Specifically, the proposed gates directly short connect the extracted input features to the outputs of vanilla gates, denoted as refined gates. The refining mechanism allows enhancing gradient back-propagation as well as extending the gating activation scope, which, although simple, can guide RNN to reach possibly deeper minima. We verify the proposed gating mechanism on three popular types of gated RNNs including LSTM, GRU and MGU. Extensive experiments on 3 synthetic tasks, 3 language modeling tasks and 5 scene text recognition benchmarks demonstrate the effectiveness of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：递归神经网络（RNN）已被广泛研究序列学习任务，而主流机型（例如，LSTM和GRU）靠门控机制（在信息隐藏状态之间的流动控制）。然而，在RNN香草栅极（例如输入门在LSTM）从栅极undertraining主要的问题是由于饱和激活功能，这可能会导致学习门控作用和因此弱性能的故障受到影响。在本文中，我们提出了普通门回归神经网络中的一个新的控制机制来处理这个问题。具体地，所提出的栅极直接短的提取的输入特征连接到香草门，表示为精制门的输出。精炼机制允许增强梯度反向传播以及延伸的选通启动范围，该范围，虽然简单，可以引导RNN到达更深的可能极小。我们验证上的三个热门类型的门控RNNs包括LSTM，GRU和MGU所提出的门控机制。 3个合成任务，3语言建模任务和5现场文字识别基准测试大量的实验证明我们的方法的有效性。</font>
</div>


<hr>
<div id="paper23"> <b>23. Self-supervised Image Enhancement Network: Training with Low Light  Images Only</b>  <a href="https://arxiv.org/pdf/2002.11300" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Di%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoguang Di</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunhui Wang</a><br>
<font size="3">
Abstract: This paper proposes a self-supervised low light image enhancement method based on deep learning. Inspired by information entropy theory and Retinex model, we proposed a maximum entropy based Retinex model. With this model, a very simple network can separate the illumination and reflectance, and the network can be trained with low light images only. We introduce a constraint that the maximum channel of the reflectance conforms to the maximum channel of the low light image and its entropy should be largest in our model to achieve self-supervised learning. Our model is very simple and does not rely on any well-designed data set (even one low light image can complete the training). The network only needs minute-level training to achieve image enhancement. It can be proved through experiments that the proposed method has reached the state-of-the-art in terms of processing speed and effect. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种基于深度学习自我监督的低光图像增强方法。通过信息熵理论和视网膜皮层模型的启发，我们提出了一个最大熵基于视网膜皮层模型。与这个模型中，一个非常简单的网络可以分离照明和反射率，并且网络可以仅具有低光图像进行训练。我们引入一个约束的反射率符合的低光图像的最大渠道，它的熵的最大通道应该在我们的模型中最大的，达到自我监督学习。我们的模型是非常简单的，不依赖于任何精心设计的数据集（即使是一个低光图像可以完成训练）。该网络只需要分层次的培训来实现图像增强。它可以通过实验证实，所提出的方法已达到的状态的最先进的处理速度和效果方面。</font>
</div>


<hr>
<div id="paper24"> <b>24. Generalized ODIN: Detecting Out-of-distribution Image without Learning  from Out-of-distribution Data</b>  <a href="https://arxiv.org/pdf/2002.11297" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yen-Chang Hsu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yilin Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongxia Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kira%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zsolt Kira</a><br>
<font size="3">
Abstract: Deep neural networks have attained remarkable performance when applied to data that comes from the same distribution as that of the training set, but can significantly degrade otherwise. Therefore, detecting whether an example is out-of-distribution (OoD) is crucial to enable a system that can reject such samples or alert users. Recent works have made significant progress on OoD benchmarks consisting of small image datasets. However, many recent methods based on neural networks rely on training or tuning with both in-distribution and out-of-distribution data. The latter is generally hard to define a-priori, and its selection can easily bias the learning. We base our work on a popular method ODIN, proposing two strategies for freeing it from the needs of tuning with OoD data, while improving its OoD detection performance. We specifically propose to decompose confidence scoring as well as a modified input pre-processing method. We show that both of these significantly help in detection performance. Our further analysis on a larger scale image dataset shows that the two types of distribution shifts, specifically semantic shift and non-semantic shift, present a significant difference in the difficulty of the problem, providing an analysis of when ODIN-like strategies do or do not work. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络已经达到时，适用于来自分布相同，训练组数据的表现可圈可点，但可以以其他方式显著降低。因此，检测的例子是外的分布（OOD）是至关重要的，以使可以拒绝这样的样品或提醒用户的系统。最近的作品在由小图像数据集的OOD基准取得显著的进展。然而，基于神经网络最近的许多方法依赖于培训或调整两者的分布和外的分布数据。后者通常是难以定义的先验，并且其选择可以很容易地偏压所述学习。我们立足我们的工作在一个流行的方法ODIN，提出两种策略从与面向对象的数据调整的需求中解放出来，同时改善其OOD的检测性能。我们特别提出来分解信心得分以及经修改的输入处理前的方法。我们发现，这两种检测性能显著帮助。我们在更大的规模的图像数据集示出了进一步的分析，该两种类型的分布的变化的，具体语义移位和非语义移，呈现出显著差异在问题的难度，从而提供当ODIN样策略做的分析或做不行。</font>
</div>


<hr>
<div id="paper25"> <b>25. Adversarial Ranking Attack and Defense</b>  <a href="https://arxiv.org/pdf/2002.11293" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mo Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenxing Niu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Le Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qilin Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gang Hua</a><br>
<font size="3">
Abstract: Deep Neural Network (DNN) classifiers are vulnerable to adversarial attack, where an imperceptible perturbation could result in misclassification. However, the vulnerability of DNN-based image ranking systems remains under-explored. In this paper, we propose two attacks against deep ranking systems, i.e., Candidate Attack and Query Attack, that can raise or lower the rank of chosen candidates by adversarial perturbations. Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation. Conversely, a defense method is also proposed to improve the ranking system robustness, which can mitigate all the proposed attacks simultaneously. Our adversarial ranking attacks and defense are evaluated on datasets including MNIST, Fashion-MNIST, and Stanford-Online-Products. Experimental results demonstrate that a typical deep ranking system can be effectively compromised by our attacks. Meanwhile, the system robustness can be moderately improved with our defense. Furthermore, the transferable and universal properties of our adversary illustrate the possibility of realistic black-box attack. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络（DNN）分类很容易受到攻击对抗，其中一个难以觉察的扰动可能会导致错误分类。然而，基于DNN图像排名系统遗骸的脆弱性充分开发。在本文中，我们提出了对深排名系统，即候选人攻击和查询的攻击，可以提高或敌对的扰动降低选择候选人的排名两起袭击事件。具体地，预期排列顺序，首先表示为一组不等式，然后三重态样的目标函数被设计以获得最佳扰动。相反，防御方法也提出了提高排名系统的健壮性，可同时降低所有的提议攻击。我们的对抗排名的攻击和防守队员的数据集，包括MNIST，时尚MNIST，和斯坦福在线产品评估。实验结果表明，一个典型的深排名系统可以通过我们的攻击被有效破坏。同时，系统的健壮性，可以适度地用我们的防守得到改善。此外，我们的对手的转让和通用性说明现实的黑箱攻击的可能性。</font>
</div>


<hr>
<div id="paper26"> <b>26. Generalized Product Quantization Network for Semi-supervised Hashing</b>  <a href="https://arxiv.org/pdf/2002.11281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jang%2C+Y+K" target="_blank" rel="noopener" style="color:#0000EE;">Young Kyun Jang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+N+I" target="_blank" rel="noopener" style="color:#0000EE;">Nam Ik Cho</a><br>
<font size="3">
Abstract: Learning to hash has achieved great success in image retrieval due to its low storage cost and fast search speed. In recent years, hashing methods that take advantage of deep learning have come into the spotlight with some positive outcomes. However, these approaches do not meet expectations unless expensive label information is sufficient. To resolve this issue, we propose the first quantization-based semi-supervised hashing scheme: Generalized Product Quantization (\textbf{GPQ}) network. We design a novel metric learning strategy that preserves semantic similarity between labeled data, and employ entropy regularization term to fully exploit inherent potentials of unlabeled data. Our solution increases the generalization capacity of the hash function, which allows overcoming previous limitations in the retrieval community. Extensive experimental results demonstrate that GPQ yields state-of-the-art performance on large-scale real image benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：学习哈希也由于它的低存储成本实现了图像检索了巨大的成功和快速的搜索速度。近年来，散列是利用深度学习的方法已经成为焦点的一些积极成果。然而，除非使用昂贵的标签信息就足够了，这些方法并没有达到预期。要解决此问题，我们提出了第一个基于量化半监督散列方案：通用产品量化（\ textbf {} GPQ）网络。我们设计了一个新的度量学习策略，要充分标记数据，并采用熵调整项之间的蜜饯语义相似性利用无标签数据的内在潜力。我们的解决方案增加了哈希函数，它允许在检索社区克服以前限制的泛化能力。大量的实验结果表明，GPQ产生于大型实景图像基准数据集的国家的最先进的性能。</font>
</div>


<hr>
<div id="paper27"> <b>27. Learning Light Field Angular Super-Resolution via a Geometry-Aware  Network</b>  <a href="https://arxiv.org/pdf/2002.11263" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junhui Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kwong%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sam Kwong</a><br>
<font size="3">
Abstract: The acquisition of light field images with high angular resolution is costly. Although many methods have been proposed to improve the angular resolution of a sparsely-sampled light field, they always focus on the light field with a small baseline, which is captured by a consumer light field camera. By making full use of the intrinsic \textit{geometry} information of light fields, in this paper we propose an end-to-end learning-based approach aiming at angularly super-resolving a sparsely-sampled light field with a large baseline. Our model consists of two learnable modules and a physically-based module. Specifically, it includes a depth estimation module for explicitly modeling the scene geometry, a physically-based warping for novel views synthesis, and a light field blending module specifically designed for light field reconstruction. Moreover, we introduce a novel loss function to promote the preservation of the light field parallax structure. Experimental results over various light field datasets including large baseline light field images demonstrate the significant superiority of our method when compared with state-of-the-art ones, i.e., our method improves the PSNR of the second best method up to 2 dB in average, while saves the execution time 48$\times$. In addition, our method preserves the light field parallax structure better. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：光场图像以高角分辨率采集是昂贵的。虽然许多方法已被提出来改善稀疏采样的光场的角分辨率，他们总是着眼于光场的很小的基线，这是由消费者光场照相机捕获。充分利用固有的\ {textit几何}光场的信息，在本文中，我们提出了一个终端到终端基于学习的方法针对角度超分辨率的大基线稀疏采样的光场。我们的模型包括两个可以学习模块和基于物理的模块。具体而言，它包括用于明确地建模场景的几何形状，用于新颖的观点合成基于物理的翘曲，并专门用于光场重构设计了一个光场混合模块的深度估计模块。此外，我们介绍一种新颖的损失函数，以促进光场视差结构的保存。当与国家的最先进的，即，与在不同的光场数据集的实验结果，包括大基线光场图像表明了该方法的显著优势，我们的方法提高了第二最佳方法向上的，以平均2分贝的PSNR ，同时节省了执行时间48 $ \ $倍。此外，我们的方法保留了光场视差结构更好。</font>
</div>


<hr>
<div id="paper28"> <b>28. Multi-Attribute Guided Painting Generation</b>  <a href="https://arxiv.org/pdf/2002.11261" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minxuan Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingying Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fan Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weiming Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changsheng Xu</a><br>
<font size="3">
Abstract: Controllable painting generation plays a pivotal role in image stylization. Currently, the control way of style transfer is subject to exemplar-based reference or a random one-hot vector guidance. Few works focus on decoupling the intrinsic properties of painting as control conditions, e.g., artist, genre and period. Under this circumstance, we propose a novel framework adopting multiple attributes from the painting to control the stylized results. An asymmetrical cycle structure is equipped to preserve the fidelity, associating with style preserving and attribute regression loss to keep the unique distinction of colors and textures between domains. Several qualitative and quantitative results demonstrate the effect of the combinations of multiple attributes and achieve satisfactory performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可控画一代起着图像程式化了举足轻重的作用。目前，风格转移的控制方式是受到基于标本引用或随机一个热载体指导。很少有作品专注于绘画解耦作为对照条件，例如，艺术家，流派和周期的内在属性。在这种情况下，我们提出了一种新颖的框架采用从涂装控制程式化结果多个属性。不对称的周期结构配备保留保真度，风格保存和属性回归的损失，以保持的域之间的颜色和纹理的独特区别关联。一些定性和定量的结果证明了多个属性的组合效果，达到满意的性能。</font>
</div>


<hr>
<div id="paper29"> <b>29. Back to the Future: Joint Aware Temporal Deep Learning 3D Human Pose  Estimation</b>  <a href="https://arxiv.org/pdf/2002.11251" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vikas Gupta</a><br>
<font size="3">
Abstract: We propose a new deep learning network that introduces a deeper CNN channel filter and constraints as losses to reduce joint position and motion errors for 3D video human body pose estimation. Our model outperforms the previous best result from the literature based on mean per-joint position error, velocity error, and acceleration errors on the Human 3.6M benchmark corresponding to a new state-of-the-art mean error reduction in all protocols and motion metrics. Mean per joint error is reduced by 1%, velocity error by 7% and acceleration by 13% compared to the best results from the literature. Our contribution increasing positional accuracy and motion smoothness in video can be integrated with future end to end networks without increasing network complexity. Our model and code are available at this https URL Keywords: 3D, human, image, pose, action, detection, object, video, visual, supervised, joint, kinematic </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种新的深学习网络，它引入了一个更深层次的CNN信道滤波器和约束损失降低3D影像的人体姿态估计关节位置及运动误差。我们的性能优于由文献先前最佳结果模型基于平均每个关节的位置误差，速度误差，并对应于在所有协议和运动的新的国家的最先进的平均误差减少人力3.6M基准加速度误差指标。通过1％，速度误差减小的平均每关节误差为7％和加速度由13％相比，从文献中最好的结果。我们的贡献增加视频定位精度和运动平滑度可以与未来的最终集成到最终的网络在不增加网络复杂性。我们的模型和代码可在此HTTPS URL关键词：3D，人类，形象，姿态，动作，检测，对象，影，视，监督，关节，运动</font>
</div>


<hr>
<div id="paper30"> <b>30. Super-Resolving Commercial Satellite Imagery Using Realistic Training  Data</b>  <a href="https://arxiv.org/pdf/2002.11248" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Talebi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossein Talebi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinwei Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milanfar%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peyman Milanfar</a><br>
<font size="3">
Abstract: In machine learning based single image super-resolution, the degradation model is embedded in training data generation. However, most existing satellite image super-resolution methods use a simple down-sampling model with a fixed kernel to create training images. These methods work fine on synthetic data, but do not perform well on real satellite images. We propose a realistic training data generation model for commercial satellite imagery products, which includes not only the imaging process on satellites but also the post-process on the ground. We also propose a convolutional neural network optimized for satellite images. Experiments show that the proposed training data generation model is able to improve super-resolution performance on real satellite images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于机器学习的单幅图像超分辨率的退化模型嵌入在训练数据产生。然而，大多数现有的卫星影像超分辨率方法使用一个简单的具有固定内核下采样模型来创建训练图像。对合成数据，这些方法做工精细，但并不真实的卫星图片表现良好。我们提出了商业卫星图像产品真实的训练数据生成模式，这不仅包括对卫星成像过程，而且地面上的后处理。我们还提出了卫星图像优化卷积神经网络。实验结果表明，所提出的训练数据生成模型能够提高真实的卫星图片超分辨率的性能。</font>
</div>


<hr>
<div id="paper31"> <b>31. Transfer Learning from Synthetic to Real-Noise Denoising with Adaptive  Instance Normalization</b>  <a href="https://arxiv.org/pdf/2002.11244" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoonsik Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soh%2C+J+W" target="_blank" rel="noopener" style="color:#0000EE;">Jae Woong Soh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+G+Y" target="_blank" rel="noopener" style="color:#0000EE;">Gu Yong Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+N+I" target="_blank" rel="noopener" style="color:#0000EE;">Nam Ik Cho</a><br>
<font size="3">
Abstract: Real-noise denoising is a challenging task because the statistics of real-noise do not follow the normal distribution, and they are also spatially and temporally changing. In order to cope with various and complex real-noise, we propose a well-generalized denoising architecture and a transfer learning scheme. Specifically, we adopt an adaptive instance normalization to build a denoiser, which can regularize the feature map and prevent the network from overfitting to the training set. We also introduce a transfer learning scheme that transfers knowledge learned from synthetic-noise data to the real-noise denoiser. From the proposed transfer learning, the synthetic-noise denoiser can learn general features from various synthetic-noise data, and the real-noise denoiser can learn the real-noise characteristics from real data. From the experiments, we find that the proposed denoising method has great generalization ability, such that our network trained with synthetic-noise achieves the best performance for Darmstadt Noise Dataset (DND) among the methods from published papers. We can also see that the proposed transfer learning scheme robustly works for real-noise images through the learning with a very small number of labeled data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：实时噪声降噪是一项具有挑战性的任务，因为真正的噪声统计不遵循正态分布，他们也是在空间和时间发生变化。为了应对各种复杂的实际噪声，我们提出了一个良好的广义降噪结构和转移的学习方案。具体来说，我们采用了自适应实例规范化建设降噪，可以正规化特征地图和防止网络过度拟合训练集。我们还引进了转移学习计划，该转移的知识从合成噪声数据学会了真正的噪声降噪。从提出迁移学习，合成噪声降噪可以从各种合成噪声数据学习一般特征，而真正的噪声降噪可以从中学到真实的数据实时噪声特性。从实验中，我们发现，所提出的去噪方法具有很大的推广能力，使得我们的网络与合成噪声训练有素达到达姆施塔特噪声数据集（DND）从发表论文的方法中最好的性能。我们还可以看到，所提出的迁移学习方案，通过强劲的学习工作真正噪声的图像有极少数的标签数据。</font>
</div>


<hr>
<div id="paper32"> <b>32. Style Transfer for Light Field Photography</b>  <a href="https://arxiv.org/pdf/2002.11220" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hart%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Hart</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Greenland%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jessica Greenland</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morse%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bryan Morse</a><br>
<font size="3">
Abstract: As light field images continue to increase in use and application, it becomes necessary to adapt existing image processing methods to this unique form of photography. In this paper we explore methods for applying neural style transfer to light field images. Feed-forward style transfer networks provide fast, high-quality results for monocular images, but no such networks exist for full light field images. Because of the size of these images, current light field data sets are small and are insufficient for training purely feed-forward style-transfer networks from scratch. Thus, it is necessary to adapt existing monocular style transfer networks in a way that allows for the stylization of each view of the light field while maintaining visual consistencies between views. Instead, the proposed method backpropagates the loss through the network, and the process is iterated to optimize (essentially overfit) the resulting stylization for a single light field image alone. The network architecture allows for the incorporation of pre-trained fast monocular stylization networks while avoiding the need for a large light field training set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着光场图像继续使用和应用的增加，有必要对现有的图像处理方法，以适应摄影这一独特的形式。在本文中，我们探讨了应用神经风格转移到光场图像的方法。前馈式的传送网络提供快速，单眼图像的高质量的结果，但对于全亮场图像不存在这样的网络。由于这些图像的尺寸，当前光场数据集较小且不足以从头训练纯粹前馈式传输网络。因此，有必要的方式，允许的光场的每个视图的风格化，同时维持视图之间的视觉一致性，以适应现有的单眼式传送网络。取而代之的是，所提出的方法backpropagates通过网络的损失，并且该处理被重复，以优化（基本上过拟合）将所得程式化为单独的单个光场图像。该网络架构允许预先训练快单眼风格化网络的整合，同时避免了大光场训练集的需要。</font>
</div>


<hr>
<div id="paper33"> <b>33. Geometric Fusion via Joint Delay Embeddings</b>  <a href="https://arxiv.org/pdf/2002.11201" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Solomon%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elchanan Solomon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bendich%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Bendich</a><br>
<font size="3">
Abstract: We introduce geometric and topological methods to develop a new framework for fusing multi-sensor time series. This framework consists of two steps: (1) a joint delay embedding, which reconstructs a high-dimensional state space in which our sensors correspond to observation functions, and (2) a simple orthogonalization scheme, which accounts for tangencies between such observation functions, and produces a more diversified geometry on the embedding space. We conclude with some synthetic and real-world experiments demonstrating that our framework outperforms traditional metric fusion methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍几何和拓扑方法开发融合多传感器时间序列的新框架。该框架包括两个步骤：（1）联合延迟嵌入，其中重构高维状态空间，其中我们的传感器对应于观察功能，和（2）一个简单的正交化方案，其占这样的观察功能之间相切，并产生关于所述嵌入空间的更多样化的几何形状。我们的结论与一些合成和真实世界的实验，证明了我们的框架优于传统指标的融合方法。</font>
</div>


<hr>
<div id="paper34"> <b>34. Unsupervised Semantic Attribute Discovery and Control in Generative  Models</b>  <a href="https://arxiv.org/pdf/2002.11169" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Paul</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">I-Jeng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alajaji%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fady Alajaji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Burlina%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philippe Burlina</a><br>
<font size="3">
Abstract: This work focuses on the ability to control via latent space factors semantic image attributes in generative models, and the faculty to discover mappings from factors to attributes in an unsupervised fashion. The discovery of controllable semantic attributes is of special importance, as it would facilitate higher level tasks such as unsupervised representation learning to improve anomaly detection, or the controlled generation of novel data for domain shift and imbalanced datasets. The ability to control semantic attributes is related to the disentanglement of latent factors, which dictates that latent factors be "uncorrelated" in their effects. Unfortunately, despite past progress, the connection between control and disentanglement remains, at best, confused and entangled, requiring clarifications we hope to provide in this work. To this end, we study the design of algorithms for image generation that allow unsupervised discovery and control of semantic attributes.We make several contributions: a) We bring order to the concepts of control and disentanglement, by providing an analytical derivation that connects mutual information maximization, which promotes attribute control, to total correlation minimization, which relates to disentanglement. b) We propose hybrid generative model architectures that use mutual information maximization with multi-scale style transfer. c) We introduce a novel metric to characterize the performance of semantic attributes control. We report experiments that appear to demonstrate, quantitatively and qualitatively, the ability of the proposed model to perform satisfactory control while still preserving competitive visual quality. We compare to other state of the art methods (e.g., Frechet inception distance (FID)= 9.90 on CelebA and 4.52 on EyePACS). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：今年工作重点放在通过潜在空间因素的生成模型图像语义属性和教师发现从因素映射到属性在无监督形式的控制能力。可控语义属性的发现是特别重要的，因为这将有利于更高级别的任务，如无监督表示学习提高异常检测，或受控代域移位和不平衡数据集新颖的数据。控制语义属性的能力，是关系到潜在因素，这决定了潜在的因素在他们的影响“不相关”解开。不幸的是，尽管过去的进度，控制和解开遗迹之间的连接，充其量困惑和纠结，需要我们希望在这项工作中，以提供澄清。为此，我们研究的算法生成图像，使监督的发现和语义attributes.We的控制设计，使一些贡献：a）我们带来为了控制和解开的概念，通过提供相互连接信息的分析推导最大化，从而促进属性控制，总相关性最小化，其涉及解开。 b）我们建议使用具有多尺度的风格转移互信息最大化混合生成模型架构。 c）我们介绍一种新颖的度量来表征语义属性控制的性能。我们报告似乎证实了，定量和定性实验，该模型的能力进行良好的控制，同时仍保持有竞争力的视觉质量。我们比较的现有技术方法（例如，Frechet可开始距离（FID）=上EyePACS 9.90上CelebA和4.52）等的状态。</font>
</div>


<hr>
<div id="paper35"> <b>35. CLARA: Clinical Report Auto-completion</b>  <a href="https://arxiv.org/pdf/2002.11701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Biswal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siddharth Biswal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cao Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Glass%2C+L+M" target="_blank" rel="noopener" style="color:#0000EE;">Lucas M. Glass</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Westover%2C+M+B" target="_blank" rel="noopener" style="color:#0000EE;">M. Brandon Westover</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jimeng Sun</a><br>
<font size="3">
Abstract: Generating clinical reports from raw recordings such as X-rays and electroencephalogram (EEG) is an essential and routine task for doctors. However, it is often time-consuming to write accurate and detailed reports. Most existing methods try to generate the whole reports from the raw input with limited success because 1) generated reports often contain errors that need manual review and correction, 2) it does not save time when doctors want to write additional information into the report, and 3) the generated reports are not customized based on individual doctors' preference. We propose {\it CL}inic{\it A}l {\it R}eport {\it A}uto-completion (CLARA), an interactive method that generates reports in a sentence by sentence fashion based on doctors' anchor words and partially completed sentences. CLARA searches for most relevant sentences from existing reports as the template for the current report. The retrieved sentences are sequentially modified by combining with the input feature representations to create the final report. In our experimental evaluation, CLARA achieved 0.393 CIDEr and 0.248 BLEU-4 on X-ray reports and 0.482 CIDEr and 0.491 BLEU-4 for EEG reports for sentence-level generation, which is up to 35% improvement over the best baseline. Also via our qualitative evaluation, CLARA is shown to produce reports which have a significantly higher level of approval by doctors in a user study (3.74 out of 5 for CLARA vs 2.52 out of 5 for the baseline). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从原材料的录音，如X射线和脑电图（EEG）生成的临床报告是对医生的基本和例行的任务。然而，它往往是耗费时间来写准确和详细的报告。大多数现有的方法试图生成有限的成功原始输入整个报告，因为1）生成的报告一般都需要人工审核和修正，2个错误），它不会把时间浪费在医生想写更多的信息到报告，并3）将所生成的报告不是定制的基于个体医生的偏好。我们提出{\它CL} INIC {\它A}升{\它R}扩展端口{\它A} UTO完成（圣克拉拉），其基于医生的锚词语由句子的方式在一个句子中报告的交互式方法并部分完成句子。 CLARA搜索从作为模板当前报告的现有报告最相关的句子。检索到的句子与输入特征表示相结合，创造了最终报告相继修改。在我们的实验评价，CLARA达到0.393苹果酒和X射线报告0.248 BLEU-4和0.482苹果酒和0.491 BLEU-4的句子级的一代，是在最好的基线高达35％的改善脑电图报告。也可以通过我们的定性评价，CLARA显示，其具有通过医生在用户研究中显著较高水平批准生产报告（3.74出5 CLARA VS 2.52，满分5为基准）。</font>
</div>


<hr>
<div id="paper36"> <b>36. Inceptive Event Time-Surfaces for Object Classification Using  Neuromorphic Cameras</b>  <a href="https://arxiv.org/pdf/2002.11656" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Baldwin%2C+R+W" target="_blank" rel="noopener" style="color:#0000EE;">R Wes Baldwin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Almatrafi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammed Almatrafi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaufman%2C+J+R" target="_blank" rel="noopener" style="color:#0000EE;">Jason R Kaufman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Asari%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vijayan Asari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hirakawa%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Keigo Hirakawa</a><br>
<font size="3">
Abstract: This paper presents a novel fusion of low-level approaches for dimensionality reduction into an effective approach for high-level objects in neuromorphic camera data called Inceptive Event Time-Surfaces (IETS). IETSs overcome several limitations of conventional time-surfaces by increasing robustness to noise, promoting spatial consistency, and improving the temporal localization of (moving) edges. Combining IETS with transfer learning improves state-of-the-art performance on the challenging problem of object classification utilizing event camera data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文呈现低级别的新的融合方法为维数降低到用于高级对象中神经形态相机数据称为表始动词事件时间的表面（IETS）的有效方法。 IETSs通过增加鲁棒性噪声，促进空间一致性，改善的（移动）的边缘的时间本地化克服传统的时间表面的一些局限性。与转印学习结合IETS提高了利用事件相机数据对象分类的具有挑战性的问题状态的最先进的性能。</font>
</div>


<hr>
<div id="paper37"> <b>37. Revisiting Ensembles in an Adversarial Context: Improving Natural  Accuracy</b>  <a href="https://arxiv.org/pdf/2002.11572" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/stat?searchtype=author&query=Saligrama%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aditya Saligrama</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Leclerc%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillaume Leclerc</a><br>
<font size="3">
Abstract: A necessary characteristic for the deployment of deep learning models in real world applications is resistance to small adversarial perturbations while maintaining accuracy on non-malicious inputs. While robust training provides models that exhibit better adversarial accuracy than standard models, there is still a significant gap in natural accuracy between robust and non-robust models which we aim to bridge. We consider a number of ensemble methods designed to mitigate this performance difference. Our key insight is that model trained to withstand small attacks, when ensembled, can often withstand significantly larger attacks, and this concept can in turn be leveraged to optimize natural accuracy. We consider two schemes, one that combines predictions from several randomly initialized robust models, and the other that fuses features from robust and standard models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习模式在实际应用部署的一个必要特征是小对抗性干扰性，同时保持对非恶意输入的准确性。虽然强大的培训提供了表现出比标准车型更好的对抗精度的模型，还有在我们的目标是弥合稳健和非可靠的模型之间的自然精度显著的差距。我们考虑了一些旨在缓解这种性能上的差异集成方法。我们的主要观点是，模型中训练的承受小的攻击，合奏的时候，往往能承受较大的显著攻击，这个概念又可以被利用来优化自然准确性。我们考虑两个方案，一个是从几个随机初始化可靠的模型预测，联合收割机，另一项融合了功能强大的从标准模型。</font>
</div>


<hr>
<div id="paper38"> <b>38. Region of Interest Identification for Brain Tumors in Magnetic Resonance  Images</b>  <a href="https://arxiv.org/pdf/2002.11509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Mostafaie%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fateme Mostafaie</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Teimouri%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Reihaneh Teimouri</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Nabizadeh%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zahra Nabizadeh</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Karimi%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nader Karimi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Samavi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shadrokh Samavi</a><br>
<font size="3">
Abstract: Glioma is a common type of brain tumor, and accurate detection of it plays a vital role in the diagnosis and treatment process. Despite advances in medical image analyzing, accurate tumor segmentation in brain magnetic resonance (MR) images remains a challenge due to variations in tumor texture, position, and shape. In this paper, we propose a fast, automated method, with light computational complexity, to find the smallest bounding box around the tumor region. This region-of-interest can be used as a preprocessing step in training networks for subregion tumor segmentation. By adopting the outputs of this algorithm, redundant information is removed; hence the network can focus on learning notable features related to subregions' classes. The proposed method has six main stages, in which the brain segmentation is the most vital step. Expectation-maximization (EM) and K-means algorithms are used for brain segmentation. The proposed method is evaluated on the BraTS 2015 dataset, and the average gained DICE score is 0.73, which is an acceptable result for this application. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经胶质瘤是脑瘤的一种常见类型，它的精确检测对诊断和治疗过程中的重要作用。尽管在脑磁共振在医学图像分析，准确肿瘤分割的进展（MR）图像仍然是一个挑战，因为在肿瘤的纹理，位置，和形状的变化。在本文中，我们提出了一个快速，自动化的方法，以较轻的运算复杂度，找到肿瘤周围区域的最小边界框。这个区域的兴趣可以被用作在训练网络的子区域肿瘤分割的预处理步骤。通过采用该算法的输出，冗余信息被去除;因此网络可以集中精力学习有关次区域班显着特点。该方法有六个主要阶段，在大脑分割是最关键的一步。期望最大化（EM）和K均值算法用于脑分割。所提出的方法在评价臭小子2015年数据集，以及平均获得DICE得分为0.73，这是本申请中可接受的结果。</font>
</div>


<hr>
<div id="paper39"> <b>39. Unpaired Image Super-Resolution using Pseudo-Supervision</b>  <a href="https://arxiv.org/pdf/2002.11397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Maeda%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shunta Maeda</a><br>
<font size="3">
Abstract: In most studies on learning-based image super-resolution (SR), the paired training dataset is created by downscaling high-resolution (HR) images with a predetermined operation (e.g., bicubic). However, these methods fail to super-resolve real-world low-resolution (LR) images, for which the degradation process is much more complicated and unknown. In this paper, we propose an unpaired SR method using a generative adversarial network that does not require a paired/aligned training dataset. Our network consists of an unpaired kernel/noise correction network and a pseudo-paired SR network. The correction network removes noise and adjusts the kernel of the inputted LR image; then, the corrected clean LR image is upscaled by the SR network. In the training phase, the correction network also produces a pseudo-clean LR image from the inputted HR image, and then a mapping from the pseudo-clean LR image to the inputted HR image is learned by the SR network in a paired manner. Because our SR network is independent of the correction network, well-studied existing network architectures and pixel-wise loss functions can be integrated with the proposed framework. Experiments on diverse datasets show that the proposed method is superior to existing solutions to the unpaired SR problem. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对基于学习图像超分辨率（SR）大多数研究中，配对的训练数据集由缩减的高分辨率（HR）图像与预定的操作（例如，双三次）创建。然而，这些方法不能超解决真实世界的低分辨率（LR）图像，其降解过程要复杂得多的和未知的。在本文中，我们建议使用不需要配对/对准训练数据集生成对抗性网络未配对的SR方法。我们的网络是由一个未成对内核/噪声校正网络和伪配对SR网络。校正网络噪声去除，并调整所输入的LR图像的内核;然后，校正后的清洁LR图像通过SR网络放大的。在训练阶段，校正网络还从输入的HR图像产生的伪干净LR图像，然后从伪清洁LR图像与输入的HR图像的映射是通过以成对方式的SR网络获知。由于我们的SR网络是独立的修正网络，充分研究现有的网络架构和像素方面的损失函数可以用建议的框架进行集成。对不同的数据集实验表明，该方法是优于对非成对SR问题的现有解决方案。</font>
</div>


<hr>
<div id="paper40"> <b>40. CheXpedition: Investigating Generalization Challenges for Translation of  Chest X-Ray Algorithms to the Clinical Setting</b>  <a href="https://arxiv.org/pdf/2002.11379" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Rajpurkar%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pranav Rajpurkar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Joshi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anirudh Joshi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Pareek%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anuj Pareek</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phil Chen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kiani%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amirhossein Kiani</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Irvin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeremy Irvin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ng%2C+A+Y" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Y. Ng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lungren%2C+M+P" target="_blank" rel="noopener" style="color:#0000EE;">Matthew P. Lungren</a><br>
<font size="3">
Abstract: Although there have been several recent advances in the application of deep learning algorithms to chest x-ray interpretation, we identify three major challenges for the translation of chest x-ray algorithms to the clinical setting. We examine the performance of the top 10 performing models on the CheXpert challenge leaderboard on three tasks: (1) TB detection, (2) pathology detection on photos of chest x-rays, and (3) pathology detection on data from an external institution. First, we find that the top 10 chest x-ray models on the CheXpert competition achieve an average AUC of 0.851 on the task of detecting TB on two public TB datasets without fine-tuning or including the TB labels in training data. Second, we find that the average performance of the models on photos of x-rays (AUC = 0.916) is similar to their performance on the original chest x-ray images (AUC = 0.924). Third, we find that the models tested on an external dataset either perform comparably to or exceed the average performance of radiologists. We believe that our investigation will inform rapid translation of deep learning algorithms to safe and effective clinical decision support tools that can be validated prospectively with large impact studies and clinical trials. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然有过在深学习算法应用到X线胸片解释一些最新进展，我们确定的胸部X射线算法临床设置翻译三大挑战。我们研究在CheXpert挑战排行榜前10执行模型的三个任务的性能：（1）TB检测，（2）胸部X射线照片病理学检测，和（3）病理上的数据从外部机构检测。首先，我们发现，在CheXpert比赛的前10胸部X射线模型上检测两个市民对结核病TB的数据集没有微调或包括训练数据的TB标签任务达到0.851的平均AUC。其次，我们发现模型对X射线（AUC = 0.916）的照片的平均表现是相似，他们对原胸部X射线图像（AUC = 0.924）的性能。第三，我们发现，在外部数据集测试的模型或者执行相当或超过放射科医生的平均表现。我们相信，我们的调查会通知的深度学习算法，快速翻译，可以前瞻性地有大影响的研究和临床试验验证安全有效的临床决策支持工具。</font>
</div>


<hr>
<div id="paper41"> <b>41. Invariance vs. Robustness of Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.11318" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kamath%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sandesh Kamath</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deshpande%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amit Deshpande</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Subrahmanyam%2C+K+V" target="_blank" rel="noopener" style="color:#0000EE;">K V Subrahmanyam</a><br>
<font size="3">
Abstract: We study the performance of neural network models on random geometric transformations and adversarial perturbations. Invariance means that the model's prediction remains unchanged when a geometric transformation is applied to an input. Adversarial robustness means that the model's prediction remains unchanged after small adversarial perturbations of an input. In this paper, we show a quantitative trade-off between rotation invariance and robustness. We empirically study the following two cases: (a) change in adversarial robustness as we improve only the invariance of equivariant models via training augmentation, (b) change in invariance as we improve only the adversarial robustness using adversarial training. We observe that the rotation invariance of equivariant models (StdCNNs and GCNNs) improves by training augmentation with progressively larger random rotations but while doing so, their adversarial robustness drops progressively, and very significantly on MNIST. We take adversarially trained LeNet and ResNet models which have good $L_\infty$ adversarial robustness on MNIST and CIFAR-10, respectively, and observe that adversarial training with progressively larger perturbations results in a progressive drop in their rotation invariance profiles. Similar to the trade-off between accuracy and robustness known in previous work, we give a theoretical justification for the invariance vs. robustness trade-off observed in our experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们研究的神经网络模型的随机几何变换和对抗性干扰性能。不变性装置，当一个几何变换应用于输入模型预测保持不变。对抗性鲁棒性意味着该模型的预测之后仍然输入的小扰动对抗性不变。在本文中，我们将展示旋转不变性和鲁棒性之间的定量权衡。我们实证研究了以下两种情况：在对抗鲁棒性（一）变更为我们改善只能通过训练增强等变模型的不变性，在不变性（B）变化我们使用对抗训练来提高只有敌对的鲁棒性。我们观察到，等变模型（StdCNNs和GCNNs）的旋转不变性通过培训增强改善了与逐渐变大的随机轮换，但在这样做时，他们的对抗性稳健性逐步下降，而且非常显著的MNIST。我们采取adversarially训练有素这对分别MNIST和CIFAR-10，良好的$ L_ \ infty $对抗性的鲁棒性LeNet和RESNET模型，并观察对抗性训练，在他们的旋转不变性曲线渐进下降逐渐变大扰动的结果。在权衡准确性和鲁棒性在以往的工作称为相似，我们给权衡在我们的实验中观察到的不变性与鲁棒性的理论论证。</font>
</div>


<hr>
<div id="paper42"> <b>42. From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation  (VIN)</b>  <a href="https://arxiv.org/pdf/2002.11310" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Ye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yezhou Yang</a><br>
<font size="3">
Abstract: Visual Indoor Navigation (VIN) task has drawn increasing attentions from the data-driven machine learning communities especially with the recent reported success from learning-based methods. Due to the innate complexity of this task, researchers have tried approaching the problem from a variety of different angles, the full scope of which has not yet been captured within an overarching report. In this survey, we discuss the representative work of learning-based approaches for visual navigation and its related tasks. Firstly, we summarize the current work in terms of task representations and applied methods along with their properties. We then further identify and discuss lingering issues impeding the performance of VIN tasks and motivate future research in these key areas worth exploring in the future for the community. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可视室内导航（VIN）的任务已经引起了数据驱动的机器学习社区越来越多的关注特别是从学习基础的方法，最近成功报道。由于这项任务的复杂性与生俱来，研究人员一直试图从各种不同的角度接近问题的全部范围，其中还没有一个总体报告内捕获。在本次调查中，我们讨论的视觉导航及其相关任务基于学习的方法的代表作。首先，我们总结了任务交涉和应用方法方面目前的工作与自己的属性一起。然后，我们进一步确定和讨论遗留问题阻碍在这些关键领域的价值在未来的探索，为社会VIN任务和激励未来研究的性能。</font>
</div>


<hr>
<div id="paper43"> <b>43. Deep Learning and Statistical Models for Time-Critical Pedestrian  Behaviour Prediction</b>  <a href="https://arxiv.org/pdf/2002.11226" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dabrowski%2C+J+J" target="_blank" rel="noopener" style="color:#0000EE;">Joel Janek Dabrowski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Villiers%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Johan Pieter de Villiers</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashfaqur Rahman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Beyers%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Conrad Beyers</a><br>
<font size="3">
Abstract: The time it takes for a classifier to make an accurate prediction can be crucial in many behaviour recognition problems. For example, an autonomous vehicle should detect hazardous pedestrian behaviour early enough for it to take appropriate measures. In this context, we compare the switching linear dynamical system (SLDS) and a three-layered bi-directional long short-term memory (LSTM) neural network, which are applied to infer pedestrian behaviour from motion tracks. We show that, though the neural network model achieves an accuracy of 80%, it requires long sequences to achieve this (100 samples or more). The SLDS, has a lower accuracy of 74%, but it achieves this result with short sequences (10 samples). To our knowledge, such a comparison on sequence length has not been considered in the literature before. The results provide a key intuition of the suitability of the models in time-critical problems. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：花费分类的时间做出准确的预测可以在许多行为识别问题的关键。例如，自主车辆应检测危险行人行为及早为它采取适当的措施。在这方面，我们比较线性动态系统（SLDS）和三层双向长短期记忆（LSTM）神经网络，这是从运动轨道施加到推断行人行为的切换。我们表明，虽然神经网络模型实现了80％的准确度，这需要很长的序列来实现这一目标（100个样本以上）。该SLDS，具有74％的较低的精度，但它达到这个结果与短序列（10个样品）。据我们所知，在序列长度这样的比较并没有在之前的文献中考虑。结果提供了模型的适用性的关键直觉在时间紧迫的问题。</font>
</div>


<hr>
<div id="paper44"> <b>44. End-to-End Models for the Analysis of System 1 and System 2 Interactions  based on Eye-Tracking Data</b>  <a href="https://arxiv.org/pdf/2002.11192" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Rossi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessandro Rossi</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Ermini%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sara Ermini</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Bernabini%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dario Bernabini</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Zanca%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dario Zanca</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Todisco%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marino Todisco</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Genovese%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessandro Genovese</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Rizzo%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonio Rizzo</a><br>
<font size="3">
Abstract: While theories postulating a dual cognitive system take hold, quantitative confirmations are still needed to understand and identify interactions between the two systems or conflict events. Eye movements are among the most direct markers of the individual attentive load and may serve as an important proxy of information. In this work we propose a computational method, within a modified visual version of the well-known Stroop test, for the identification of different tasks and potential conflicts events between the two systems through the collection and processing of data related to eye movements. A statistical analysis shows that the selected variables can characterize the variation of attentive load within different scenarios. Moreover, we show that Machine Learning techniques allow to distinguish between different tasks with a good classification accuracy and to investigate more in depth the gaze dynamics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管理论postulating双认知系统搦，定量确认仍然需要了解和识别两个系统或冲突事件之间的相互作用。眼球运动个别细心负荷的最直接的指标中，可作为一个重要的信息代理。在这项工作中，我们提出了一种计算方法，众所周知的斯特鲁普测试的修改版本的视觉范围内，针对不同的任务，并通过相关的眼睛运动数据的收集和处理两个系统之间的潜在冲突事件的鉴定。统计分析显示，所选择的变量可以不同的方案中表征细心负载的变化。此外，我们表明，机器学习技术允许具有良好的分类准确率和不同的任务之间进行区分，调查更深入的凝视动态。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-27</title>
    <url>/2020/02/27/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-27/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Marathi To English Neural Machine Translation With Near Perfect Corpus  And Transformers <a href="https://arxiv.org/pdf/2002.11643" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Using Distributional Thesaurus Embedding for Co-hyponymy Detection <a href="https://arxiv.org/pdf/2002.11506" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Detecting Potential Topics In News Using BERT, CRF and Wikipedia <a href="https://arxiv.org/pdf/2002.11402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Speech2Phone: A Multilingual and Text Independent Speaker Identification  Model <a href="https://arxiv.org/pdf/2002.11213" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> End-to-End Entity Linking and Disambiguation leveraging Word and  Knowledge Graph Embeddings <a href="https://arxiv.org/pdf/2002.11143" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Trends of digitalization and adoption of big data &amp; analytics among UK  SMEs: Analysis and lessons drawn from a case study of 53 SMEs <a href="https://arxiv.org/pdf/2002.11623" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Object Relational Graph with Teacher-Recommended Learning for Video  Captioning <a href="https://arxiv.org/pdf/2002.11566" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Sparse Sinkhorn Attention <a href="https://arxiv.org/pdf/2002.11296" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> A Density Ratio Approach to Language Model Fusion in End-To-End  Automatic Speech Recognition <a href="https://arxiv.org/pdf/2002.11268" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Marathi To English Neural Machine Translation With Near Perfect Corpus  And Transformers</b>  <a href="https://arxiv.org/pdf/2002.11643" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jadhav%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Swapnil Ashok Jadhav</a><br>
<font size="3">
Abstract: There have been very few attempts to benchmark performances of state-of-the-art algorithms for Neural Machine Translation task on Indian Languages. Google, Bing, Facebook and Yandex are some of the very few companies which have built translation systems for few of the Indian Languages. Among them, translation results from Google are supposed to be better, based on general inspection. Bing-Translator do not even support Marathi language which has around 95 million speakers and ranks 15th in the world in terms of combined primary and secondary speakers. In this exercise, we trained and compared variety of Neural Machine Marathi to English Translators trained with BERT-tokenizer by huggingface and various Transformer based architectures using Facebook's Fairseq platform with limited but almost correct parallel corpus to achieve better BLEU scores than Google on Tatoeba and Wikimedia open datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目前已经很少有企图的国家的最先进的算法基准演出在印度语言神经机器翻译的任务。谷歌，必应，Facebook和Yandex的是一些已建成的翻译系统几个印度语言的企业很少。其中，从谷歌翻译的结果应该是更好的，基于一般检查。炳翻译甚至不支持其中有大约95亿人讲和中小学综合扬声器方面排名第15位的世界马拉语言。在这个练习中，我们训练和比较各种神经机马拉的英文翻译与BERT-标记者训练有素的huggingface并使用Facebook的Fairseq平台限制各种基于变压器的架构，但几乎正确的平行语料库，以达到更好的BLEU得分高于谷歌在Tatoeba和维基媒体开放的数据集。</font>
</div>


<hr>
<div id="paper2"> <b>2. Using Distributional Thesaurus Embedding for Co-hyponymy Detection</b>  <a href="https://arxiv.org/pdf/2002.11506" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jana%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhik Jana</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Varimalla%2C+N+R" target="_blank" rel="noopener" style="color:#0000EE;">Nikhil Reddy Varimalla</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pawan Goyal</a><br>
<font size="3">
Abstract: Discriminating lexical relations among distributionally similar words has always been a challenge for natural language processing (NLP) community. In this paper, we investigate whether the network embedding of distributional thesaurus can be effectively utilized to detect co-hyponymy relations. By extensive experiments over three benchmark datasets, we show that the vector representation obtained by applying node2vec on distributional thesaurus outperforms the state-of-the-art models for binary classification of co-hyponymy vs. hypernymy, as well as co-hyponymy vs. meronymy, by huge margins. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：中分布式地相似的单词词汇辨析的关系一直是自然语言处理（NLP）社会面临的挑战。在本文中，我们调查是否在网络中嵌入分布式词库可以有效地利用检测共下义关系。通过在三个标准数据集大量的实验，我们表明，通过应用node2vec获得的矢量表示分布式词库性能优于国家的最先进的型号为共同下义与hypernymy的二元分类，以及共同下义与分体法，通过巨大的利润。</font>
</div>


<hr>
<div id="paper3"> <b>3. Detecting Potential Topics In News Using BERT, CRF and Wikipedia</b>  <a href="https://arxiv.org/pdf/2002.11402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jadhav%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Swapnil Ashok Jadhav</a><br>
<font size="3">
Abstract: For a news content distribution platform like Dailyhunt, Named Entity Recognition is a pivotal task for building better user recommendation and notification algorithms. Apart from identifying names, locations, organisations from the news for 13+ Indian languages and use them in algorithms, we also need to identify n-grams which do not necessarily fit in the definition of Named-Entity, yet they are important. For example, "me too movement", "beef ban", "alwar mob lynching". In this exercise, given an English language text, we are trying to detect case-less n-grams which convey important information and can be used as topics and/or hashtags for a news. Model is built using Wikipedia titles data, private English news corpus and BERT-Multilingual pre-trained model, Bi-GRU and CRF architecture. It shows promising results when compared with industry best Flair, Spacy and Stanford-caseless-NER in terms of F1 and especially Recall. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对于新闻内容发布平台像Dailyhunt，命名实体识别是建立更好的用户建议，并通知算法的关键任务。除了从13+印度语新闻识别名称，位置，组织和算法使用它们，我们还需要确定的n-gram它不一定适合在命名实体的定义，但他们却是非常重要的。例如，“我也运动”，“牛肉禁令”，“暴民阿尔瓦尔私刑”。在这个练习中，由于英语语言的文本，我们试图检测无外壳正克传达重要信息，可作为主题和/或主题标签的消息。模型使用维基百科标题数据建，私人英语新闻语料和BERT，多语种预训练模式，双GRU和CRF架构。当同行业相比，它显示了可喜的成果最好的天才，Spacy和F1，尤其是召回方面斯坦福无壳-ER。</font>
</div>


<hr>
<div id="paper4"> <b>4. Speech2Phone: A Multilingual and Text Independent Speaker Identification  Model</b>  <a href="https://arxiv.org/pdf/2002.11213" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Casanova%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Edresson Casanova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Junior%2C+A+C" target="_blank" rel="noopener" style="color:#0000EE;">Arnaldo Candido Junior</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shulby%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christopher Shulby</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=da+Silva%2C+H+P" target="_blank" rel="noopener" style="color:#0000EE;">Hamilton Pereira da Silva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Paula+Filho%2C+P+L" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Luiz de Paula Filho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cordeiro%2C+A+F" target="_blank" rel="noopener" style="color:#0000EE;">Alessandro Ferreira Cordeiro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Oliveira+Guedes%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Victor de Oliveira Guedes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Aluisio%2C+S+M" target="_blank" rel="noopener" style="color:#0000EE;">Sandra Maria Aluisio</a><br>
<font size="3">
Abstract: Voice recognition is an area with a wide application potential. Speaker identification is useful in several voice recognition tasks, as seen in voice-based authentication, transcription systems and intelligent personal assistants. Some tasks benefit from open-set models which can handle new speakers without the need of retraining. Audio embeddings for speaker identification is a proposal to solve this issue. However, choosing a suitable model is a difficult task, especially when the training resources are scarce. Besides, it is not always clear whether embeddings are as good as more traditional methods. In this work, we propose the Speech2Phone and compare several embedding models for open-set speaker identification, as well as traditional closed-set models. The models were investigated in the scenario of small datasets, which makes them more applicable to languages in which data scarceness is an issue. The results show that embeddings generated by artificial neural networks are competitive when compared to classical approaches for the task. Considering a testing dataset composed of 20 speakers, the best models reach accuracies of 100% and 76.96% for closed an open set scenarios, respectively. Results suggest that the models can perform language independent speaker identification. Among the tested models, a fully connected one, here presented as Speech2Phone, led to the higher accuracy. Furthermore, the models were tested for different languages showing that the knowledge learned was successfully transferred for close and distant languages to Portuguese (in terms of vocabulary). Finally, the models can scale and can handle more speakers than they were trained for, identifying 150% more speakers while still maintaining 55% accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语音识别是具有广泛应用前景的领域。说话人识别是在几个语音识别任务非常有用，如基于语音的认证，转录系统和智能个人助理看到。有些任务受益于开放式集的模型，它可以处理新的扬声器，而不需要再培训的。对于说话人识别音频的嵌入是解决这一问题的建议。然而，选择一个合适的模型是一项艰巨的任务，尤其是在培训资源是稀缺的。此外，它并不总是很清楚的嵌入是否不如更传统的方法。在这项工作中，我们提出了Speech2Phone并比较开集说话人识别几个嵌入模型，以及传统的闭集模型。该车型在小型数据集的情况下，这使得它们更适用于数据珍异是一个问题的语言进行了调查。结果表明，相对于任务的经典方法时，人工神经网络产生的嵌入具有竞争力。考虑到20个扬声器组成的测试数据集，最好的车型分别达到100％和76.96％，精度为封闭的开集情景。结果表明，该模型可以执行独立于语言的说话人识别。在这些测试的模型，完全连接的一个，这里作为Speech2Phone，导致了更高的精度。此外，该机型是为表明学过的知识被成功地转移了靠近和远离的语言葡萄牙语（在词汇方面）不同的语言测试。最后，该模型可以扩展，并且可以处理更多的扬声器比他们进行了培训，辨认更150％的扬声器，同时仍保持55％的准确率。</font>
</div>


<hr>
<div id="paper5"> <b>5. End-to-End Entity Linking and Disambiguation leveraging Word and  Knowledge Graph Embeddings</b>  <a href="https://arxiv.org/pdf/2002.11143" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nedelchev%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rostislav Nedelchev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaudhuri%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Debanjan Chaudhuri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lehmann%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jens Lehmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fischer%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asja Fischer</a><br>
<font size="3">
Abstract: Entity linking - connecting entity mentions in a natural language utterance to knowledge graph (KG) entities is a crucial step for question answering over KGs. It is often based on measuring the string similarity between the entity label and its mention in the question. The relation referred to in the question can help to disambiguate between entities with the same label. This can be misleading if an incorrect relation has been identified in the relation linking step. However, an incorrect relation may still be semantically similar to the relation in which the correct entity forms a triple within the KG; which could be captured by the similarity of their KG embeddings. Based on this idea, we propose the first end-to-end neural network approach that employs KG as well as word embeddings to perform joint relation and entity classification of simple questions while implicitly performing entity disambiguation with the help of a novel gating mechanism. An empirical evaluation shows that the proposed approach achieves a performance comparable to state-of-the-art entity linking while requiring less post-processing. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：实体连接 - 连接实体的自然语言语句知识图（KG）实体提到是问题回答了幼儿园的关键一步。它往往是基于测量的实体标签，它在这个问题提到之间的串相似度。的关系中提到的问题，可以帮助具有相同标签的实体之间的歧义。这可能是，如果一个不正确的关系已经在相关联的步骤已经确定误导。然而，不正确的关系仍然可以是语义上相似，其中正确的实体形成内KG三重的关系;这可以通过他们的KG的嵌入的相似性被捕获。基于这一思路，我们建议采用KG和字的嵌入，同时含蓄地用新的门控机制的帮助下进行实体消歧进行的简单问题的联合关系，实体分类第一终端到终端的神经网络方法。一种经验评估表明，所提出的方法实现了相当先进的最先进的实体，同时要求较少的后处理联性能。</font>
</div>


<hr>
<div id="paper6"> <b>6. Trends of digitalization and adoption of big data &amp; analytics among UK  SMEs: Analysis and lessons drawn from a case study of 53 SMEs</b>  <a href="https://arxiv.org/pdf/2002.11623" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohamed%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Muhidin Mohamed</a><br>
<font size="3">
Abstract: Small and Medium Enterprises (SMEs) now generate digital data at an unprecedented rate from online transactions, social media marketing and associated customer interactions, online product or service reviews and feedback, clinical diagnosis, Internet of Things (IoT) sensors, and production processes. All these forms of data can be transformed into monetary value if put into a proper data value chain. This requires both skills and IT investments for the long-term benefit of businesses. However, such spending is beyond the capacity of most SMEs due to their limited resources and restricted access to finances. This paper presents lessons learned from a case study of 53 UK SMEs, mostly from the West Midlands region of England, supported as part of a 3-year ERDF project, Big Data Corridor, in the areas of big data management, analytics and related IT issues. Based on our study's sample companies, several perspectives including the digital technology trends, challenges facing the UK SMEs, and the state of their adoption in data analytics and big data, are presented in the paper. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：中小型企业（SMEs）现在可以生成从网上交易以前所未有的速度数字数据，社会化媒体营销和相关的客户互动，在线产品或服务的评论和反馈，临床诊断，物联网（IOT）互联网传感器，以及生产流程。所有这些形式的数据可如果放到一个正确的数据价值链转化成货币价值。这既需要技能和企业的长期效益的IT投资。然而，这样的支出超出了大多数中小企业的能力，由于其有限的资源和资金限制访问。本文礼物教训53个英国中小企业，大多来自英国西米德兰地区，支持为3年ERDF项目，大数据通道的一部分的情况下节课的学习，在大数据管理，分析的领域和相关的IT的问题。根据我们研究的样本公司，几个方面，包括数字化技术的发展趋势，面临的挑战英国中小企业，以及它们在数据分析和大数据采用的状态，在论文分别介绍。</font>
</div>


<hr>
<div id="paper7"> <b>7. Object Relational Graph with Teacher-Recommended Learning for Video  Captioning</b>  <a href="https://arxiv.org/pdf/2002.11566" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaya Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunfeng Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bing Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peijin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weiming Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zha%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengjun Zha</a><br>
<font size="3">
Abstract: Taking full advantage of the information from both vision and language is critical for the video captioning task. Existing models lack adequate visual representation due to the neglect of interaction between object, and sufficient training for content-related words due to long-tailed problems. In this paper, we propose a complete video captioning system including both a novel model and an effective training strategy. Specifically, we propose an object relational graph (ORG) based encoder, which captures more detailed interaction features to enrich visual representation. Meanwhile, we design a teacher-recommended learning (TRL) method to make full use of the successful external language model (ELM) to integrate the abundant linguistic knowledge into the caption model. The ELM generates more semantically similar word proposals which extend the ground-truth words used for training to deal with the long-tailed problem. Experimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the proposed ORG-TRL system achieves state-of-the-art performance. Extensive ablation studies and visualizations illustrate the effectiveness of our system. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在充分考虑来自视觉和语言的信息，充分利用是视频字幕任务的关键。现有车型缺乏足够的可视化表示，由于物体之间的相互作用的忽视，以及由于长尾问题内容有关的词汇足够的培训。在本文中，我们提出了一个完整的视频字幕系统，既包括新的模型和有效的培训战略。具体地，我们提出了一种对象关系图（ORG）基于编码器，其捕获更详细交互功能来丰富视觉表示。同时，我们设计了一个老师推荐的学习（TRL）方法，以充分利用外部成功的语言模型（ELM）与丰富的语言知识融入到标题的模式。榆树产生更多的语义延伸用于训练应对长尾问题的地面实况话相似字的建议。在三个基准实验评估：MSVD，MSR-VTT和VATEX表明了该ORG-TRL系统实现国家的最先进的性能。广泛切除研究和可视化说明我们系统的有效性。</font>
</div>


<hr>
<div id="paper8"> <b>8. Sparse Sinkhorn Attention</b>  <a href="https://arxiv.org/pdf/2002.11296" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tay%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Tay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bahri%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dara Bahri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liu Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Metzler%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Donald Metzler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Juan%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Da-Cheng Juan</a><br>
<font size="3">
Abstract: We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出稀疏Sinkhorn注意，学习去参加一个新的高效和稀疏的方法。我们的方法是基于内部表示微排序。具体而言，我们引入一个元排序网络学会产生超过序列潜排列。鉴于排序序列，我们就能够计算准全球的关注，只有本地窗口，提高注意力模块的存储效率。为此，我们提出了新的算法创新，如因果Sinkhorn平衡和SortCut，用于定制Sinkhorn注意，用于编码和/或解码目的的动态序列截断方法。通过对算法seq2seq排序，语言建模，逐像素图像生成，文档分类和自然语言推理广泛的实验，我们证明了我们的记忆效率Sinkhorn注意力的方法是用香草关注竞争力和持续优于最近提出的高效变压器等车型稀疏变形金刚。</font>
</div>


<hr>
<div id="paper9"> <b>9. A Density Ratio Approach to Language Model Fusion in End-To-End  Automatic Speech Recognition</b>  <a href="https://arxiv.org/pdf/2002.11268" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=McDermott%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erik McDermott</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sak%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hasim Sak</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Variani%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ehsan Variani</a><br>
<font size="3">
Abstract: This article describes a density ratio approach to integrating external Language Models (LMs) into end-to-end models for Automatic Speech Recognition (ASR). Applied to a Recurrent Neural Network Transducer (RNN-T) ASR model trained on a given domain, a matched in-domain RNN-LM, and a target domain RNN-LM, the proposed method uses Bayes' Rule to define RNN-T posteriors for the target domain, in a manner directly analogous to the classic hybrid model for ASR based on Deep Neural Networks (DNNs) or LSTMs in the Hidden Markov Model (HMM) framework (Bourlard & Morgan, 1994). The proposed approach is evaluated in cross-domain and limited-data scenarios, for which a significant amount of target domain text data is used for LM training, but only limited (or no) {audio, transcript} training data pairs are used to train the RNN-T. Specifically, an RNN-T model trained on paired audio & transcript data from YouTube is evaluated for its ability to generalize to Voice Search data. The Density Ratio method was found to consistently outperform the dominant approach to LM and end-to-end ASR integration, Shallow Fusion. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了一种密度比逼近外部语言模型（LMS）集成到终端到高端机型的自动语音识别（ASR）。施加到上训练给定域中的回归神经网络传感器（RNN-T）ASR模型，匹配在域RNN-LM，和目标域RNN-LM，所提出的方法使用贝叶斯规则来定义RNN-T后验目标域，在直接类似于基于深神经网络（DNNs）或在隐马尔可夫模型（HMM）的框架（Bourlard和Morgan，1994）LSTMs对ASR的经典混合模型的方式。所提出的方法在跨域和有限的数据情况下，对于其用于LM训练目标域的文本数据的一个显著量进行评价，但只有有限的（或没有）{音频，转录}训练数据对被用来训练所述RNN-T。具体而言，从YouTube训练的成对的音频和成绩单数据RNN-T模式是其推广到语音搜索数据的能力进行评估。密度比的方法，发现持续超越的主要途径LM和终端到终端的整合ASR，浅层融合。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>python 发送带附件邮件</title>
    <url>/2020/02/27/python-%E5%8F%91%E9%80%81%E5%B8%A6%E9%99%84%E4%BB%B6%E9%82%AE%E4%BB%B6/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> smtplib</span><br><span class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</span><br><span class="line"><span class="keyword">from</span> email.mime.application <span class="keyword">import</span> MIMEApplication</span><br><span class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_email</span><span class="params">(filename, message_type=<span class="string">'html'</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 第三方 SMTP 服务</span></span><br><span class="line">    mail_host = <span class="string">"smtp.xxx.xxx.xxx"</span>  <span class="comment"># 设置服务器</span></span><br><span class="line">    mail_user = <span class="string">"name@xxx.xxx.xxx"</span>  <span class="comment"># 发件邮箱</span></span><br><span class="line">    mail_pass = <span class="string">"xxxxxx"</span>  <span class="comment"># 密码</span></span><br><span class="line">    receivers = [<span class="string">'name@xxx.xxx.xxx'</span>,] <span class="comment"># 收件邮箱</span></span><br><span class="line"></span><br><span class="line">    sender = <span class="string">'name@xxx.xxx.xxx'</span> <span class="comment"># 发件邮箱</span></span><br><span class="line"></span><br><span class="line">    message=filename <span class="comment"># 邮件内容</span></span><br><span class="line">    header=filename <span class="comment"># 发件人title</span></span><br><span class="line">    subject=filename <span class="comment"># 邮件标题</span></span><br><span class="line">    </span><br><span class="line">    part = MIMEText(message, message_type, <span class="string">'utf-8'</span>)</span><br><span class="line">    message = MIMEMultipart()</span><br><span class="line"></span><br><span class="line">    message[<span class="string">'From'</span>] = Header(header, <span class="string">'utf-8'</span>)</span><br><span class="line">    message[<span class="string">'Subject'</span>] = Header(subject, <span class="string">'utf-8'</span>)</span><br><span class="line">    message.attach(part)</span><br><span class="line"></span><br><span class="line">    attachment=MIMEApplication(open(filename, <span class="string">'rb'</span>).read())</span><br><span class="line">    attachment.add_header(<span class="string">'Content-Disposition'</span>, <span class="string">'attachment'</span>, filename=filename)</span><br><span class="line">    message.attach(attachment)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        smtpObj = smtplib.SMTP()</span><br><span class="line">        smtpObj.connect(mail_host, <span class="number">25</span>)  <span class="comment"># 25 为 SMTP 端口号</span></span><br><span class="line">        smtpObj.login(mail_user, mail_pass)</span><br><span class="line">        smtpObj.sendmail(sender, receivers, message.as_string())</span><br><span class="line">        print(<span class="string">"邮件发送成功"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">except</span> smtplib.SMTPException:</span><br><span class="line">        print(<span class="string">"Error: 无法发送邮件"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    send_email(<span class="string">'test.txt'</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>发送邮件</tag>
        <tag>附件</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-26</title>
    <url>/2020/02/26/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-26/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Semantic Relatedness for Keyword Disambiguation: Exploiting Different  Embeddings <a href="https://arxiv.org/pdf/2002.11023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Language-Independent Tokenisation Rivals Language-Specific Tokenisation  for Word Similarity Prediction <a href="https://arxiv.org/pdf/2002.11004" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> A more abstractive summarization model <a href="https://arxiv.org/pdf/2002.10959" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression  of Pre-Trained Transformers <a href="https://arxiv.org/pdf/2002.10957" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Detecting Asks in SE attacks: Impact of Linguistic and Structural  Knowledge <a href="https://arxiv.org/pdf/2002.10931" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> KEML: A Knowledge-Enriched Meta-Learning Framework for Lexical Relation  Classification <a href="https://arxiv.org/pdf/2002.10903" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Small-Footprint Open-Vocabulary Keyword Spotting with Quantized LSTM  Networks <a href="https://arxiv.org/pdf/2002.10851" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> BERT Can See Out of the Box: On the Cross-modal Transferability of Text  Representations <a href="https://arxiv.org/pdf/2002.10832" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> MuST-Cinema: a Speech-to-Subtitles corpus <a href="https://arxiv.org/pdf/2002.10829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Label-guided Learning for Text Classification <a href="https://arxiv.org/pdf/2002.10772" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Event Detection with Relation-Aware Graph Convolutional Neural Networks <a href="https://arxiv.org/pdf/2002.10757" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> End-to-end Emotion-Cause Pair Extraction via Learning to Link <a href="https://arxiv.org/pdf/2002.10710" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge <a href="https://arxiv.org/pdf/2002.10695" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Exploring BERT Parameter Efficiency on the Stanford Question Answering  Dataset v2.0 <a href="https://arxiv.org/pdf/2002.10670" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Differentiable Reasoning over a Virtual Knowledge Base <a href="https://arxiv.org/pdf/2002.10640" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Parsing Early Modern English for Linguistic Search <a href="https://arxiv.org/pdf/2002.10546" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> On Feature Normalization and Data Augmentation <a href="https://arxiv.org/pdf/2002.11102" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Diversity-Based Generalization for Neural Unsupervised Text  Classification under Domain Shift <a href="https://arxiv.org/pdf/2002.10937" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Abstractive Snippet Generation <a href="https://arxiv.org/pdf/2002.10782" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Declarative Memory-based Structure for the Representation of Text Data <a href="https://arxiv.org/pdf/2002.10665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Towards Learning a Generic Agent for Vision-and-Language Navigation via  Pre-training <a href="https://arxiv.org/pdf/2002.10638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Automating Discovery of Dominance in Synchronous Computer-Mediated  Communication <a href="https://arxiv.org/pdf/2002.10582" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Semantic Relatedness for Keyword Disambiguation: Exploiting Different  Embeddings</b>  <a href="https://arxiv.org/pdf/2002.11023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Buey%2C+M+G" target="_blank" rel="noopener" style="color:#0000EE;">María G. Buey</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bobed%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Bobed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gracia%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jorge Gracia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mena%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eduardo Mena</a><br>
<font size="3">
Abstract: Understanding the meaning of words is crucial for many tasks that involve human-machine interaction. This has been tackled by research in Word Sense Disambiguation (WSD) in the Natural Language Processing (NLP) field. Recently, WSD and many other NLP tasks have taken advantage of embeddings-based representation of words, sentences, and documents. However, when it comes to WSD, most embeddings models suffer from ambiguity as they do not capture the different possible meanings of the words. Even when they do, the list of possible meanings for a word (sense inventory) has to be known in advance at training time to be included in the embeddings space. Unfortunately, there are situations in which such a sense inventory is not known in advance (e.g., an ontology selected at run-time), or it evolves with time and its status diverges from the one at training time. This hampers the use of embeddings models for WSD. Furthermore, traditional WSD techniques do not perform well in situations in which the available linguistic information is very scarce, such as the case of keyword-based queries. In this paper, we propose an approach to keyword disambiguation which grounds on a semantic relatedness between words and senses provided by an external inventory (ontology) that is not known at training time. Building on previous works, we present a semantic relatedness measure that uses word embeddings, and explore different disambiguation algorithms to also exploit both word and sentence representations. Experimental results show that this approach achieves results comparable with the state of the art when applied for WSD, without training for a particular domain. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：了解话中的意思是至关重要的涉及人机交互的许多任务。这在词义消歧（WSD）在自然语言处理（NLP）领域得到了解决调研。近日，WSD和许多其他NLP任务已经采取单词，句子和文档的基于嵌入物的表示的优势。然而，当涉及到WSD，大多数的嵌入模型从歧义苦，因为他们没有抓住的话可能的不同含义。即使他们这样做，对一个字（感库存）可能的含义名单必须提前在训练时间被包含在嵌入物空间是已知的。不幸的是，在其中这样的意识库存事先不知道的情况下（例如，在运行时选择的本体），或者将其与时间和从所述一个在训练时间其状态发散演变。这阻碍了WSD使用的嵌入模型。此外，传统的WSD技术并不在其中可用的语言信息非常匮乏，比如基于关键字查询的情况的情况下表现良好。在本文中，我们提出了一种方法来关键字歧义消除其上通过未在训练时间已知的外部资源（本体）提供单词和感官之间的语义相关性的理由。在以前的作品的基础上，我们提出了一个语义相关措施，使用Word的嵌入，探索不同的消歧算法，还利用两个单词和句子表示。实验结果表明，当施加用于WSD，无需为特定域训练这个方法实现了结果与现有技术的状态相当。</font>
</div>


<hr>
<div id="paper2"> <b>2. Language-Independent Tokenisation Rivals Language-Specific Tokenisation  for Word Similarity Prediction</b>  <a href="https://arxiv.org/pdf/2002.11004" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bollegala%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danushka Bollegala</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiryo%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryuichi Kiryo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tsujino%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kosuke Tsujino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yukawa%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haruki Yukawa</a><br>
<font size="3">
Abstract: Language-independent tokenisation (LIT) methods that do not require labelled language resources or lexicons have recently gained popularity because of their applicability in resource-poor languages. Moreover, they compactly represent a language using a fixed size vocabulary and can efficiently handle unseen or rare words. On the other hand, language-specific tokenisation (LST) methods have a long and established history, and are developed using carefully created lexicons and training resources. Unlike subtokens produced by LIT methods, LST methods produce valid morphological subwords. Despite the contrasting trade-offs between LIT vs. LST methods, their performance on downstream NLP tasks remain unclear. In this paper, we empirically compare the two approaches using semantic similarity measurement as an evaluation task across a diverse set of languages. Our experimental results covering eight languages show that LST consistently outperforms LIT when the vocabulary size is large, but LIT can produce comparable or better results than LST in many languages with comparatively smaller (i.e. less than 100K words) vocabulary sizes, encouraging the use of LIT when language-specific resources are unavailable, incomplete or a smaller model is required. Moreover, we find that smoothed inverse frequency (SIF) to be an accurate method to create word embeddings from subword embeddings for multilingual semantic similarity prediction tasks. Further analysis of the nearest neighbours of tokens show that semantically and syntactically related tokens are closely embedded in subword embedding spaces </font>
<br>
<font size="2" style="line-height:30px;">
摘要：不需要标记语言资源或词典语言无关的断词（LIT）方法最近获得了，因为他们在资源贫乏的语言应用的普及。此外，它们紧凑地使用固定大小的词汇表示一种语言，并且可以有效地处理看不见或罕见词语。在另一方面，特定于语言的断词（LST）的方法有很长的和成立历史，并用精心打造的词典和培训资源的开发。不同于由LIT方法产生subtokens，LST方法产生有效形态学子词。尽管LIT与LST方法之间的对比权衡，它们对下游NLP任务上的表现仍不清楚。在本文中，我们经验比较使用语义相似性测量为在一组不同的语言评估任务的两种方法。我们覆盖八种语言的实验结果表明，LST的性能一直优于LIT当词汇量很大，但LIT可以产生许多语言比LST相当或更好的效果相对较小（即小于10万个字）词汇量，鼓励采用LIT的当特定语言资源不可用，不完整或需要更小的模型。此外，我们发现，平滑逆频率（SIF）为创建从多语言的语义相似度预测任务的嵌入子字的嵌入字的准确方法。令牌的最近邻居的进一步分析表明，在语义和语法相关令牌紧密嵌入子字嵌入空间</font>
</div>


<hr>
<div id="paper3"> <b>3. A more abstractive summarization model</b>  <a href="https://arxiv.org/pdf/2002.10959" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chakraborty%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Satyaki Chakraborty</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinya Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chakraborty%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sayak Chakraborty</a><br>
<font size="3">
Abstract: Pointer-generator network is an extremely popular method of text summarization. More recent works in this domain still build on top of the baseline pointer generator by augmenting a content selection phase, or by decomposing the decoder into a contextual network and a language model. However, all such models that are based on the pointer-generator base architecture cannot generate novel words in the summary and mostly copy words from the source text. In our work, we first thoroughly investigate why the pointer-generator network is unable to generate novel words, and then address that by adding an Out-of-vocabulary (OOV) penalty. This enables us to improve the amount of novelty/abstraction significantly. We use normalized n-gram novelty scores as a metric for determining the level of abstraction. Moreover, we also report rouge scores of our model since most summarization models are evaluated with R-1, R-2, R-L scores. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：指针发电机网络文本摘要的一个非常流行的方法。最近在这一领域的作品仍然建立在基准指针生成的顶部通过增强内容选择阶段，或通过分解成解码器的上下文网络和语言模型。但是，基于指针发电机基础结构中，所有这样的模型不能生成在摘要新颖的单词和大多来自源文本复制的话。在我们的工作中，我们首先彻底调查为什么指针发生器网络是无法生成新的单词，然后地址通过添加外的词汇（OOV）罚款。这使我们能够显著改善新奇/抽象的量。我们使用标准化的n-gram新奇的分数作为指标来确定的抽象水平。此外，由于大多数总结模型与R-1，R-2，R-L的分数评价我们还报告我们的模型的胭脂分数。</font>
</div>


<hr>
<div id="paper4"> <b>4. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression  of Pre-Trained Transformers</b>  <a href="https://arxiv.org/pdf/2002.10957" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenhui Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hangbo Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. The code and models are publicly available at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预先训练的语言模型（例如，BERT（Devlin等，2018）和它的变体）都实现了品种的NLP任务显着成效。然而，这些模型通常由数以百万计的参数带来的微调挑战和在线在现实生活中的应用，由于等待时间和能力的限制服务。在这项工作中，我们提出了一个简单而有效的方法来压缩大型变压器（瓦斯瓦尼等人，2017年）的预训练的模型，称为深自注意蒸馏。小模型（学生）被深深模仿自注意模块，其在变压器网络至关重要的作用，大模型（教师）的培训。具体来说，我们建议蒸馏老师的最后一个变压器层，这是有效的，灵活的为学生的自我关注的模块。此外，我们的自我关注模块作为新的深自重视知识引入值之间的缩放点的产品，除了关注分布（即，查询和键缩放点积）已经在使用了现有工程。此外，我们表明，引入教师助理（Mirzadeh等，2019）也有助于大型预训练Transformer模型蒸馏。实验结果表明，我们的模型优于国家的最先进的基线的学生机型的不同参数的大小。特别是，它保持了使用的变压器参数和教师模型计算的50％的阵容2.0超过99％的准确度和一些胶基准任务。代码和模式是公开的，在此HTTPS URL</font>
</div>


<hr>
<div id="paper5"> <b>5. Detecting Asks in SE attacks: Impact of Linguistic and Structural  Knowledge</b>  <a href="https://arxiv.org/pdf/2002.10931" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dorr%2C+B+J" target="_blank" rel="noopener" style="color:#0000EE;">Bonnie J. Dorr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhatia%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Archna Bhatia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dalton%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adam Dalton</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mather%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brodie Mather</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hebenstreit%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bryanna Hebenstreit</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Santhanam%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sashank Santhanam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuo Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shaikh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samira Shaikh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zemel%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alan Zemel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Strzalkowski%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomek Strzalkowski</a><br>
<font size="3">
Abstract: Social engineers attempt to manipulate users into undertaking actions such as downloading malware by clicking links or providing access to money or sensitive information. Natural language processing, computational sociolinguistics, and media-specific structural clues provide a means for detecting both the ask (e.g., buy gift card) and the risk/reward implied by the ask, which we call framing (e.g., lose your job, get a raise). We apply linguistic resources such as Lexical Conceptual Structure to tackle ask detection and also leverage structural clues such as links and their proximity to identified asks to improve confidence in our results. Our experiments indicate that the performance of ask detection, framing detection, and identification of the top ask is improved by linguistically motivated classes coupled with structural clues such as links. Our approach is implemented in a system that informs users about social engineering risk situations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：社会工程师试图操纵用户进入创业行动，比如通过点击链接或提供存取款或敏感信息，下载恶意软件。自然语言处理，计算语言学和媒体特有的结构线索，提供了一种用于检测两个要求（例如，购买礼品卡）和风险/回报在问，我们称之为框暗示（例如，失去工作，GET加薪）。我们运用语言的资源，如词汇概念结构，以解决问检测也利用结构线索，如链接和它们接近鉴定要求，以改善我们的结果的信心。我们的实验表明，ASK检测，成帧检测，并且顶部的识别性能要求的是通过加上结构线索如链接语言动机类提高。我们的方法是在系统中实现，大约社会工程风险的情况下通知用户。</font>
</div>


<hr>
<div id="paper6"> <b>6. KEML: A Knowledge-Enriched Meta-Learning Framework for Lexical Relation  Classification</b>  <a href="https://arxiv.org/pdf/2002.10903" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengyu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghui Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaofeng He</a><br>
<font size="3">
Abstract: Lexical relations describe how concepts are semantically related, in the form of relation triples. The accurate prediction of lexical relations between concepts is challenging, due to the sparsity of patterns indicating the existence of such relations. We propose the Knowledge-Enriched Meta-Learning (KEML) framework to address the task of lexical relation classification. In KEML, the LKB-BERT (Lexical Knowledge Base-BERT) model is presented to learn concept representations from massive text corpora, with rich lexical knowledge injected by distant supervision. A probabilistic distribution of auxiliary tasks is defined to increase the model's ability to recognize different types of lexical relations. We further combine a meta-learning process over the auxiliary task distribution and supervised learning to train the neural lexical relation classifier. Experiments over multiple datasets show that KEML outperforms state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：词汇关系描述概念如何语义相关的，关于三元组的形式。概念之间词法关系的精确预测是具有挑战性，由于指示这种关系的存在模式的稀疏性。我们提出了知识富集元学习（KEML）框架来处理词汇的关系分类的任务。在KEML，提出了LKB-BERT（词汇知识库-BERT）模式，从大规模语料库学习概念表示，与远处的监督注入丰富的词汇知识。辅助任务的概率分布被定义为增加模型的识别不同类型的词汇关系的能力。我们进一步结合了元学习的过程在辅助任务分配和监督学习训练神经的词汇关系的分类。在多个数据集实验表明KEML优于状态的最先进的方法。</font>
</div>


<hr>
<div id="paper7"> <b>7. Small-Footprint Open-Vocabulary Keyword Spotting with Quantized LSTM  Networks</b>  <a href="https://arxiv.org/pdf/2002.10851" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bluche%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Théodore Bluche</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Primet%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maël Primet</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gisselbrecht%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thibault Gisselbrecht</a><br>
<font size="3">
Abstract: We explore a keyword-based spoken language understanding system, in which the intent of the user can directly be derived from the detection of a sequence of keywords in the query. In this paper, we focus on an open-vocabulary keyword spotting method, allowing the user to define their own keywords without having to retrain the whole model. We describe the different design choices leading to a fast and small-footprint system, able to run on tiny devices, for any arbitrary set of user-defined keywords, without training data specific to those keywords. The model, based on a quantized long short-term memory (LSTM) neural network, trained with connectionist temporal classification (CTC), weighs less than 500KB. Our approach takes advantage of some properties of the predictions of CTC-trained networks to calibrate the confidence scores and implement a fast detection algorithm. The proposed system outperforms a standard keyword-filler model approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文探讨基于关键字的口语理解系统，其中意向的用户可以直接从检测的关键字的查询序列的衍生。在本文中，我们专注于一个开放式的词汇关键词识别方法，允许用户自己定义的关键字，而无需重新培训整个模型。我们描述了不同的设计选择导致一个快速和小尺寸系统，能够在很小的设备上运行，为用户自定义关键字的任意设定，无需培训数据具体到这些关键字。该模型的基础上，量化长短期存储器（LSTM）神经网络，训练有素的联结时间分类（CTC），重量小于500KB。我们的方法采用CTC训练网络的预测校准的置信度，并实现了快速检测算法的一些性能优势。所提出的系统优于标准的关键字填料模型的方法。</font>
</div>


<hr>
<div id="paper8"> <b>8. BERT Can See Out of the Box: On the Cross-modal Transferability of Text  Representations</b>  <a href="https://arxiv.org/pdf/2002.10832" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Scialom%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Scialom</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bordes%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Bordes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dray%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul-Alexis Dray</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Staiano%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jacopo Staiano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gallinari%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Gallinari</a><br>
<font size="3">
Abstract: Pre-trained language models such as BERT have recently contributed to significant advances in Natural Language Processing tasks. Interestingly, while multilingual BERT models have demonstrated impressive results, recent works have shown how monolingual BERT can also be competitive in zero-shot cross-lingual settings. This suggests that the abstractions learned by these models can transfer across languages, even when trained on monolingual data. In this paper, we investigate whether such generalization potential applies to other modalities, such as vision: does BERT contain abstractions that generalize beyond text? We introduce BERT-gen, an architecture for text generation based on BERT, able to leverage on either mono- or multi- modal representations. The results reported under different configurations indicate a positive answer to our research question, and the proposed model obtains substantial improvements over the state-of-the-art on two established Visual Question Generation datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预先训练语言模型，如BERT最近促成了自然语言处理任务显著的进步。有趣的是，多语种BERT模式已经证明了不俗的业绩，近期的作品表现出一种语言BERT怎么还可以在零次跨语言设置的竞争力。这表明，通过这些模型学到的抽象可以在单语数据训练时跨语言的传递，甚至。在本文中，我们调查这种泛化的潜力是否适用于其他方式，如视力：不BERT包含抽象，超越文字期广义？我们引进BERT根，用于文本生成的架构基础上BERT，能够充分利用在任单或多式联运表示。根据不同的配置报告的结果表明正面回答我们的研究问题，提出的模型获得了两个建立的视觉问题生成的数据集的国家的最先进的实质性改善。</font>
</div>


<hr>
<div id="paper9"> <b>9. MuST-Cinema: a Speech-to-Subtitles corpus</b>  <a href="https://arxiv.org/pdf/2002.10829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Karakanta%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alina Karakanta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Negri%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matteo Negri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Turchi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marco Turchi</a><br>
<font size="3">
Abstract: Growing needs in localising audiovisual content in multiple languages through subtitles call for the development of automatic solutions for human subtitling. Neural Machine Translation (NMT) can contribute to the automatisation of subtitling, facilitating the work of human subtitlers and reducing turn-around times and related costs. NMT requires high-quality, large, task-specific training data. The existing subtitling corpora, however, are missing both alignments to the source language audio and important information about subtitle breaks. This poses a significant limitation for developing efficient automatic approaches for subtitling, since the length and form of a subtitle directly depends on the duration of the utterance. In this work, we present MuST-Cinema, a multilingual speech translation corpus built from TED subtitles. The corpus is comprised of (audio, transcription, translation) triplets. Subtitle breaks are preserved by inserting special symbols. We show that the corpus can be used to build models that efficiently segment sentences into subtitles and propose a method for annotating existing subtitling corpora with subtitle breaks, conforming to the constraint of length. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在日益本地化通过调用字幕多语言视听内容的自动解决方案的发展，为人类的字幕需求。神经机器翻译（NMT）可以有助于字幕的automatisation，促进人类subtitlers的工作和减少周转时间和相关费用。 NMT需要高品质，大，特定任务的训练数据。现有的字幕语料库，然而，缺少这两个路线源语言音频和字幕左右休息的重要信息。这对用于开发高效字幕自动方法一个显著限制，因为字幕的长度和形式直接依赖于发声的持续时间。在这项工作中，我们存在，必须影院，从TED字幕建成多语种的语音翻译语料。该文集是由（音频，转录，翻译）三胞胎。字幕符插入特殊符号保留。我们表明，语料库可以用来构建模型，有效段句子翻译成字幕，并提出对现有的标注语料字幕与字幕休息，符合长度的限制的方法。</font>
</div>


<hr>
<div id="paper10"> <b>10. Label-guided Learning for Text Classification</b>  <a href="https://arxiv.org/pdf/2002.10772" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xien Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Song Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=You%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinxin You</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Ji Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dejing Dou</a><br>
<font size="3">
Abstract: Text classification is one of the most important and fundamental tasks in natural language processing. Performance of this task mainly dependents on text representation learning. Currently, most existing learning frameworks mainly focus on encoding local contextual information between words. These methods always neglect to exploit global clues, such as label information, for encoding text information. In this study, we propose a label-guided learning framework LguidedLearn for text representation and classification. Our method is novel but simple that we only insert a label-guided encoding layer into the commonly used text representation learning schemas. That label-guided layer performs label-based attentive encoding to map the universal text embedding (encoded by a contextual information learner) into different label spaces, resulting in label-wise embeddings. In our proposed framework, the label-guided layer can be easily and directly applied with a contextual encoding method to perform jointly learning. Text information is encoded based on both the local contextual information and the global label clues. Therefore, the obtained text embeddings are more robust and discriminative for text classification. Extensive experiments are conducted on benchmark datasets to illustrate the effectiveness of our proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文本分类是自然语言处理的最重要和最基本的任务之一。这个任务的性能主要眷的文本表示学习。目前，大多数现有的学习框架，主要集中在编码词之间的本地上下文信息。这些方法往往忽略利用全球的线索，如标签信息，编码的文本信息。在这项研究中，我们提出了文本表示和分类标签引导学习框架LguidedLearn。我们的方法是新颖的，但简单，我们只插入一个标签引导编码层到常用的文本表示学习模式。也就是说，基于标签的标签引导层进行细心编码映射通用文本嵌入（由上下文信息学习者编码）成不同的标签空间，从而导致标签的嵌入明智。在我们提出的框架，该标签引导层可以很容易地和直接地与上下文编码方法来执行共同学习应用。文本信息是基于本地的上下文信息和全球标签线索都进行编码。因此，所获取文本的嵌入更健壮，歧视性的文本分类。大量的实验是在基准数据集进行说明我们提出的方法的有效性。</font>
</div>


<hr>
<div id="paper11"> <b>11. Event Detection with Relation-Aware Graph Convolutional Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.10757" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiyao Cui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bowen Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tingwen Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenyu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuebin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinqiao Shi</a><br>
<font size="3">
Abstract: Event detection (ED), a key subtask of information extraction, aims to recognize instances of specific types of events in text. Recently, graph convolutional networks (GCNs) over dependency trees have been widely used to capture syntactic structure information and get convincing performances in event detection. However, these works ignore the syntactic relation labels on the tree, which convey rich and useful linguistic knowledge for event detection. In this paper, we investigate a novel architecture named Relation-Aware GCN (RA-GCN), which efficiently exploits syntactic relation labels and models the relation between words specifically. We first propose a relation-aware aggregation module to produce expressive word representation by aggregating syntactically connected words through specific relation. Furthermore, a context-aware relation update module is designed to explicitly update the relation representation between words, and these two modules work in the mutual promotion way. Experimental results on the ACE2005 dataset show that our model achieves a new state-of-the-art performance for event detection. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事件检测（ED），信息提取的关键子任务，目的是识别特定类型的文本事件的实例。近日，图形上依赖树卷积网络（GCNs）已被广泛用于捕获句法结构信息，并得到有说服力的事件检测性能。然而，这些作品忽略树上的语法关系的标签，传达了事件检测丰富实用的语言知识。在本文中，我们研究了一个名为关系感知GCN新颖的架构（RA-GCN），它有效地利用语法关系的标签和型号词之间的关系明确。我们首先提出了一个关系感知汇聚模块通过特定关系聚合语法连接的话，产生的表达字表示。此外，上下文感知关系更新模块旨在明确更新词之间的关系表示，这两个模块相互促进的方式工作。在ACE2005数据集上，我们的模型实现了新的国家的最先进的性能事件检测实验结果。</font>
</div>


<hr>
<div id="paper12"> <b>12. End-to-end Emotion-Cause Pair Extraction via Learning to Link</b>  <a href="https://arxiv.org/pdf/2002.10710" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haolin Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiuchi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dawei Song</a><br>
<font size="3">
Abstract: Emotion-cause pair extraction (ECPE), as an emergent natural language processing task, aims at jointly investigating emotions and their underlying causes in documents. It extends the previous emotion cause extraction (ECE) task, yet without requiring a set of pre-given emotion clauses as in ECE. Existing approaches to ECPE generally adopt a two-stage method, i.e., (1) emotion and cause detection, and then (2) pairing the detected emotions and causes. Such pipeline method, while intuitive, suffers from two critical issues, including error propagation across stages that may hinder the effectiveness, and high computational cost that would limit the practical application of the method. To tackle these issues, we propose a multi-task learning model that can extract emotions, causes and emotion-cause pairs simultaneously in an end-to-end manner. Specifically, our model regards pair extraction as a link prediction task, and learns to link from emotion clauses to cause clauses, i.e., the links are directional. Emotion extraction and cause extraction are incorporated into the model as auxiliary tasks, which further boost the pair extraction. Experiments are conducted on an ECPE benchmarking dataset. The results show that our proposed model outperforms a range of state-of-the-art approaches in terms of both effectiveness and efficiency. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：情感原因对的提取（ECPE），作为一个新兴的自然语言处理任务，旨在联合调查的情绪和文件的根本原因。它扩展了以前的情感原因提取（ECE）的任务，但不要求一组预先给定的情感条文，ECE。现有方法ECPE一般采用两阶段方法，即，（1）情感和原因的检测，然后（2）配对所检测的情绪和原因。这种管道的方法，同时直观的，从两个关键问题，包括跨可能会阻碍有效性阶段错误传播，并计算成本高，将限制该方法的实际应用受到影响。为了解决这些问题，我们提出了一个多任务的学习模式，可以在一个终端到终端的方式同时提取的情绪，原因和情感原因对。具体地，我们的模型关于对提取作为链接预测任务，并获知从情感子句原因条款，即链接，链接是定向。情感提取和原因提取被并入模型作为辅助任务，这进一步增强了对萃取。实验是在一个ECPE基准数据集进行。结果表明，该模型优于一系列国家的最先进的有效性和效率方面的方法。</font>
</div>


<hr>
<div id="paper13"> <b>13. Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge</b>  <a href="https://arxiv.org/pdf/2002.10695" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hung Le</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+N+F" target="_blank" rel="noopener" style="color:#0000EE;">Nancy F. Chen</a><br>
<font size="3">
Abstract: Audio-Visual Scene-Aware Dialog (AVSD) is an extension from Video Question Answering (QA) whereby the dialogue agent is required to generate natural language responses to address user queries and carry on conversations. This is a challenging task as it consists of video features of multiple modalities, including text, visual, and audio features. The agent also needs to learn semantic dependencies among user utterances and system responses to make coherent conversations with humans. In this work, we describe our submission to the AVSD track of the 8th Dialogue System Technology Challenge. We adopt dot-product attention to combine text and non-text features of input video. We further enhance the generation capability of the dialogue agent by adopting pointer networks to point to tokens from multiple source sequences in each generation step. Our systems achieve high performance in automatic metrics and obtain 5th and 6th place in human evaluation among all submissions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视听场景感知对话框（AVSD）是从视频问答的延伸（QA），由此对话代理需要生成自然语言应答对谈话地址用户查询和携带。这是一项艰巨的任务，因为它是由多种方式，包括文本，视频和音频功能，视频功能。代理还需要学习用户话语和系统响应之间的语义依赖关系，以连贯的对话与人类。在这项工作中，我们描述了我们提交的8对话系统技术挑战的AVSD轨道。我们采用点积注意结合输入视频的文本和非文本的功能。我们通过采用指针网络指向从在每一代步骤多个源序列的标记进一步增强了对话剂的产生能力。我们的系统实现自动度量高性能和获得所有提交中的人评价第5和第6位。</font>
</div>


<hr>
<div id="paper14"> <b>14. Exploring BERT Parameter Efficiency on the Stanford Question Answering  Dataset v2.0</b>  <a href="https://arxiv.org/pdf/2002.10670" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hulburd%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Hulburd</a><br>
<font size="3">
Abstract: In this paper we explore the parameter efficiency of BERT $arXiv:1810.04805$ on version 2.0 of the Stanford Question Answering dataset (SQuAD2.0). We evaluate the parameter efficiency of BERT while freezing a varying number of final transformer layers as well as including the adapter layers proposed in $arXiv:1902.00751$. Additionally, we experiment with the use of context-aware convolutional (CACNN) filters, as described in $arXiv:1709.08294v3$, as a final augmentation layer for the SQuAD2.0 tasks. This exploration is motivated in part by $arXiv:1907.10597$, which made a compelling case for broadening the evaluation criteria of artificial intelligence models to include various measures of resource efficiency. While we do not evaluate these models based on their floating point operation efficiency as proposed in arXiv:1907.10597, we examine efficiency with respect to training time, inference time, and total number of model parameters. Our results largely corroborate those of $arXiv:1902.00751$ for adapter modules, while also demonstrating that gains in F1 score from adding context-aware convolutional filters are not practical due to the increase in training and inference time. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文探讨BERT $的arXiv的参数效率：在斯坦福问题的2.0版本1810.04805 $应答数据集（SQuAD2.0）。我们评估BERT的参数效率，同时冻结不同数量的最终变压器层，以及包括在$提出的arXiv适配器层：1902.00751 $。此外，我们实验使用的上下文感知卷积（CACNN）过滤器，在$的arXiv描述：1709.08294v3 $，作为SQuAD2.0任务的最后的增强层。 1907.10597 $，这使得拓宽的人工智能模型的评价标准包括的资源效率的各项措施令人信服的理由：这种探索部分是由$的arXiv的动机。虽然我们不评价基于其浮点运算效率的arXiv中提出的这些模型：1907.10597，我们考察相对于训练时间，推断时间，以及模型参数总数的效率。我们的研究结果在很大程度上证实了那些$的arXiv的：1902.00751 $为适配器模块，同时也证明了在F1的收益从加入上下文感知卷积过滤器得分都没有在训练和推理时间的增加实用所致。</font>
</div>


<hr>
<div id="paper15"> <b>15. Differentiable Reasoning over a Virtual Knowledge Base</b>  <a href="https://arxiv.org/pdf/2002.10640" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dhingra%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bhuwan Dhingra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zaheer%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manzil Zaheer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Balachandran%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vidhisha Balachandran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Graham Neubig</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Salakhutdinov%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruslan Salakhutdinov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+W+W" target="_blank" rel="noopener" style="color:#0000EE;">William W. Cohen</a><br>
<font size="3">
Abstract: We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search (MIPS) on a special index of contextual representations of the mentions. This module is differentiable, so the full system can be trained end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the contextual representation encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. On HotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking approach to retrieving the relevant passages required to answer a question. DrKIT is also very efficient, processing 10-100x more queries per second than existing multi-hop systems. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们认为回答使用语料库作为虚拟知识库（KB）复杂的多跳问题的任务。尤其是，我们描述了一个神经模块，DrKIT，横穿文本数据如KB，轻声以下关系的路径提到在语料库中的实体之间。在每个步骤中的模块使用稀疏矩阵TFIDF指数和上的提及上下文表示中的一个特殊的索引的最大内积搜索（MIPS）的组合。该模块是可微的，因此整个系统可使用基于梯度的方法，从自然语言输入开始进行训练的端至端。我们还通过生成使用现有的知识基础硬反面例子描述了上下文表示编码器预训练方案。我们发现，DrKIT 9点就在MetaQA数据集3跳的问题提高了精度，切削之间的差距为基础的KB国家的最先进的70％，基于文本的和。在HotpotQA，DrKIT导致在基于BERT-重新排序的方法来检索相关段落有10％的改善需要回答的问题。 DrKIT也是非常有效的，处理每秒比现有的多跳系统10-100多个查询。</font>
</div>


<hr>
<div id="paper16"> <b>16. Parsing Early Modern English for Linguistic Search</b>  <a href="https://arxiv.org/pdf/2002.10546" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kulick%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seth Kulick</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ryant%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Neville Ryant</a><br>
<font size="3">
Abstract: We investigate the question of whether advances in NLP over the last few years make it possible to vastly increase the size of data usable for research in historical syntax. This brings together many of the usual tools in NLP word embeddings, tagging, and parsing - in the service of linguistic queries over automatically annotated corpora. We train a part-of-speech (POS) tagger and parser on a corpus of historical English, using ELMo embeddings trained over a billion words of similar text. The evaluation is based on the standard metrics, as well as on the accuracy of the query searches using the parsed data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们调查的NLP在过去几年中的进步是否有可能，极大地增加大小的数据可用于在历史语法研究的问题。这个汇集了众多的NLP字的嵌入常用的工具，标记和解析 - 在语言学上的查询自动标注的语料的服务。我们培养对历史英语语料库中的部分的语音（POS）恶搞和分析器，使用培训了类似的文本的十亿字的嵌入ELMO。评价是基于标准指标，以及关于使用该解析数据的查询搜索的准确度。</font>
</div>


<hr>
<div id="paper17"> <b>17. On Feature Normalization and Data Augmentation</b>  <a href="https://arxiv.org/pdf/2002.11102" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Boyi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Felix Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lim%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Ser-Nam Lim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Belongie%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Serge Belongie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weinberger%2C+K+Q" target="_blank" rel="noopener" style="color:#0000EE;">Kilian Q. Weinberger</a><br>
<font size="3">
Abstract: Modern neural network training relies heavily on data augmentation for improved generalization. After the initial success of label-preserving augmentations, there has been a recent surge of interest in label-perturbing approaches, which combine features and labels across training samples to smooth the learned decision surface. In this paper, we propose a new augmentation method that leverages the first and second moments extracted and re-injected by feature normalization. We replace the moments of the learned features of one training image by those of another, and also interpolate the target labels. As our approach is fast, operates entirely in feature space, and mixes different signals than prior methods, one can effectively combine it with existing augmentation methods. We demonstrate its efficacy across benchmark data sets in computer vision, speech, and natural language processing, where it consistently improves the generalization performance of highly competitive baseline networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现代神经网络训练在很大程度上依赖于改善泛化数据增强。标签保留扩充的初步成功之后，最近一直在标签扰动方法，横跨训练样本，以平滑了解到决策相结合的表面特征和标签兴趣大增。在本文中，我们提议利用所述第一和第二时刻提取并通过特征正规化再注入一个新的增强方法。我们通过这些的另一个替代的一个训练图像的特征学到的时刻，也是插值目标标签。我们的方法是快速的，在功能空间全部工作，混合和不同的信号比以前的方法，可以有效地与现有的隆胸方法结合起来。我们证明其疗效跨越基准数据集在计算机视觉，语音和自然语言处理，它始终提高竞争激烈的基线网络的泛化性能。</font>
</div>


<hr>
<div id="paper18"> <b>18. Diversity-Based Generalization for Neural Unsupervised Text  Classification under Domain Shift</b>  <a href="https://arxiv.org/pdf/2002.10937" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Krishnan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jitin Krishnan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Purohit%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hemant Purohit</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rangwala%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huzefa Rangwala</a><br>
<font size="3">
Abstract: Domain adaptation approaches seek to learn from a source domain and generalize it to an unseen target domain. At present, the state-of-the-art domain adaptation approaches for subjective text classification problems are semi-supervised; and use unlabeled target data along with labeled source data. In this paper, we propose a novel method for domain adaptation of single-task text classification problems based on a simple but effective idea of diversity-based generalization that does not require unlabeled target data. Diversity plays the role of promoting the model to better generalize and be indiscriminate towards domain shift by forcing the model not to rely on same features for prediction. We apply this concept on the most explainable component of neural networks, the attention layer. To generate sufficient diversity, we create a multi-head attention model and infuse a diversity constraint between the attention heads such that each head will learn differently. We further expand upon our model by tri-training and designing a procedure with an additional diversity constraint between the attention heads of the tri-trained classifiers. Extensive evaluation using the standard benchmark dataset of Amazon reviews and a newly constructed dataset of Crisis events shows that our fully unsupervised method matches with the competing semi-supervised baselines. Our results demonstrate that machine learning architectures that ensure sufficient diversity can generalize better; encouraging future research to design ubiquitously usable learning models without using unlabeled target data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：域名适应办法寻求从源域学习并推广到一个看不见的目标域。目前，国家的最先进的域适应接近主观文本分类问题半监督;并使用未标记的目标数据与标记的源数据一起。在本文中，我们提出了基于一个简单但基于多样性的推广有效的想法，不需要未标记的目标数据单任务文本分类问题领域适应性的新方法。多样性起到促进模型，以便更好地推广和通过强制模式不依赖于预测相同的功能不分青红皂对域转移的作用。我们运用神经网络，关注层的最可以解释组件这个概念。为了产生足够的多样性，我们创建了一个多头注意模型和灌输注意头之间的多样性约束，使得每头将学习不同。我们进一步用三训练，并与三训练的分类注意头部之间附加的多样性约束设计一个程序，在我们的模型扩展。使用亚马逊的评论标准的基准数据集和危机事件显示了一个新建的数据集广泛的评估，我们与竞争的半监督基线完全无监督的方法匹配。我们的研究结果表明，学习机架构，确保有足够的多样性能够更好地推广;鼓励未来的研究，设计无处不可用的学习模式，而无需使用未标记的目标数据。</font>
</div>


<hr>
<div id="paper19"> <b>19. Abstractive Snippet Generation</b>  <a href="https://arxiv.org/pdf/2002.10782" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Fan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Syed%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shahbaz Syed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stein%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benno Stein</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hagen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Hagen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Potthast%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Potthast</a><br>
<font size="3">
Abstract: An abstractive snippet is an originally created piece of text to summarize a web page on a search engine results page. Compared to the conventional extractive snippets, which are generated by extracting phrases and sentences verbatim from a web page, abstractive snippets circumvent copyright issues; even more interesting is the fact that they open the door for personalization. Abstractive snippets have been evaluated as equally powerful in terms of user acceptance and expressiveness---but the key question remains: Can abstractive snippets be automatically generated with sufficient quality? This paper introduces a new approach to abstractive snippet generation: We identify the first two large-scale sources for distant supervision, namely anchor contexts and web directories. By mining the entire ClueWeb09 and ClueWeb12 for anchor contexts and by utilizing the DMOZ Open Directory Project, we compile the Webis Abstractive Snippet Corpus 2020, comprising more than 3.5 million triples of the form $\langle$query, snippet, document$\rangle$ as training examples, where the snippet is either an anchor context or a web directory description in lieu of a genuine query-biased abstractive snippet of the web document. We propose a bidirectional abstractive snippet generation model and assess the quality of both our corpus and the generated abstractive snippets with standard measures, crowdsourcing, and in comparison to the state of the art. The evaluation shows that our novel data sources along with the proposed model allow for producing usable query-biased abstractive snippets while minimizing text reuse. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一个抽象的片段是最初创建的文本块总结出搜索引擎结果页面上的网页。相对于传统的提取片段，这是由一个网页，抽象片段规避版权问题，提取的短语和句子逐字产生;更有意思的是，他们打开了个性化门的事实。抽象的片段被评价为在用户接受度和表现力方面同样强大---但关键的问题是：能抽象化片段以足够的质量自动生成？本文介绍一种新的方法，以抽象的片断代：我们确定遥远的监督，即锚背景和网站目录前两个大型的来源。通过挖掘整个ClueWeb09和ClueWeb12锚上下文和利用DMOZ开放目录项目，我们编译Webis写意片段语料库2020年，包括形式$ \ langle $查询超过350万点的三倍，片段，文件$ \ rangle $作为训练样例，其中该片断或者是一个锚上下文或代替所述web文档的一个真正的查询偏置抽象片断的web目录描述。我们提出了一个双向抽象片段生成模型，并评估了我们的语料库，并与标准的措施，众包的产生抽象片段的质量，并与现有技术相比的状态。评估表明，我们所提出的模型一起新的数据源允许生产使用的查询偏向抽象的片段，同时尽量减少文字重用。</font>
</div>


<hr>
<div id="paper20"> <b>20. Declarative Memory-based Structure for the Representation of Text Data</b>  <a href="https://arxiv.org/pdf/2002.10665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pushp%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sumant Pushp</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kashmira%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pragya Kashmira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hazarika%2C+S+M" target="_blank" rel="noopener" style="color:#0000EE;">Shyamanta M Hazarika</a><br>
<font size="3">
Abstract: In the era of intelligent computing, computational progress in text processing is an essential consideration. Many systems have been developed to process text over different languages. Though, there is considerable development, they still lack in understanding of the text, i.e., instead of keeping text as knowledge, many treat text as a data. In this work we introduce a text representation scheme which is influenced by human memory infrastructure. Since texts are declarative in nature, a structural organization would foster efficient computation over text. We exploit long term episodic memory to keep text information observed over time. This not only keep fragments of text in an organized fashion but also reduces redundancy and stores the temporal relation among them. Wordnet has been used to imitate semantic memory, which works at word level to facilitate the understanding about individual words within text. Experimental results of various operation performed over episodic memory and growth of knowledge infrastructure over time is reported. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：智能计算时代，在文本处理计算的进展是一个重要的考虑因素。许多系统已发展到在处理不同语言的文字。虽然，有相当大的发展，他们仍然缺乏文字的理解，即，而不是保持文本的知识，许多处理文本的数据。在这项工作中，我们介绍这是由人类记忆的基础设施影响了文本表示方法。由于文本在本质上声明，一个组织结构将促进在文本高效的计算。我们利用长期的情景记忆，以保持文本信息随时间观察。这不仅保持文本的片段以有组织的方式，但也减少了冗余并存储它们之间的时间关系。共发现已被用来模仿语义记忆，这在字的级别工作，以促进有关文本中各个单词的理解。报道在情节记忆随着时间的推移知识基础设施的发展进行各种操作的实验结果。</font>
</div>


<hr>
<div id="paper21"> <b>21. Towards Learning a Generic Agent for Vision-and-Language Navigation via  Pre-training</b>  <a href="https://arxiv.org/pdf/2002.10638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weituo Hao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunyuan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiujun Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lawrence Carin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Gao</a><br>
<font size="3">
Abstract: Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a new task is often limited. In this paper, we present the first pre-training and fine-tuning paradigm for vision-and-language navigation (VLN) tasks. By training on a large amount of image-text-action triplets in a self-supervised learning manner, the pre-trained model provides generic representations of visual environments and language instructions. It can be easily used as a drop-in for existing VLN frameworks, leading to the proposed agent called Prevalent. It learns more effectively in new tasks and generalizes better in a previously unseen environment. The performance is validated on three VLN tasks. On the Room-to-Room benchmark, our model improves the state-of-the-art from 47% to 51% on success rate weighted by path length. Further, the learned representation is transferable to other VLN tasks. On two recent tasks, vision-and-dialog navigation and ``Help, Anna!'' the proposed Prevalent leads to significant improvement over existing methods, achieving a new state of the art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：学习到以下自然语言指令的视觉环境中导航是一项艰巨的任务，因为多模式输入到代理是高度可变的，并在新的任务训练数据往往是有限的。在本文中，我们提出的第一个前培训及微调范式视觉和语言导航（VLN）的任务。通过自我监督学习方式上大量的图像，文本动作三胞胎的训练，预先训练模型提供了可视化的环境和语言指令的通用表示。它可以方便地作为一个下拉现有VLN框架，从而导致所谓的流行提出的代理。它在新的任务和推广更有效地学习好于以前看不见的环境。性能验证三个VLN任务。在房间到房间基准，我们的模型改进了从47％的状态下的最先进的51％由路径长度加权的成功率。此外，学会表示是转移到其他VLN任务。在最近的两个任务，视觉和对话框导航和``帮助，安娜！“”所提出的盛行导致显著改善了现有的方法，实现了新的艺术状态。</font>
</div>


<hr>
<div id="paper22"> <b>22. Automating Discovery of Dominance in Synchronous Computer-Mediated  Communication</b>  <a href="https://arxiv.org/pdf/2002.10582" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Samuel%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jim Samuel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Holowczak%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Holowczak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Benbunan-Fich%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raquel Benbunan-Fich</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Levine%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ilan Levine</a><br>
<font size="3">
Abstract: With the advent of electronic interaction, dominance (or the assertion of control over others) has acquired new dimensions. This study investigates the dynamics and characteristics of dominance in virtual interaction by analyzing electronic chat transcripts of groups solving a hidden profile task. We investigate computer-mediated communication behavior patterns that demonstrate dominance and identify a number of relevant variables. These indicators are calculated with automatic and manual coding of text transcripts. A comparison of both sets of variables indicates that automatic text analysis methods yield similar conclusions than manual coding. These findings are encouraging to advance research in text analysis methods in general, and in the study of virtual team dominance in particular. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着电子相互作用的到来，显性（或控制权人的说法）已经获得了新的维度。本研究通过分析解决隐藏轮廓任务组的电子聊天记录调查了虚拟交互主导地位的动态和特点。我们调查演示的优势，并提出了若干相关变量的计算机为媒介的沟通行为模式。这些指标与文本转录的自动和手动编码计算。两组变量的比较表明，自动文本分析方法产生比手动编码类似的结论。这些结果是令人鼓舞的推进在文本分析方法的研究一般，而在特定的虚拟团队主导地位的研究。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-25</title>
    <url>/2020/02/25/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-25/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Resources for Turkish Dependency Parsing: Introducing the BOUN Treebank  and the BoAT Annotation Tool <a href="https://arxiv.org/pdf/2002.10416" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Discriminative Adversarial Search for Abstractive Summarization <a href="https://arxiv.org/pdf/2002.10375" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Multilingual Twitter Corpus and Baselines for Evaluating Demographic  Bias in Hate Speech Recognition <a href="https://arxiv.org/pdf/2002.10361" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Low-Resource Knowledge-Grounded Dialogue Generation <a href="https://arxiv.org/pdf/2002.10348" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation <a href="https://arxiv.org/pdf/2002.10345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Semi-Supervised Speech Recognition via Local Prior Matching <a href="https://arxiv.org/pdf/2002.10336" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Word Embeddings Inherently Recover the Conceptual Organization of the  Human Mind <a href="https://arxiv.org/pdf/2002.10284" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Fixed Encoder Self-Attention Patterns in Transformer-Based Machine  Translation <a href="https://arxiv.org/pdf/2002.10260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Learning to Select Bi-Aspect Information for Document-Scale Text Content  Manipulation <a href="https://arxiv.org/pdf/2002.10210" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> A Hybrid Approach to Dependency Parsing: Combining Rules and Morphology  with Deep Learning <a href="https://arxiv.org/pdf/2002.10116" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Predicting Subjective Features from Questions on QA Websites using BERT <a href="https://arxiv.org/pdf/2002.10107" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> GRET: Global Representation Enhanced Transformer <a href="https://arxiv.org/pdf/2002.10101" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Do Multi-Hop Question Answering Systems Know How to Answer the  Single-Hop Sub-Questions? <a href="https://arxiv.org/pdf/2002.09919" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> A Nepali Rule Based Stemmer and its performance on different NLP  applications <a href="https://arxiv.org/pdf/2002.09901" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Fill in the BLANC: Human-free quality estimation of document summaries <a href="https://arxiv.org/pdf/2002.09836" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Unsupervised Question Decomposition for Question Answering <a href="https://arxiv.org/pdf/2002.09758" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Exploiting Typed Syntactic Dependencies for Targeted Sentiment  Classification Using Graph Attention Neural Network <a href="https://arxiv.org/pdf/2002.09685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Incorporating Effective Global Information via Adaptive Gate Attention  for Text Classification <a href="https://arxiv.org/pdf/2002.09673" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Machine Translation System Selection from Bandit Feedback <a href="https://arxiv.org/pdf/2002.09646" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Markov Chain Monte-Carlo Phylogenetic Inference Construction in  Computational Historical Linguistics <a href="https://arxiv.org/pdf/2002.09637" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Data Augmentation for Copy-Mechanism in Dialogue State Tracking <a href="https://arxiv.org/pdf/2002.09634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Efficient Sentence Embedding via Semantic Subspace Analysis <a href="https://arxiv.org/pdf/2002.09620" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> "Wait, I'm Still Talking!" Predicting the Dialogue Interaction Behavior  Using Imagine-Then-Arbitrate Model <a href="https://arxiv.org/pdf/2002.09616" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Emergent Communication with World Models <a href="https://arxiv.org/pdf/2002.09604" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Training Question Answering Models From Synthetic Data <a href="https://arxiv.org/pdf/2002.09599" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Extracting and Validating Explanatory Word Archipelagoes using Dual  Entropy <a href="https://arxiv.org/pdf/2002.09581" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Modelling Latent Skills for Multitask Language Generation <a href="https://arxiv.org/pdf/2002.09543" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> KBSET -- Knowledge-Based Support for Scholarly Editing and Text  Processing with Declarative LaTeX Markup and a Core Written in SWI-Prolog <a href="https://arxiv.org/pdf/2002.10329" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Uncertainty based Class Activation Maps for Visual Question Answering <a href="https://arxiv.org/pdf/2002.10309" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Rhythm, Chord and Melody Generation for Lead Sheets using Recurrent  Neural Networks <a href="https://arxiv.org/pdf/2002.10266" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Leveraging Code Generation to Improve Code Retrieval and Summarization  via Dual Learning <a href="https://arxiv.org/pdf/2002.10198" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> FONDUE: A Framework for Node Disambiguation Using Network Embeddings <a href="https://arxiv.org/pdf/2002.10127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Emosaic: Visualizing Affective Content of Text at Varying Granularity <a href="https://arxiv.org/pdf/2002.10096" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Deep Multimodal Image-Text Embeddings for Automatic Cross-Media  Retrieval <a href="https://arxiv.org/pdf/2002.10016" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Automata for Hyperlanguages <a href="https://arxiv.org/pdf/2002.09877" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Sketching Transformed Matrices with Applications to Natural Language  Processing <a href="https://arxiv.org/pdf/2002.09812" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Resources for Turkish Dependency Parsing: Introducing the BOUN Treebank  and the BoAT Annotation Tool</b>  <a href="https://arxiv.org/pdf/2002.10416" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=T%C3%BCrk%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Utku Türk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Atmaca%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furkan Atmaca</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=%C3%96zate%C5%9F%2C+%C5%9E+B" target="_blank" rel="noopener" style="color:#0000EE;">Şaziye Betül Özateş</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Berk%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gözde Berk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bedir%2C+S+T" target="_blank" rel="noopener" style="color:#0000EE;">Seyyit Talha Bedir</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=K%C3%B6ksal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abdullatif Köksal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ba%C5%9Faran%2C+B+%C3%96" target="_blank" rel="noopener" style="color:#0000EE;">Balkız Öztürk Başaran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%BCng%C3%B6r%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tunga Güngör</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=%C3%96zg%C3%BCr%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arzucan Özgür</a><br>
<font size="3">
Abstract: In this paper, we describe our contributions and efforts to develop Turkish resources, which include a new treebank (BOUN Treebank) with novel sentences, along with the guidelines we adopted and a new annotation tool we developed (BoAT). The manual annotation process we employed was shaped and implemented by a team of four linguists and five NLP specialists. Decisions regarding the annotation of the BOUN Treebank were made in line with the Universal Dependencies framework, which originated from the works of De Marneffe et al. (2014) and Nivre et al. (2016). We took into account the recent unifying efforts based on the re-annotation of other Turkish treebanks in the UD framework (Türk et al., 2019). Through the BOUN Treebank, we introduced a total of 9,757 sentences from various topics including biographical texts, national newspapers, instructional texts, popular culture articles, and essays. In addition, we report the parsing results of a graph-based dependency parser obtained over each text type, the total of the BOUN Treebank, and all Turkish treebanks that we either re-annotated or introduced. We show that a state-of-the-art dependency parser has improved scores for identifying the proper head and the syntactic relationships between the heads and the dependents. In light of these results, we have observed that the unification of the Turkish annotation scheme and introducing a more comprehensive treebank improves performance with regards to dependency parsing </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们描述了我们的贡献和努力发展土耳其的资源，其中包括一个新的树库（奔树库）与新颖的句子，与我们所采用的准则和新的注释工具，我们开发（船）一起。我们所采用的人工标注过程塑造和一队四个语言学家和五个NLP专业执行。关于奔树库的标注决定与通用依赖性框架，它源于德Marneffe等人的作品行作了。 （2014）和Nivre等。 （2016）。我们考虑到了（蒂尔克等，2019）基于UD框架其他土耳其树库的重新注解近期统一的努力。通过奔树库，我们一共从各种主题的9757句，包括传记文本，全国性的报纸，教学文本，流行文化的文章，和文章的介绍。此外，我们报告了每一个文本类型，共奔树库获得的基于图形的依赖解析器的解析结果，所有土耳其树库，我们要么重新注解或介绍。我们表明，一个国家的最先进的依赖解析器具有识别正确的头和头和家属之间的关系语法改进分数。根据这些结果，我们已观察到土耳其标注方案，并引入更全面的树库的统一提高了与关于依存分析性能</font>
</div>


<hr>
<div id="paper2"> <b>2. Discriminative Adversarial Search for Abstractive Summarization</b>  <a href="https://arxiv.org/pdf/2002.10375" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Scialom%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Scialom</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dray%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul-Alexis Dray</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lamprier%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sylvain Lamprier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Piwowarski%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin Piwowarski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Staiano%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jacopo Staiano</a><br>
<font size="3">
Abstract: We introduce a novel approach for sequence decoding, Discriminative Adversarial Search (DAS), which has the desirable properties of alleviating the effects of exposure bias without requiring external metrics. Inspired by Generative Adversarial Networks (GANs), wherein a discriminator is used to improve the generator, our method differs from GANs in that the generator parameters are not updated at training time and the discriminator is only used to drive sequence generation at inference time. We investigate the effectiveness of the proposed approach on the task of Abstractive Summarization: the results obtained show that a naive application of DAS improves over the state-of-the-art methods, with further gains obtained via discriminator retraining. Moreover, we show how DAS can be effective for cross-domain adaptation. Finally, all results reported are obtained without additional rule-based filtering strategies, commonly used by the best performing systems available: this indicates that DAS can effectively be deployed without relying on post-hoc modifications of the generated outputs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们介绍用于序列解码，判别对抗性搜索（DAS），其具有减轻曝光偏压的效果，而不需要外部度量的所需性质的新方法。通过剖成对抗性网络（甘斯），其特征在于，鉴别器被用于提高发电机，我们的方法从甘斯不同之处在于所述发电机参数不被更新，在训练时间和鉴别器仅用于在推理时间来驱动序列生成的启发。我们调查所提出的方法对写意总结的任务成效：获得表明DAS的天真应用改善了国家的最先进的方法，通过鉴别再培训获得进一步上涨的结果。此外，我们将展示如何DAS可以有效跨域适应。最后，无需额外的基于规则的过滤策略，获得了报告的所有结果，普遍采用现有的最佳执行系统：这表明DAS可以有效地不依赖于产生输出的事后修改进行部署。</font>
</div>


<hr>
<div id="paper3"> <b>3. Multilingual Twitter Corpus and Baselines for Evaluating Demographic  Bias in Hate Speech Recognition</b>  <a href="https://arxiv.org/pdf/2002.10361" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolei Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linzi Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dernoncourt%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Franck Dernoncourt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Paul%2C+M+J" target="_blank" rel="noopener" style="color:#0000EE;">Michael J. Paul</a><br>
<font size="3">
Abstract: Existing research on fairness evaluation of document classification models mainly uses synthetic monolingual data without ground truth for author demographic attributes. In this work, we assemble and publish a multilingual Twitter corpus for the task of hate speech detection with inferred four author demographic factors: age, country, gender and race/ethnicity. The corpus covers five languages: English, Italian, Polish, Portuguese and Spanish. We evaluate the inferred demographic labels with a crowdsourcing platform, Figure Eight. To examine factors that can cause biases, we take an empirical analysis of demographic predictability on the English corpus. We measure the performance of four popular document classifiers and evaluate the fairness and bias of the baseline classifiers on the author-level demographic attributes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有的文档分类模型的公平性评价研究主要采用合成的单语数据，而无需地面实测作者人口属性。在这项工作中，我们组装并发布多语种的Twitter语料库仇恨言论检测与推断4笔者人口因素的任务：年龄，国家，性别和种族/族裔。语料库包括五种语言：英语，意大利语，波兰语，葡萄牙语和西班牙语。我们评估与众包平台，图八的推断人口统计标签。要检查可能会导致偏差的因素，我们把人口预测对英语语料库进行了实证分析。我们衡量的四大流行的文档分类的性能和评价笔者级人口属性基线分类的公平和偏见。</font>
</div>


<hr>
<div id="paper4"> <b>4. Low-Resource Knowledge-Grounded Dialogue Generation</b>  <a href="https://arxiv.org/pdf/2002.10348" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xueliang Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chongyang Tao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Can Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongyan Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Yan</a><br>
<font size="3">
Abstract: Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge-grounded dialogues, as training data for learning such a response generation model, are difficult to obtain. Motivated by the challenge in practice, we consider knowledge-grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a disentangled response decoder in order to isolate parameters that depend on knowledge-grounded dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be well fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only 1/8 training data, our model can achieve the state-of-the-art performance and generalize well on out-of-domain knowledge. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：知识应对已被确认为一个智能会话代理的重要能力。然而，知识接地对话，作为训练数据学习这种反应生成模型，都很难获得。通过在实践中的挑战的推动下，我们认为自然假设只有有限的训练例子都可以在知识接地对话产生。在这样的低资源设定，制定在顺序的解缠结的响应解码器依赖于从整个一代模型知识接地对话分离物的参数。通过这种方式，该模型的主要部分可以从大量不接地对话和非结构化文档中可以得知，而其余的小参数可以通过有限的训练例子很好地拟合。两个基准测试评价结果表明，只有1/8的训练数据，我们的模型可以实现国家的最先进的性能和域外的知识推广好。</font>
</div>


<hr>
<div id="paper5"> <b>5. Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation</b>  <a href="https://arxiv.org/pdf/2002.10345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yige Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xipeng Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Ligao Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuanjing Huang</a><br>
<font size="3">
Abstract: Fine-tuning pre-trained language models like BERT has become an effective way in NLP and yields state-of-the-art results on many downstream tasks. Recent studies on adapting BERT to new tasks mainly focus on modifying the model structure, re-designing the pre-train tasks, and leveraging external data and knowledge. The fine-tuning strategy itself has yet to be fully explored. In this paper, we improve the fine-tuning of BERT with two effective mechanisms: self-ensemble and self-distillation. The experiments on text classification and natural language inference tasks show our proposed methods can significantly improve the adaption of BERT without any external data or knowledge. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：微调预训练的语言模型，如BERT已经成为国家的最先进的NLP和产量上许多下游任务结果的有效途径。在BERT适应新任务最近的研究主要集中在修改模型结构，重新设计的前车的任务，并利用外部数据和知识。微调战略本身尚未得到充分探讨。在本文中，我们提高BERT的微调有两个有效机制：自合奏和自我升华。文本分类和自然语言推理任务的实验表明，我们所提出的方法可以显著提高BERT的适应，无需任何外部数据和知识。</font>
</div>


<hr>
<div id="paper6"> <b>6. Semi-Supervised Speech Recognition via Local Prior Matching</b>  <a href="https://arxiv.org/pdf/2002.10336" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Ning Hsu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ann Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Synnaeve%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabriel Synnaeve</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hannun%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Awni Hannun</a><br>
<font size="3">
Abstract: For sequence transduction tasks like speech recognition, a strong structured prior model encodes rich information about the target space, implicitly ruling out invalid sequences by assigning them low probability. In this work, we propose local prior matching (LPM), a semi-supervised objective that distills knowledge from a strong prior (e.g. a language model) to provide learning signal to a discriminative model trained on unlabeled speech. We demonstrate that LPM is theoretically well-motivated, simple to implement, and superior to existing knowledge distillation techniques under comparable settings. Starting from a baseline trained on 100 hours of labeled speech, with an additional 360 hours of unlabeled data, LPM recovers 54% and 73% of the word error rate on clean and noisy test sets relative to a fully supervised model on the same data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对于像语音识别序列转任务，强大的结构化模型之前编码关于目标空间的丰富信息，通过赋予它们低概率隐含排除无效序列。在这项工作中，我们提出地方之前匹配（LPM），半监督客观的说，从强之前（例如语言模型）蒸馏出知识，以提供学习信号的培训上未标记语音的判别模型。我们证明LPM理论上良好的动机，实现简单，且优于可比下设置现有的知识蒸馏技术。从上训练100小时标记语音的基线起，与另外的360小时未标记的数据，LPM复苏54％和在干净的和有噪声的测试集相对于在相同的数据的完全监控模型字错误率的73％的。</font>
</div>


<hr>
<div id="paper7"> <b>7. Word Embeddings Inherently Recover the Conceptual Organization of the  Human Mind</b>  <a href="https://arxiv.org/pdf/2002.10284" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Swift%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Victor Swift</a><br>
<font size="3">
Abstract: Machine learning is a means to uncover deep patterns from rich sources of data. Here, we find that machine learning can recover the conceptual organization of the human mind when applied to the natural language use of millions of people. Utilizing text from billions of webpages, we recover most of the concepts contained in English, Dutch, and Japanese, as represented in large scale Word Association networks. Our results justify machine learning as a means to probe the human mind, at a depth and scale that has been unattainable using self-report and observational methods. Beyond direct psychological applications, our methods may prove useful for projects concerned with defining, assessing, relating, or uncovering concepts in any scientific field. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器学习是从丰富的数据来源揪出深模式的一种手段。在这里，我们发现当应用到自然语言使用的数以百万计的人们，机器学习可以恢复人的心灵的概念组织。从数十亿网页的利用文字，我们收回大部分的概念，包含英语，荷兰语和日语，在大规模词语联想网表示。我们的研究结果证明机器学习，以探测人的心灵，在深度和广度已经高不可攀使用自我报告和观测方法的手段。除了直接的心理应用，我们的方法可以证明对涉及定义，评估，涉及，或以任何科学领域揭示概念的项目非常有用。</font>
</div>


<hr>
<div id="paper8"> <b>8. Fixed Encoder Self-Attention Patterns in Transformer-Based Machine  Translation</b>  <a href="https://arxiv.org/pdf/2002.10260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Raganato%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alessandro Raganato</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scherrer%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yves Scherrer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tiedemann%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jörg Tiedemann</a><br>
<font size="3">
Abstract: Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that attention heads learn simple positional patterns which are often redundant. In this paper, we propose to replace all but one attention head of each encoder layer with fixed -- non-learnable -- attentive patterns that are solely based on position and do not require any external knowledge. Our experiments show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于变压器的模型带来了神经机器翻译的根本改变。变压器结构的一个关键特性是所谓的多磁头注意机制，其允许模型同时着眼于输入的不同部分。然而，最近的作品表明，重视学习的头简单位置的图形，这往往是多余的。在本文中，我们建议全部更换，但有固定每个编码器层的一个关注头 - 非可学习 - 这是完全基于位置周到的模式，不需要任何外部知识。我们的实验表明，在训练时间上变压器的编码器侧固定注意头不影响翻译质量，甚至高达3点在资源匮乏的情况下增加的BLEU分数。</font>
</div>


<hr>
<div id="paper9"> <b>9. Learning to Select Bi-Aspect Information for Document-Scale Text Content  Manipulation</b>  <a href="https://arxiv.org/pdf/2002.10210" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaocheng Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yawei Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bing Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heng Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yibo Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bi%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Bi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojiang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Ting Liu</a><br>
<font size="3">
Abstract: In this paper, we focus on a new practical task, document-scale text content manipulation, which is the opposite of text style transfer and aims to preserve text styles while altering the content. In detail, the input is a set of structured records and a reference text for describing another recordset. The output is a summary that accurately describes the partial content in the source recordset with the same writing style of the reference. The task is unsupervised due to lack of parallel data, and is challenging to select suitable records and style words from bi-aspect inputs respectively and generate a high-fidelity long document. To tackle those problems, we first build a dataset based on a basketball game report corpus as our testbed, and present an unsupervised neural model with interactive attention mechanism, which is used for learning the semantic relationship between records and reference texts to achieve better content transfer and better style preservation. In addition, we also explore the effectiveness of the back-translation in our task for constructing some pseudo-training pairs. Empirical results show superiority of our approaches over competitive methods, and the models also yield a new state-of-the-art result on a sentence-level dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们着眼于新的实践任务，文件规模的文本内容的操作，这是文字风格转移和目标的相对保留文本样式而改变的内容。详细地，输入是一组结构化的记录，并用于描述另一个记录的参考文本。输出是准确地描述在源记录与基准相同的写作风格的部分内容的摘要。任务被无人看管由于缺乏并行数据，并且是具有挑战性的分别从双方面的输入选择合适的记录和风格字和产生高保真长文档。为了解决这些问题，我们首先建立一个数据集的基础上一场篮球比赛报告文集作为我们的测试平台，并展示互动注意机制，这是用于学习记录和参考文本之间的语义关系，以达到更好的内容传输的无监督神经网络模型和更好的风格保存。此外，我们还探索回译的有效性，我们的任务构建一些伪训练对。实证结果表明，我们在竞争的方式方法的优势，而车型也产生在句子层面的数据集一个新的国家的最先进的结果。</font>
</div>


<hr>
<div id="paper10"> <b>10. A Hybrid Approach to Dependency Parsing: Combining Rules and Morphology  with Deep Learning</b>  <a href="https://arxiv.org/pdf/2002.10116" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=%C3%96zate%C5%9F%2C+%C5%9E+B" target="_blank" rel="noopener" style="color:#0000EE;">Şaziye Betül Özateş</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=%C3%96zg%C3%BCr%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arzucan Özgür</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%BCng%C3%B6r%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tunga Güngör</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=%C3%96zt%C3%BCrk%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Balkız Öztürk</a><br>
<font size="3">
Abstract: Fully data-driven, deep learning-based models are usually designed as language-independent and have been shown to be successful for many natural language processing tasks. However, when the studied language is low-resourced and the amount of training data is insufficient, these models can benefit from the integration of natural language grammar-based information. We propose two approaches to dependency parsing especially for languages with restricted amount of training data. Our first approach combines a state-of-the-art deep learning-based parser with a rule-based approach and the second one incorporates morphological information into the parser. In the rule-based approach, the parsing decisions made by the rules are encoded and concatenated with the vector representations of the input words as additional information to the deep network. The morphology-based approach proposes different methods to include the morphological structure of words into the parser network. Experiments are conducted on the IMST-UD Treebank and the results suggest that integration of explicit knowledge about the target language to a neural parser through a rule-based parsing system and morphological analysis leads to more accurate annotations and hence, increases the parsing performance in terms of attachment scores. The proposed methods are developed for Turkish, but can be adapted to other languages as well. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：完全数据驱动，深学习型模型通常被设计为与语言无关，并已被证明是成功的为众多的自然语言处理任务。然而，当研究语言资源不足地区和训练数据的量不足，这些模型可以从中受益的自然语言基于语法的信息的整合。我们提出了两种方法的依赖尤其是分析与训练数据的限制量语言。我们的第一种方法相结合的状态下的最先进的深基于学习的语法分析器与基于规则的方法，第二个结合形态信息到所述解析器。在基于规则的方法，按规则进行的分析决策编码，并与输入字的附加信息的深网络矢量表示串联。基于形态学的方法提出了不同的方法来包括词的形态结构到解析器网络。实验是在IMST-UD树库进行，结果表明，整合有关通过基于规则的分析系统和形态分析，导致更准确的注解，因此目标语言的神经解析器显性知识，增加了术语解析性能附着分数。所提出的方法是为土耳其的发展，但也可以适用于其他语言。</font>
</div>


<hr>
<div id="paper11"> <b>11. Predicting Subjective Features from Questions on QA Websites using BERT</b>  <a href="https://arxiv.org/pdf/2002.10107" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Annamoradnejad%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Issa Annamoradnejad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fazli%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammadamin Fazli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Habibi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jafar Habibi</a><br>
<font size="3">
Abstract: Modern Question-Answering websites, such as StackOverflow and Quora, have specific user rules to maintain their content quality. These systems rely on user reports for accessing new contents, which has serious problems including the slow handling of violations, the loss of normal and experienced users' time, the low quality of some reports, and discouraging feedback to new users. Therefore, with the overall goal of providing solutions for automating moderation actions in Q&A websites, we aim to provide a model to predict 20 quality or subjective aspects of questions in QA websites. To this end, we used data gathered by the CrowdSource team at Google Research in 2019 and fine-tuned pre-trained BERT model on our problem. Model achieves 95.4% accuracy after 2 epochs of training and did not improve substantially in the next ones. Results confirm that by simple fine-tuning, we can achieve accurate models, in little time, and on less amount of data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现代答疑网站，如StackOverflow的和Quora的，有特定的用户规则来维护自己的内容质量。这些系统依赖于用户报告访问新的内容，其中有严重的问题，包括慢处理违法行为，正常和有经验的用户的时间损失，一些报告质量低，和劝阻反馈给新用户。因此，自动化的Q＆A网站中庸行动提供解决方案的总体目标，我们的目标是提供一个模型来预测20质量或在QA网站问题的主观方面。为此，我们使用了我们的问题在2019年由众包团队在谷歌研究收集的数据和微调预训练BERT模式。型号达到95.4％的准确率在2个时期的训练，并没有在接下来的那些大幅度提高。结果证实，通过简单的微调，就可以实现精确的模型，在很少的时间，并在数据量较少。</font>
</div>


<hr>
<div id="paper12"> <b>12. GRET: Global Representation Enhanced Transformer</b>  <a href="https://arxiv.org/pdf/2002.10101" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Weng%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rongxiang Weng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoran Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shujian Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heng Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bing%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lidong Bing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weihua Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiajun Chen</a><br>
<font size="3">
Abstract: Transformer, based on the encoder-decoder framework, has achieved state-of-the-art performance on several natural language generation tasks. The encoder maps the words in the input sentence into a sequence of hidden states, which are then fed into the decoder to generate the output sentence. These hidden states usually correspond to the input words and focus on capturing local information. However, the global (sentence level) information is seldom explored, leaving room for the improvement of generation quality. In this paper, we propose a novel global representation enhanced Transformer (GRET) to explicitly model global representation in the Transformer network. Specifically, in the proposed model, an external state is generated for the global representation from the encoder. The global representation is then fused into the decoder during the decoding process to improve generation quality. We conduct experiments in two text generation tasks: machine translation and text summarization. Experimental results on four WMT machine translation tasks and LCSTS text summarization task demonstrate the effectiveness of the proposed approach on natural language generation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：变压器的基础上，编码器，解码器框架，取得了几个自然语言生成任务的国家的最先进的性能。编码器在输入句子的单词映射到隐藏状态，然后将其馈送到解码器中，以产生输出语句的序列。这些隐藏的状态通常对应于捕捉本地信息的输入单词和重点。然而，全球（句子层面）的信息很少探索，留有余地产生质量的提高。在本文中，我们提出了增强型变压器（GRET）一种新型的全球代表全球代表性变压器网络中明确建模。具体而言，在所提出的模型中，对来自编码器的全球代表所产生的外部的状态。则全局表示被在解码过程中熔合到解码器中，以提高生成质量。我们进行了两个文本生成任务实验：机器翻译和文本摘要。四个WMT机器翻译任务和LCSTS文本摘要任务的实验结果表明，在自然语言生成了该方法的有效性。</font>
</div>


<hr>
<div id="paper13"> <b>13. Do Multi-Hop Question Answering Systems Know How to Answer the  Single-Hop Sub-Questions?</b>  <a href="https://arxiv.org/pdf/2002.09919" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yixuan Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+H+T" target="_blank" rel="noopener" style="color:#0000EE;">Hwee Tou Ng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tung%2C+A+K+H" target="_blank" rel="noopener" style="color:#0000EE;">Anthony K.H. Tung</a><br>
<font size="3">
Abstract: Multi-hop question answering (QA) requires a model to retrieve and integrate information from different parts of a long text to answer a question. Humans answer this kind of complex questions via a divide-and-conquer approach. In this paper, we investigate whether top-performing models for multi-hop questions understand the underlying sub-questions like humans. We adopt a neural decomposition model to generate sub-questions for a multi-hop complex question, followed by extracting the corresponding sub-answers. We show that multiple state-of-the-art multi-hop QA models fail to correctly answer a large portion of sub-questions, although their corresponding multi-hop questions are correctly answered. This indicates that these models manage to answer the multi-hop questions using some partial clues, instead of truly understanding the reasoning paths. We also propose a new model which significantly improves the performance on answering the sub-questions. Our work takes a step forward towards building a more explainable multi-hop QA system. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多跳问答（QA）需要一个模型来检索和长文本的不同部分整合信息来回答的问题。人类回答这种通过分而治之的方法复杂的问题。在本文中，我们研究了多跳的问题表现最佳车型是否了解底层的子问题，像人类一样。我们采用神经分解模型来生成子问题的一个多跳复杂的问题，其次是提取相应子的答案。我们发现，多个国家的最先进的多跳QA模型不能正确回答的子问题的很大一部分，但其对应的多跳问题都回答正确。这表明，这些模型能答对使用一些局部的线索多跳的问题，而不是真正理解推理路径。我们也建议其显著提高了回答小问题的性能的新模式。我们的工作采取稳步前进，建立一个更可解释的多跳的质量保证体系。</font>
</div>


<hr>
<div id="paper14"> <b>14. A Nepali Rule Based Stemmer and its performance on different NLP  applications</b>  <a href="https://arxiv.org/pdf/2002.09901" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Koirala%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pravesh Koirala</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shakya%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aman Shakya</a><br>
<font size="3">
Abstract: Stemming is an integral part of Natural Language Processing (NLP). It's a preprocessing step in almost every NLP application. Arguably, the most important usage of stemming is in Information Retrieval (IR). While there are lots of work done on stemming in languages like English, Nepali stemming has only a few works. This study focuses on creating a Rule Based stemmer for Nepali text. Specifically, it is an affix stripping system that identifies two different class of suffixes in Nepali grammar and strips them separately. Only a single negativity prefix (Na) is identified and stripped. This study focuses on a number of techniques like exception word identification, morphological normalization and word transformation to increase stemming performance. The stemmer is tested intrinsically using Paice's method and extrinsically on a basic tf-idf based IR system and an elementary news topic classifier using Multinomial Naive Bayes Classifier. The difference in performance of these systems with and without using the stemmer is analysed. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：词干是自然语言处理（NLP）的一个组成部分。这是在几乎每一个NLP应用预处理步骤。可以说，所产生的最重要的用法是在信息检索（IR）。虽然有很多工作对像英语，尼泊尔语词干语言所产生做过只有少数作品。这项研究的重点是建立一个以规则为基础的词干尼泊尔文本。具体而言，它是一种汽提词缀系统，识别两个不同的类中尼泊尔语法后缀和钢带它们分开。只有一个消极的前缀（Na）的标识和剥离。本研究着重于一些像例外词识别，形态归一化和转化这个词来提高所产生的表现技法。该词干是利用Paice的方法和外在基本TF-IDF基于IR系统，并使用多项朴素贝叶斯分类器的基本新闻话题分类本质上测试。在使用和不使用词干这些系统的性能差异进行了分析。</font>
</div>


<hr>
<div id="paper15"> <b>15. Fill in the BLANC: Human-free quality estimation of document summaries</b>  <a href="https://arxiv.org/pdf/2002.09836" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Vasilyev%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oleg Vasilyev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dharnidharka%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vedant Dharnidharka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bohannon%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Bohannon</a><br>
<font size="3">
Abstract: We present BLANC, a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document's text. We present evidence that BLANC scores have at least as good correlation with human evaluations as do the ROUGE family of summary quality measurements. And unlike ROUGE, the BLANC method does not require human-written reference summaries, allowing for fully human-free summary quality estimation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出BLANC，一种新的方法，以文档摘要质量的自动估计。我们的目标是一种客观的，可重现，和全自动的方法来测量的概要的功能性能。我们的方法通过测量同时开展对文档的文本的语言理解任务由预先训练的语言模型访问文档摘要获得的性能提升达到这一点。我们目前的证据表明，BLANC分数至少有与人评价为做胭脂家族的总结质量测量良好的相关性。不像ROUGE，布兰克方法不需要人工编写的参考摘要，允许完全自由人总结质量估计。</font>
</div>


<hr>
<div id="paper16"> <b>16. Unsupervised Question Decomposition for Question Answering</b>  <a href="https://arxiv.org/pdf/2002.09758" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Perez%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ethan Perez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lewis%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Lewis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yih%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen-tau Yih</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kyunghyun Cho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Douwe Kiela</a><br>
<font size="3">
Abstract: We aim to improve question answering (QA) by decomposing hard questions into easier sub-questions that existing QA systems can answer. Since collecting labeled decompositions is cumbersome, we propose an unsupervised approach to produce sub-questions. Specifically, by leveraging >10M questions from Common Crawl, we learn to map from the distribution of multi-hop questions to the distribution of single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and incorporate the resulting answers in a downstream, multi-hop QA system. On a popular multi-hop QA dataset, HotpotQA, we show large improvements over a strong baseline, especially on adversarial and out-of-domain questions. Our method is generally applicable and automatically learns to decompose questions of different classes, while matching the performance of decomposition methods that rely heavily on hand-engineering and annotation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们的目标是通过分解难的问题更容易进入子的问题，现有的质量保证系统能回答改善问答（QA）。由于收集标记分解很麻烦，我们提出了一种无监督的方法来产生子问题。具体来说，由通用抓取借力> 10M的问题，我们学会从多跳问题分布的单跳子问题的分布图。我们回答小问题有一个现成的，现成的QA模型，并在下游，多跳的质量保证体系结合所产生的答案。在一个流行的多跳QA数据集，HotpotQA，我们表现出了强烈的基线大的改进，特别是在对抗和外的域的问题。我们的方法是普遍适用的，自动学习不同类别的分解问题，同时匹配的严重依赖手工工程和注释的分解方法的性能。</font>
</div>


<hr>
<div id="paper17"> <b>17. Exploiting Typed Syntactic Dependencies for Targeted Sentiment  Classification Using Graph Attention Neural Network</b>  <a href="https://arxiv.org/pdf/2002.09685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuefeng Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengbo Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Zhang</a><br>
<font size="3">
Abstract: Targeted sentiment classification predicts the sentiment polarity on given target mentions in input texts. Dominant methods employ neural networks for encoding the input sentence and extracting relations between target mentions and their contexts. Recently, graph neural network has been investigated for integrating dependency syntax for the task, achieving the state-of-the-art results. However, existing methods do not consider dependency label information, which can be intuitively useful. To solve the problem, we investigate a novel relational graph attention network that integrates typed syntactic dependency information. Results on standard benchmarks show that our method can effectively leverage label information for improving targeted sentiment classification performances. Our final model significantly outperforms state-of-the-art syntax-based approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：有针对性的情感分类预测在给定的目标情感极性在输入文本中提到。占主导地位的方法采用神经网络编码输入句子之间的目标提到了和他们的背景提取关系。近日，图表神经网络已经被研究用于集成依赖语法任务，实现国家的最先进的成果。然而，现有的方法没有考虑依赖标签信息，它可以直观地有用。为了解决这个问题，我们研究了一种新型的关系图关注网络整合类型的语法结构信息。关于标准的基准测试结果表明，该方法可以有效地利用标签信息以提高针对性情感分类表演。我们最后的模型显著优于国家的最先进的基于语法的方法。</font>
</div>


<hr>
<div id="paper18"> <b>18. Incorporating Effective Global Information via Adaptive Gate Attention  for Text Classification</b>  <a href="https://arxiv.org/pdf/2002.09673" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xianming Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zongxi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingbin Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoran Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qing Li</a><br>
<font size="3">
Abstract: The dominant text classification studies focus on training classifiers using textual instances only or introducing external knowledge (e.g., hand-craft features and domain expert knowledge). In contrast, some corpus-level statistical features, like word frequency and distribution, are not well exploited. Our work shows that such simple statistical information can enhance classification performance both efficiently and significantly compared with several baseline models. In this paper, we propose a classifier with gate mechanism named Adaptive Gate Attention model with Global Information (AGA+GI), in which the adaptive gate mechanism incorporates global statistical features into latent semantic features and the attention layer captures dependency relationship within the sentence. To alleviate the overfitting issue, we propose a novel Leaky Dropout mechanism to improve generalization ability and performance stability. Our experiments show that the proposed method can achieve better accuracy than CNN-based and RNN-based approaches without global information on several benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：占主导地位的文本分类的研究集中在仅使用或引入外部知识（例如，手工工艺的特点和领域专家知识）文本实例培训分类。相反，一些语料库级别的统计特征，如词频和分布，没有得到很好的利用。我们的工作表明，这种简单的统计信息，可以提高分类性能与一些基准模型有效且显著比较。在本文中，我们提出了与全球信息（AGA + GI），其中自适应栅极机制整合全球统计功能集成到潜在语义特征和句子中的关注层捕获依赖关系称为自适应栅极注意模型门机构的分类。为了减轻过拟合问题，我们提出了一个新颖的漏降机制，提高推广能力和性能的稳定性。我们的实验表明，该方法可以实现比不上几个基准全球信息化CNN和基于RNN方法更好的精度。</font>
</div>


<hr>
<div id="paper19"> <b>19. Machine Translation System Selection from Bandit Feedback</b>  <a href="https://arxiv.org/pdf/2002.09646" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Naradowsky%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Naradowsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Duh</a><br>
<font size="3">
Abstract: Adapting machine translation systems in the real world is a difficult problem. In contrast to offline training, users cannot provide the type of fine-grained feedback typically used for improving the system. Moreover, users have different translation needs, and even a single user's needs may change over time. In this work we take a different approach, treating the problem of adapting as one of selection. Instead of adapting a single system, we train many translation systems using different architectures and data partitions. Using bandit learning techniques on simulated user feedback, we learn a policy to choose which system to use for a particular translation task. We show that our approach can (1) quickly adapt to address domain changes in translation tasks, (2) outperform the single best system in mixed-domain translation tasks, and (3) make effective instance-specific decisions when using contextual bandit strategies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在现实世界中适应机器翻译系统是一个棘手的问题。相较于离线训练，用户不能提供通常用于提高系统的细粒度反馈类型。此外，用户有不同的翻译需求，甚至是单个用户的需求可能会随时间而改变。在这项工作中，我们采取不同的方法，治疗适应作为选择的一个问题。相反，适应一个单一的系统中，我们培养使用不同的架构和数据分区很多翻译系统。使用匪学习上模拟用户反馈技术，我们学习的策略选择使用特定的翻译任务，系统。我们表明，我们的方法可以（1）快速适应地址域的变化在翻译任务，（2）混合域翻译任务跑赢大单最好的系统，和（3）利用上下文匪策略时做出有效的情况下，具体的决定。</font>
</div>


<hr>
<div id="paper20"> <b>20. Markov Chain Monte-Carlo Phylogenetic Inference Construction in  Computational Historical Linguistics</b>  <a href="https://arxiv.org/pdf/2002.09637" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianyi Ni</a><br>
<font size="3">
Abstract: More and more languages in the world are under study nowadays, as a result, the traditional way of historical linguistics study is facing some challenges. For example, the linguistic comparative research among languages needs manual annotation, which becomes more and more impossible with the increasing amount of language data coming out all around the world. Although it could hardly replace linguists work, the automatic computational methods have been taken into consideration and it can help people reduce their workload. One of the most important work in historical linguistics is word comparison from different languages and find the cognate words for them, which means people try to figure out if the two languages are related to each other or not. In this paper, I am going to use computational method to cluster the languages and use Markov Chain Monte Carlo (MCMC) method to build the language typology relationship tree based on the clusters. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：越来越多的世界语言正在研究时下，因此，历史语言学研究的传统方式正面临着一些挑战。例如，语言之间的语言比较研究需要人工注释，成为与语言数据的现身在世界各地的越来越多越来越不可能。虽然很难取代语言学家的工作，自动计算方法已经被考虑到，它可以帮助人们减少他们的工作量。一个在历史语言学中最重要的工作是由不同的语言文字比较，并找到他们的同源词，这意味着人们揣摩，如果两种语言相互关联与否。在本文中，我将用计算方法进行聚类的语言和使用马尔可夫链蒙特卡罗（MCMC）方法来构建基于集群的语言类型学关系树。</font>
</div>


<hr>
<div id="paper21"> <b>21. Data Augmentation for Copy-Mechanism in Dialogue State Tracking</b>  <a href="https://arxiv.org/pdf/2002.09634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaohui Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangjun Zang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yipeng Su</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xing Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jizhong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Songlin Hu</a><br>
<font size="3">
Abstract: While several state-of-the-art approaches to dialogue state tracking (DST) have shown promising performances on several benchmarks, there is still a significant performance gap between seen slot values (i.e., values that occur in both training set and test set) and unseen ones (values that occur in training set but not in test set). Recently, the copy-mechanism has been widely used in DST models to handle unseen slot values, which copies slot values from user utterance directly. In this paper, we aim to find out the factors that influence the generalization ability of a common copy-mechanism model for DST. Our key observations include: 1) the copy-mechanism tends to memorize values rather than infer them from contexts, which is the primary reason for unsatisfactory generalization performance; 2) greater diversity of slot values in the training set increase the performance on unseen values but slightly decrease the performance on seen values. Moreover, we propose a simple but effective algorithm of data augmentation to train copy-mechanism models, which augments the input dataset by copying user utterances and replacing the real slot values with randomly generated strings. Users could use two hyper-parameters to realize a trade-off between the performances on seen values and unseen ones, as well as a trade-off between overall performance and computational cost. Experimental results on three widely used datasets (WoZ 2.0, DSTC2, and Multi-WoZ 2.0) show the effectiveness of our approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然一些国家的最先进的方法，以对话状态跟踪（DST）已经在几个基准显示出大有希望的演出，但仍然发生在两个训练集和测试看出槽值（即，值之间的显著性能差距集）和看不见的人（出现在训练组值而不是在测试集）。近日，复制机制已被广泛应用于DST车型可供使用者说话直接处理看不见的槽值，其副本槽值。在本文中，我们的目标是找出影响了DST共同的复制机制模型的泛化能力的因素。我们的关键观察包括：1）复制机制倾向于从上下文记忆值，而不是他们推断，这是不令人满意的推广性能的主要原因; 2）在训练集中槽值的更大的多样性增加看不见值性能，但稍微降低上看到的值的性能。此外，我们提出了一个简单的，但数据扩充的有效算法训练复制机制的模型，其中通过复制用户话语，并用随机生成的字符串替换真实槽值增强了输入数据集。用户可以使用两个超参数，实现对看到的价值观和看不见的那些性能之间的权衡，以及整体性能和计算成本之间的权衡。三个广泛使用的数据集实验结果（WOZ 2.0，DSTC2，和多沃兹2.0）表明我们的方法的有效性。</font>
</div>


<hr>
<div id="paper22"> <b>22. Efficient Sentence Embedding via Semantic Subspace Analysis</b>  <a href="https://arxiv.org/pdf/2002.09620" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fenxiao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuncheng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuo%2C+C+-+J" target="_blank" rel="noopener" style="color:#0000EE;">C.-C. Jay Kuo</a><br>
<font size="3">
Abstract: A novel sentence embedding method built upon semantic subspace analysis, called semantic subspace sentence embedding (S3E), is proposed in this work. Given the fact that word embeddings can capture semantic relationship while semantically similar words tend to form semantic groups in a high-dimensional embedding space, we develop a sentence representation scheme by analyzing semantic subspaces of its constituent words. Specifically, we construct a sentence model from two aspects. First, we represent words that lie in the same semantic group using the intra-group descriptor. Second, we characterize the interaction between multiple semantic groups with the inter-group descriptor. The proposed S3E method is evaluated on both textual similarity tasks and supervised tasks. Experimental results show that it offers comparable or better performance than the state-of-the-art. The complexity of our S3E method is also much lower than other parameterized models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：根据语义子空间分析，建立了一种新的句子埋线法，称为语义子空间句子嵌入（S3E），在这项工作中提出。鉴于字的嵌入可以捕捉语义关系，而语义相似的词往往会形成语义组在高维嵌入空间，我们开发了通过分析它的构成词的语义子空间的句子表达方式。具体来说，我们从两个方面来构建一个句模型。首先，我们表示位于同一语义组使用帧内组描述符的话。其次，表征与组间描述多个语义组之间的相互作用。所提出的S3E方法在两个文本相似的任务和监督的任务进行评估。实验结果表明，它提供了比国家的最先进的相当或更好的性能。本店S3E方法的复杂性也比其它参数化模型低得多。</font>
</div>


<hr>
<div id="paper23"> <b>23. "Wait, I'm Still Talking!" Predicting the Dialogue Interaction Behavior  Using Imagine-Then-Arbitrate Model</b>  <a href="https://arxiv.org/pdf/2002.09616" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zehao Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoming Kang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guodun Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haiqing Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yin Zhang</a><br>
<font size="3">
Abstract: Producing natural and accurate responses like human beings is the ultimate goal of intelligent dialogue agents. So far, most of the past works concentrate on selecting or generating one pertinent and fluent response according to current query and its context. These models work on a one-to-one environment, making one response to one utterance each round. However, in real human-human conversations, human often sequentially sends several short messages for readability instead of a long message in one turn. Thus messages will not end with an explicit ending signal, which is crucial for agents to decide when to reply. So the first step for an intelligent dialogue agent is not replying but deciding if it should reply at the moment. To address this issue, in this paper, we propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to help the agent decide whether to wait or to make a response directly. Our method has two imaginator modules and an arbitrator module. The two imaginators will learn the agent's and user's speaking style respectively, generate possible utterances as the input of the arbitrator, combining with dialogue history. And the arbitrator decides whether to wait or to make a response to the user directly. To verify the performance and effectiveness of our method, we prepared two dialogue datasets and compared our approach with several popular models. Experimental results show that our model performs well on addressing ending prediction issue and outperforms baseline models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生产人类一样自然而准确的答复是智能对话代理的终极目标。到目前为止，大多数过去的作品集中选择或生成一个相关的，并根据当前查询及其上下文流畅的响应。这些模型在一到一个环境中工作，使一个响应一个话语每一轮。然而，在实际人 - 人交谈，人类经常依次发送几个短消息是为了便于阅读，而不是在一匝的长消息。因此消息不会有一个明确的结束信号，这是至关重要的代理商时，答复决定而结束。因此，对于一个智能代理对话的第一步是不回答，但决定是否应该在此刻答复。为了解决这个问题，在本文中，我们提出了一个新颖的想象，然后仲裁的（ITA）的神经对话模式，以帮助代理商决定是否要等待或者直接作出回应。我们的方法具有两个imaginator模块和仲裁模块。这两个imaginators将分别学习代理人和用户的说话方式，可能产生的话语仲裁员的输入，以对话的历史相结合。而该仲裁员决定是否要等待或使直接用户的响应。为了验证方法的性能和效率，我们准备了两个对话数据集和我们相比，使用一些流行的模型法。实验结果表明，我们的模型进行很好的处理结束的预测问题，优于基准模型。</font>
</div>


<hr>
<div id="paper24"> <b>24. Emergent Communication with World Models</b>  <a href="https://arxiv.org/pdf/2002.09604" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cowen-Rivers%2C+A+I" target="_blank" rel="noopener" style="color:#0000EE;">Alexander I. Cowen-Rivers</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Naradowsky%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Naradowsky</a><br>
<font size="3">
Abstract: We introduce Language World Models, a class of language-conditional generative model which interpret natural language messages by predicting latent codes of future observations. This provides a visual grounding of the message, similar to an enhanced observation of the world, which may include objects outside of the listening agent's field-of-view. We incorporate this "observation" into a persistent memory state, and allow the listening agent's policy to condition on it, akin to the relationship between memory and controller in a World Model. We show this improves effective communication and task success in 2D gridworld speaker-listener navigation tasks. In addition, we develop two losses framed specifically for our model-based formulation to promote positive signalling and positive listening. Finally, because messages are interpreted in a generative model, we can visualize the model beliefs to gain insight into how the communication channel is utilized. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍语言世界图式，一类语言条件生成模型的其中通过预测未来的观测潜代码解释自然语言的消息。这提供了该消息，类似于世界的增强观测，其可以包括对象听音代理的字段的视图以外的可视接地。我们将这一“观察”到持久性存储器的状态，使听音代理的政策条件就可以了，类似于在世界模特内存和控制器之间的关系。我们发现这提高了2D gridworld扬声器收听导航任务的有效沟通和任务的成功。此外，我们还开发专门为我们推动积极的信号，积极的倾听基于模型的框架制定两负。最后，因为消息是在生成模型的解释，我们可以可视化模型的信念深入了解通信信道如何利用。</font>
</div>


<hr>
<div id="paper25"> <b>25. Training Question Answering Models From Synthetic Data</b>  <a href="https://arxiv.org/pdf/2002.09599" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Puri%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raul Puri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Spring%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryan Spring</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Patwary%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mostofa Patwary</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Shoeybi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Catanzaro%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bryan Catanzaro</a><br>
<font size="3">
Abstract: Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：问题和答案产生是一个数据增强方法，旨在改善问答（QA）模型给出的人类标注的数据量有限。然而，相当大的差距保持合成的和人类生成的问答配对之间。这项工作旨在通过利用大型语言模型，探讨多种因素，如模型的大小，预先训练模型的质量，合成数据的规模，和算法的选择优势，缩小这个差距。在SQuAD1.1问答任务，我们使用单独使用SQuAD1.1训练集的问题时，不是单纯的合成问题和解答达到更高的精度。删除访问维基百科的实际数据，我们综合问题和答案由8.3十亿参数GPT-2模型产生的合成语料库。由于没有进入人类的监管，只有其他机型的访问，我们能够在艺术问题上达到88.4精确匹配（EM）和93.9 F1得分上SQuAD1.1开发一套完全模型生成数据回答网络的列车状态。我们我们的方法还适用于SQuAD2.0和EM得分使用合成数据相比以前的工作表现出2.8的绝对收益。</font>
</div>


<hr>
<div id="paper26"> <b>26. Extracting and Validating Explanatory Word Archipelagoes using Dual  Entropy</b>  <a href="https://arxiv.org/pdf/2002.09581" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ohsawa%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yukio Ohsawa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hayashi%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Teruaki Hayashi</a><br>
<font size="3">
Abstract: The logical connectivity of text is represented by the connectivity of words that form archipelagoes. Here, each archipelago is a sequence of islands of the occurrences of a certain word. An island here means the local sequence of sentences where the word is emphasized, and an archipelago of a length comparable to the target text is extracted using the co-variation of entropy A (the window-based entropy) on the distribution of the word's occurrences with the width of each time window. Then, the logical connectivity of text is evaluated on entropy B (the graph-based entropy) computed on the distribution of sentences to connected word-clusters obtained on the co-occurrence of words. The results show the parts of the target text with words forming archipelagoes extracted on entropy A, without learned or prepared knowledge, form an explanatory part of the text that is of smaller entropy B than the parts extracted by the baseline methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文本的逻辑连接是由形成群岛的字的连接来表示。在这里，每个群岛是某个词的出现岛屿的序列。这里的一个岛是指其中字强调句子的局部序列，并在字的出现的分布使用熵A的共变（所述基于窗口的熵）被提取媲美的目标文本的长度的群岛与每个时间窗的宽度。然后，文本的逻辑连接是在计算上的句子的分布上的字的同现得到的连接的字集群熵B（基于图形的熵）来评价。结果表明与形成于熵甲萃取群岛，没有获悉或者准备知识字的目标文本的部分，形成比由基线方法提取的部分更小的熵B的文本的说明部分。</font>
</div>


<hr>
<div id="paper27"> <b>27. Modelling Latent Skills for Multitask Language Generation</b>  <a href="https://arxiv.org/pdf/2002.09543" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kris Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yogatama%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dani Yogatama</a><br>
<font size="3">
Abstract: We present a generative model for multitask conditional language generation. Our guiding hypothesis is that a shared set of latent skills underlies many disparate language generation tasks, and that explicitly modelling these skills in a task embedding space can help with both positive transfer across tasks and with efficient adaptation to new tasks. We instantiate this task embedding space as a latent variable in a latent variable sequence-to-sequence model. We evaluate this hypothesis by curating a series of monolingual text-to-text language generation datasets - covering a broad range of tasks and domains and comparing the performance of models both in the multitask and few-shot regimes. We show that our latent task variable model outperforms other sequence-to-sequence baselines on average across tasks in the multitask setting. In the few-shot learning setting on an unseen test dataset (i.e., a new task), we demonstrate that model adaptation based on inference in the latent task space is more robust than standard fine-tuning based parameter adaptation and performs comparably in terms of overall performance. Finally, we examine the latent task representations learnt by our model and show that they cluster tasks in a natural way. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了一个生成模型的多任务条件语言的产生。我们的指导假设是潜伏技能underlies许多不同的语言生成任务组共享，并在任务中嵌入空间明确建模这些技能可以跨越任务，并以高效的适应新的任务正传递帮助。我们实例化这个任务嵌入空间的潜在变量序列到序列模型中的潜在变量。覆盖范围广泛的任务和域的和比较的车型无论是在多任务和几拍政权的表现 - 我们通过策划一系列单语文本到文本的语言生成的数据集的评估这一假说。我们证明了我们的潜在任务变量模型优于其他序列到序列基线平均横跨任务在多任务环境。在一个看不见的测试数据集（即，新任务）的几个拍的学习环境，我们展示了基于潜在的任务空间推理这种模式适应比标准微调基于参数的适应和执行更强大的同等来讲整体表现。最后，我们检查我们的模型学到的潜伏任务陈述，并表明他们以自然的方式聚集任务。</font>
</div>


<hr>
<div id="paper28"> <b>28. KBSET -- Knowledge-Based Support for Scholarly Editing and Text  Processing with Declarative LaTeX Markup and a Core Written in SWI-Prolog</b>  <a href="https://arxiv.org/pdf/2002.10329" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kittelmann%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jana Kittelmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wernhard%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christoph Wernhard</a><br>
<font size="3">
Abstract: KBSET is an environment that provides support for scholarly editing in two flavors: First, as a practical tool KBSET/Letters that accompanies the development of editions of correspondences (in particular from the 18th and 19th century), completely from source documents to PDF and HTML presentations. Second, as a prototypical tool KBSET/NER for experimentally investigating novel forms of working on editions that are centered around automated named entity recognition. KBSET can process declarative application-specific markup that is expressed in LaTeX notation and incorporate large external fact bases that are typically provided in RDF. KBSET includes specially developed LaTeX styles and a core system that is written in SWI-Prolog, which is used there in many roles, utilizing that it realizes the potential of Prolog as a unifying language. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：KBSET的是，在两种形式为学术编辑支持的环境：首先，作为一种实用工具KBSET /信函，伴随着对应的版本的发展（特别是从18世纪和19世纪），完全从源文件为PDF和HTML演示。其次，作为实验研究上都围绕着自动命名实体识别版本的工作新形式的原型工具KBSET / ER。 KBSET可以处理在乳胶符号表示声明性应用程序特定标记，并纳入其通常在RDF提供大的外部事实碱基。 KBSET包括专门开发的LaTeX风格和上所写的SWI-Prolog的，这是在许多角色使用有一个核心系统，利用它实现的Prolog作为统一语言的潜力。</font>
</div>


<hr>
<div id="paper29"> <b>29. Uncertainty based Class Activation Maps for Visual Question Answering</b>  <a href="https://arxiv.org/pdf/2002.10309" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Patro%2C+B+N" target="_blank" rel="noopener" style="color:#0000EE;">Badri N. Patro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lunayach%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mayank Lunayach</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Namboodiri%2C+V+P" target="_blank" rel="noopener" style="color:#0000EE;">Vinay P. Namboodiri</a><br>
<font size="3">
Abstract: Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a recipe for obtaining improved certainty estimates and explanations for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：理解和解释深学习模型是一个势在必行的任务。为了实现这个，我们建议，取得基于梯度确定性估计，也提供视觉注意力图的方法。尤其是，我们解决了视觉问答任务。我们结合现代化的概率深的学习方法，我们通过使用梯度为这些估计进一步提高。这有两方面的好处：在获得确定性估计与错误分类样本和b）提高注意力的地图，提供结果的国家的最先进的与人类关注点区域相关的关联方面更好）的改善。改进后的注意地图导致对于视觉答疑各种方法持续改进。因此，提出的技术可以被看作是获得改善的确定性的估计和解释深学习模型的配方。我们提供详细的实证分析，在所有标准基准测试，并与现有技术方法相比，状态可视化问答任务。</font>
</div>


<hr>
<div id="paper30"> <b>30. Rhythm, Chord and Melody Generation for Lead Sheets using Recurrent  Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.10266" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=De+Boom%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cedric De Boom</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Laere%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephanie Van Laere</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Verbelen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tim Verbelen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dhoedt%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bart Dhoedt</a><br>
<font size="3">
Abstract: Music that is generated by recurrent neural networks often lacks a sense of direction and coherence. We therefore propose a two-stage LSTM-based model for lead sheet generation, in which the harmonic and rhythmic templates of the song are produced first, after which, in a second stage, a sequence of melody notes is generated conditioned on these templates. A subjective listening test shows that our approach outperforms the baselines and increases perceived musical coherence. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由递归神经网络产生的音乐往往缺乏方向和连贯性的感觉。因此，我们提出一种用于铅片产生一个两阶段的基于LSTM模型，其中，所述歌曲的和声和节奏模板首先产生，在这之后，在第二阶段中，的旋律音符序列来生成调节这些模板。主观听音测试表明，我们的方法比基线和增加感知音乐的连贯性。</font>
</div>


<hr>
<div id="paper31"> <b>31. Leveraging Code Generation to Improve Code Retrieval and Summarization  via Dual Learning</b>  <a href="https://arxiv.org/pdf/2002.10198" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Ye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinglei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianxiang Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shikun Zhang</a><br>
<font size="3">
Abstract: Code summarization generates brief natural language description given a source code snippet, while code retrieval fetches relevant source code given a natural language query. Since both tasks aim to model the association between natural language and program-ming language, recent studies have combined these two tasks to improve their performance. However, researchers have yet been able to effectively leverage the intrinsic connection between the two tasks as they train these tasks in a separate or pipeline manner, which means their performance can not be well balanced. In this paper, we propose a novel end-to-end model for the two tasks by introducing an additional code generation task. More specifically, we explicitly exploit the probabilistic correlation between code summarization and code generation with dual learning, and utilize the two encoders for code summarization and code generation to train the code retrieval task via multi-task learning. We have carried out extensive experiments on an existing dataset of SQL andPython, and results show that our model can significantly improve the results of the code retrieval task over the-state-of-art models, as well as achieve competitive performance in terms of BLEU score for the code summarization task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：代码生成汇总给源代码片段简要自然语言描述，而代码检索获取给定的自然语言查询相关的源代码。既然两个任务的目标是自然语言和程序铭语言之间的关系进行建模，最近的研究结合这两个任务，以提高其性能。然而，研究人员尚未能有效地利用这两个任务之间的内在联系，因为他们在一个单独或管道的方式，这意味着它们的性能不能得到很好的平衡训练这些任务。在本文中，我们通过引入额外的代码生成任务提出了一种新的终端到终端型号为两个任务。更具体地说，我们明确地利用了双码学习总结和代码生成之间的概率相关性，并利用两个编码器的代码总结和代码生成通过多任务学习训练码检索任务。我们已经进行了大量的实验对SQL andPython的现有数据集，结果表明，该模型可以显著提高代码检索任务在国家的最先进的模型的结果，以及实现BLEU方面竞争力的性能分数代码汇总任务。</font>
</div>


<hr>
<div id="paper32"> <b>32. FONDUE: A Framework for Node Disambiguation Using Network Embeddings</b>  <a href="https://arxiv.org/pdf/2002.10127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mel%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmad Mel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Kang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lijffijt%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jefrey Lijffijt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=De+Bie%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tijl De Bie</a><br>
<font size="3">
Abstract: Real-world data often presents itself in the form of a network. Examples include social networks, citation networks, biological networks, and knowledge graphs. In their simplest form, networks represent real-life entities (e.g. people, papers, proteins, concepts) as nodes, and describe them in terms of their relations with other entities by means of edges between these nodes. This can be valuable for a range of purposes from the study of information diffusion to bibliographic analysis, bioinformatics research, and question-answering. The quality of networks is often problematic though, affecting downstream tasks. This paper focuses on the common problem where a node in the network in fact corresponds to multiple real-life entities. In particular, we introduce FONDUE, an algorithm based on network embedding for node disambiguation. Given a network, FONDUE identifies nodes that correspond to multiple entities, for subsequent splitting. Extensive experiments on twelve benchmark datasets demonstrate that FONDUE is substantially and uniformly more accurate for ambiguous node identification compared to the existing state-of-the-art, at a comparable computational cost, while less optimal for determining the best way to split ambiguous nodes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：真实世界的数据通常表示其本身在网络的形式。例子包括社交网络，引文网络，生物网络和知识图。在最简单的形式，网络代表现实生活中的实体（如人，论文，蛋白质，概念）为节点，通过这些节点之间的边的方式描述他们在与其他机构的关系方面。这可以为各种目的，从信息传播的书目分析，生物信息学研究，问题回答的研究价值。网络的质量往往是有问题的，虽然，影响下游任务。本文重点介绍了常见的问题，即实际上对应多个真实的实体网络中的节点。特别是，我们引进火锅，基于网络的嵌入节点消歧的算法。给定的网络中，FONDUE识别节点对应于多个实体，用于随后的分裂。十二个基准数据集大量的实验证明，FONDUE基本上且均匀地为相比于现有状态的最先进的暧昧节点识别更精确，在相当的计算成本，而用于确定的最佳方式以下最佳分裂暧昧节点。</font>
</div>


<hr>
<div id="paper33"> <b>33. Emosaic: Visualizing Affective Content of Text at Varying Granularity</b>  <a href="https://arxiv.org/pdf/2002.10096" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Geuder%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philipp Geuder</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leidinger%2C+M+C" target="_blank" rel="noopener" style="color:#0000EE;">Marie Claire Leidinger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Lupin%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin von Lupin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=D%C3%B6rk%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marian Dörk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schr%C3%B6der%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tobias Schröder</a><br>
<font size="3">
Abstract: This paper presents Emosaic, a tool for visualizing the emotional tone of text documents, considering multiple dimensions of emotion and varying levels of semantic granularity. Emosaic is grounded in psychological research on the relationship between language, affect, and color perception. We capitalize on an established three-dimensional model of human emotion: valence (good, nice vs. bad, awful), arousal (calm, passive vs. exciting, active) and dominance (weak, controlled vs. strong, in control). Previously, multi-dimensional models of emotion have been used rarely in visualizations of textual data, due to the perceptual challenges involved. Furthermore, until recently most text visualizations remained at a high level, precluding closer engagement with the deep semantic content of the text. Informed by empirical studies, we introduce a color mapping that translates any point in three-dimensional affective space into a unique color. Emosaic uses affective dictionaries of words annotated with the three emotional parameters of the valence-arousal-dominance model to extract emotional meanings from texts and then assigns to them corresponding color parameters of the hue-saturation-brightness color space. This approach of mapping emotion to color is aimed at helping readers to more easily grasp the emotional tone of the text. Several features of Emosaic allow readers to interactively explore the affective content of the text in more detail; e.g., in aggregated form as histograms, in sequential form following the order of text, and in detail embedded into the text display itself. Interaction techniques have been included to allow for filtering and navigating of text and visualizations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍Emosaic，用于可视化情绪状态的文本文档，考虑到情绪的多维度和不同的语义粒度级别的工具。 Emosaic在心理学研究接地的语言，情感，和颜色的感知之间的关系。我们利用人类情感的建立三维模型：价（好，好和差的，可怕的），觉醒（平静，被动与精彩的活动）和显性（弱，控制与强，中控）。此前，情感的多维模型已经很少使用的文本数据的可视化，由于涉及到的感性挑战。此外，直到最近，大多数文本可视化仍处于较高水平，与文本的深层语义内容排除更密切地参与。通过实证研究知情，我们介绍翻译在三维空间情感的任何一点到一个独特的颜色的颜色映射。 Emosaic使用与价觉醒-显性模型的三个参数的情绪注释来提取文本的情感的含义，然后受让人将它们对应的色相 - 饱和度 - 亮度颜色空间的颜色参数字的情感词典。映射情感色彩这种方法旨在帮助读者更容易把握感情基调的文字。 Emosaic的几个特点让读者以交互方式更详细地探讨文本的情感内容;例如，在下面的文本的顺序聚集形式作为直方图，以顺序的形式，并详细嵌入到文本显示器本身。互动技术已被列入允许用于过滤和文本和可视化的导航。</font>
</div>


<hr>
<div id="paper34"> <b>34. Deep Multimodal Image-Text Embeddings for Automatic Cross-Media  Retrieval</b>  <a href="https://arxiv.org/pdf/2002.10016" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Khojasteh%2C+H+A" target="_blank" rel="noopener" style="color:#0000EE;">Hadi Abdi Khojasteh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ansari%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ebrahim Ansari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Razzaghi%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Parvin Razzaghi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Karimi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Akbar Karimi</a><br>
<font size="3">
Abstract: This paper considers the task of matching images and sentences by learning a visual-textual embedding space for cross-modal retrieval. Finding such a space is a challenging task since the features and representations of text and image are not comparable. In this work, we introduce an end-to-end deep multimodal convolutional-recurrent network for learning both vision and language representations simultaneously to infer image-text similarity. The model learns which pairs are a match (positive) and which ones are a mismatch (negative) using a hinge-based triplet ranking. To learn about the joint representations, we leverage our newly extracted collection of tweets from Twitter. The main characteristic of our dataset is that the images and tweets are not standardized the same as the benchmarks. Furthermore, there can be a higher semantic correlation between the pictures and tweets contrary to benchmarks in which the descriptions are well-organized. Experimental results on MS-COCO benchmark dataset show that our model outperforms certain methods presented previously and has competitive performance compared to the state-of-the-art. The code and dataset have been made available publicly. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文通过学习跨模态获取一个视觉文本嵌入空间考虑匹配的图像和句子的任务。找到这样的空间是一个具有挑战性的任务，因为功能和文本和图像的表示是不可比的。在这项工作中，我们介绍一个终端到终端的多深卷积经常性网络同时学习两种视觉和语言表述来推断图像文本相似性。该模型获知哪个对是一个匹配（正）和哪些是使用基于铰链 - 三重态排名的失配（负）。要了解联合表示，我们充分利用我们的新提取的来自Twitter的鸣叫的集合。我们的数据集的主要特点是图像和鸣叫不规范一样的基准。此外，还可以是图片和鸣叫违背基准之间的更高语义相关，其中描述是井井有条。在MS-COCO基准数据集显示，我们的模型优于某些方法上提交并相比于国家的最先进的竞争力的性能实验结果。代码和数据集已经可以公开获得的。</font>
</div>


<hr>
<div id="paper35"> <b>35. Automata for Hyperlanguages</b>  <a href="https://arxiv.org/pdf/2002.09877" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bonakdarpour%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Borzoo Bonakdarpour</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sheinvald%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sarai Sheinvald</a><br>
<font size="3">
Abstract: Hyperproperties lift conventional trace properties from a set of execution traces to a set of sets of execution traces. Hyperproperties have been shown to be a powerful formalism for expressing and reasoning about information-flow security policies and important properties of cyber-physical systems such as sensitivity and robustness, as well as consistency conditions in distributed computing such as linearizability. Although there is an extensive body of work on automata-based representation of trace properties, we currently lack such characterization for hyperproperties. We introduce hyperautomata for em hyperlanguages, which are languages over sets of words. Essentially, hyperautomata allow running multiple quantified words over an automaton. We propose a specific type of hyperautomata called nondeterministic finite hyperautomata (NFH), which accept regular hyperlanguages. We demonstrate the ability of regular hyperlanguages to express hyperproperties for finite traces. We then explore the fundamental properties of NFH and show their closure under the Boolean operations. We show that while nonemptiness is undecidable in general, it is decidable for several fragments of NFH. We further show the decidability of the membership problem for finite sets and regular languages for NFH, as well as the containment problem for several fragments of NFH. Finally, we introduce learning algorithms based on Angluin's L-star algorithm for the fragments NFH in which the quantification is either strictly universal or strictly existential. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：Hyperproperties解除从一组执行跟踪的一组集的执行跟踪的常规跟踪属性。 Hyperproperties已被证明是用于表达和在分布式计算如线性化推理信息流的安全策略和网络的物理系统，如灵敏度和鲁棒性的重要性能，以及一致性条件的有力形式主义。尽管有广泛的跟踪属性的基于自动机的表示工作的机构，我们目前缺乏这种hyperproperties表征。我们引进hyperautomata为EM hyperlanguages，这是在套语的语言。从本质上讲，hyperautomata允许运行在一台自动机的多个量化的话。我们提出了所谓的非确定性有限hyperautomata（NFH）hyperautomata的特殊类型，它接受定期hyperlanguages。我们证明定期hyperlanguages的表达对有限的痕迹hyperproperties的能力。然后，我们探索NFH的基本性质并根据布尔运算展示自己关闭。我们表明，尽管非空性是一般不可判定的，它是可判定为五丰行的几个片段。我们进一步显示了有限集和NFH正规语言的成员问题的可判定性，以及遏制问题的NFH的几个片段。最后，介绍了基于Angluin的L-星算法的片段NFH其中定量或者是严格通用或严格存在学习算法。</font>
</div>


<hr>
<div id="paper36"> <b>36. Sketching Transformed Matrices with Applications to Natural Language  Processing</b>  <a href="https://arxiv.org/pdf/2002.09812" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingyu Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhao Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengdi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+L+F" target="_blank" rel="noopener" style="color:#0000EE;">Lin F. Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Yang</a><br>
<font size="3">
Abstract: Suppose we are given a large matrix $A=(a_{i,j})$ that cannot be stored in memory but is in a disk or is presented in a data stream. However, we need to compute a matrix decomposition of the entry-wisely transformed matrix, $f(A):=(f(a_{i,j}))$ for some function $f$. Is it possible to do it in a space efficient way? Many machine learning applications indeed need to deal with such large transformed matrices, for example word embedding method in NLP needs to work with the pointwise mutual information (PMI) matrix, while the entrywise transformation makes it difficult to apply known linear algebraic tools. Existing approaches for this problem either need to store the whole matrix and perform the entry-wise transformation afterwards, which is space consuming or infeasible, or need to redesign the learning method, which is application specific and requires substantial remodeling. In this paper, we first propose a space-efficient sketching algorithm for computing the product of a given small matrix with the transformed matrix. It works for a general family of transformations with provable small error bounds and thus can be used as a primitive in downstream learning tasks. We then apply this primitive to a concrete application: low-rank approximation. We show that our approach obtains small error and is efficient in both space and time. We complement our theoretical results with experiments on synthetic and real data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：假设我们给出一个大的矩阵$ A =（A_ {I，J}）$不能被存储在存储器中，但是在一个磁盘或在数据流中被呈现。然而，我们需要计算入门明智地变换矩阵的矩阵分解，$ F（A）：=（F（A_ {I，J}））$对于某些功能$ F $。是否有可能做一个空间有效的方式？很多机器学习的应用程序确实需要用这么大的转变矩阵，例如字嵌入方法在NLP需要与逐点互信息（PMI）矩阵工作，而entrywise改造使其难以适用已知的线性代数工具。现有的方法对这个问题要么需要存储整个矩阵，之后执行入门明智的转型，这是占用空间或不可行，或者需要重新设计学习方法，这是特定于应用需要大量的重塑。在本文中，我们首先提出了用转换矩阵计算给定的小矩阵的产品节省空间的素描算法。它的工作原理与可证实的小误差界变革的一般家庭，因此可以用作下游学习任务的原始。接着，我们采用这种原始的具体应用：低等级近似。我们证明了我们的方法获得误差小，是在空间和时间效率。我们对合成和真实数据的实验补充了我们的理论结果。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-21</title>
    <url>/2020/02/21/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-21/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Spatiotemporal Relationship Reasoning for Pedestrian Intent Prediction <a href="https://arxiv.org/pdf/2002.08945" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Strategy to Increase the Safety of a DNN-based Perception for HAD  Systems <a href="https://arxiv.org/pdf/2002.08935" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Deep Learning-Based Feature Extraction in Iris Recognition: Use Existing  Models, Fine-tune or Train From Scratch? <a href="https://arxiv.org/pdf/2002.08916" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Automatic Shortcut Removal for Self-Supervised Representation Learning <a href="https://arxiv.org/pdf/2002.08822" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Object 6D Pose Estimation with Non-local Attention <a href="https://arxiv.org/pdf/2002.08749" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Roto-Translation Equivariant Convolutional Networks: Application to  Histopathology Image Analysis <a href="https://arxiv.org/pdf/2002.08725" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> A survey on Semi-, Self- and Unsupervised Techniques in Image  Classification <a href="https://arxiv.org/pdf/2002.08721" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Photorealistic Lip Sync with Adversarial Temporal Convolutional Networks <a href="https://arxiv.org/pdf/2002.08700" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Bi-directional Dermoscopic Feature Learning and Multi-scale Consistent  Decision Fusion for Skin Lesion Segmentation <a href="https://arxiv.org/pdf/2002.08694" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Neural Network Compression Framework for fast model inference <a href="https://arxiv.org/pdf/2002.08679" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Stroke Constrained Attention Network for Online Handwritten Mathematical  Expression Recognition <a href="https://arxiv.org/pdf/2002.08670" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Focus on Semantic Consistency for Cross-domain Crowd Understanding <a href="https://arxiv.org/pdf/2002.08623" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> KaoKore: A Pre-modern Japanese Art Facial Expression Dataset <a href="https://arxiv.org/pdf/2002.08595" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Captioning Images Taken by People Who Are Blind <a href="https://arxiv.org/pdf/2002.08565" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Learning Object Scale With Click Supervision for Object Detection <a href="https://arxiv.org/pdf/2002.08555" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Fast and Regularized Reconstruction of Building Façades from  Street-View Images using Binary Integer Programming <a href="https://arxiv.org/pdf/2002.08549" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Do We Really Need to Access the Source Data? Source Hypothesis Transfer  for Unsupervised Domain Adaptation <a href="https://arxiv.org/pdf/2002.08546" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Expressing Objects just like Words: Recurrent Visual Embedding for  Image-Text Matching <a href="https://arxiv.org/pdf/2002.08510" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Modelling response to trypophobia trigger using intermediate layers of  ImageNet networks <a href="https://arxiv.org/pdf/2002.08490" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Revisiting Training Strategies and Generalization Performance in Deep  Metric Learning <a href="https://arxiv.org/pdf/2002.08473" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> SD-GAN: Structural and Denoising GAN reveals facial parts under  occlusion <a href="https://arxiv.org/pdf/2002.08448" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Cooperative LIDAR Object Detection via Feature Sharing in Deep Networks <a href="https://arxiv.org/pdf/2002.08440" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Residual-Sparse Fuzzy $C$-Means Clustering Incorporating Morphological  Reconstruction and Wavelet frames <a href="https://arxiv.org/pdf/2002.08418" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Table-Top Scene Analysis Using Knowledge-Supervised MCMC <a href="https://arxiv.org/pdf/2002.08417" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> A Generalizable Knowledge Framework for Semantic Indoor Mapping Based on  Markov Logic Networks and Data Driven MCMC <a href="https://arxiv.org/pdf/2002.08402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset <a href="https://arxiv.org/pdf/2002.08397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> MonoLayout: Amodal scene layout from a single image <a href="https://arxiv.org/pdf/2002.08394" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> SynFi: Automatic Synthetic Fingerprint Generation <a href="https://arxiv.org/pdf/2002.08900" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> A Bayes-Optimal View on Adversarial Examples <a href="https://arxiv.org/pdf/2002.08859" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Deep Learning Estimation of Multi-Tissue Constrained Spherical  Deconvolution with Limited Single Shell DW-MRI <a href="https://arxiv.org/pdf/2002.08820" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Pruning untrained neural networks: Principles and Analysis <a href="https://arxiv.org/pdf/2002.08797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Disentangled Speech Embeddings using Cross-modal Self-supervision <a href="https://arxiv.org/pdf/2002.08742" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Bimodal Distribution Removal and Genetic Algorithm in Neural Network for  Breast Cancer Diagnosis <a href="https://arxiv.org/pdf/2002.08729" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> An empirical study of Conv-TasNet <a href="https://arxiv.org/pdf/2002.08688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and  Practice <a href="https://arxiv.org/pdf/2002.08681" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Unsupervised Domain Adaptation via Discriminative Manifold Embedding and  Alignment <a href="https://arxiv.org/pdf/2002.08675" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> A Novel Framework for Selection of GANs for an Application <a href="https://arxiv.org/pdf/2002.08641" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Boosting Adversarial Training with Hypersphere Embedding <a href="https://arxiv.org/pdf/2002.08619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Cross-stained Segmentation from Renal Biopsy Images Using Multi-level  Adversarial Learning <a href="https://arxiv.org/pdf/2002.08587" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Deep Fusion of Local and Non-Local Features for Precision Landslide  Recognition <a href="https://arxiv.org/pdf/2002.08547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> AdvMS: A Multi-source Multi-cost Defense Against Adversarial Attacks <a href="https://arxiv.org/pdf/2002.08439" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Fine tuning U-Net for ultrasound image segmentation: which layers? <a href="https://arxiv.org/pdf/2002.08438" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Interactive Natural Language-based Person Search <a href="https://arxiv.org/pdf/2002.08434" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> T-Net: A Template-Supervised Network for Task-specific Feature  Extraction in Biomedical Image Analysis <a href="https://arxiv.org/pdf/2002.08406" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Algorithm-hardware Co-design for Deformable Convolution <a href="https://arxiv.org/pdf/2002.08357" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Spatiotemporal Relationship Reasoning for Pedestrian Intent Prediction</b>  <a href="https://arxiv.org/pdf/2002.08945" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bingbin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Adeli%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ehsan Adeli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhangjie Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kuan-Hui Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shenoi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhijeet Shenoi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gaidon%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrien Gaidon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niebles%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">Juan Carlos Niebles</a><br>
<font size="3">
Abstract: Reasoning over visual data is a desirable capability for robotics and vision-based applications. Such reasoning enables forecasting of the next events or actions in videos. In recent years, various models have been developed based on convolution operations for prediction or forecasting, but they lack the ability to reason over spatiotemporal data and infer the relationships of different objects in the scene. In this paper, we present a framework based on graph convolution to uncover the spatiotemporal relationships in the scene for reasoning about pedestrian intent. A scene graph is built on top of segmented object instances within and across video frames. Pedestrian intent, defined as the future action of crossing or not-crossing the street, is a very crucial piece of information for autonomous vehicles to navigate safely and more smoothly. We approach the problem of intent prediction from two different perspectives and anticipate the intention-to-cross within both pedestrian-centric and location-centric scenarios. In addition, we introduce a new dataset designed specifically for autonomous-driving scenarios in areas with dense pedestrian populations: the Stanford-TRI Intent Prediction (STIP) dataset. Our experiments on STIP and another benchmark dataset show that our graph modeling framework is able to predict the intention-to-cross of the pedestrians with an accuracy of 79.10% on STIP and 79.28% on \rev{Joint Attention for Autonomous Driving (JAAD) dataset up to one second earlier than when the actual crossing happens. These results outperform the baseline and previous work. Please refer to this http URL for the dataset and code. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在推理可视化数据是机器人和基于视觉的应用的期望能力。这样的推理能够在视频的未来事件或行为的预测。近年来，各种型号都基于对预测或预报卷积运算得到了发展，但他们缺乏对时空数据，并推断场景中的不同对象之间的关系的能力的原因。在本文中，我们提出了基于图形卷积揭露在现场为约行人意图推理的时空关系的框架。的场景图是建立在内部和跨视频帧分割的对象实例的顶部。行人意图，定义为交叉或不渡街道的未来行动，是信息的自动驾驶汽车能够安全，更顺利浏览一个非常关键的部分。我们已接近于从两个不同的角度意向预测的问题，并预测两行人为中心和以位置为中心的场景内的意向交叉。此外，我们引入了一个新的数据集与行人密集人口的地区自主驾驶的情况专门设计：斯坦福-TRI意向预测（STIP）数据集。我们对科技革新和另一基准数据集显示，我们的图形建模框架能够预测的\转{共同注意的自动驾驶意向交与对科技革新79.10％和79.28％的精度行人的实验（JAAD）数据集最大一秒钟时比实际发生交叉更早。这些结果跑赢基准和以前的工作。请参阅本HTTP URL的数据集和代码。</font>
</div>


<hr>
<div id="paper2"> <b>2. Strategy to Increase the Safety of a DNN-based Perception for HAD  Systems</b>  <a href="https://arxiv.org/pdf/2002.08935" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=S%C3%A4mann%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timo Sämann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schlicht%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Schlicht</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=H%C3%BCger%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fabian Hüger</a><br>
<font size="3">
Abstract: Safety is one of the most important development goals for highly automated driving (HAD) systems. This applies in particular to the perception function driven by Deep Neural Networks (DNNs). For these, large parts of the traditional safety processes and requirements are not fully applicable or sufficient. The aim of this paper is to present a framework for the description and mitigation of DNN insufficiencies and the derivation of relevant safety mechanisms to increase the safety of DNNs. To assess the effectiveness of these safety mechanisms, we present a categorization scheme for evaluation metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：安全是高度自动驾驶（HAD）系统中最重要的发展目标之一。这尤其适用于通过深层神经网络（DNNs）驱动的感知功能。对于这些，传统的安全流程和要求的大部分地区都没有完全适用或足够了。本文的目的是提出一个框架DNN不足的描述和缓解和相关的安全机制的推导来增加DNNs的安全性。为了评估这些安全机制的有效性，我们提出了评价指标，一个分类方案。</font>
</div>


<hr>
<div id="paper3"> <b>3. Deep Learning-Based Feature Extraction in Iris Recognition: Use Existing  Models, Fine-tune or Train From Scratch?</b>  <a href="https://arxiv.org/pdf/2002.08916" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Boyd%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aidan Boyd</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Czajka%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adam Czajka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bowyer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Bowyer</a><br>
<font size="3">
Abstract: Modern deep learning techniques can be employed to generate effective feature extractors for the task of iris recognition. The question arises: should we train such structures from scratch on a relatively large iris image dataset, or it is better to fine-tune the existing models to adapt them to a new domain? In this work we explore five different sets of weights for the popular ResNet-50 architecture to find out whether iris-specific feature extractors perform better than models trained for non-iris tasks. Features are extracted from each convolutional layer and the classification accuracy achieved by a Support Vector Machine is measured on a dataset that is disjoint from the samples used in training of the ResNet-50 model. We show that the optimal training strategy is to fine-tune an off-the-shelf set of weights to the iris recognition domain. This approach results in greater accuracy than both off-the-shelf weights and a model trained from scratch. The winning, fine-tuned approach also shows an increase in performance when compared to previous work, in which only off-the-shelf (not fine-tuned) models were used in iris feature extraction. We make the best-performing ResNet-50 model, fine-tuned with more than 360,000 iris images, publicly available along with this paper. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现代深学习技术可以用来生成虹膜识别的任务有效特征提取。问题在于：我们要培养从头这种结构上相对较大的虹膜图像数据集，或者最好是微调现有的模式，使其适应新的领域？在这项工作中，我们探讨五种不同的权重集流行的RESNET-50架构，找出具体的虹膜特征提取是否比训练有素的非光圈任务机型更好的表现。特征从各个卷积层萃取并通过支持向量机实现的分类精度上的数据集是从在RESNET-50模型的训练中使用的样本不相交的测量。我们表明，最佳的训练策略是微调关闭的，现成的设置权重来虹膜识别领域的。这种做法的结果比关闭的，现成的权重和从头开始训练的模型都更高的精度。相比以前的工作，其中只有现成的现货（未微调）模型中使用的虹膜特征提取时获胜，微调的做法也显示性能的提高。我们做效果最好的RESNET-50模型，微调有超过36万点的虹膜图像，与本文一起公开。</font>
</div>


<hr>
<div id="paper4"> <b>4. Automatic Shortcut Removal for Self-Supervised Representation Learning</b>  <a href="https://arxiv.org/pdf/2002.08822" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Minderer%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Minderer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bachem%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Olivier Bachem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Houlsby%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Neil Houlsby</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tschannen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Tschannen</a><br>
<font size="3">
Abstract: In self-supervised visual representation learning, a feature extractor is trained on a "pretext task" for which labels can be generated cheaply. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such "shortcut" features and hand-designing schemes to reduce their effect. Here, we propose a general framework for removing shortcut features automatically. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a "lens" network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在自我监督的可视化表示学习，特征提取器对其中标签可以廉价地产生“借口任务”训练。这种方法的一个主要挑战是，特征提取很快学会了利用低层次的视觉特征，如色差或水印，然后将无法学到有用的语义表示。很多工作已经进入识别这样的“快捷方式”功能和手工设计方案，降低了它们的影响。在这里，我们提出了删除快捷方式的总体框架自动功能。我们的主要假设是，那些第一功能，以解决为借口，任务也可能是最脆弱的训练费力气的对手加以利用。我们表明，这种假设，通过训练“镜头”的网络，使小图像的变化是最大程度减少借口任务绩效跨越常见借口任务和数据集成立。与修正后的图像了解到交涉优于那些没有在所有测试的情况下的经验教训。此外，通过镜头所做的修改显示借口任务和数据集的选择如何影响自我监督学习的特点。</font>
</div>


<hr>
<div id="paper5"> <b>5. Object 6D Pose Estimation with Non-local Attention</b>  <a href="https://arxiv.org/pdf/2002.08749" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mei%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianhan Mei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Henghui Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xudong Jiang</a><br>
<font size="3">
Abstract: In this paper, we address the challenging task of estimating 6D object pose from a single RGB image. Motivated by the deep learning based object detection methods, we propose a concise and efficient network that integrate 6D object pose parameter estimation into the object detection framework. Furthermore, for more robust estimation to occlusion, a non-local self-attention module is introduced. The experimental results show that the proposed method reaches the state-of-the-art performance on the YCB-video and the Linemod datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们来从一个RGB图像估计6D对象姿势的具有挑战性的任务。由深基于学习对象的检测方法的启发，我们提出了一个简洁高效的网络6D对象姿态参数估计融入对象检测框架。此外，对于更稳健估计遮挡，非本地自注意模块介绍。实验结果表明，所提出的方法达到的YCB视频和Linemod数据集的状态的最先进的性能。</font>
</div>


<hr>
<div id="paper6"> <b>6. Roto-Translation Equivariant Convolutional Networks: Application to  Histopathology Image Analysis</b>  <a href="https://arxiv.org/pdf/2002.08725" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lafarge%2C+M+W" target="_blank" rel="noopener" style="color:#0000EE;">Maxime W. Lafarge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bekkers%2C+E+J" target="_blank" rel="noopener" style="color:#0000EE;">Erik J. Bekkers</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pluim%2C+J+P+W" target="_blank" rel="noopener" style="color:#0000EE;">Josien P.W. Pluim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duits%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Remco Duits</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Veta%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mitko Veta</a><br>
<font size="3">
Abstract: Rotation-invariance is a desired property of machine-learning models for medical image analysis and in particular for computational pathology applications. We propose a framework to encode the geometric structure of the special Euclidean motion group SE(2) in convolutional networks to yield translation and rotation equivariance via the introduction of SE(2)-group convolution layers. This structure enables models to learn feature representations with a discretized orientation dimension that guarantees that their outputs are invariant under a discrete set of rotations. Conventional approaches for rotation invariance rely mostly on data augmentation, but this does not guarantee the robustness of the output when the input is rotated. At that, trained conventional CNNs may require test-time rotation augmentation to reach their full capability. This study is focused on histopathology image analysis applications for which it is desirable that the arbitrary global orientation information of the imaged tissues is not captured by the machine learning models. The proposed framework is evaluated on three different histopathology image analysis tasks (mitosis detection, nuclei segmentation and tumor classification). We present a comparative analysis for each problem and show that consistent increase of performances can be achieved when using the proposed framework. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：旋转不变性是机器学习模型，医学图像分析为计算病理学应用所需的性能，尤其是。我们提出了一个框架通过引入SE（2） - 基团卷积层的编码中卷积网络产量平移和旋转同变性特殊欧几里德运动组SE（2）的几何结构。这种结构使模型的学习功能交涉，离散化取向层面，保证它们的输出正在一组离散旋转不变。传统的方法对旋转不变性主要依靠数据增强，但是当输入旋转，这并不保证输出的稳定性。当时，受过训练的传统细胞神经网络可能需要测试时间旋转增强，以充分发挥其能力。这项研究主要集中于它希望的是，成像组织的全球任意方位信息不是由机器学习模型拍摄的组织病理学图像分析应用。所提出的框架在三个不同的组织病理学图像分析任务（有丝分裂检测，细胞核分割和肿瘤分类）来评价。我们提出了比较分析每个问题，并显示使用拟议的框架时，就可以实现的性能是一致的增加。</font>
</div>


<hr>
<div id="paper7"> <b>7. A survey on Semi-, Self- and Unsupervised Techniques in Image  Classification</b>  <a href="https://arxiv.org/pdf/2002.08721" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schmarje%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lars Schmarje</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Santarossa%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Monty Santarossa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schr%C3%B6der%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon-Martin Schröder</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koch%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Reinhard Koch</a><br>
<font size="3">
Abstract: While deep learning strategies achieve outstanding results in computer vision tasks, one issue remains. The current strategies rely heavily on a huge amount of labeled data. In many real-world problems it is not feasible to create such an amount of labeled training data. Therefore, researchers try to incorporate unlabeled data into the training process to reach equal results with fewer labels. Due to a lot of concurrent research, it is difficult to keep track of recent developments. In this survey we provide an overview of often used techniques and methods in image classification with fewer labels. We compare 21 methods. In our analysis we identify three major trends. 1. State-of-the-art methods are scaleable to real world applications based on their accuracy. 2. The degree of supervision which is needed to achieve comparable results to the usage of all labels is decreasing. 3. All methods share common techniques while only few methods combine these techniques to achieve better performance. Based on all of these three trends we discover future research opportunities. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然深学习策略，实现在计算机视觉任务，但仍有一个问题优异成绩。目前的策略在很大程度上依赖于一个巨大的标记数据量。在许多现实世界的问题，这是不可行的创建标记的训练数据的量。因此，研究人员尝试将未标记的数据纳入训练过程以更少的标签，以达到相同的效果。由于大量的并发研究的，它是很难保持的最新发展轨道。在本次调查中，我们提供了经常使用的技术和较少的标签图像分类方法的概述。我们比较21点的方法。在我们的分析，我们确定三大发展趋势。 1，国家的最先进的方法是可扩展的，基于其准确性现实世界的应用。 2.这是需要达到比较的结果，所有标签的使用监督的程度正在下降。 3.所有的方法都有着共同的技术，而只有少数方法结合这些技术，以实现更好的性能。基于所有这些三个趋势，我们发现未来的研究机会。</font>
</div>


<hr>
<div id="paper8"> <b>8. Photorealistic Lip Sync with Adversarial Temporal Convolutional Networks</b>  <a href="https://arxiv.org/pdf/2002.08700" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruobing Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhou Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changjiang Ji</a><br>
<font size="3">
Abstract: Lip sync has emerged as a promising technique to generate mouth movements on a talking head. However, synthesizing a clear, accurate and human-like performance is still challenging. In this paper, we present a novel lip-sync solution for producing a high-quality and photorealistic talking head from speech. We focus on capturing the specific lip movement and talking style of the target person. We model the seq-to-seq mapping from audio signals to mouth features by two adversarial temporal convolutional networks. Experiments show our model outperforms traditional RNN-based baselines in both accuracy and speed. We also propose an image-to-image translation-based approach for generating high-resolution photoreal face appearance from synthetic facial maps. This fully-trainable framework not only avoids the cumbersome steps like candidate-frame selection in graphics-based rendering methods but also solves some existing issues in recent neural network-based solutions. Our work will benefit related applications such as conversational agent, virtual anchor, tele-presence and gaming. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：唇型同步已成为一个有前途的技术来生成上说话的头部嘴巴的动作。然而，合成清晰，准确和人类一样的性能仍然是具有挑战性的。在本文中，我们提出了用于从语音的高品质和照片拟真头部特写的新型唇同步溶液。我们专注于捕捉特定的嘴唇运动和说话的目标人物的风格。我们由两个对立的时间卷积网络从音频信号序列到序列映射模型的嘴部特征。实验表明，在精度和速度我们的模型优于传统的基于RNN-基线。我们还提出了从生成合成面部贴图高分辨率的真实感面部外观图像 - 图像基于平移的方法。这完全可训练框架不仅避免了像在基于图形的绘制方法候选帧选择的繁琐步骤，也解决了最近基于神经网络的解决方案存在的一些问题。我们的工作将有利于相关的应用，如会话代理，虚拟锚，临场感和游戏。</font>
</div>


<hr>
<div id="paper9"> <b>9. Bi-directional Dermoscopic Feature Learning and Multi-scale Consistent  Decision Fusion for Skin Lesion Segmentation</b>  <a href="https://arxiv.org/pdf/2002.08694" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaohong Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xudong Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Henghui Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Liu</a><br>
<font size="3">
Abstract: Accurate segmentation of skin lesion from dermoscopic images is a crucial part of computer-aided diagnosis of melanoma. It is challenging due to the fact that dermoscopic images from different patients have non-negligible lesion variation, which causes difficulties in anatomical structure learning and consistent skin lesion delineation. In this paper, we propose a novel bi-directional dermoscopic feature learning (biDFL) framework to model the complex correlation between skin lesions and their informative context. By controlling feature information passing through two complementary directions, a substantially rich and discriminative feature representation is achieved. Specifically, we place biDFL module on the top of a CNN network to enhance high-level parsing performance. Furthermore, we propose a multi-scale consistent decision fusion (mCDF) that is capable of selectively focusing on the informative decisions generated from multiple classification layers. By analysis of the consistency of the decision at each position, mCDF automatically adjusts the reliability of decisions and thus allows a more insightful skin lesion delineation. The comprehensive experimental results show the effectiveness of the proposed method on skin lesion segmentation, achieving state-of-the-art performance consistently on two publicly available dermoscopic image databases. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从皮肤镜图像皮肤损伤的精确分割是黑素瘤的计算机辅助诊断的重要部分。它是具有挑战性由于来自不同患者的皮肤镜图像有不可忽略的病变变化，这导致在解剖结构的学习和一致的皮肤损伤划分的困难。在本文中，我们提出了一个新颖的双向皮肤镜功能学习（biDFL）框架，皮肤损伤和他们的信息语境之间的复杂关联性模型。通过控制通过两个互补的方向特征的信息，基本上丰富和判别特征表示得以实现。具体而言，我们将biDFL模块上的CNN网络，以加强高层解析性能的顶部。此外，我们提出了一个多尺度一致决策融合（MCDF），其能够选择性地集中在从多个分类层中产生的信息决定。通过在每个位置的决定的一致性的分析，MCDF自动调整的决定的可靠性，并因此允许更见地皮肤损伤的划分。综合实验结果表明，对皮肤损伤分割算法的有效性，在两个公开可用的皮肤镜图像数据库持续取得国家的最先进的性能。</font>
</div>


<hr>
<div id="paper10"> <b>10. Neural Network Compression Framework for fast model inference</b>  <a href="https://arxiv.org/pdf/2002.08679" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kozlov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Kozlov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lazarevich%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivan Lazarevich</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shamporov%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasily Shamporov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lyalyushkin%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikolay Lyalyushkin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gorbachev%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yury Gorbachev</a><br>
<font size="3">
Abstract: In this work we present a new framework for neural networks compression with fine-tuning, which we called Neural Network Compression Framework (NNCF). It leverages recent advances of various network compression methods and implements some of them, such as sparsity, quantization, and binarization. These methods allow getting more hardware-friendly models which can be efficiently run on general-purpose hardware computation units (CPU, GPU) or special Deep Learning accelerators. We show that the developed methods can be successfully applied to a wide range of models to accelerate the inference time while keeping the original accuracy. The framework can be used within the training samples, which are supplied with it, or as a standalone package that can be seamlessly integrated into the existing training code with minimal adaptations. Currently, a PyTorch \cite{PyTorch} version of NNCF is available as a part of OpenVINO Training Extensions at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们提出了神经网络与压缩微调，我们称之为神经网络压缩框架（NNCF）的新框架。它利用最近的各种网络的压缩方法和器具其中的一些，例如稀疏性，量化和二进制化的进步。这些方法允许获得可在通用的硬件运算单元（CPU，GPU）或特殊的深度学习加速器有效地运行更多硬件友好的模式。我们表明，该方法可以成功地应用于多种型号，加快推理时间，同时保持原有的精度。该框架可以在训练样本，这是与它提供，或者作为独立的软件包，可以无缝地集成到具有最小适应现有的训练码内使用。目前，PyTorch \ {引用} PyTorch NNCF的版本可以在此HTTPS URL OpenVINO培训扩展的一部分</font>
</div>


<hr>
<div id="paper11"> <b>11. Stroke Constrained Attention Network for Online Handwritten Mathematical  Expression Recognition</b>  <a href="https://arxiv.org/pdf/2002.08670" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaming Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianshu Zhang</a><br>
<font size="3">
Abstract: In this paper, we propose a novel stroke constrained attention network (SCAN) which treats stroke as the basic unit for encoder-decoder based online handwritten mathematical expression recognition (HMER). Unlike previous methods which use trace points or image pixels as basic units, SCAN makes full use of stroke-level information for better alignment and representation. The proposed SCAN can be adopted in both single-modal (online or offline) and multi-modal HMER. For single-modal HMER, SCAN first employs a CNN-GRU encoder to extract point-level features from input traces in online mode and employs a CNN encoder to extract pixel-level features from input images in offline mode, then use stroke constrained information to convert them into online and offline stroke-level features. Using stroke-level features can explicitly group points or pixels belonging to the same stroke, therefore reduces the difficulty of symbol segmentation and recognition via the decoder with attention mechanism. For multi-modal HMER, other than fusing multi-modal information in decoder, SCAN can also fuse multi-modal information in encoder by utilizing the stroke based alignments between online and offline modalities. The encoder fusion is a better way for combining multi-modal information as it implements the information interaction one step before the decoder fusion so that the advantages of multiple modalities can be exploited earlier and more adequately when training the encoder-decoder model. Evaluated on a benchmark published by CROHME competition, the proposed SCAN achieves the state-of-the-art performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出一种治疗中风为基本单位为编码器 - 解码器基于联机手写的数学式识别（HMER）一种新颖的行程约束注意网络（SCAN）。不同于以往的方法，它使用的跟踪点或图像的像素为基本单位，SCAN实现更好的定位和代表性，充分利用行程级信息。所提出的SCAN可以在单峰（在线或离线）和多模态HMER通过。对于单峰HMER，SCAN第一采用CNN-GRU编码器提取的点电平从在在线模式下输入迹线的特性和采用CNN编码器提取的像素级从在离线模式下输入图像中的特征，然后使用行程限制信息它们转换成在线和离线中风级功能。使用行程级功能可以明确组点或属于同一行程的像素，因此减少了经由与注意机制的解码器符号分割和识别的难度。对于多模态HMER，比熔合在解码器的多模态信息之外，还SCAN保险丝可以通过利用在线和离线模式之间的基于笔划的比对在编码器的多模态的信息。该编码器的融合是因为它实现了解码器融合前的信息交互一个步骤，使得多个模态的优点，可更早和更充分地训练编码器 - 解码器模型时利用相结合的多模态信息的更好的方法。评估由CROHME竞争公布的基准，建议SCAN实现国家的最先进的性能。</font>
</div>


<hr>
<div id="paper12"> <b>12. Focus on Semantic Consistency for Cross-domain Crowd Understanding</b>  <a href="https://arxiv.org/pdf/2002.08623" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junyu Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Wang</a><br>
<font size="3">
Abstract: For pixel-level crowd understanding, it is time-consuming and laborious in data collection and annotation. Some domain adaptation algorithms try to liberate it by training models with synthetic data, and the results in some recent works have proved the feasibility. However, we found that a mass of estimation errors in the background areas impede the performance of the existing methods. In this paper, we propose a domain adaptation method to eliminate it. According to the semantic consistency, a similar distribution in deep layer's features of the synthetic and real-world crowd area, we first introduce a semantic extractor to effectively distinguish crowd and background in high-level semantic information. Besides, to further enhance the adapted model, we adopt adversarial learning to align features in the semantic space. Experiments on three representative real datasets show that the proposed domain adaptation scheme achieves the state-of-the-art for cross-domain counting problems. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对于像素级人群的理解，这是耗时的数据收集和注释中费力。有些域自适应算法尝试通过与合成训练数据模型中解脱出来，并在最近的一些作品中的结果证明是可行的。然而，我们发现，在背景区域估计误差的质量阻碍了现有方法的性能。在本文中，我们提出了一个领域适应性的方法来消除它。根据语义一致性，在合成和真实世界的人群区域的深层的功能相似的分布，我们首先介绍一个语义提取到有效区分人群和背景中的高层语义信息。此外，为进一步提高适应模型，我们采取敌对学习对齐功能，在语义空间。对三种具有代表性的真实数据集的实验表明，该领域适应性方案实现了国家的最先进的跨域计数问题。</font>
</div>


<hr>
<div id="paper13"> <b>13. KaoKore: A Pre-modern Japanese Art Facial Expression Dataset</b>  <a href="https://arxiv.org/pdf/2002.08595" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingtao Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chikahiko Suzuki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clanuwat%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tarin Clanuwat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bober-Irizar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mikel Bober-Irizar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lamb%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alex Lamb</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kitamoto%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asanobu Kitamoto</a><br>
<font size="3">
Abstract: From classifying handwritten digits to generating strings of text, the datasets which have received long-time focus from the machine learning community vary greatly in their subject matter. This has motivated a renewed interest in building datasets which are socially and culturally relevant, so that algorithmic research may have a more direct and immediate impact on society. One such area is in history and the humanities, where better and relevant machine learning models can accelerate research across various fields. To this end, newly released benchmarks and models have been proposed for transcribing historical Japanese cursive writing, yet for the field as a whole using machine learning for historical Japanese artworks still remains largely uncharted. To bridge this gap, in this work we propose a new dataset KaoKore which consists of faces extracted from pre-modern Japanese artwork. We demonstrate its value as both a dataset for image classification as well as a creative and artistic dataset, which we explore using generative models. Dataset available at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从手写数字进行分类，以生成文本字符串，均获得长期焦点从机器学习领域的数据集在题材方面差异很大。这促使在建立数据集，其在社会和文化相关的新的兴趣，使算法的研究可能对社会产生更直接，更直接的影响。就是这样一个地区的历史和人文，更好的地方及相关机器学习模型可以加速在各个领域的研究。为此，新公布的基准和模型已被提出用于转录历史日本草书书写，但对于该领域的整体使用机器学习历史的日本艺术品仍然在很大程度上仍然未知。为了弥补这一差距，在这项工作中，我们提出了一个新的数据集KaoKore其中包括由前现代的日本艺术品提取的面孔。我们展示了其既是对图像分类数据集以及一个创造性和艺术性的数据集，这是我们探索利用生成模型值。数据集可以在这个HTTPS URL</font>
</div>


<hr>
<div id="paper14"> <b>14. Captioning Images Taken by People Who Are Blind</b>  <a href="https://arxiv.org/pdf/2002.08565" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gurari%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danna Gurari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinan Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhattacharya%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nilavra Bhattacharya</a><br>
<font size="3">
Abstract: While an important problem in the vision community is to design algorithms that can automatically caption images, few publicly-available datasets for algorithm development directly address the interests of real users. Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of over 39,000 images originating from people who are blind that are each paired with five captions. We analyze this dataset to (1) characterize the typical captions, (2) characterize the diversity of content found in the images, and (3) compare its content to that found in eight popular vision datasets. We also analyze modern image captioning algorithms to identify what makes this new dataset challenging for the vision community. We publicly-share the dataset with captioning challenge instructions at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然在视觉领域的一个重要问题是设计算法，可以自动字幕图像，算法开发一些公开可用的数据集，直接解决实际用户的利益。观察，人们谁是盲目的对（以人为本）图像字幕服务，以了解他们采取了近十年的图像依赖，我们引进了第一图像数据集字幕表示此实际使用情况。这个新的数据集，我们称之为VizWiz字幕功能，包括从人谁是盲目的，它们各自成对五个字幕39000图像发起的。我们分析这些数据集（1）表征典型的字幕，（2）描述的内容的多样性在图像中发现，和（3）比较其内容中所发现的八大热门视觉数据集。我们还分析了现代的形象字幕算法，以确定是什么使这个新的数据集为愿景社会挑战。与此HTTPS URL字幕挑战说明我们的公开，共享数据集</font>
</div>


<hr>
<div id="paper15"> <b>15. Learning Object Scale With Click Supervision for Object Detection</b>  <a href="https://arxiv.org/pdf/2002.08555" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanzi Wang</a><br>
<font size="3">
Abstract: Weakly-supervised object detection has recently attracted increasing attention since it only requires image-levelannotations. However, the performance obtained by existingmethods is still far from being satisfactory compared with fully-supervised object detection methods. To achieve a good trade-off between annotation cost and object detection performance,we propose a simple yet effective method which incorporatesCNN visualization with click supervision to generate the pseudoground-truths (i.e., bounding boxes). These pseudo ground-truthscan be used to train a fully-supervised detector. To estimatethe object scale, we firstly adopt a proposal selection algorithmto preserve high-quality proposals, and then generate ClassActivation Maps (CAMs) for these preserved proposals by theproposed CNN visualization algorithm called Spatial AttentionCAM. Finally, we fuse these CAMs together to generate pseudoground-truths and train a fully-supervised object detector withthese ground-truths. Experimental results on the PASCAL VOC2007 and VOC 2012 datasets show that the proposed methodcan obtain much higher accuracy for estimating the object scale,compared with the state-of-the-art image-level based methodsand the center-click based method </font>
<br>
<font size="2" style="line-height:30px;">
摘要：弱监督对象检测最近吸引了越来越多的关注，因为它仅需要图像levelannotations。然而，通过existingmethods所获得的性能是从具有完全监督对象检测方法相比，不能令人满意仍远。为了达到良好的权衡注释成本和物体检测性能之间，我们提出了一个简单而有效的方法，它incorporatesCNN可视化与点击监管产生pseudoground-真理（即，边界框）。这些伪接地truthscan用来训练完全监督检测器。为了estimatethe对象的规模，我们首先采取建议选择algorithmto保持高质量的提案，然后生成ClassActivation地图（凸轮）通过所谓的空间AttentionCAM theproposed CNN可视化算法，这些保留的建议。最后，我们这些融合在一起的CAM生成pseudoground，真理和培养标价的地面真理完全监督对象检测器。在PASCAL VOC2007和VOC 2012数据集的实验结果表明，所提出methodcan获得高得多的精确度来估计对象的规模，与基于所述状态的最先进的图像级methodsand的中心基于点击的方法相比</font>
</div>


<hr>
<div id="paper16"> <b>16. Fast and Regularized Reconstruction of Building Façades from  Street-View Images using Binary Integer Programming</b>  <a href="https://arxiv.org/pdf/2002.08549" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Libin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulin Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qing Zhu</a><br>
<font size="3">
Abstract: Regularized arrangement of primitives on building façades to aligned locations and consistent sizes is important towards structured reconstruction of urban environment. Mixed integer linear programing was used to solve the problem, however, it is extreamly time consuming even for state-of-the-art commercial solvers. Aiming to alleviate this issue, we cast the problem into binary integer programming, which omits the requirements for real value parameters and is more efficient to be solved . Firstly, the bounding boxes of the primitives are detected using the YOLOv3 architecture in real-time. Secondly, the coordinates of the upper left corners and the sizes of the bounding boxes are automatically clustered in a binary integer programming optimization, which jointly considers the geometric fitness, regularity and additional constraints; this step does not require \emph{a priori} knowledge, such as the number of clusters or pre-defined grammars. Finally, the regularized bounding boxes can be directly used to guide the façade reconstruction in an interactive envinronment. Experimental evaluations have revealed that the accuracies for the extraction of primitives are above 0.85, which is sufficient for the following 3D reconstruction. The proposed approach only takes about $ 10\% $ to $ 20\% $ of the runtime than previous approach and reduces the diversity of the bounding boxes to about $20\%$ to $50\%$ </font>
<br>
<font size="2" style="line-height:30px;">
摘要：建筑立面，以排列位置和大小一致元的正则安排是对城市环境的结构重建重要。使用线性规划混合整数来解决这个问题，但是，它成绩不理想时间即使对于国家的最先进的商业求解器耗时。旨在缓解这一问题，我们投的问题转化为二进制整数规划，其中省略了真正的价值参数要求，并更有效地得到解决。首先，使用YOLOv3架构中的实时检测到的图元的边界框。其次，左上角的坐标和所述边界框的尺寸在一个二进制整数规划优化，这共同考虑几何健身，规律性和附加约束自动群集;此步骤不需要\ EMPH {先验}知识，如簇或预先定义的语法的数量。最后，将正则边界框可直接用于指导外墙重建在交互式envinronment。实验评估揭示，对于基元的提取的精度是在0.85以上，这足以用于以下3D重建。所提出的方法只需要大约$ 10 \％$至$ 20 \％的运行时间比以前的方法$，降低了边框的多样性，约$ 20 \％$至$ 50 \％$</font>
</div>


<hr>
<div id="paper17"> <b>17. Do We Really Need to Access the Source Data? Source Hypothesis Transfer  for Unsupervised Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2002.08546" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dapeng Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiashi Feng</a><br>
<font size="3">
Abstract: Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. In this work we tackle a novel setting where only a trained source model is available and investigate how we can effectively utilize such a model without source data to solve UDA problems. To this end, we propose a simple yet generic representation learning framework, named \emph{Source HypOthesis Transfer} (SHOT). Specifically, SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. In this way, the learned target model can directly predict the labels of target data. We further investigate several techniques to refine the network architecture to parameterize the source model for better transfer performance. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督领域适应性（UDA）的目标是利用从标记源数据集学会了解决类似的任务在一个新的未标记领域的知识。此前UDA方法通常需要访问学习，以适应模型时的源数据，并使这些风险和低效分散的私人数据。在这项工作中，我们解决一个新的环境，让只受过训练的源模型可用，并探讨如何才能有效利用这样的模型，而源数据解决UDA问题。为此，我们提出了一个简单而通用表示学习框架，名为\ {EMPH源假设传输}（SHOT）。具体而言，SHOT冻结源模型的分类器模块（假说），通过双方的信息最大化和自我监督伪标记利用从目标域到源假设隐含地对准表示获知目标特异性特征提取模块。通过这种方式，学习目标模型可以直接预测目标数据的标签。我们进一步研究了几种技术来改进网络结构参数更好的传输性能源模型。为了验证它的多功能性，我们在各种改编例，其中闭集，偏集，和开集领域适应性评估SHOT。实验表明，SHOT产生多个域自适应基准中的国家的最先进的结果。</font>
</div>


<hr>
<div id="paper18"> <b>18. Expressing Objects just like Words: Recurrent Visual Embedding for  Image-Text Matching</b>  <a href="https://arxiv.org/pdf/2002.08510" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianlang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiebo Luo</a><br>
<font size="3">
Abstract: Existing image-text matching approaches typically infer the similarity of an image-text pair by capturing and aggregating the affinities between the text and each independent object of the image. However, they ignore the connections between the objects that are semantically related. These objects may collectively determine whether the image corresponds to a text or not. To address this problem, we propose a Dual Path Recurrent Neural Network (DP-RNN) which processes images and sentences symmetrically by recurrent neural networks (RNN). In particular, given an input image-text pair, our model reorders the image objects based on the positions of their most related words in the text. In the same way as extracting the hidden features from word embeddings, the model leverages RNN to extract high-level object features from the reordered object inputs. We validate that the high-level object features contain useful joint information of semantically related objects, which benefit the retrieval task. To compute the image-text similarity, we incorporate a Multi-attention Cross Matching Model into DP-RNN. It aggregates the affinity between objects and words with cross-modality guided attention and self-attention. Our model achieves the state-of-the-art performance on Flickr30K dataset and competitive performance on MS-COCO dataset. Extensive experiments demonstrate the effectiveness of our model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有图像文本匹配通过捕获和聚集所述文本和图像的每个独立的对象之间的亲和度通常接近推断的图像文本对的相似性。然而，他们忽略那些语义相关的对象之间的连接。这些对象可以共同确定图像是否对应于文本或没有。为了解决这个问题，我们提出了一个双路径回归神经网络（DP-RNN），其采用递归神经网络（RNN）处理图像和句子对称。特别是，通过在输入图像文本对，我们的模型重新排序基于对自己最相关的词在文中的位置的图像对象。以相同的方式从字的嵌入提取隐藏的特征中，该模型利用RNN到提取高级对象从重新排序的对象的输入功能。我们验证了高层次的对象特征包含语义相关的对象，这有利于检索任务的有用的联合信息。为了计算图像文本相似性，我们结合了多注意交叉配型号为DP-RNN。它聚合的对象和文字与跨模态引导注意和自关注之间的亲和力。我们的模型实现了对Flickr30K数据集和有竞争力的表现上MS-COCO数据集的国家的最先进的性能。大量的实验证明我们的模型的有效性。</font>
</div>


<hr>
<div id="paper19"> <b>19. Modelling response to trypophobia trigger using intermediate layers of  ImageNet networks</b>  <a href="https://arxiv.org/pdf/2002.08490" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wo%C5%BAnicki%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Woźnicki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ku%C5%BAba%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michał Kuźba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Migda%C5%82%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Migdał</a><br>
<font size="3">
Abstract: In this paper, we approach the problem of detecting trypophobia triggers using Convolutional neural networks. We show that standard architectures such as VGG or ResNet are capable of recognizing trypophobia patterns. We also conduct experiments to analyze the nature of this phenomenon. To do that, we dissect the network decreasing the number of its layers and parameters. We prove, that even significantly reduced networks have accuracy above 91\% and focus their attention on the trypophobia patterns as presented on the visual explanations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们接近的使用卷积神经网络检测密集恐惧症的触发器的问题。我们发现，标准体系结构，如VGG或RESNET能够识别密集恐惧症的图案。我们还进行实验来分析这一现象的本质。要做到这一点，我们剖析了网络降低其层和参数的数目。我们证明，即使显著降低网络拥有上述91 \％的准确度和集中自己的注意力集中在密集恐惧症的图案视觉上的解释为呈现。</font>
</div>


<hr>
<div id="paper20"> <b>20. Revisiting Training Strategies and Generalization Performance in Deep  Metric Learning</b>  <a href="https://arxiv.org/pdf/2002.08473" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Karsten Roth</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milbich%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timo Milbich</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sinha%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samarth Sinha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prateek Gupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ommer%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bjoern Ommer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Joseph Paul Cohen</a><br>
<font size="3">
Abstract: Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Based on our analysis, we uncover a correlation between the embedding space compression and the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深度量学习（DML）可以说是研究最有影响力的线学习视觉相似之处很多提出一个每年接近。虽然从快速进步领域的好处，在培训协议，架构和参数选择的分歧做出公正的比较困难。为了提供一个一致的参考点，我们重温最广泛使用的DML目标函数和进行关键参数选择的研究，以及常用忽视小批量采样过程。根据我们的分析，我们发现嵌入空间压缩和DML模型的泛化性能之间的相关性。利用这些见解，我们提出了一个简单而有效的训练正规化可靠地提高排名为基础的DML模型对各种标准的基准数据集的性能。</font>
</div>


<hr>
<div id="paper21"> <b>21. SD-GAN: Structural and Denoising GAN reveals facial parts under  occlusion</b>  <a href="https://arxiv.org/pdf/2002.08448" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samik Banerjee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Das%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sukhendu Das</a><br>
<font size="3">
Abstract: Certain facial parts are salient (unique) in appearance, which substantially contribute to the holistic recognition of a subject. Occlusion of these salient parts deteriorates the performance of face recognition algorithms. In this paper, we propose a generative model to reconstruct the missing parts of the face which are under occlusion. The proposed generative model (SD-GAN) reconstructs a face preserving the illumination variation and identity of the face. A novel adversarial training algorithm has been designed for a bimodal mutually exclusive Generative Adversarial Network (GAN) model, for faster convergence. A novel adversarial "structural" loss function is also proposed, comprising of two components: a holistic and a local loss, characterized by SSIM and patch-wise MSE. Ablation studies on real and synthetically occluded face datasets reveal that our proposed technique outperforms the competing methods by a considerable margin, even for boosting the performance of Face Recognition. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：某些面部部分在外观上显着的（唯一的），其基本上与整体识别的对象的贡献。这些突出的部分阻塞恶化的人脸识别算法的性能。在本文中，我们提出了一个生成模型重建它们遮挡情况下，面对残缺的部分。所提出的生成模型（SD-GAN）重构面保留面的照明变化和身份。一种新的对抗训练算法已被设计为双峰互斥剖成对抗性网络（GAN）的模型，更快的收敛。一种新的对抗式“结构的”损失函数还提出，其包括两个部分：一个整体和局部损失，其特征在于，SSIM和patch-明智MSE。真实和综合遮挡脸部数据集消融的研究表明，我们所提出的技术通过一个相当幅度优于竞争的方法，即使是提高人脸识别的性能。</font>
</div>


<hr>
<div id="paper22"> <b>22. Cooperative LIDAR Object Detection via Feature Sharing in Deep Networks</b>  <a href="https://arxiv.org/pdf/2002.08440" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Marvasti%2C+E+E" target="_blank" rel="noopener" style="color:#0000EE;">Ehsan Emad Marvasti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Raftari%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arash Raftari</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marvasti%2C+A+E" target="_blank" rel="noopener" style="color:#0000EE;">Amir Emad Marvasti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fallah%2C+Y+P" target="_blank" rel="noopener" style="color:#0000EE;">Yaser P. Fallah</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">HongSheng Lu</a><br>
<font size="3">
Abstract: The recent advancements in communication and computational systems has led to significant improvement of situational awareness in connected and autonomous vehicles. Computationally efficient neural networks and high speed wireless vehicular networks have been some of the main contributors to this improvement. However, scalability and reliability issues caused by inherent limitations of sensory and communication systems are still challenging problems. In this paper, we aim to mitigate the effects of these limitations by introducing the concept of feature sharing for cooperative object detection (FS-COD). In our proposed approach, a better understanding of the environment is achieved by sharing partially processed data between cooperative vehicles while maintaining a balance between computation and communication load. This approach is different from current methods of map sharing, or sharing of raw data which are not scalable. The performance of the proposed approach is verified through experiments on Volony dataset. It is shown that the proposed approach has significant performance superiority over the conventional single-vehicle object detection approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在通信和计算系统中的最新进展已经导致在连接和自动驾驶车辆的态势感知显著的改善。计算效率的神经网络和高速无线车载网络中已经有了一些主要贡献者的这一改进。然而，因感觉和通信系统的固有局限性可扩展性和可靠性等问题仍然具有挑战性的问题。在本文中，我们的目标是通过引入的特征共享的概念用于协作的对象检测（FS-COD），以减轻这些限制的影响。在我们提出的方法，更好地理解环境是由协作车辆之间共享部分处理的数据，同时保持计算和通信负载之间的平衡来实现的。这种方法是从地图电流共享的方法，或原始数据的共享其是不可扩展的不同。该方法的性能是通过对Volony数据集实验验证。结果表明，所提出的方法具有优于常规单车辆的物体检测方法显著性能的优越性。</font>
</div>


<hr>
<div id="paper23"> <b>23. Residual-Sparse Fuzzy $C$-Means Clustering Incorporating Morphological  Reconstruction and Wavelet frames</b>  <a href="https://arxiv.org/pdf/2002.08418" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cong Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pedrycz%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Witold Pedrycz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">ZhiWu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">MengChu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Zhao</a><br>
<font size="3">
Abstract: Instead of directly utilizing an observed image including some outliers, noise or intensity inhomogeneity, the use of its ideal value (e.g. noise-free image) has a favorable impact on clustering. Hence, the accurate estimation of the residual (e.g. unknown noise) between the observed image and its ideal value is an important task. To do so, we propose an $\ell_0$ regularization-based Fuzzy $C$-Means (FCM) algorithm incorporating a morphological reconstruction operation and a tight wavelet frame transform. To achieve a sound trade-off between detail preservation and noise suppression, morphological reconstruction is used to filter an observed image. By combining the observed and filtered images, a weighted sum image is generated. Since a tight wavelet frame system has sparse representations of an image, it is employed to decompose the weighted sum image, thus forming its corresponding feature set. Taking it as data for clustering, we present an improved FCM algorithm by imposing an $\ell_0$ regularization term on the residual between the feature set and its ideal value, which implies that the favorable estimation of the residual is obtained and the ideal value participates in clustering. Spatial information is also introduced into clustering since it is naturally encountered in image segmentation. Furthermore, it makes the estimation of the residual more reliable. To further enhance the segmentation effects of the improved FCM algorithm, we also employ the morphological reconstruction to smoothen the labels generated by clustering. Finally, based on the prototypes and smoothed labels, the segmented image is reconstructed by using a tight wavelet frame reconstruction operation. Experimental results reported for synthetic, medical, and color images show that the proposed algorithm is effective and efficient, and outperforms other algorithms. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：而不是直接利用的观察图像包括一些离群值，噪声或强度不均匀性的，利用它的理想值（例如无噪声图像）的对聚类产生有利的影响。因此，残留的（例如未知噪声）的观察图像和它的理想值之间的准确的估计是一项重要任务。要做到这一点，我们提出了一个基于正则$ \ $ ell_0模糊$ C $ -Means（FCM）结合形态学重建操作算法和从紧的小波变换框架。为了实现细节保留和噪声抑制之间的声音权衡，形态重构用于过滤的观察图像。通过组合所观察到的和滤波后的图像，将生成的加权和图像。由于小波紧框架系统具有图像的稀疏表示，它被用来分解加权和图像，从而形成其相应的功能集。以它作为聚类的数据，我们通过对特征组和它的理想值，这意味着得到的残余的有利估计和理想值参与之间的残余强加一个$ \ ell_0 $正则化项呈现的改进的FCM算法在集群。空间信息也被引入到集群，因为它在图像分割自然遇到。此外，它使剩余的估计更加可靠。为了进一步增强改进的FCM算法的分割的影响，我们还采用了形态重构以平滑由聚类生成的标签。最后，基于该原型和平滑标签，所述分割的图像，通过使用小波紧帧重构操作重建。实验结果报道为合成的，医疗，和彩色图像显示，该算法是有效和高效率，并且优于其它算法。</font>
</div>


<hr>
<div id="paper24"> <b>24. Table-Top Scene Analysis Using Knowledge-Supervised MCMC</b>  <a href="https://arxiv.org/pdf/2002.08417" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziyuan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dong Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wurm%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">Kai M. Wurm</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Wichert%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Georg von Wichert</a><br>
<font size="3">
Abstract: In this paper, we propose a probabilistic method to generate abstract scene graphs for table-top scenes from 6D object pose estimates. We explicitly make use of task-specfic context knowledge by encoding this knowledge as descriptive rules in Markov logic networks. Our approach to generate scene graphs is probabilistic: Uncertainty in the object poses is addressed by a probabilistic sensor model that is embedded in a data driven MCMC process. We apply Markov logic inference to reason about hidden objects and to detect false estimates of object poses. The effectiveness of our approach is demonstrated and evaluated in real world experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个概率的方法来产生从6D对象姿势估计桌面场景抽象的场景图。通过编码这方面的知识，如马氏逻辑网络中描述的规则，我们明确地利用任务specfic背景知识。我们以生成场景图的方法是概率性：不确定性中的对象的姿势是由嵌入在数据驱动MCMC处理的概率测量模型解决。我们应用马氏逻辑推理推理隐藏的对象，并检测对象姿势的错误估计。我们的方法的有效性证明，并在现实世界中的实验评估。</font>
</div>


<hr>
<div id="paper25"> <b>25. A Generalizable Knowledge Framework for Semantic Indoor Mapping Based on  Markov Logic Networks and Data Driven MCMC</b>  <a href="https://arxiv.org/pdf/2002.08402" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziyuan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Wichert%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Georg von Wichert</a><br>
<font size="3">
Abstract: In this paper, we propose a generalizable knowledge framework for data abstraction, i.e. finding compact abstract model for input data using predefined abstract terms. Based on these abstract terms, intelligent autonomous systems, such as a robot, should be able to make inference according to specific knowledge base, so that they can better handle the complexity and uncertainty of the real world. We propose to realize this framework by combining Markov logic networks (MLNs) and data driven MCMC sampling, because the former are a powerful tool for modelling uncertain knowledge and the latter provides an efficient way to draw samples from unknown complex distributions. Furthermore, we show in detail how to adapt this framework to a certain task, in particular, semantic robot mapping. Based on MLNs, we formulate task-specific context knowledge as descriptive soft rules. Experiments on real world data and simulated data confirm the usefulness of our framework. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个概括性的知识框架，数据抽象，即发现使用预定义抽象的术语输入数据紧凑抽象模型。基于这些抽象的术语，智能自治系统，如机器人，应该能够根据具体的知识基础，使推理，使他们能更好地处理现实世界的复杂性和不确定性。我们建议结合马尔科夫逻辑网络（MLNS）和驱动MCMC采样数据，实现这个框架，因为前者是不确定的知识建模的有力工具，而后者提供了一种有效的方式来吸取未知复杂分布样本。此外，我们还详细显示了如何将这个框架适应特定的任务，特别是语义机器人映射。基于MLNS，我们制定任务的特定背景知识，描述性的软规则。对现实世界的数据的实验和模拟数据证实了我们的框架的有效性。</font>
</div>


<hr>
<div id="paper26"> <b>26. JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset</b>  <a href="https://arxiv.org/pdf/2002.08397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shenoi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhijeet Shenoi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Patel%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mihir Patel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gwak%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">JunYoung Gwak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goebel%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Goebel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sadeghian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amir Sadeghian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rezatofighi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hamid Rezatofighi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martin-Martin%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roberto Martin-Martin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Savarese%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Silvio Savarese</a><br>
<font size="3">
Abstract: An autonomous navigating agent needs to perceive and track the motion of objects and other agents in its surroundings to achieve robust and safe motion planning and execution. While autonomous navigation requires a multi-object tracking (MOT) system to provide 3D information, most research has been done in 2D MOT from RGB videos. In this work we present JRMOT, a novel 3D MOT system that integrates information from 2D RGB images and 3D point clouds into a real-time performing framework. Our system leverages advancements in neural-network based re-identification as well as 2D and 3D detection and descriptors. We incorporate this into a joint probabilistic data-association framework within a multi-modal recursive Kalman architecture to achieve online, real-time 3D MOT. As part of our work, we release the JRDB dataset, a novel large scale 2D+3D dataset and benchmark annotated with over 2 million boxes and 3500 time consistent 2D+3D trajectories across 54 indoor and outdoor scenes. The dataset contains over 60 minutes of data including 360 degree cylindrical RGB video and 3D pointclouds. The presented 3D MOT system demonstrates state-of-the-art performance against competing methods on the popular 2D tracking KITTI benchmark and serves as a competitive 3D tracking baseline for our dataset and benchmark. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自主导航代理需要感知和跟踪对象和其他代理的运动在它的周围，实现稳健和安全的运动规划和执行。虽然自主导航需要一个多目标追踪（MOT）系统，提供3D信息，大多数的研究已经从RGB视频2D MOT完成。在这项工作中，我们目前JRMOT，一种新颖的3D MOT系统，从2D RGB图像和三维点云信息集成到一个实时的执行框架。我们的系统利用神经网络基于重新鉴定的进展，以及2D和3D检测和描述符。我们的多模态递归卡尔曼架构中整合到这个联合概率数据关联框架来实现在线，实时3D MOT。作为我们工作的一部分，我们发布JRDB数据集，一个新的大型2D + 3D数据集和基准超过200万箱和整个54个室内和室外场景3500个时间一致的2D + 3D轨迹进行注释。该数据集包含在60分钟内的数据，包括360度圆柱RGB视频和3D点云。所提出的3D MOT系统证明了对抗上流行的二维跟踪KITTI基准竞争法国家的最先进的性能，并作为我们的数据集和基准具有竞争力的3D跟踪基准。</font>
</div>


<hr>
<div id="paper27"> <b>27. MonoLayout: Amodal scene layout from a single image</b>  <a href="https://arxiv.org/pdf/2002.08394" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mani%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaustubh Mani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Daga%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Swapnil Daga</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garg%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shubhika Garg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shankar%2C+N+S" target="_blank" rel="noopener" style="color:#0000EE;">N. Sai Shankar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jatavallabhula%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">Krishna Murthy Jatavallabhula</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krishna%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">K. Madhava Krishna</a><br>
<font size="3">
Abstract: In this paper, we address the novel, highly challenging problem of estimating the layout of a complex urban driving scenario. Given a single color image captured from a driving platform, we aim to predict the bird's-eye view layout of the road and other traffic participants. The estimated layout should reason beyond what is visible in the image, and compensate for the loss of 3D information due to projection. We dub this problem amodal scene layout estimation, which involves "hallucinating" scene layout for even parts of the world that are occluded in the image. To this end, we present MonoLayout, a deep neural network for real-time amodal scene layout estimation from a single image. We represent scene layout as a multi-channel semantic occupancy grid, and leverage adversarial feature learning to hallucinate plausible completions for occluded image parts. Due to the lack of fair baseline methods, we extend several state-of-the-art approaches for road-layout estimation and vehicle occupancy estimation in bird's-eye view to the amodal setup for rigorous evaluation. By leveraging temporal sensor fusion to generate training labels, we significantly outperform current art over a number of datasets. On the KITTI and Argoverse datasets, we outperform all baselines by a significant margin. We also make all our annotations, and code publicly available. A video abstract of this paper is available this https URL . </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们解决估计复杂的城市驾驶情形的布局新颖，极具挑战性的问题。鉴于从驾驶平台捕获的单色图像时，我们的目标是预测道路和其他交通参与者的鸟瞰视图布局。估计布局应理性超出了图像中可见，并赔偿3D信息，由于投影损失。我们配音这个问题amodal场景布置估计，其中涉及对于在图像中闭塞的世界，甚至部分“幻觉”场景布置。为此，我们提出MonoLayout，从单一的图像实时amodal场景布置估计的深层神经网络。我们代表现场布置的多通道语义网格占用，并充分利用对抗性特征的学习产生幻觉的遮挡图像部分合理的完成。由于缺乏公平基线方法，我们延长几个国家的最先进的在鸟瞰到amodal设置了严格的评估道路布局估计和车辆占用估计方法。通过利用时间传感器融合以生成训练标签，我们显著优于在数数据集的现有技术。在KITTI和Argoverse数据集，我们通过一个显著保证金胜过所有的基线。我们也使我们的所有注释和代码公开。本文的视频摘要可用此HTTPS URL。</font>
</div>


<hr>
<div id="paper28"> <b>28. SynFi: Automatic Synthetic Fingerprint Generation</b>  <a href="https://arxiv.org/pdf/2002.08900" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Riazi%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">M. Sadegh Riazi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chavoshian%2C+S+M" target="_blank" rel="noopener" style="color:#0000EE;">Seyed M. Chavoshian</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Koushanfar%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Farinaz Koushanfar</a><br>
<font size="3">
Abstract: Authentication and identification methods based on human fingerprints are ubiquitous in several systems ranging from government organizations to consumer products. The performance and reliability of such systems directly rely on the volume of data on which they have been verified. Unfortunately, a large volume of fingerprint databases is not publicly available due to many privacy and security concerns. In this paper, we introduce a new approach to automatically generate high-fidelity synthetic fingerprints at scale. Our approach relies on (i) Generative Adversarial Networks to estimate the probability distribution of human fingerprints and (ii) Super-Resolution methods to synthesize fine-grained textures. We rigorously test our system and show that our methodology is the first to generate fingerprints that are computationally indistinguishable from real ones, a task that prior art could not accomplish. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于人的指纹身份验证和识别方法在几个系统，从政府机构到消费者的产品无处不在。这种系统的性能和可靠性直接依赖于它们已被验证的数据量上。不幸的是，大容量指纹数据库是不公开的，由于很多隐私和安全问题。在本文中，我们介绍一种新的方法来大规模自动生成高保真合成指纹。我们的方法依赖于（I）剖成对抗性网络对人类的指纹及（ii）超分辨率方法的概率分布估计合成细粒度纹理。我们严格的测试我们的系统，并表明我们的方法是首先生成从以假乱真，任务是现有技术无法实现的计算区分指纹。</font>
</div>


<hr>
<div id="paper29"> <b>29. A Bayes-Optimal View on Adversarial Examples</b>  <a href="https://arxiv.org/pdf/2002.08859" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Richardson%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eitan Richardson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weiss%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yair Weiss</a><br>
<font size="3">
Abstract: The ability to fool modern CNN classifiers with tiny perturbations of the input has lead to the development of a large number of candidate defenses and often conflicting explanations. In this paper, we argue for examining adversarial examples from the perspective of Bayes-Optimal classification. We construct realistic image datasets for which the Bayes-Optimal classifier can be efficiently computed and derive analytic conditions on the distributions so that the optimal classifier is either robust or vulnerable. By training different classifiers on these datasets (for which the "gold standard" optimal classifiers are known), we can disentangle the possible sources of vulnerability and avoid the accuracy-robustness tradeoff that may occur in commonly used datasets. Our results show that even when the optimal classifier is robust, standard CNN training consistently learns a vulnerable classifier. At the same time, for exactly the same training data, RBF SVMs consistently learn a robust classifier. The same trend is observed in experiments with real images. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：愚弄现代CNN分类与输入的微小扰动导致了大量候选防御，并经常互相冲突的解释发展的能力。在本文中，我们认为从贝叶斯最优分类的角度审查对抗的例子。我们构建的量，贝叶斯分类器优化可以有效地计算并导出上分布的分析条件，使得最优分类器或者是健壮或脆弱的逼真的图像数据集。通过对这些数据集（对于其中的“金标准”最佳分类器是公知的）训练不同的分类器，我们可以解开漏洞的可能来源，并避免可能出现在通常使用的数据集，该精度鲁棒性的权衡。我们的研究结果显示，即使在最佳的分类器是强大的，标准的CNN一贯的培训学习脆弱的分类。与此同时，对于完全相同的训练数据，RBF支持向量机一贯学习强大的分类。同样的趋势在真实图像实验中观察到。</font>
</div>


<hr>
<div id="paper30"> <b>30. Deep Learning Estimation of Multi-Tissue Constrained Spherical  Deconvolution with Limited Single Shell DW-MRI</b>  <a href="https://arxiv.org/pdf/2002.08820" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Nath%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vishwesh Nath</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Pathak%2C+S+K" target="_blank" rel="noopener" style="color:#0000EE;">Sudhir K. Pathak</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Schilling%2C+K+G" target="_blank" rel="noopener" style="color:#0000EE;">Kurt G. Schilling</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Schneider%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Walt Schneider</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Landman%2C+B+A" target="_blank" rel="noopener" style="color:#0000EE;">Bennett A. Landman</a><br>
<font size="3">
Abstract: Diffusion-weighted magnetic resonance imaging (DW-MRI) is the only non-invasive approach for estimation of intra-voxel tissue microarchitecture and reconstruction of in vivo neural pathways for the human brain. With improvement in accelerated MRI acquisition technologies, DW-MRI protocols that make use of multiple levels of diffusion sensitization have gained popularity. A well-known advanced method for reconstruction of white matter microstructure that uses multi-shell data is multi-tissue constrained spherical deconvolution (MT-CSD). MT-CSD substantially improves the resolution of intra-voxel structure over the traditional single shell version, constrained spherical deconvolution (CSD). Herein, we explore the possibility of using deep learning on single shell data (using the b=1000 s/mm2 from the Human Connectome Project (HCP)) to estimate the information content captured by 8th order MT-CSD using the full three shell data (b=1000, 2000, and 3000 s/mm2 from HCP). Briefly, we examine two network architectures: 1.) Sequential network of fully connected dense layers with a residual block in the middle (ResDNN), 2.) Patch based convolutional neural network with a residual block (ResCNN). For both networks an additional output block for estimation of voxel fraction was used with a modified loss function. Each approach was compared against the baseline of using MT-CSD on all data on 15 subjects from the HCP divided into 5 training, 2 validation, and 8 testing subjects with a total of 6.7 million voxels. The fiber orientation distribution function (fODF) can be recovered with high correlation (0.77 vs 0.74 and 0.65) as compared to the ground truth of MT-CST, which was derived from the multi-shell DW-MRI acquisitions. Source code and models have been made publicly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：弥散加权磁共振成像（DW-MRI）是用于帧内体素的组织微架构的估计和对人类大脑在体内的神经通路的重建唯一的非侵入性的方法。随着加速MRI采集技术的改进，DW-MRI方案，使利用扩散敏感的多层次的获得了普及。对于白质组织重建，使用多壳数据是多组织的公知方法先进约束球形去卷积（MT-CSD）。 MT-CSD显着改善内部体素的结构相对于传统的单壳版本分辨率，约束球形去卷积（CSD）。在此，我们探讨使用单壳数据深度学习（使用从人类连接组项目（HCP）的B = 1000秒/平方毫米）来估计使用足三个壳数据由8阶MT-CSD捕获的信息内容的可能性（b = 1000，2000，和3000秒/从HCP平方毫米）。简要地说，我们检查两个网络架构：1）与在中间（ResDNN），2。）修补基于卷积神经与残余块（ResCNN）网络中的残余块完全连接致密层的顺序网络。对于这两个网络用改进的损失函数用于体素部分的估计的额外的输出块。每一种方法是使用对关于从HCP分成5训练，2验证和8名测试受试者具有总共670万点的体素15名受试者的所有数据MT-CSD的基线比较。纤维取向分布函数（fODF）可以以高的相关性（0.77 VS 0.74和0.65）相比，MT-CST，将其从多壳DW-MRI采集衍生的基础事实作为回收。源代码和模型已经被公布于众。</font>
</div>


<hr>
<div id="paper31"> <b>31. Pruning untrained neural networks: Principles and Analysis</b>  <a href="https://arxiv.org/pdf/2002.08797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/stat?searchtype=author&query=Hayou%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Soufiane Hayou</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Ton%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-Francois Ton</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Doucet%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arnaud Doucet</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Teh%2C+Y+W" target="_blank" rel="noopener" style="color:#0000EE;">Yee Whye Teh</a><br>
<font size="3">
Abstract: Overparameterized neural networks display state-of-the art performance. However, there is a growing need for smaller, energy-efficient, neural networks to be able to use machine learning applications on devices with limited computational resources. A popular approach consists of using pruning techniques. While these techniques have traditionally focused on pruning pre-trained neural networks (e.g. LeCun et al. (1990) and Hassabi et al. (1993)), recent work by Lee et al. (2018) showed promising results where pruning is performed at initialization. However, such procedures remain unsatisfactory as the resulting pruned networks can be difficult to train and, for instance, these procedures do not prevent one layer being fully pruned. In this paper we provide a comprehensive theoretical analysis of pruning at initialization and training sparse architectures. This analysis allows us to propose novel principled approaches which we validate experimentally on a variety of network architectures. We particularly show that we can prune up to 99.9% of the weights while keeping the model trainable. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：Overparameterized神经网络显示国家的本领域的性能。然而，越来越需要小型，节能，神经网络能够与计算资源有限的设备使用机器学习应用。一种流行的方法包括使用修剪技术。虽然这些技术历来集中在修剪预训练神经网络（例如LeCun等人（1990）和Hassabi等人（1993）），最近由Lee等人的工作。 （2018）显示，有前途的地方修剪在初始化执行结果。然而，这样的程序仍然不能令人满意的所得到的修剪网络可以是很难培养，并且例如，这些程序并不防止层与层之间充分修剪。在本文中，我们提供的初始化，修剪和培育稀疏架构进行了全面的理论分析。这种分析使我们能够提出我们通过实验验证多种网络架构的新原则的方法。我们特别表明，我们可以修剪到权重的99.9％，同时保持模型训练的。</font>
</div>


<hr>
<div id="paper32"> <b>32. Disentangled Speech Embeddings using Cross-modal Self-supervision</b>  <a href="https://arxiv.org/pdf/2002.08742" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Nagrani%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arsha Nagrani</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chung%2C+J+S" target="_blank" rel="noopener" style="color:#0000EE;">Joon Son Chung</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Albanie%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samuel Albanie</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zisserman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Zisserman</a><br>
<font size="3">
Abstract: The objective of this paper is to learn representations of speaker identity without access to manually annotated data. To do so, we develop a self-supervised learning objective that exploits the natural cross-modal synchrony between faces and audio in video. The key idea behind our approach is to tease apart---without annotation---the representations of linguistic content and speaker identity. We construct a two-stream architecture which: (1) shares low-level features common to both representations; and (2) provides a natural mechanism for explicitly disentangling these factors, offering the potential for greater generalisation to novel combinations of content and identity and ultimately producing speaker identity representations that are more robust. We train our method on a large-scale audio-visual dataset of talking heads `in the wild', and demonstrate its efficacy by evaluating the learned speaker representations for standard speaker recognition performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文的目的是学习的发言者身份的表示，而没有进入手动注释的数据。要做到这一点，我们开发了一个自我监督学习的目标是利用面孔和音频视频之间的天然跨模式同步。我们的做法背后的关键思想是，以梳理出---没有注释---的语言内容和演讲人的身份表示。我们构建了一个两流体系结构，其中：（1）股低级特征常见的两种表示;和（2）提供了一种自然的机制明确地解开这些因素，提供了潜在的更大的推广到的内容和身份的新组合，并最终产生扬声器身份表示是更稳健。我们培养的野生”名嘴'的大型视听数据集我们的方法，并通过评估标准说话人识别性能了解到扬声器陈述证明其疗效。</font>
</div>


<hr>
<div id="paper33"> <b>33. Bimodal Distribution Removal and Genetic Algorithm in Neural Network for  Breast Cancer Diagnosis</b>  <a href="https://arxiv.org/pdf/2002.08729" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Quan%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Quan</a><br>
<font size="3">
Abstract: Diagnosis of breast cancer has been well studied in the past. Multiple linear programming models have been devised to approximate the relationship between cell features and tumour malignancy. However, these models are less capable in handling non-linear correlations. Neural networks instead are powerful in processing complex non-linear correlations. It is thus certainly beneficial to approach this cancer diagnosis problem with a model based on neural network. Particularly, introducing bias to neural network training process is deemed as an important means to increase training efficiency. Out of a number of popular proposed methods for introducing artificial bias, Bimodal Distribution Removal (BDR) presents ideal efficiency improvement results and fair simplicity in implementation. However, this paper examines the effectiveness of BDR against the target cancer diagnosis classification problem and shows that BDR process in fact negatively impacts classification performance. In addition, this paper also explores genetic algorithm as an efficient tool for feature selection and produced significantly better results comparing to baseline model that without any feature selection in place </font>
<br>
<font size="2" style="line-height:30px;">
摘要：乳腺癌的诊断已经得到很好的研究中来。多个线性规划模型已经被设计来近似细胞特征和肿瘤恶性之间的关系。然而，这些模型是在处理非线性关系能力较差。神经网络代替是在处理复杂的非线性相关性的强大。因此肯定有益基于神经网络模型来处理这个癌症诊断问题。特别地，引入偏压神经网络的训练过程被视为提高训练效率的重要手段。出了一些在执行引入人工偏置，双峰分布去除（BDR）呈现理想的效率提高的结果，公平简单通俗提出的方法。然而，本文考察BDR的针对目标癌症诊断分类问题，并说明BDR过程其实负面影响分类性能的有效性。此外，本文还探讨了遗传算法作为特征选择的有效工具，并产生显著更好的效果比较基准模型，没有采取任何适当的特征选择</font>
</div>


<hr>
<div id="paper34"> <b>34. An empirical study of Conv-TasNet</b>  <a href="https://arxiv.org/pdf/2002.08688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Kadioglu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Berkan Kadioglu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Horgan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Horgan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyu Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Pons%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jordi Pons</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Darcy%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dan Darcy</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vivek Kumar</a><br>
<font size="3">
Abstract: Conv-TasNet is a recently proposed waveform-based deep neural network that achieves state-of-the-art performance in speech source separation. Its architecture consists of a learnable encoder/decoder and a separator that operates on top of this learned space. Various improvements have been proposed to Conv-TasNet. However, they mostly focus on the separator, leaving its encoder/decoder as a (shallow) linear operator. In this paper, we conduct an empirical study of Conv-TasNet and propose an enhancement to the encoder/decoder that is based on a (deep) non-linear variant of it. In addition, we experiment with the larger and more diverse LibriTTS dataset and investigate the generalization capabilities of the studied models when trained on a much larger dataset. We propose cross-dataset evaluation that includes assessing separations from the WSJ0-2mix, LibriTTS and VCTK databases. Our results show that enhancements to the encoder/decoder can improve average SI-SNR performance by more than 1 dB. Furthermore, we offer insights into the generalization capabilities of Conv-TasNet and the potential value of improvements to the encoder/decoder. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：转化率，TasNet是实现语音源分离状态的最先进的性能最近提出的基于波形的深层神经网络。它的架构是由一个可以学习的编码器/解码器，并且在这里学到的空间的顶部运行的分隔符。各种改进已经提出了转化率，TasNet。然而，它们主要集中在分离器，留下其编码器/解码器作为（浅）线性算子。在本文中，我们进行转化率-TasNet的实证研究提出的增强编码器/解码器是基于它的一个（深）的非线性变体。此外，我们尝试用更大和更多样化LibriTTS在更大的数据集训练的时候数据集和调查研究模型的推广能力。我们提出的跨数据集的评估，其中包括从WSJ0-2mix，LibriTTS和VCTK数据库评估分离。我们的研究结果表明，增强了编码器/解码器可以提高超过1dB平均SI-SNR性能。此外，我们还提供见解转化率，TasNet和改进的编码器/解码器的潜在价值的概括能力。</font>
</div>


<hr>
<div id="paper35"> <b>35. Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and  Practice</b>  <a href="https://arxiv.org/pdf/2002.08681" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yabin Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kui Jia</a><br>
<font size="3">
Abstract: In this paper, we study the formalism of unsupervised multi-class domain adaptation (multi-class UDA), which underlies some recent algorithms whose learning objectives are only motivated empirically. A Multi-Class Scoring Disagreement (MCSD) divergence is presented by aggregating the absolute margin violations in multi-class classification; the proposed MCSD is able to fully characterize the relations between any pair of multi-class scoring hypotheses. By using MCSD as a measure of domain distance, we develop a new domain adaptation bound for multi-class UDA as well as its data-dependent, probably approximately correct bound, which naturally suggest adversarial learning objectives to align conditional feature distributions across the source and target domains. Consequently, an algorithmic framework of Multi-class Domain-adversarial learning Networks (McDalNets) is developed, whose different instantiations via surrogate learning objectives either coincide with or resemble a few of recently popular methods, thus (partially) underscoring their practical effectiveness. Based on our same theory of multi-class UDA, we also introduce a new algorithm of Domain-Symmetric Networks (SymmNets), which is featured by a novel adversarial strategy of domain confusion and discrimination. SymmNets afford simple extensions that work equally well under the problem settings of either closed set, partial, or open set UDA. We conduct careful empirical studies to compare different algorithms of McDalNets and our newly introduced SymmNets. Experiments verify our theoretical analysis and show the efficacy of our proposed SymmNets. We make our implementation codes publicly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们研究了无监督多类领域适应性（多级UDA），这underlies最近的一些算法，其学习目标仅是出于经验的形式主义。多类别得分不一致（MCSD）发散通过聚集绝对余量违反多类分类呈现;所提出的MCSD是能够充分体现任何一组多级得分假设之间的关系。通过MCSD作为域距离的测量，我们开发必将为多类UDA以及它的数据依赖，可能近似正确绑定，在源这自然意味着敌对的学习目标对齐有条件的功能分布和一个新的领域适应性目标域。因此，多级域对抗性学习网络（McDalNets）的算法框架得以确立，通过替代学习目标或者重合，其不同实例有或类似几个最近流行的方法，从而（部分地）强调它们的实际效果。基于我们的多级UDA的同样的理论，我们也介绍域的对称网络（SymmNets），这是由域的混乱和歧视的一种新的对抗策略精选的新算法。 SymmNets提供简单的扩展，它的工作同样在任一闭集，部分或开集UDA的问题设置。我们进行认真的实证研究比较McDalNets和我们新推出的SymmNets不同的算法。实验结果验证了我们的理论分析和展示我们提出SymmNets的功效。我们使我们的实现代码公开。</font>
</div>


<hr>
<div id="paper36"> <b>36. Unsupervised Domain Adaptation via Discriminative Manifold Embedding and  Alignment</b>  <a href="https://arxiv.org/pdf/2002.08675" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">You-Wei Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuan-Xian Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke-Kun Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu-Feng Yu</a><br>
<font size="3">
Abstract: Unsupervised domain adaptation is effective in leveraging the rich information from the source domain to the unsupervised target domain. Though deep learning and adversarial strategy make an important breakthrough in the adaptability of features, there are two issues to be further explored. First, the hard-assigned pseudo labels on the target domain are risky to the intrinsic data structure. Second, the batch-wise training manner in deep learning limits the description of the global structure. In this paper, a Riemannian manifold learning framework is proposed to achieve transferability and discriminability consistently. As to the first problem, this method establishes a probabilistic discriminant criterion on the target domain via soft labels. Further, this criterion is extended to a global approximation scheme for the second issue; such approximation is also memory-saving. The manifold metric alignment is exploited to be compatible with the embedding space. A theoretical error bound is derived to facilitate the alignment. Extensive experiments have been conducted to investigate the proposal and results of the comparison study manifest the superiority of consistent manifold learning framework. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督领域适应性是有效地利用从源域到无监督目标域的丰富信息。虽然深学习和对抗性的策略使功能适应性的一个重要突破，有两个问题有待进一步探讨。首先，在目标域硬分配的假标签是有风险的内在数据结构。其次，在深度学习间歇式培训的方式限制了全球结构的描述。在本文中，黎曼流形学习框架，提出了实现可转让性和识别性一致。作为第一个问题，这种方法建立经由软标签目标域中的概率的判别标准。此外，该标准被推广到了第二个问题，一个全球性的近似方案;这种近似也是存储器节约。歧管度量对准被利用以与嵌入空间兼容。结合的理论误差导出以便于对准。大量的实验已经进行了调查对比研究清单一致流形学习框架的优越性的建议和结果。</font>
</div>


<hr>
<div id="paper37"> <b>37. A Novel Framework for Selection of GANs for an Application</b>  <a href="https://arxiv.org/pdf/2002.08641" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Motwani%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tanya Motwani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Parmar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manojkumar Parmar</a><br>
<font size="3">
Abstract: Generative Adversarial Network (GAN) is a current focal point of research. The body of knowledge is fragmented, leading to a trial-error method while selecting an appropriate GAN for a given scenario. We provide a comprehensive summary of the evolution of GANs starting from its inception addressing issues like mode collapse, vanishing gradient, unstable training and non-convergence. We also provide a comparison of various GANs from the application point of view, its behaviour and implementation details. We propose a novel framework to identify candidate GANs for a specific use case based on architecture, loss, regularization and divergence. We also discuss application of the framework using an example, and we demonstrate a significant reduction in search space. This efficient way to determine potential GANs lowers unit economics of AI development for organizations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：创成对抗性网络（GAN）是研究当前的焦点。知识的身体支离破碎，导致试错法，而选择合适的GAN对于给定的情景。我们提供甘斯的进化全面总结了自成立以来的寻址模式一样崩溃的问题出发，消失梯度，不稳定的培训和非收敛。我们还提供各种甘斯从来看，它的行为和实施细则的应用点的比较。我们提出了一个新的框架，以确定基于架构的损失，正规化和发散具体的使用情况下，候选人甘斯。我们还讨论了使用一个例子框架的应用，我们证明搜索空间显著减少。这种高效的方式来确定潜在甘斯降低AI开发单位为经济组织。</font>
</div>


<hr>
<div id="paper38"> <b>38. Boosting Adversarial Training with Hypersphere Embedding</b>  <a href="https://arxiv.org/pdf/2002.08619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianyu Pang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinpeng Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Su</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Zhu</a><br>
<font size="3">
Abstract: Adversarial training (AT) is one of the most effective defenses to improve the adversarial robustness of deep learning models. In order to promote the reliability of the adversarially trained models, we propose to boost AT via incorporating hypersphere embedding (HE), which can regularize the adversarial features onto compact hypersphere manifolds. We formally demonstrate that AT and HE are well coupled, which tunes up the learning dynamics of AT from several aspects. We comprehensively validate the effectiveness and universality of HE by embedding it into the popular AT frameworks including PGD-AT, ALP, and TRADES, as well as the FreeAT and FastAT strategies. In experiments, we evaluate our methods on the CIFAR-10 and ImageNet datasets, and verify that integrating HE can consistently enhance the performance of the models trained by each AT framework with little extra computation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对抗性训练（AT）是最有效的防御系统，以提高深学习模式的对抗稳健性的一个。为了促进adversarially训练的模型的可靠性，提出了通过引入超球面包埋（HE），其可正规化对抗特征到紧凑超球面的歧管，以提高AT。我们正式表明，AT和HE很好耦合，这从几个方面曲调了AT的学习动力。我们全面通过嵌入流行的AT框架，包括PGD-AT，ALP，和行业，以及在FreeAT和FastAT策略验证HE的有效性和普遍性。在实验中，我们评估的CIFAR-10和ImageNet数据集的方法，并验证集成HE可以持续提升与一些额外的计算每个AT框架训练模型的性能。</font>
</div>


<hr>
<div id="paper39"> <b>39. Cross-stained Segmentation from Renal Biopsy Images Using Multi-level  Adversarial Learning</b>  <a href="https://arxiv.org/pdf/2002.08587" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Mei%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Mei</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuang Zhu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Jiang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Jiang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Qiao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanyuan Qiao</a><br>
<font size="3">
Abstract: Segmentation from renal pathological images is a key step in automatic analyzing the renal histological characteristics. However, the performance of models varies significantly in different types of stained datasets due to the appearance variations. In this paper, we design a robust and flexible model for cross-stained segmentation. It is a novel multi-level deep adversarial network architecture that consists of three sub-networks: (i) a segmentation network; (ii) a pair of multi-level mirrored discriminators for guiding the segmentation network to extract domain-invariant features; (iii) a shape discriminator that is utilized to further identify the output of the segmentation network and the ground truth. Experimental results on glomeruli segmentation from renal biopsy images indicate that our network is able to improve segmentation performance on target type of stained images and use unlabeled data to achieve similar accuracy to labeled data. In addition, this method can be easily applied to other tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：分割从肾脏病理图像是在自动分析肾的组织学特征的关键步骤。然而，模型的性能显著不同类型染色的数据集，由于外观的变化而变化。在本文中，我们设计了交叉染色分割一个强大的和灵活的模式。它是一种新颖的多级深对抗网络架构，可包括三个子网络：（ⅰ）一分割网络; （ⅱ）一对多级的镜像用于引导分割网络来提取域不变特征鉴别器; （ⅲ）被利用来进一步识别分割网络的输出和所述地面真值的形状鉴别器。从肾活检图像分割肾小球实验结果表明，我们的网络能够提高染色图像的目标类型划分的性能和使用无标签数据来实现类似的准确性标签的数据。此外，该方法可以很容易地应用到其他的任务。</font>
</div>


<hr>
<div id="paper40"> <b>40. Deep Fusion of Local and Non-Local Features for Precision Landslide  Recognition</b>  <a href="https://arxiv.org/pdf/2002.08547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qing Zhu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Chen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Hu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Binzhi Xu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yeting Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haifeng Li</a><br>
<font size="3">
Abstract: Precision mapping of landslide inventory is crucial for hazard mitigation. Most landslides generally co-exist with other confusing geological features, and the presence of such areas can only be inferred unambiguously at a large scale. In addition, local information is also important for the preservation of object boundaries. Aiming to solve this problem, this paper proposes an effective approach to fuse both local and non-local features to surmount the contextual problem. Built upon the U-Net architecture that is widely adopted in the remote sensing community, we utilize two additional modules. The first one uses dilated convolution and the corresponding atrous spatial pyramid pooling, which enlarged the receptive field without sacrificing spatial resolution or increasing memory usage. The second uses a scale attention mechanism to guide the up-sampling of features from the coarse level by a learned weight map. In implementation, the computational overhead against the original U-Net was only a few convolutional layers. Experimental evaluations revealed that the proposed method outperformed state-of-the-art general-purpose semantic segmentation approaches. Furthermore, ablation studies have shown that the two models afforded extensive enhancements in landslide-recognition performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：滑坡库存的精确映射是减轻灾害的关键。最通常滑坡并存与其它混淆地质特征，并且这些区域的存在只能在大规模明确地推断。此外，本地信息也是对象边界的保护非常重要。旨在解决这一问题，本文提出了融合本地和非本地特色超越情境问题的有效途径。建立在是在遥感界广泛采用U型网络架构，我们利用两个附加模块。第一个用途扩张卷积和相应atrous空间金字塔池，其扩大了感受域，而不会牺牲的空间分辨率或增加内存使用情况。第二个使用的刻度注意机制通过学习权重映射来指导的特征上采样从所述粗级。在实施中，对原来的掌中计算开销只有几卷积层。实验评估揭示了所提出的方法优于状态的最先进的通用语义分割方法。此外，消融的研究表明，这两个模型在滑坡识别性能得到了广泛的改进。</font>
</div>


<hr>
<div id="paper41"> <b>41. AdvMS: A Multi-source Multi-cost Defense Against Adversarial Attacks</b>  <a href="https://arxiv.org/pdf/2002.08439" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyue Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pin-Yu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xue Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chin%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Chin</a><br>
<font size="3">
Abstract: Designing effective defense against adversarial attacks is a crucial topic as deep neural networks have been proliferated rapidly in many security-critical domains such as malware detection and self-driving cars. Conventional defense methods, although shown to be promising, are largely limited by their single-source single-cost nature: The robustness promotion tends to plateau when the defenses are made increasingly stronger while the cost tends to amplify. In this paper, we study principles of designing multi-source and multi-cost schemes where defense performance is boosted from multiple defending components. Based on this motivation, we propose a multi-source and multi-cost defense scheme, Adversarially Trained Model Switching (AdvMS), that inherits advantages from two leading schemes: adversarial training and random model switching. We show that the multi-source nature of AdvMS mitigates the performance plateauing issue and the multi-cost nature enables improving robustness at a flexible and adjustable combination of costs over different factors which can better suit specific restrictions and needs in practice. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：针对设计对抗攻击的有效防御是深层神经网络已在许多安全关键领域迅速激增，如恶意软件检测和自动驾驶汽车的一个关键话题。传统的防御方法，虽然显示是有前途，在很大程度上是由他们的单源单成本性质的限制：稳健性促销趋于平稳时，防御是由日益增强，而成本趋于放大。在本文中，我们研究设计的多源，多成本方案，其中防御性能是由多个组件卫冕提振原则。在此基础上的动机，我们提出了一种多源，多成本的防御方案，Adversarially受训模型交换（AdvMS），由两个主要方案继承优点：对抗训练和随机模型切换。我们发现，AdvMS减轻了性能停滞不前的问题，多成本自然的多源特性使得以超过其在实践中更好地满足特定的限制和需求的不同因素成本灵活可调组合改进健壮性。</font>
</div>


<hr>
<div id="paper42"> <b>42. Fine tuning U-Net for ultrasound image segmentation: which layers?</b>  <a href="https://arxiv.org/pdf/2002.08438" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Amiri%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mina Amiri</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Brooks%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rupert Brooks</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Rivaz%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hassan Rivaz</a><br>
<font size="3">
Abstract: Fine-tuning a network which has been trained on a large dataset is an alternative to full training in order to overcome the problem of scarce and expensive data in medical applications. While the shallow layers of the network are usually kept unchanged, deeper layers are modified according to the new dataset. This approach may not work for ultrasound images due to their drastically different appearance. In this study, we investigated the effect of fine-tuning different layers of a U-Net which was trained on segmentation of natural images in breast ultrasound image segmentation. Tuning the contracting part and fixing the expanding part resulted in substantially better results compared to fixing the contracting part and tuning the expanding part. Furthermore, we showed that starting to fine-tune the U-Net from the shallow layers and gradually including more layers will lead to a better performance compared to fine-tuning the network from the deep layers moving back to shallow layers. We did not observe the same results on segmentation of X-ray images, which have different salient features compared to ultrasound, it may therefore be more appropriate to fine-tune the shallow layers rather than deep layers. Shallow layers learn lower level features (including speckle pattern, and probably the noise and artifact properties) which are critical in automatic segmentation in this modality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：微调已经培训了大量的数据集是完整的训练替代，以克服在医疗应用稀缺和昂贵的数据问题的网络。虽然网络的浅层通常保持不变，更深的层根据新的数据集进行修改。这种方法可能不适合超声图像，由于其显着不同的外观正常工作。在这项研究中，我们调查的U网这是在乳腺超声图像分割自然图像的分割训练有素的微调不同层次的效果。调整收缩部和固定的膨胀部导致基本上更好的结果相比，固定收缩部和调整的膨胀部。此外，我们发现，从浅层开始微调掌中逐步包括更多的层会导致相较于微调，从深层搬回浅层网络具有更好的性能。我们没有观察X射线图像的分割同样的结果，不同的显着特征相比，超声具有，因此可能更适合于微调浅层而不是深层。浅层学习较低级特征（包括散斑图案，并且可能是噪声和假象性能），这是在该模态自动分割关键的。</font>
</div>


<hr>
<div id="paper43"> <b>43. Interactive Natural Language-based Person Search</b>  <a href="https://arxiv.org/pdf/2002.08434" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shree%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vikram Shree</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Lun Chao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mark Campbell</a><br>
<font size="3">
Abstract: In this work, we consider the problem of searching people in an unconstrained environment, with natural language descriptions. Specifically, we study how to systematically design an algorithm to effectively acquire descriptions from humans. An algorithm is proposed by adapting models, used for visual and language understanding, to search a person of interest (POI) in a principled way, achieving promising results without the need to re-design another complicated model. We then investigate an iterative question-answering (QA) strategy that enable robots to request additional information about the POI's appearance from the user. To this end, we introduce a greedy algorithm to rank questions in terms of their significance, and equip the algorithm with the capability to dynamically adjust the length of human-robot interaction according to model's uncertainty. Our approach is validated not only on benchmark datasets but on a mobile robot, moving in a dynamic and crowded environment. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们考虑搜索不受约束的环境中的人，用自然语言描述的问题。具体来说，我们研究如何系统地设计一个算法，从人类有效地获取描述。一种算法是通过调整模型提出的，用于视觉和语言的理解，有原则的方式对人的搜索兴趣点（POI），取得可喜的成果，而不需要重新设计的另一个复杂的模型。然后，我们调查的迭代答疑（QA）的战略，使机器人能够请求关于该用户的POI的外观的其他信息。为此，我们在他们的意义方面引入贪心算法来排名的问题，并装备了算法的能力，动态调整，根据模型的不确定性人类与机器人互动的长度。我们的方法是有效的，不仅在标准数据集，但在移动机器人上，在一个充满活力和拥挤的环境中移动。</font>
</div>


<hr>
<div id="paper44"> <b>44. T-Net: A Template-Supervised Network for Task-specific Feature  Extraction in Biomedical Image Analysis</b>  <a href="https://arxiv.org/pdf/2002.08406" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Song%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weinan Song</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Liang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=He%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei He</a><br>
<font size="3">
Abstract: Existing deep learning methods depend on an encoder-decoder structure to learn feature representation from the segmentation annotation in biomedical image analysis. However, the effectiveness of feature extraction under this structure decreases due to the indirect optimization process, limited training data size, and simplex supervision method. In this paper, we propose a template-supervised network T-Net for task-specific feature extraction. Specifically, we first obtain templates from pixel-level annotations by down-sampling binary masks of recognition targets according to specific tasks. Then, we directly train the encoding network under the supervision of the derived task-specific templates. Finally, we combine the resulting encoding network with a posterior network for the specific task, e.g. an up-sampling network for segmentation or a region proposal network for detection. Extensive experiments on three public datasets (BraTS-17, MoNuSeg and IDRiD) show that T-Net achieves competitive results to the state-of-the-art methods and superior performance to an encoder-decoder based network. To the best of our knowledge, this is the first in-depth study to improve feature extraction by directly supervise the encoding network and by applying task-specific supervision in biomedical image analysis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有深学习方法依赖于编码器 - 译码器结构从在生物医学图像分析中的分割注释学特征表示。然而，在此结构之下特征提取的有效性降低，由于间接优化过程中，有限的训练数据的大小，和单纯的监督方法。在本文中，我们提出了一个模板监督网络T-网任务特定的特征提取。具体而言，我们首先根据具体任务分解采样的识别目标二进掩模获得像素级的注解模板。然后，我们直接培养的衍生任务特定模板的监督下进行编码网络。最后，我们将得到的编码网络与后网络的特定任务，例如结合上采样网络分段或区域提案网络进行检测。在三个数据集的公共（臭小子-17，MoNuSeg和IDRiD）表明，T-网实现有竞争力的结果，以国家的最先进的方法和优越的性能给编码器 - 解码器基于网络广泛的实验。据我们所知，这是第一次深入研究由直接管理的编码网络，并通过在生物医学图像分析应用特定任务的监督，以提高特征提取。</font>
</div>


<hr>
<div id="paper45"> <b>45. Algorithm-hardware Co-design for Deformable Convolution</b>  <a href="https://arxiv.org/pdf/2002.08357" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title45" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Huang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qijing Huang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dequan Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Gao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yizhao Gao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cai%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaohui Cai</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dong%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Dong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wu%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bichen Wu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Keutzer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kurt Keutzer</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wawrzynek%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Wawrzynek</a><br>
<font size="3">
Abstract: FPGAs provide a flexible and efficient platform to accelerate rapidly-changing algorithms for computer vision. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, including object detection and instance segmentation, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this, recent work proposes dynamic deformable convolution to augment regular convolutions. Regular convolutions process a fixed grid of pixels across all the spatial locations in an image, while dynamic deformable convolutions may access arbitrary pixels in the image and the access pattern is input-dependent and varies per spatial location. These properties lead to inefficient memory accesses of inputs with existing hardware. In this work, we first investigate the overhead of the deformable convolution on embedded FPGA SoCs, and then show the accuracy-latency tradeoffs for a set of algorithm modifications including full versus depthwise, fixed-shape, and limited-range. These modifications benefit the energy efficiency for embedded devices in general as they reduce the compute complexity. We then build an efficient object detection network with modified deformable convolutions and quantize the network using state-of-the-art quantization methods. We implement a unified hardware engine on FPGA to support all the operations in the network. Preliminary experiments show that little accuracy is compromised and speedup can be achieved with our co-design optimization for the deformable convolution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：FPGA提供了灵活，高效的平台，以加速计算机视觉快速变化的算法。大多数现有工作的重点放在加快图像分类，而其他基本视力问题，包括目标检测和实例分割，没有得到充分解决。与图像分类相比，检测问题是到对象的空间方差更敏感，因此，需要专门的卷积到聚集空间信息。为了解决这个问题，最近的工作提出了动态变形卷积，以增加普通卷积。常规的卷积处理整个图像中的所有空间位置的像素的定格，而动态变形卷积可以访问任意的像素的图像中和所述接入模式是输入依赖性与每个空间位置而变化。这些性质导致的与现有的硬件输入低效的存储器存取。在这项工作中，我们首先调查对嵌入式FPGA的SoC可变形卷积的开销，然后显示该精度延时权衡用于一组算法的修改包括完全与深度方向，固定形状，和有限范围的。这些修改有利于在一般的嵌入式设备的能源效率，因为他们减少了计算的复杂性。然后，我们构建具有修饰的变形卷积的有效对象检测网络和使用状态的最先进的量化方法量化该网络。我们实施基于FPGA的统一硬件引擎，支持网络中的所有操作。初步实验表明，小准确性受到影响，加速可以用我们的可变形卷积协同设计优化来实现。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-21</title>
    <url>/2020/02/21/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-21/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Measuring Social Biases in Grounded Vision and Language Embeddings <a href="https://arxiv.org/pdf/2002.08911" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> How Much Knowledge Can You Pack Into the Parameters of a Language Model? <a href="https://arxiv.org/pdf/2002.08910" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> REALM: Retrieval-Augmented Language Model Pre-Training <a href="https://arxiv.org/pdf/2002.08909" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Application of Pre-training Models in Named Entity Recognition <a href="https://arxiv.org/pdf/2002.08902" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Identifying physical health comorbidities in a cohort of individuals  with severe mental illness: An application of SemEHR <a href="https://arxiv.org/pdf/2002.08901" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Compositional Neural Machine Translation by Removing the Lexicon from  Syntax <a href="https://arxiv.org/pdf/2002.08899" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> MA-DST: Multi-Attention Based Scalable Dialog State Tracking <a href="https://arxiv.org/pdf/2002.08898" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> The Fluidity of Concept Representations in Human Brain Signals <a href="https://arxiv.org/pdf/2002.08880" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Contextual Lensing of Universal Sentence Representations <a href="https://arxiv.org/pdf/2002.08866" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Guiding attention in Sequence-to-sequence models for Dialogue Act  prediction <a href="https://arxiv.org/pdf/2002.08801" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Balancing Cost and Benefit with Tied-Multi Transformers <a href="https://arxiv.org/pdf/2002.08614" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding <a href="https://arxiv.org/pdf/2002.08608" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Federated pretraining and fine tuning of BERT using clinical notes from  multiple silos <a href="https://arxiv.org/pdf/2002.08562" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Wavesplit: End-to-End Speech Separation by Speaker Clustering <a href="https://arxiv.org/pdf/2002.08933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Imputer: Sequence Modelling via Imputation and Dynamic Programming <a href="https://arxiv.org/pdf/2002.08926" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Multi-Agent Reinforcement Learning as a Computational Tool for Language  Evolution Research: Historical Context and Future Challenges <a href="https://arxiv.org/pdf/2002.08878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> How To Avoid Being Eaten By a Grue: Exploration Strategies for  Text-Adventure Agents <a href="https://arxiv.org/pdf/2002.08795" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Interactive Natural Language-based Person Search <a href="https://arxiv.org/pdf/2002.08434" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Measuring Social Biases in Grounded Vision and Language Embeddings</b>  <a href="https://arxiv.org/pdf/2002.08911" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ross%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Candace Ross</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Katz%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Boris Katz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barbu%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrei Barbu</a><br>
<font size="3">
Abstract: We generalize the notion of social biases from language embeddings to grounded vision and language embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting extending standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从一般化的嵌入语言到接地视觉和语言的嵌入社会偏见的概念。偏见存在于接地的嵌入，也确实似乎同样适用或比不接地的嵌入更显著。尽管这是一个事实，即视觉和语言可以从不同的偏见，人们可能希望能够减轻在两个偏差受到影响。多种方式存在概括的指标测量字的嵌入偏置到这个新的设置。我们引进概括的空间（接地WEAT和接地SEAT），并展示三个概括回答有关如何偏见，语言和视觉互动不同但重要的问题。这些指标都上了一个新的数据集，第一个为接地偏差，通过扩大与COCO 10,228图像，概念字幕，和谷歌图片扩展标准语言偏置基准创建使用。数据集建设是具有挑战性的，因为视觉数据集本身非常偏颇。在系统中，这些偏见的存在会开始有真实世界的后果，因为他们部署，精心制作测量偏差，然后减轻它来建立一个公平的社会是至关重要的。</font>
</div>


<hr>
<div id="paper2"> <b>2. How Much Knowledge Can You Pack Into the Parameters of a Language Model?</b>  <a href="https://arxiv.org/pdf/2002.08910" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Roberts%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adam Roberts</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Colin Raffel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Noam Shazeer</a><br>
<font size="3">
Abstract: It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近有人发现，在训练的非结构化文本神经语言模型可以隐式存储和使用自然语言查询检索知识。在这短短的文章中，我们衡量这种方法通过微调预训练模型的实用性，回答问题不访问任何外部环境或知识。我们表明，这种做法尺度出奇地好与模型的大小，优于模型，明确查找知识的开放域自然问题和WebQuestions的变体。</font>
</div>


<hr>
<div id="paper3"> <b>3. REALM: Retrieval-Augmented Language Model Pre-Training</b>  <a href="https://arxiv.org/pdf/2002.08909" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kelvin Guu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kenton Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tung%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zora Tung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pasupat%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Panupong Pasupat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Wei Chang</a><br>
<font size="3">
Abstract: Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言模型前培训已被证明是捕捉世界的知识数量惊人，对NLP任务，如问答至关重要。但是，这方面的知识是隐含存储在神经网络的参数，需要越来越大的网络，以覆盖更多的事实。要在一个更加模块化和解释的方式获取知识，我们扩充语言模型前培训与潜在的知识猎犬，它允许模型检索，并出席了由大语料库，如维基百科文档，在预培训使用，细调整和推理。这是第一次，我们将展示如何在无人监督的方式，预先培养这样的知识猎犬使用屏蔽语言建模作为学习信号，并通过一种考虑数百万文档的检索步骤backpropagating。我们展示的开放域问答系统的具有挑战性的任务（开放-QA）检索，增强语言模型的有效性前培训（REALM）的微调。我们比较反对国家的最先进的模型上的三个热门打开-QA基准显性和隐性知识的存储，并发现我们超越由显著保证金（4-16％的绝对精度）以前的所有方法，同时还提供定性的好处，如可解释性和模块化。</font>
</div>


<hr>
<div id="paper4"> <b>4. Application of Pre-training Models in Named Entity Recognition</b>  <a href="https://arxiv.org/pdf/2002.08902" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yining Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zuchang Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lisheng Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Ting Sun</a><br>
<font size="3">
Abstract: Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task to extract entities from unstructured data. The previous methods for NER were based on machine learning or deep learning. Recently, pre-training models have significantly improved performance on multiple NLP tasks. In this paper, firstly, we introduce the architecture and pre-training tasks of four common pre-training models: BERT, ERNIE, ERNIE2.0-tiny, and RoBERTa. Then, we apply these pre-training models to a NER task by fine-tuning, and compare the effects of the different model architecture and pre-training tasks on the NER task. The experiment results showed that RoBERTa achieved state-of-the-art results on the MSRA-2006 dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：命名实体识别（NER）是一个基本的自然语言处理（NLP）任务从非结构化数据提取的实体。对于NER以前的方法是基于机器学习或深度学习。近日，前培训模型已显著提高多任务NLP性能。在本文中，首先，我们介绍的架构和预培训的四种常见的预培训模式的任务：BERT，摇奖，ERNIE2.0纤巧，和罗伯塔。然后，我们将这些前培训模式，以通过微调一个NER任务，并比较不同的模型结构的影响，前培训在NER任务的任务。实验结果表明，罗伯塔实现在MSRA-2006数据集的国家的最先进的成果。</font>
</div>


<hr>
<div id="paper5"> <b>5. Identifying physical health comorbidities in a cohort of individuals  with severe mental illness: An application of SemEHR</b>  <a href="https://arxiv.org/pdf/2002.08901" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bendayan%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rebecca Bendayan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Honghan Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kraljevic%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zeljko Kraljevic</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stewart%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Stewart</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Searle%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Searle</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaturvedi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaya Chaturvedi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Das-Munshi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jayati Das-Munshi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ibrahim%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zina Ibrahim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mascio%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aurelie Mascio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roberts%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Angus Roberts</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bean%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Bean</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dobson%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Dobson</a><br>
<font size="3">
Abstract: Multimorbidity research in mental health services requires data from physical health conditions which is traditionally limited in mental health care electronic health records. In this study, we aimed to extract data from physical health conditions from clinical notes using SemEHR. Data was extracted from Clinical Record Interactive Search (CRIS) system at South London and Maudsley Biomedical Research Centre (SLaM BRC) and the cohort consisted of all individuals who had received a primary or secondary diagnosis of severe mental illness between 2007 and 2018. Three pairs of annotators annotated 2403 documents with an average Cohen's Kappa of 0.757. Results show that the NLP performance varies across different diseases areas (F1 0.601 - 0.954) suggesting that the language patterns or terminologies of different condition groups entail different technical challenges to the same NLP task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：Multimorbidity研究精神卫生服务，需要从身体健康条件，在精神卫生保健电子健康记录传统有限的数据。在这项研究中，我们的目的是提取使用SemEHR临床笔记身体健康状况的数据。数据来自于伦敦南部和莫兹利生物医学研究中心（SLAM BRC）临床记录交互式搜索（CRIS）系统中提取和对象包括谁收到了2007年和2018年三对之间的严重精神疾病的原发性或继发性诊断的所有个人的注释的注释2403个文档与0.757的平均Cohen的κ。结果表明，NLP性能在不同疾病领域（F1 0.601  -  0.954）变化提示语言模式或不同条件组的术语意味着相同的NLP任务不同的技术挑战。</font>
</div>


<hr>
<div id="paper6"> <b>6. Compositional Neural Machine Translation by Removing the Lexicon from  Syntax</b>  <a href="https://arxiv.org/pdf/2002.08899" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Thrush%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tristan Thrush</a><br>
<font size="3">
Abstract: The meaning of a natural language utterance is largely determined from its syntax and words. Additionally, there is evidence that humans process an utterance by separating knowledge about the lexicon from syntax knowledge. Theories from semantics and neuroscience claim that complete word meanings are not encoded in the representation of syntax. In this paper, we propose neural units that can enforce this constraint over an LSTM encoder and decoder. We demonstrate that our model achieves competitive performance across a variety of domains including semantic parsing, syntactic parsing, and English to Mandarin Chinese translation. In these cases, our model outperforms the standard LSTM encoder and decoder architecture on many or all of our metrics. To demonstrate that our model achieves the desired separation between the lexicon and syntax, we analyze its weights and explore its behavior when different neural modules are damaged. When damaged, we find that the model displays the knowledge distortions that aphasics are evidenced to have. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一个自然语言语句的含义主要是从它的语法和单词确定。此外，有证据表明，人类大约从语法知识的词汇知识分开处理的话语。从语义和神经科学理论要求的是完整的单词的含义在语法的表示不被编码。在本文中，我们建议可以通过一个LSTM编码器和解码器强制执行此约束的神经单元。我们表明，我们的模型在各种领域，包括语义分析，句法分析实现竞争力的性能，以及英语翻译成汉语中国的翻译。在这种情况下，我们的模型优于标准LSTM编码器和解码器架构在很多或所有的指标。为了证明我们的模型实现了词汇和语法之间所需的间隔，我们分析它的权重，并探讨其行为时，不同的神经模块损坏。当损坏，我们发现，该模型显示知识的扭曲是失语症患者被证实有。</font>
</div>


<hr>
<div id="paper7"> <b>7. MA-DST: Multi-Attention Based Scalable Dialog State Tracking</b>  <a href="https://arxiv.org/pdf/2002.08898" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adarsh Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ku%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Ku</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+A+K" target="_blank" rel="noopener" style="color:#0000EE;">Anuj Kumar Goyal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Metallinou%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Angeliki Metallinou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hakkani-Tur%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dilek Hakkani-Tur</a><br>
<font size="3">
Abstract: Task oriented dialog agents provide a natural language interface for users to complete their goal. Dialog State Tracking (DST), which is often a core component of these systems, tracks the system's understanding of the user's goal throughout the conversation. To enable accurate multi-domain DST, the model needs to encode dependencies between past utterances and slot semantics and understand the dialog context, including long-range cross-domain references. We introduce a novel architecture for this task to encode the conversation history and slot semantics more robustly by using attention mechanisms at multiple granularities. In particular, we use cross-attention to model relationships between the context and slots at different semantic levels and self-attention to resolve cross-domain coreferences. In addition, our proposed architecture does not rely on knowing the domain ontologies beforehand and can also be used in a zero-shot setting for new domains or unseen slot values. Our model improves the joint goal accuracy by 5% (absolute) in the full-data setting and by up to 2% (absolute) in the zero-shot setting over the present state-of-the-art on the MultiWoZ 2.1 dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于任务的对话框代理商提供自然语言界面，供用户完成他们的目标。对话状态跟踪（DST），这往往是这些系统的核心部件，追踪整个谈话的用户目标系统的理解。为了能够精确的多域DST，过去的话语和狭槽之间的语义模型需要编码的依赖关系和理解对话上下文，包括远程跨域引用。我们引入新的架构，这个任务通过使用注意机制在多粒度编码的对话历史和插槽语义更有力。特别是，我们使用交叉注意在不同的语义水平和自我关注的背景和插槽之间的关系进行建模来解决跨域coreferences。此外，我们提出的架构不依赖于知道领域本体事前也可以在零射门设置新的域或看不见的槽值使用。我们的模型通过在全数据设定5％（绝对值）提高了联合目标精度和由高达2％（绝对值）在零拍设置在存在于MultiWoZ 2.1数据集状态的最先进的。</font>
</div>


<hr>
<div id="paper8"> <b>8. The Fluidity of Concept Representations in Human Brain Signals</b>  <a href="https://arxiv.org/pdf/2002.08880" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hendrikx%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eva Hendrikx</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Beinborn%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lisa Beinborn</a><br>
<font size="3">
Abstract: Cognitive theories of human language processing often distinguish between concrete and abstract concepts. In this work, we analyze the discriminability of concrete and abstract concepts in fMRI data using a range of analysis methods. We find that the distinction can be decoded from the signal with an accuracy significantly above chance, but it is not found to be a relevant structuring factor in clustering and relational analyses. From our detailed comparison, we obtain the impression that human concept representations are more fluid than dichotomous categories can capture. We argue that fluid concept representations lead to more realistic models of human language processing because they better capture the ambiguity and underspecification present in natural language use. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类语言处理的认知理论往往具象与抽象的概念区分。在这项工作中，我们分析了具体的和抽象的概念，fMRI数据使用一系列的分析方法可辨。我们发现，区分可从显著上述机会的精度信号进行解码，但不认为是在集群和关联分析相关的结构因素。从我们的详细的对比，我们得到的印象是，人的概念表示比二分类别可以捕捉更多的流体。我们认为，流体概念表示导致人类语言处理，以更现实的模型，因为他们更好地捕捉模糊和标示不足出现在自然语言使用。</font>
</div>


<hr>
<div id="paper9"> <b>9. Contextual Lensing of Universal Sentence Representations</b>  <a href="https://arxiv.org/pdf/2002.08866" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiros%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jamie Kiros</a><br>
<font size="3">
Abstract: What makes a universal sentence encoder universal? The notion of a generic encoder of text appears to be at odds with the inherent contextualization and non-permanence of language use in a dynamic world. However, mapping sentences into generic fixed-length vectors for downstream similarity and retrieval tasks has been fruitful, particularly for multilingual applications. How do we manage this dilemma? In this work we propose Contextual Lensing, a methodology for inducing context-oriented universal sentence vectors. We break the construction of universal sentence vectors into a core, variable length, sentence matrix representation equipped with an adaptable `lens' from which fixed-length vectors can be induced as a function of the lens context. We show that it is possible to focus notions of language similarity into a small number of lens parameters given a core universal matrix representation. For example, we demonstrate the ability to encode translation similarity of sentences across several languages into a single weight matrix, even when the core encoder has not seen parallel data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：是什么让一个普遍的一句编码器通用？文本的通用编码器的概念似乎是在与固有的语境和语言运用的非永久性在这个瞬息万变的世界的赔率。然而，映射到句子下游相似度和检索任务的通用固定长度矢量取得了成果，特别是用于多语言应用程序。我们如何管理这个难题呢？在这项工作中，我们提出语境伦辛，诱导面向环境的万能句子向量的方法。我们打破普遍句子矢量成芯，可变长度，句子矩阵表示装备有从该固定长度矢量可以诱导作为透镜上下文的功能的适应性`透镜的结构。我们表明，它可能集中语言相似的概念变成少数赋予了核心的通用矩阵表示镜头参数。例如，我们证明能力跨越多种语言的句子翻译编码相似度成一个单一的权重矩阵，即使在核心编码器还没有见过的并行数据。</font>
</div>


<hr>
<div id="paper10"> <b>10. Guiding attention in Sequence-to-sequence models for Dialogue Act  prediction</b>  <a href="https://arxiv.org/pdf/2002.08801" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Colombo%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre Colombo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chapuis%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emile Chapuis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manica%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matteo Manica</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vignon%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emmanuel Vignon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Varni%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giovanna Varni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clavel%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chloe Clavel</a><br>
<font size="3">
Abstract: The task of predicting dialog acts (DA) based on conversational dialog is a key component in the development of conversational agents. Accurately predicting DAs requires a precise modeling of both the conversation and the global tag dependencies. We leverage seq2seq approaches widely adopted in Neural Machine Translation (NMT) to improve the modelling of tag sequentiality. Seq2seq models are known to learn complex global dependencies while currently proposed approaches using linear conditional random fields (CRF) only model local tag dependencies. In this work, we introduce a seq2seq model tailored for DA classification using: a hierarchical encoder, a novel guided attention mechanism and beam search applied to both training and inference. Compared to the state of the art our model does not require handcrafted features and is trained end-to-end. Furthermore, the proposed approach achieves an unmatched accuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on MRDA. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预测基于会话对话对话框行为（DA）的任务是在对话代理商发展的重要组成部分。准确预测的DA需要对话和全局变量依赖两者的精确建模。我们充分利用seq2seq办法在神经机器翻译（NMT）广泛采用，以提高标签的顺序性的造型。 Seq2seq型号，会同时使用线性条件随机场（CRF）只有模型局部变量的依赖目前提出的方法去学习复杂的全球性依赖。在这项工作中，我们介绍了使用DA分类量身打造的一款seq2seq模式：分层编码器，一种新型的注意力引导机制和束搜索应用到训练和推理。与现有技术相比我们的模型不需要手工的特性和训练端至端的状态。此外，所提出的方法达到的85％的上SWDA不匹配的准确度得分，并在MRDA状态的最先进的准确度得分的91.6％。</font>
</div>


<hr>
<div id="paper11"> <b>11. Balancing Cost and Benefit with Tied-Multi Transformers</b>  <a href="https://arxiv.org/pdf/2002.08614" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dabre%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raj Dabre</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rubino%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raphael Rubino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fujita%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Atsushi Fujita</a><br>
<font size="3">
Abstract: We propose and evaluate a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. In sequence-to-sequence modeling, typically, the output of the last layer of the N-layer encoder is fed to the M-layer decoder, and the output of the last decoder layer is used to compute loss. Instead, our method computes a single loss consisting of NxM losses, where each loss is computed from the output of one of the M decoder layers connected to one of the N encoder layers. Such a model subsumes NxM models with different number of encoder and decoder layers, and can be used for decoding with fewer than the maximum number of encoder and decoder layers. We then propose a mechanism to choose a priori the number of encoder and decoder layers for faster decoding, and also explore recurrent stacking of layers and knowledge distillation for model compression. We present a cost-benefit analysis of applying the proposed approaches for neural machine translation and show that they reduce decoding costs while preserving translation quality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出并评估用于训练多个变压器捆绑带参数的新的方法来压缩多个模型为一个能够在解码期间动态选择的编码器和译码器层的数目。在序列到序列建模，通常，N型层编码器的最后一层的输出被馈送到M-层解码器，最后解码器层的输出被用来计算损失。相反，我们的方法计算的单个损失由N×M个损失，其中每个损失从连接到所述N个编码器层中的一个的M个译码器层中的一个的输出来计算。这样的模型涵括N×M个具有不同数量的编码器和译码器层模型，并且可以被用于具有比的编码器和译码器层的最大数目更少的解码。然后，我们提出了一种机制来选择的编码器和解码器层的先验数更快解码，并且还探讨了层和知识蒸馏模型压缩的反复堆叠。我们提出申请神经机器翻译所提出的方案的成本效益分析和展示，同时保持翻译质量，他们降低成本的解码。</font>
</div>


<hr>
<div id="paper12"> <b>12. FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding</b>  <a href="https://arxiv.org/pdf/2002.08608" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kwak%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haewoon Kwak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=An%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jisun An</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ahn%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong-Yeol Ahn</a><br>
<font size="3">
Abstract: We propose FrameAxis, a method of characterizing the framing of a given text by identifying the most relevant semantic axes ("microframes") defined by antonym word pairs. In contrast to the traditional framing analysis, which has been constrained by a small number of manually annotated general frames, our unsupervised approach provides much more detailed insights, by considering a host of semantic axes. Our method is capable of quantitatively teasing out framing bias -- how biased a text is in each microframe -- and framing intensity -- how much each microframe is used -- from the text, offering a nuanced characterization of framing. We evaluate our approach using SemEval datasets as well as three other datasets and human evaluations, demonstrating that FrameAxis can reliably characterize documents with relevant microframes. Our method may allow scalable and nuanced computational analyses of framing across disciplines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出FrameAxis，通过识别反义词词对定义的最相关的语义轴（“微帧”）表征给定文本的框架的方法。相较于传统的框架分析，这已经限制由少数手动注释一般框架，我们的无监督的方法提供更详细的见解，通过考虑语义轴的主机。我们的方法是能够定量地梳理出成帧偏压 - 如何偏置的文本是在每个微帧 - 和成帧强度 - 每个微帧使用了多少 - 从所述文本，提供成帧的细致入微的表征。我们使用的数据集SemEval以及其他三个数据集和人的评价，这表明FrameAxis能够可靠地表征相关微帧文件评估我们的做法。我们的方法可以允许跨学科框架的可扩展性和细致入微的计算分析。</font>
</div>


<hr>
<div id="paper13"> <b>13. Federated pretraining and fine tuning of BERT using clinical notes from  multiple silos</b>  <a href="https://arxiv.org/pdf/2002.08562" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dianbo Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Miller%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tim Miller</a><br>
<font size="3">
Abstract: Large scale contextual representation models, such as BERT, have significantly advanced natural language processing (NLP) in recently years. However, in certain area like healthcare, accessing diverse large scale text data from multiple institutions is extremely challenging due to privacy and regulatory reasons. In this article, we show that it is possible to both pretrain and fine tune BERT models in a federated manner using clinical texts from different silos without moving the data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型上下文表示模型，如BERT，在最近几年有显著先进的自然语言处理（NLP）。然而，在某些领域诸如医疗保健，访问来自多个机构不同的大规模文本数据极为由于隐私和管理上的原因挑战。在这篇文章中，我们表明，它使用来自不同筒仓临床文本，而无需移动数据可能既pretrain和微调BERT模型以联合方式。</font>
</div>


<hr>
<div id="paper14"> <b>14. Wavesplit: End-to-End Speech Separation by Speaker Clustering</b>  <a href="https://arxiv.org/pdf/2002.08933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zeghidour%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Neil Zeghidour</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Grangier%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Grangier</a><br>
<font size="3">
Abstract: We introduce Wavesplit, an end-to-end speech separation system. From a single recording of mixed speech, the model infers and clusters representations of each speaker and then estimates each source signal conditioned on the inferred representations. The model is trained on the raw waveform to jointly perform the two tasks. Our model infers a set of speaker representations through clustering, which addresses the fundamental permutation problem of speech separation. Moreover, the sequence-wide speaker representations provide a more robust separation of long, challenging sequences, compared to previous approaches. We show that Wavesplit outperforms the previous state-of-the-art on clean mixtures of 2 or 3 speakers (WSJ0-2mix, WSJ0-3mix), as well as in noisy (WHAM!) and reverberated (WHAMR!) conditions. As an additional contribution, we further improve our model by introducing online data augmentation for separation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：介绍Wavesplit，最终到终端的语音分离系统。从混合的语音的单个记录，该模型推断和每个扬声器的簇表示，然后估计空调所述推断表示每个源信号。该模型被训练在原始波形，共同执行两项任务。我们的模型推断通过集群一组扬声器表示，这解决了语音分离的根本置换问题。此外，序列范围的扬声器表示提供长，挑战序列的更稳健的分离，比以前的方法。我们发现，Wavesplit优于上（WSJ0-2mix，WSJ0-3mix），以及在嘈杂的（开个唱！）和回荡的2个或3扬声器清洁混合物以前的国家的最先进的（WHAMR！）的条件。作为一个额外的贡献，我们进一步提高通过引入在线数据扩张分离我们的模型。</font>
</div>


<hr>
<div id="paper15"> <b>15. Imputer: Sequence Modelling via Imputation and Dynamic Programming</b>  <a href="https://arxiv.org/pdf/2002.08926" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Chan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Chan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Saharia%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chitwan Saharia</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hinton%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geoffrey Hinton</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Norouzi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Norouzi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Jaitly%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Navdeep Jaitly</a><br>
<font size="3">
Abstract: This paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generative model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了Imputer，经由插补迭代地生成输出序列的神经序列的模型。所述Imputer是一个迭代生成模型，只需要独立的输入或输出的令牌的数量的生成步骤常数。的Imputer可以训练大约边缘化在输入和输出序列，以及所有可能的代级之间的所有可能的比对。我们提出了一个听话的动态编程训练算法，这将产生一个下界日志边际可能性。当应用到终端到终端的语音识别，在Imputer优于以前的非自回归模型，并实现有竞争力的结果自回归模型。上LibriSpeech测试其他的Imputer达到11.1 WER，在12.5 WER优于CTC在13.0 WER和seq2seq。</font>
</div>


<hr>
<div id="paper16"> <b>16. Multi-Agent Reinforcement Learning as a Computational Tool for Language  Evolution Research: Historical Context and Future Challenges</b>  <a href="https://arxiv.org/pdf/2002.08878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Moulin-Frier%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Clément Moulin-Frier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oudeyer%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre-Yves Oudeyer</a><br>
<font size="3">
Abstract: Computational models of emergent communication in agent populations are currently gaining interest in the machine learning community due to recent advances in Multi-Agent Reinforcement Learning (MARL). Current contributions are however still relatively disconnected from the earlier theoretical and computational literature aiming at understanding how language might have emerged from a prelinguistic substance. The goal of this paper is to position recent MARL contributions within the historical context of language evolution research, as well as to extract from this theoretical and computational background a few challenges for future research. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在代理群体应急通信的计算模型，目前获得在机器学习领域的兴趣是由于多Agent强化学习（MARL）的最新进展。当前贡献但是还是比较从早期的理论和计算文学，旨在了解语言如何可能从一个前语言的物质出现断开。本文的目标是从这个理论和计算背景为今后的研究一些挑战定位到提取语言进化研究的历史范围内最近MARL的贡献，以及。</font>
</div>


<hr>
<div id="paper17"> <b>17. How To Avoid Being Eaten By a Grue: Exploration Strategies for  Text-Adventure Agents</b>  <a href="https://arxiv.org/pdf/2002.08795" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ammanabrolu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prithviraj Ammanabrolu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tien%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ethan Tien</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhaochen Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Riedl%2C+M+O" target="_blank" rel="noopener" style="color:#0000EE;">Mark O. Riedl</a><br>
<font size="3">
Abstract: Text-based games -- in which an agent interacts with the world through textual natural language -- present us with the problem of combinatorially-sized action-spaces. Most current reinforcement learning algorithms are not capable of effectively handling such a large number of possible actions per turn. Poor sample efficiency, consequently, results in agents that are unable to pass bottleneck states, where they are unable to proceed because they do not see the right action sequence to pass the bottleneck enough times to be sufficiently reinforced. Building on prior work using knowledge graphs in reinforcement learning, we introduce two new game state exploration strategies. We compare our exploration strategies against strong baselines on the classic text-adventure game, Zork1, where prior agent have been unable to get past a bottleneck where the agent is eaten by a Grue. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于文本的游戏 - 其中通过文本自然语言与世界的代理进行交互 - 现在我们有组合地大小的动作空间的问题。目前大多数的强化学习算法不能够有效地处理如此大量的每回合可能采取的行动。可怜样的效率，因此，在那些无法通过瓶颈状态，他们不能继续进行，因为他们看不到正确的行动顺序传递瓶颈足够的时间药的结果进行充分加固。使用知识图中强化学习之前工作的基础上，我们引入了两个新的游戏状态探索的战略。我们将我们的勘探战略对抗的经典文字冒险游戏，Zork1，其中前代理已经无法让过去在代理被怪兽吃掉的瓶颈强大的基线。</font>
</div>


<hr>
<div id="paper18"> <b>18. Interactive Natural Language-based Person Search</b>  <a href="https://arxiv.org/pdf/2002.08434" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shree%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vikram Shree</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Lun Chao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Campbell%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mark Campbell</a><br>
<font size="3">
Abstract: In this work, we consider the problem of searching people in an unconstrained environment, with natural language descriptions. Specifically, we study how to systematically design an algorithm to effectively acquire descriptions from humans. An algorithm is proposed by adapting models, used for visual and language understanding, to search a person of interest (POI) in a principled way, achieving promising results without the need to re-design another complicated model. We then investigate an iterative question-answering (QA) strategy that enable robots to request additional information about the POI's appearance from the user. To this end, we introduce a greedy algorithm to rank questions in terms of their significance, and equip the algorithm with the capability to dynamically adjust the length of human-robot interaction according to model's uncertainty. Our approach is validated not only on benchmark datasets but on a mobile robot, moving in a dynamic and crowded environment. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们考虑搜索不受约束的环境中的人，用自然语言描述的问题。具体来说，我们研究如何系统地设计一个算法，从人类有效地获取描述。一种算法是通过调整模型提出的，用于视觉和语言的理解，有原则的方式对人的搜索兴趣点（POI），取得可喜的成果，而不需要重新设计的另一个复杂的模型。然后，我们调查的迭代答疑（QA）的战略，使机器人能够请求关于该用户的POI的外观的其他信息。为此，我们在他们的意义方面引入贪心算法来排名的问题，并装备了算法的能力，动态调整，根据模型的不确定性人类与机器人互动的长度。我们的方法是有效的，不仅在标准数据集，但在移动机器人上，在一个充满活力和拥挤的环境中移动。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-20</title>
    <url>/2020/02/20/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-20/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Compressing BERT: Studying the Effects of Weight Pruning on Transfer  Learning <a href="https://arxiv.org/pdf/2002.08307" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection  and Sentiment Analysis in Conversation <a href="https://arxiv.org/pdf/2002.08267" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> CodeBERT: A Pre-Trained Model for Programming and Natural Languages <a href="https://arxiv.org/pdf/2002.08155" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Hierarchical models vs. transfer learning for document-level sentiment  classification <a href="https://arxiv.org/pdf/2002.08131" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Rnn-transducer with language bias for end-to-end Mandarin-English  code-switching speech recognition <a href="https://arxiv.org/pdf/2002.08126" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> LAMBERT: Layout-Aware language Modeling using BERT for information  extraction <a href="https://arxiv.org/pdf/2002.08087" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Toward Making the Most of Context in Neural Machine Translation <a href="https://arxiv.org/pdf/2002.07982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural  Language Understanding <a href="https://arxiv.org/pdf/2002.07972" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Studying the Effects of Cognitive Biases in Evaluation of Conversational  Agents <a href="https://arxiv.org/pdf/2002.07927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Transfer Learning for Abstractive Summarization at Controllable Budgets <a href="https://arxiv.org/pdf/2002.07845" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> VQA-LOL: Visual Question Answering under the Lens of Logic <a href="https://arxiv.org/pdf/2002.08325" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> A Differential-form Pullback Programming Language for Higher-order  Reverse-mode Automatic Differentiation <a href="https://arxiv.org/pdf/2002.08241" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Tree-structured Attention with Hierarchical Accumulation <a href="https://arxiv.org/pdf/2002.08046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Non-Autoregressive Dialog State Tracking <a href="https://arxiv.org/pdf/2002.08024" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Compressing BERT: Studying the Effects of Weight Pruning on Transfer  Learning</b>  <a href="https://arxiv.org/pdf/2002.08307" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gordon%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Mitchell A. Gordon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duh%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Duh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Andrews%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicholas Andrews</a><br>
<font size="3">
Abstract: Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40\%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：通用特征提取，如BERT自然语言处理和VGG计算机视觉，已成为提高深度学习模型有效的方法，而不需要更多的标签数据。一种常见的范例是大量数据的预培养特征提取再微调它作为部分下游任务（即转印学习）一个深度学习模型的一部分。虽然有效，像BERT特征提取可能是一些部署场景大得惊人。我们探讨重修剪的BERT，问：如何在训练前压缩影响迁移学习？我们发现，修剪影响迁移学习在三大制度。修剪（30-40 \％）水平低，不影响训练前的损失或转移到下游的所有任务。修剪增加预训练损失和中等水平的防止被传递到下游的任务有用预训练信息。修剪的高水平还防止下游拟合模型的数据集，从而导致进一步恶化。最后，我们观察到一个特定的任务，微调BERT并不能改善其prunability。我们的结论是BERT可以在一次训练前，而不是单独为在不影响性能的每个任务被修剪。</font>
</div>


<hr>
<div id="paper2"> <b>2. Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection  and Sentiment Analysis in Conversation</b>  <a href="https://arxiv.org/pdf/2002.08267" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shenoy%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aman Shenoy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sardana%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashish Sardana</a><br>
<font size="3">
Abstract: Sentiment Analysis and Emotion Detection in conversation is key in a number of real-world applications, with different applications leveraging different kinds of data to be able to achieve reasonably accurate predictions. Multimodal Emotion Detection and Sentiment Analysis can be particularly useful as applications will be able to use specific subsets of the available modalities, as per their available data, to be able to produce relevant predictions. Current systems dealing with Multimodal functionality fail to leverage and capture the context of the conversation through all modalities, the current speaker and listener(s) in the conversation, and the relevance and relationship between the available modalities through an adequate fusion mechanism. In this paper, we propose a recurrent neural network architecture that attempts to take into account all the mentioned drawbacks, and keeps track of the context of the conversation, interlocutor states, and the emotions conveyed by the speakers in the conversation. Our proposed model out performs the state of the art on two benchmark datasets on a variety of accuracy and regression metrics. Our model implementation is public and can be found at this http URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：情感分析和情感检测的谈话是在许多现实世界的应用程序的关键，与不同的应用程序利用不同类型的数据，才能够达到相当准确的预测。多式联运情绪检测和情感分析可能是特别有用的应用程序将能够使用现有的模式的特定子集，根据他们掌握的数据，要能出示相关的预测。处理多式联运功能的当前系统未能充分利用，并通过一切方式，通过适当的融合机制可用模式之间的电流扬声器和在交谈监听器（S）和相关性和关系捕捉谈话的背景。在本文中，我们提出了一个经常性的神经网络结构，试图考虑到所有的上述缺点，并跟踪谈话中，对话者状态的背景下，与情绪传达在谈话扬声器。我们提出的模型进行了艺术上的两个标准数据集上的各种准确性和回归度量的状态。我们的模型的实现是公共的，可以在这个HTTP URL中找到</font>
</div>


<hr>
<div id="paper3"> <b>3. CodeBERT: A Pre-Trained Model for Programming and Natural Languages</b>  <a href="https://arxiv.org/pdf/2002.08155" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhangyin Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daya Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duyu Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaocheng Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shou%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linjun Shou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bing Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Ting Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daxin Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出CodeBERT，用于编程语言（PL）和NAT  - 乌拉尔语言（NL）双峰预训练模式。 CodeBERT得知支持下游NL-PL应用，如自然语言于codesearch，代码文档生成等通用表示我们开发CodeBERT与基于变压器的神经结构，并使用并入前培训任务的混合目标函数训练它的替换令牌检测，这是为了检测来自发电机采样可行的替代品。这使我们能够利用NL-PL对和单峰数据，其中前者提供输入令牌模型训练，而后者有助于更好地学习发电机双模的数据。我们评估通过微调模型参数的两个NL-PL应用CodeBERT。结果表明，CodeBERT实现两个自然语言代码搜索和代码文档生成任务的国家的最先进的性能。此外，调查什么的知识型CodeBERT了解，我们构建NL-PL探测的数据集，并在零射门的设置，其中的预先训练模型的参数是固定的评价。结果表明，CodeBERT执行比NL-PL探测之前的预先训练模式更好。</font>
</div>


<hr>
<div id="paper4"> <b>4. Hierarchical models vs. transfer learning for document-level sentiment  classification</b>  <a href="https://arxiv.org/pdf/2002.08131" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Barnes%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeremy Barnes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ravishankar%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vinit Ravishankar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=%C3%98vrelid%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lilja Øvrelid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Velldal%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erik Velldal</a><br>
<font size="3">
Abstract: Documents are composed of smaller pieces - paragraphs, sentences, and tokens - that have complex relationships between one another. Sentiment classification models that take into account the structure inherent in these documents have a theoretical advantage over those that do not. At the same time, transfer learning models based on language model pretraining have shown promise for document classification. However, these two paradigms have not been systematically compared and it is not clear under which circumstances one approach is better than the other. In this work we empirically compare hierarchical models and transfer learning for document-level sentiment classification. We show that non-trivial hierarchical models outperform previous baselines and transfer learning on document-level sentiment classification in five languages. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文件是由更小的部分 - 段落，句子和令牌 - 即具有彼此之间的复杂关系。考虑到这些文件中所固有的结构情感分类模型具有理论上的优势那些不这样做。与此同时，基于语言模型训练前转让的学习模式已经显示了文档分类承诺。然而，这两种模式都没有得到系统的比较，目前尚不清楚在何种情况下一个方法是优于其他。在这项工作中，我们经验比较分层模型和转移学习文档级情感分类。我们证明了不平凡的分层模型中五种语言胜过以前的基线和迁移学习的文档级情感分类。</font>
</div>


<hr>
<div id="paper5"> <b>5. Rnn-transducer with language bias for end-to-end Mandarin-English  code-switching speech recognition</b>  <a href="https://arxiv.org/pdf/2002.08126" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuai Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiangyan Yi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengkun Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianhua Tao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ye Bai</a><br>
<font size="3">
Abstract: Recently, language identity information has been utilized to improve the performance of end-to-end code-switching (CS) speech recognition. However, previous works use an additional language identification (LID) model as an auxiliary module, which causes the system complex. In this work, we propose an improved recurrent neural network transducer (RNN-T) model with language bias to alleviate the problem. We use the language identities to bias the model to predict the CS points. This promotes the model to learn the language identity information directly from transcription, and no additional LID model is needed. We evaluate the approach on a Mandarin-English CS corpus SEAME. Compared to our RNN-T baseline, the proposed method can achieve 16.2% and 12.9% relative error reduction on two test sets, respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，语言身份信息已被用来提高终端到终端的码转换（CS）语音识别的性能。然而，先前的工作使用一个额外的语言识别（LID）模型作为辅助模块，这将导致系统复杂。在这项工作中，我们提出了一种改进的递归神经网络转换器（RNN-T）模型的语言偏缓解这一问题。我们使用的语言身份偏置模型来预测CS点。这促进了模型直接从转录学习语言的身份信息，并且不需要额外的LID模型。我们评估对国语，英语语料库CS的SEAME方法。相比，我们RNN-T基线，所提出的方法可以在两个测试组达到了16.2％和相对误差减少12.9％，分别。</font>
</div>


<hr>
<div id="paper6"> <b>6. LAMBERT: Layout-Aware language Modeling using BERT for information  extraction</b>  <a href="https://arxiv.org/pdf/2002.08087" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Garncarek%2C+%C5%81" target="_blank" rel="noopener" style="color:#0000EE;">Łukasz Garncarek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Powalski%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rafał Powalski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stanis%C5%82awek%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomasz Stanisławek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Topolski%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bartosz Topolski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Halama%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Halama</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grali%C5%84ski%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Filip Graliński</a><br>
<font size="3">
Abstract: In this paper we introduce a novel approach to the problem of understanding documents where the local semantics is influenced by non-trivial layout. Namely, we modify the Transformer architecture in a way that allows it to use the graphical features defined by the layout, without the need to re-learn the language semantics from scratch, thanks to starting the training process from a model pretrained on classical language modeling tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了一种新的方法来在当地的语义由非平凡的布局影响理解文件的问题。也就是说，我们修改的方式，允许它使用由布局定义的图形功能的变压器架构，而无需再从头学习语言的语义，得益于预训练的经典语言模型的模型开始训练过程任务。</font>
</div>


<hr>
<div id="paper7"> <b>7. Toward Making the Most of Context in Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.07982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zaixiang Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yue%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Yue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shujian Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiajun Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Birch%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandra Birch</a><br>
<font size="3">
Abstract: Document-level machine translation manages to outperform sentence level models by a small margin, but have failed to be widely adopted. We argue that previous research did not make a clear use of the global context, and propose a new document-level NMT framework that deliberately models the local context of each sentence with the awareness of the global context of the document in both source and target languages. We specifically design the model to be able to deal with documents containing any number of sentences, including single sentences. This unified approach allows our model to be trained elegantly on standard datasets without needing to train on sentence and document level data separately. Experimental results demonstrate that our model outperforms Transformer baselines and previous document-level NMT models with substantial margins of up to 2.1 BLEU on state-of-the-art baselines. We also provide analyses which show the benefit of context far beyond the neighboring two or three sentences, which previous studies have typically incorporated. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文档级机器翻译小幅设法跑赢句子级车型，但一直没有被广泛采用。我们认为，以前的研究并没有做出明确的利用全球范围内的，并提出一个新的文档级NMT框架，故意模型每个句子的本地环境与文档的全球范围内的源和目标语言意识。我们专门设计的模型，以便能够处理包含任意数量的句子，包括单句文件。这种统一的方法允许我们的模型中优雅的标准数据集，而不需要对句子和文件级数据分开训练训练。实验结果表明，我们的模型优于变压器基线和以前的文件级别的车型NMT高达2.1 BLEU在国家的最先进的基线的实质性的利润。我们还提供分析，这表明上下文的好处远远超过了相邻的两个或三个句子，其先前的研究已经纳入一般。</font>
</div>


<hr>
<div id="paper8"> <b>8. The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural  Language Understanding</b>  <a href="https://arxiv.org/pdf/2002.07972" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaodong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianshu Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xueyun Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Awa%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emmanuel Awa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengcheng He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weizhu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poon%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hoifung Poon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guihong Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Gao</a><br>
<font size="3">
Abstract: We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pre-trained models will be publicly available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前MT-DNN，一个开源的自然语言理解（NLU）工具包，便于研究人员和开发人员以培训定制的深度学习模式。建立在PyTorch和变压器，MT-DNN是为了便于快速地定制的NLU任务广谱性，利用各种目标（分类，回归，结构预测）和文本编码器（例如，RNNs，BERT，罗伯塔，UniLM） 。 MT-DNN的一大特色是其采用对抗性多任务学习的范例内置的强大的，可转让的学习支持。为了实现高效生产部署，MT-DNN支持多任务的知识蒸馏，这可以大大压缩深层神经模型，而不会显著性能下降。我们证明了广泛的跨越一般和生物医学领域的应用NLU MT-DNN的有效性。软件和预训练的模型将公开可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper9"> <b>9. Studying the Effects of Cognitive Biases in Evaluation of Conversational  Agents</b>  <a href="https://arxiv.org/pdf/2002.07927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Santhanam%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sashank Santhanam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Karduni%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alireza Karduni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shaikh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samira Shaikh</a><br>
<font size="3">
Abstract: Humans quite frequently interact with conversational agents. The rapid advancement in generative language modeling through neural networks has helped advance the creation of intelligent conversational agents. Researchers typically evaluate the output of their models through crowdsourced judgments, but there are no established best practices for conducting such studies. Moreover, it is unclear if cognitive biases in decision-making are affecting crowdsourced workers' judgments when they undertake these tasks. To investigate, we conducted a between-subjects study with 77 crowdsourced workers to understand the role of cognitive biases, specifically anchoring bias, when humans are asked to evaluate the output of conversational agents. Our results provide insight into how best to evaluate conversational agents. We find increased consistency in ratings across two experimental conditions may be a result of anchoring bias. We also determine that external factors such as time and prior experience in similar tasks have effects on inter-rater consistency. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类相当频繁与会话代理交互。通过神经网络生成的语言模型的快速进步帮助推动智能会话代理的创建。研究人员通常通过评估判断众包他们的模型的输出，但也有进行这样的研究没有建立的最佳实践。此外，如果在决策中的认知偏差是当他们承担这些任务的影响众包工人的判断，目前尚不清楚。调查中，我们与77名众包的工人被试间研究，以了解认知偏差的作用，特别是固定偏差，当人类被要求评价会话代理的输出。我们的研究结果提供深入了解如何最好地评估会话代理。我们发现，在收视率在两个实验条件增加稠度可能是锚定偏差的结果。我们还确定外部因素，如类似任务的时间和以前的经验对评估人之间的一致性效果。</font>
</div>


<hr>
<div id="paper10"> <b>10. Transfer Learning for Abstractive Summarization at Controllable Budgets</b>  <a href="https://arxiv.org/pdf/2002.07845" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sarkhel%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ritesh Sarkhel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keymanesh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Moniba Keymanesh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nandi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arnab Nandi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Parthasarathy%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Srinivasan Parthasarathy</a><br>
<font size="3">
Abstract: Summarizing a document within an allocated budget while maintaining its major concepts is a challenging task. If the budget can take any arbitrary value and not known beforehand, it becomes even more difficult. Most of the existing methods for abstractive summarization, including state-of-the-art neural networks are data intensive. If the number of available training samples becomes limited, they fail to construct high-quality summaries. We propose MLS, an end-to-end framework to generate abstractive summaries with limited training data at arbitrary compression budgets. MLS employs a pair of supervised sequence-to-sequence networks. The first network called the \textit{MFS-Net} constructs a minimal feasible summary by identifying the key concepts of the input document. The second network called the Pointer-Magnifier then generates the final summary from the minimal feasible summary by leveraging an interpretable multi-headed attention model. Experiments on two cross-domain datasets show that MLS outperforms baseline methods over a range of success metrics including ROUGE and METEOR. We observed an improvement of approximately 4% in both metrics over the state-of-art convolutional network at lower budgets. Results from a human evaluation study also establish the effectiveness of MLS in generating complete coherent summaries at arbitrary compression budgets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：分配的预算范围内总结的文档，同时保持它的主要概念是一个具有挑战性的任务。如果预算可以采取任意值，而不是事先知道，它变得更加困难。大多数用于抽象总结现有方法，包括国家的最先进的神经网络是密集的数据。如果可用的训练样本的数量变得有限，他们无法构建高品质的摘要。我们建议MLS，最终到终端的架构产生与任意压缩预算有限的训练数据抽象总结。 MLS采用一对监督序列到序列的网络。称为\ textit {MFS-Net的}第一网络通过识别输入文档的关键概念构建了一个最小的可行摘要。称为指针-放大镜第二网络然后通过利用可解释的多头注意模型生成从最小可行总结最后汇总。在两个跨域数据集实验表明，MLS性能优于在一定范围的成功指标包括胭脂METEOR的基线方法。我们在较低的预算观察到大约4％的在两个指标在国家的技术卷积网络的改进。从人的评估研究结果还建立在任意压缩预算产生完整连贯摘要MLS的有效性。</font>
</div>


<hr>
<div id="paper11"> <b>11. VQA-LOL: Visual Question Answering under the Lens of Logic</b>  <a href="https://arxiv.org/pdf/2002.08325" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gokhale%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tejas Gokhale</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pratyay Banerjee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baral%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chitta Baral</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yezhou Yang</a><br>
<font size="3">
Abstract: Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate visual question answering (VQA) through the lens of logical transformation and posit that systems that seek to answer questions about images must be robust to these transformations of the question. If a VQA system is able to answer a question, it should also be able to answer the logical composition of questions. We analyze the performance of state-of-the-art models on the VQA task under these logical operations and show that they have difficulty in correctly answering such questions. We then construct an augmentation of the VQA dataset with questions containing logical operations and retrain the same models to establish a baseline. We further propose a novel methodology to train models to learn negation, conjunction, and disjunction and show improvement in learning logical composition and retaining performance on VQA. We suggest this work as a move towards embedding logical connectives in visual understanding, along with the benefits of robustness and generalizability. Our code and dataset is available online at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：逻辑连接词及其对自然语言句子的意思含义是理解一个基本方面。在本文中，我们研究了视觉问答（VQA）通过逻辑转型断定的镜头，那些寻求回答有关图像的问题系统必须稳固的问题的这些变化。如果VQA系统能够回答的问题，它也应该能够回答问题的逻辑成分。我们分析针对这些逻辑运算的VQA任务状态的最先进机型的性能，并表明他们在正确回答这些问题的难度。然后，我们构建了VQA数据集包含逻辑运算问题的增强和再培训相同的模型来建立一个基线。我们进一步提出了一种新的方法来训练模型来了解否定，合和脱节，并显示改善学习逻辑组成，保持对VQA性能。我们认为这项工作作为对视觉的理解嵌入逻辑连接词，与坚固性和普遍性的优点外，一招。我们的代码和数据集可在网上这个HTTPS URL</font>
</div>


<hr>
<div id="paper12"> <b>12. A Differential-form Pullback Programming Language for Higher-order  Reverse-mode Automatic Differentiation</b>  <a href="https://arxiv.org/pdf/2002.08241" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mak%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carol Mak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ong%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luke Ong</a><br>
<font size="3">
Abstract: Building on the observation that reverse-mode automatic differentiation (AD) -- a generalisation of backpropagation -- can naturally be expressed as pullbacks of differential 1-forms, we design a simple higher-order programming language with a first-class differential operator, and present a reduction strategy which exactly simulates reverse-mode AD. We justify our reduction strategy by interpreting our language in any differential $\lambda$-category that satisfies the Hahn-Banach Separation Theorem, and show that the reduction strategy precisely captures reverse-mode AD in a truly higher-order setting. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：建立在观察到反向模式自动微分（AD） - 反向传播的一般化 - 可以自然被表达为差分1形式的回调，我们设计了一个简单的高阶编程语言与第一级的差动操作者，以及本的降低策略，正好模拟反向模式AD。我们证明我们可以在任何不同的$ \ $拉姆达解释-category我们的语言还原的策略，满足哈恩的Banach分离定理，并表明降价策略可以精确地捕捉反向模式AD在一个真正的高阶设置。</font>
</div>


<hr>
<div id="paper13"> <b>13. Tree-structured Attention with Hierarchical Accumulation</b>  <a href="https://arxiv.org/pdf/2002.08046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuan-Phi Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Joty%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shafiq Joty</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hoi%2C+S+C+H" target="_blank" rel="noopener" style="color:#0000EE;">Steven C.H. Hoi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Socher</a><br>
<font size="3">
Abstract: Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with "Hierarchical Accumulation" to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German translation task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结合像选区树的层次结构已被证明是有效的各种自然语言处理（NLP）的任务。然而，显而易见的是，国家的最先进的（SOTA）基于序列的模型，如变压器斗争编码这样的结构本质上。在另一方面，专用车型如树LSTM，同时明确造型的层次结构，也不能有效的执行变压器。在本文中，我们试图弥合与“分层堆积”这个差距编码解析树结构为自我的关注，在一定的时间复杂度。我们的方法比SOTA方法四个IWSLT翻译任务和WMT'14英语，德语翻译任务。这也产生了三个文本分类的任务在变压器和树LSTM改进。我们进一步表明，采用分层先验可以弥补数据不足，我们的模式更倾向于在标记级别的关注短语级的关注。</font>
</div>


<hr>
<div id="paper14"> <b>14. Non-Autoregressive Dialog State Tracking</b>  <a href="https://arxiv.org/pdf/2002.08024" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Le%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hung Le</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Socher</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hoi%2C+S+C+H" target="_blank" rel="noopener" style="color:#0000EE;">Steven C.H. Hoi</a><br>
<font size="3">
Abstract: Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among (domain, slot) pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework of Non-Autoregressive Dialog State Tracking (NADST) which can factor in potential dependencies among domains and slots to optimize the models towards better prediction of dialogue states as a complete set rather than separate slots. In particular, the non-autoregressive nature of our method not only enables decoding in parallel to significantly reduce the latency of DST for real-time dialogue response generation, but also detect dependencies among slots at token level in addition to slot and domain level. Our empirical results show that our model achieves the state-of-the-art joint accuracy across all domains on the MultiWOZ 2.1 corpus, and the latency of our model is an order of magnitude lower than the previous state of the art as the dialogue history extends over time. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在对话状态跟踪（DST）面向任务的对话最近作出的努力朝着开放式的词汇或该模型可以生成从对话历史本身插槽候选值基于代的方式推进。这些方法都显示了良好的性能增益，特别是在动态时隙值复杂对话域。然而，他们在两个方面功亏一篑：（1）他们不允许模型明确学习跨域和槽信号检测中（域，插槽）对可能依赖性; （2）现有车型遵循的付出高昂的时间成本自回归的方法时，对话的演进在多个域和多圈。在本文中，我们提出了非自回归对话状态跟踪（NADST），它可以因素域和插槽之间的可能依赖性走向对话状态更好的预测作为一套完整的，而不是单独的槽优化模型的新框架。特别地，我们的方法的非自回归性质不仅使得能够并行进行解码，以减少显著DST的实时对话响应产生的等待时间，而且在除了时隙和域级别令牌水平检测时隙中的依赖关系。我们的实证结果表明，我们的模型实现跨越的MultiWOZ 2.1语料所有域的国家的最先进的联合精确度，和我们的模型的潜伏期是一个数量级比艺术对话的历史以前的状态下延长一段时间。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-20</title>
    <url>/2020/02/20/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-20/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Extracting Semantic Indoor Maps from Occupancy Grids <a href="https://arxiv.org/pdf/2002.08348" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Towards a Complete Pipeline for Segmenting Nuclei in Feulgen-Stained  Images <a href="https://arxiv.org/pdf/2002.08331" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> VQA-LOL: Visual Question Answering under the Lens of Logic <a href="https://arxiv.org/pdf/2002.08325" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> When Radiology Report Generation Meets Knowledge Graph <a href="https://arxiv.org/pdf/2002.08277" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Weakly Supervised Semantic Segmentation of Satellite Images for Land  Cover Mapping -- Challenges and Opportunities <a href="https://arxiv.org/pdf/2002.08254" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> AI Online Filters to Real World Image Recognition <a href="https://arxiv.org/pdf/2002.08242" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> siaNMS: Non-Maximum Suppression with Siamese Networks for Multi-Camera  3D Object Detection <a href="https://arxiv.org/pdf/2002.08239" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Three-Stream Fusion Network for First-Person Interaction Recognition <a href="https://arxiv.org/pdf/2002.08219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> DeFraudNet:End2End Fingerprint Spoof Detection using Patch Level  Attention <a href="https://arxiv.org/pdf/2002.08214" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Model-Agnostic Structured Sparsification with Learnable Channel Shuffle <a href="https://arxiv.org/pdf/2002.08127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Weakly-Supervised Semantic Segmentation by Iterative Affinity Learning <a href="https://arxiv.org/pdf/2002.08098" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Unsupervised Temporal Feature Aggregation for Event Detection in  Unstructured Sports Videos <a href="https://arxiv.org/pdf/2002.08097" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Meta Segmentation Network for Ultra-Resolution Medical Images <a href="https://arxiv.org/pdf/2002.08043" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Feasibility of Video-based Sub-meter Localization on  Resource-constrained Platforms <a href="https://arxiv.org/pdf/2002.08039" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> On-line non-overlapping camera calibration net <a href="https://arxiv.org/pdf/2002.08005" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Universal Domain Adaptation through Self Supervision <a href="https://arxiv.org/pdf/2002.07953" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Dataset of Segmented Nuclei in Hematoxylin and Eosin Stained  Histopathology Images of 10 Cancer Types <a href="https://arxiv.org/pdf/2002.07913" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Lake Ice Monitoring with Webcams and Crowd-Sourced Images <a href="https://arxiv.org/pdf/2002.07875" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Fawkes: Protecting Personal Privacy against Unauthorized Deep Learning  Models <a href="https://arxiv.org/pdf/2002.08327" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Variational Encoder-based Reliable Classification <a href="https://arxiv.org/pdf/2002.08289" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> SYMOG: learning symmetric mixture of Gaussian modes for improved  fixed-point quantization <a href="https://arxiv.org/pdf/2002.08204" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Variable-Bitrate Neural Compression via Bayesian Arithmetic Coding <a href="https://arxiv.org/pdf/2002.08158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Randomized Smoothing of All Shapes and Sizes <a href="https://arxiv.org/pdf/2002.08118" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Hierarchical Quantized Autoencoders <a href="https://arxiv.org/pdf/2002.08111" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Neural Networks on Random Graphs <a href="https://arxiv.org/pdf/2002.08104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Enlarging Discriminative Power by Adding an Extra Class in Unsupervised  Domain Adaptation <a href="https://arxiv.org/pdf/2002.08041" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Globally optimal point set registration by joint symmetry plane fitting <a href="https://arxiv.org/pdf/2002.07988" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Block Switching: A Stochastic Approach for Deep Learning Security <a href="https://arxiv.org/pdf/2002.07920" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> LocoGAN -- Locally Convolutional GAN <a href="https://arxiv.org/pdf/2002.07897" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Towards Query-Efficient Black-Box Adversary with Zeroth-Order Natural  Gradient Descent <a href="https://arxiv.org/pdf/2002.07891" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> CBIR using features derived by Deep Learning <a href="https://arxiv.org/pdf/2002.07877" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Extracting Semantic Indoor Maps from Occupancy Grids</b>  <a href="https://arxiv.org/pdf/2002.08348" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziyuan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Wichert%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Georg von Wichert</a><br>
<font size="3">
Abstract: The primary challenge for any autonomous system operating in realistic, rather unconstrained scenarios is to manage the complexity and uncertainty of the real world. While it is unclear how exactly humans and other higher animals master these problems, it seems evident, that abstraction plays an important role. The use of abstract concepts allows to define the system behavior on higher levels. In this paper we focus on the semantic mapping of indoor environments. We propose a method to extract an abstracted floor plan from typical grid maps using Bayesian reasoning. The result of this procedure is a probabilistic generative model of the environment defined over abstract concepts. It is well suited for higher-level reasoning and communication purposes. We demonstrate the effectiveness of the approach using real-world data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在现实，而不受约束的情况下任何自治系统运行的主要挑战是管理的复杂性和现实世界的不确定性。虽然目前还不清楚究竟人类和其他高等动物是如何掌握了这些问题，它似乎很明显，即抽象起着重要的作用。采用抽象的概念可以定义更高层次上的系统行为。在本文中，我们专注于室内环境的语义映射。我们提出了一个方法来提取使用贝叶斯推理典型的网格地图抽象的平面图。这个过程的结果是在抽象的概念定义的环境的概率生成模型。它非常适合于更高层次的推理和沟通的目的。我们演示使用真实世界的数据的方法的有效性。</font>
</div>


<hr>
<div id="paper2"> <b>2. Towards a Complete Pipeline for Segmenting Nuclei in Feulgen-Stained  Images</b>  <a href="https://arxiv.org/pdf/2002.08331" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Macarini%2C+L+A+B" target="_blank" rel="noopener" style="color:#0000EE;">Luiz Antonio Buschetto Macarini</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=von+Wangenheim%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aldo von Wangenheim</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dalto%C3%A9%2C+F+P" target="_blank" rel="noopener" style="color:#0000EE;">Felipe Perozzo Daltoé</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Onofre%2C+A+S+C" target="_blank" rel="noopener" style="color:#0000EE;">Alexandre Sherlley Casimiro Onofre</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=de+Miranda+Onofre%2C+F+B" target="_blank" rel="noopener" style="color:#0000EE;">Fabiana Botelho de Miranda Onofre</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Stemmer%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Marcelo Ricardo Stemmer</a><br>
<font size="3">
Abstract: Cervical cancer is the second most common cancer type in women around the world. In some countries, due to non-existent or inadequate screening, it is often detected at late stages, making standard treatment options often absent or unaffordable. It is a deadly disease that could benefit from early detection approaches. It is usually done by cytological exams which consist of visually inspecting the nuclei searching for morphological alteration. Since it is done by humans, naturally, some subjectivity is introduced. Computational methods could be used to reduce this, where the first stage of the process would be the nuclei segmentation. In this context, we present a complete pipeline for the segmentation of nuclei in Feulgen-stained images using Convolutional Neural Networks. Here we show the entire process of segmentation, since the collection of the samples, passing through pre-processing, training the network, post-processing and results evaluation. We achieved an overall IoU of 0.78, showing the affordability of the approach of nuclei segmentation on Feulgen-stained images. The code is available in: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：宫颈癌是全球女性第二常见的癌症类型。在一些国家，由于不存在或不充分的筛选，它往往在后期检测，使标准治疗方案往往缺乏或负担不起。这是一种致命的疾病，可以从早期检测方法中受益。它通常是由包括目视检查为形态学改变核搜索细胞学检查完成。既然是由人完成的，当然，有些主观性介绍。计算方法可以用来降低这一点，其中该方法的第一阶段是细胞核分割。在这方面，我们提出了核的使用卷积神经网络富尔根染色的图像分割一个完整的管道。在这里我们细分的整个过程中，由于样本的采集，通过预处理，网络训练，后处理和结果评估。我们实现了整体IOU 0.78，表现出细胞核分割的上富尔根染色的图像的方法的承受能力。该代码是可用：此HTTPS URL。</font>
</div>


<hr>
<div id="paper3"> <b>3. VQA-LOL: Visual Question Answering under the Lens of Logic</b>  <a href="https://arxiv.org/pdf/2002.08325" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gokhale%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tejas Gokhale</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pratyay Banerjee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baral%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chitta Baral</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yezhou Yang</a><br>
<font size="3">
Abstract: Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate visual question answering (VQA) through the lens of logical transformation and posit that systems that seek to answer questions about images must be robust to these transformations of the question. If a VQA system is able to answer a question, it should also be able to answer the logical composition of questions. We analyze the performance of state-of-the-art models on the VQA task under these logical operations and show that they have difficulty in correctly answering such questions. We then construct an augmentation of the VQA dataset with questions containing logical operations and retrain the same models to establish a baseline. We further propose a novel methodology to train models to learn negation, conjunction, and disjunction and show improvement in learning logical composition and retaining performance on VQA. We suggest this work as a move towards embedding logical connectives in visual understanding, along with the benefits of robustness and generalizability. Our code and dataset is available online at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：逻辑连接词及其对自然语言句子的意思含义是理解一个基本方面。在本文中，我们研究了视觉问答（VQA）通过逻辑转型断定的镜头，那些寻求回答有关图像的问题系统必须稳固的问题的这些变化。如果VQA系统能够回答的问题，它也应该能够回答问题的逻辑成分。我们分析针对这些逻辑运算的VQA任务状态的最先进机型的性能，并表明他们在正确回答这些问题的难度。然后，我们构建了VQA数据集包含逻辑运算问题的增强和再培训相同的模型来建立一个基线。我们进一步提出了一种新的方法来训练模型来了解否定，合和脱节，并显示改善学习逻辑组成，保持对VQA性能。我们认为这项工作作为对视觉的理解嵌入逻辑连接词，与坚固性和普遍性的优点外，一招。我们的代码和数据集可在网上这个HTTPS URL</font>
</div>


<hr>
<div id="paper4"> <b>4. When Radiology Report Generation Meets Knowledge Graph</b>  <a href="https://arxiv.org/pdf/2002.08277" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yixiao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaosong Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziyue Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qihang Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuille%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alan Yuille</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daguang Xu</a><br>
<font size="3">
Abstract: Automatic radiology report generation has been an attracting research problem towards computer-aided diagnosis to alleviate the workload of doctors in recent years. Deep learning techniques for natural image captioning are successfully adapted to generating radiology reports. However, radiology image reporting is different from the natural image captioning task in two aspects: 1) the accuracy of positive disease keyword mentions is critical in radiology image reporting in comparison to the equivalent importance of every single word in a natural image caption; 2) the evaluation of reporting quality should focus more on matching the disease keywords and their associated attributes instead of counting the occurrence of N-gram. Based on these concerns, we propose to utilize a pre-constructed graph embedding module (modeled with a graph convolutional neural network) on multiple disease findings to assist the generation of reports in this work. The incorporation of knowledge graph allows for dedicated feature learning for each disease finding and the relationship modeling between them. In addition, we proposed a new evaluation metric for radiology image reporting with the assistance of the same composed graph. Experimental results demonstrate the superior performance of the methods integrated with the proposed graph embedding module on a publicly accessible dataset (IU-RR) of chest radiographs compared with previous approaches using both the conventional evaluation metrics commonly adopted for image captioning and our proposed ones. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动影像报告生成已朝向计算机辅助诊断的研究吸引了问题减轻医生的工作量在最近几年。自然图像字幕深度学习技术成功地适应产生的放射学报告。然而，放射影像报告是从自然图像字幕任务不同在两个方面：1）积极疾病关键词的准确性提到是在比较中自然图像标题的每一个单词的重要性相当于放射影像报告至关重要; 2）报告质量评估应更注重匹配疾病关键字及其相关联的属性，而不是计数的N-gram的发生。基于这些问题，我们提出了利用多个疾病的发现预构建的图形嵌入模块（有图卷积神经网络模型），以帮助生成报告了这项工作。知识图的结合允许对每一种疾病的发现和关系，它们之间建模专用功能的学习。此外，我们建议对于具有相同组成的图形的协助放射图像报告新的评价指标。实验结果表明，与所提出的图形胸片的可公开访问的数据集（IU-RR），但以两个图像字幕和我们提出的那些普遍采用的传统评价指标以前的方法相比，嵌入模块集成的方法的优越性能。</font>
</div>


<hr>
<div id="paper5"> <b>5. Weakly Supervised Semantic Segmentation of Satellite Images for Land  Cover Mapping -- Challenges and Opportunities</b>  <a href="https://arxiv.org/pdf/2002.08254" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schmitt%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Schmitt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prexl%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Prexl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ebel%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Ebel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liebel%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lukas Liebel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Xiang Zhu</a><br>
<font size="3">
Abstract: Fully automatic large-scale land cover mapping belongs to the core challenges addressed by the remote sensing community. Usually, the basis of this task is formed by (supervised) machine learning models. However, in spite of recent growth in the availability of satellite observations, accurate training data remains comparably scarce. On the other hand, numerous global land cover products exist and can be accessed often free-of-charge. Unfortunately, these maps are typically of a much lower resolution than modern day satellite imagery. Besides, they always come with a significant amount of noise, as they cannot be considered ground truth, but are products of previous (semi-)automatic prediction tasks. Therefore, this paper seeks to make a case for the application of weakly supervised learning strategies to get the most out of available data sources and achieve progress in high-resolution large-scale land cover mapping. Challenges and opportunities are discussed based on the SEN12MS dataset, for which also some baseline results are shown. These baselines indicate that there is still a lot of potential for dedicated approaches designed to deal with remote sensing-specific forms of weak supervision. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：全自动大型土地覆盖制图属于核心挑战谈到了遥感社区。通常情况下，这一任务的基础是由（监督），机器学习模型形成。然而，尽管在卫星观测的可用性最近的增长的，准确的训练数据仍然相当稀少。在另一方面，众多全球土地覆盖产品的存在，经常可以访问免费的充电。不幸的是，这些地图通常是低得多的分辨率比现今的卫星图像。此外，他们总是有噪音的显著量，因为它们不能被认为是基本事实，但以前的（半）自动预测任务的产品。因此，本文试图以充分的理由的弱监督学习策略，以充分利用现有的数据源，实现高分辨率的大型土地覆盖制图进步的应用程序。挑战与机遇是基于SEN12MS数据集，为此，也有一些基准结果显示了讨论。这些基准表明，仍然有很多的专门用来对付监管不力的遥感特定形式的专用方法的潜力。</font>
</div>


<hr>
<div id="paper6"> <b>6. AI Online Filters to Real World Image Recognition</b>  <a href="https://arxiv.org/pdf/2002.08242" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hai Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jin Shang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengyuan Huang</a><br>
<font size="3">
Abstract: Deep artificial neural networks, trained with labeled data sets are widely used in numerous vision and robotics applications today. In terms of AI, these are called reflex models, referring to the fact that they do not self-evolve or actively adapt to environmental changes. As demand for intelligent robot control expands to many high level tasks, reinforcement learning and state based models play an increasingly important role. Herein, in computer vision and robotics domain, we study a novel approach to add reinforcement controls onto the image recognition reflex models to attain better overall performance, specifically to a wider environment range beyond what is expected of the task reflex models. Follow a common infrastructure with environment sensing and AI based modeling of self-adaptive agents, we implement multiple types of AI control agents. To the end, we provide comparative results of these agents with baseline, and an insightful analysis of their benefit to improve overall image recognition performance in real world. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深人工神经网络，与标记的数据集的培训被广泛应用于众多视觉和机器人应用的今天。在AI方面，这些被称为反射模式，指的是事实，他们不会自我进化或主动适应环境变化。至于智能机器人控制的扩张需求，很多高级别任务，强化学习和基于状态的模型中扮演着越来越重要的作用。在此，计算机视觉和机器人领域，我们研究了一种新的方法来添加增强控件拖到图像识别反射模式，以获得更好的整体性能，具体到超出预期的任务反射模型的更广泛的环境范围。遵循环境感知和自适应代理的AI基于建模一个通用的基础设施，我们实现了多种类型的AI控制剂。到最后，我们提供基线这些药物的对比结果，以及他们的利益的精辟分析，以改善在现实世界中的整体形象识别性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. siaNMS: Non-Maximum Suppression with Siamese Networks for Multi-Camera  3D Object Detection</b>  <a href="https://arxiv.org/pdf/2002.08239" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cortes%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Irene Cortes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Beltran%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jorge Beltran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+la+Escalera%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arturo de la Escalera</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fernando Garcia</a><br>
<font size="3">
Abstract: The rapid development of embedded hardware in autonomous vehicles broadens their computational capabilities, thus bringing the possibility to mount more complete sensor setups able to handle driving scenarios of higher complexity. As a result, new challenges such as multiple detections of the same object have to be addressed. In this work, a siamese network is integrated into the pipeline of a well-known 3D object detector approach to suppress duplicate proposals coming from different cameras via re-identification. Additionally, associations are exploited to enhance the 3D box regression of the object by aggregating their corresponding LiDAR frustums. The experimental evaluation on the nuScenes dataset shows that the proposed method outperforms traditional NMS approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：嵌入式硬件的自主汽车的快速发展拓宽了他们的计算能力，从而把可能安装能够处理更复杂驾驶情况下更完整的传感器设置。其结果是，如在同一对象的多个检测新的挑战必须得到解决。在这项工作中，连体网络被集成到一个公知的3D对象检测器的方法来通过重新鉴定从不同的摄像机来抑制重复的提案的管道。此外，关联是利用通过聚合它们相应的激光雷达平截头体以增强物体的3D箱回归。在nuScenes数据集显示了试验评价，认为该方法优于传统方法NMS。</font>
</div>


<hr>
<div id="paper8"> <b>8. Three-Stream Fusion Network for First-Person Interaction Recognition</b>  <a href="https://arxiv.org/pdf/2002.08219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ye-Ji Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dong-Gyu Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seong-Whan Lee</a><br>
<font size="3">
Abstract: First-person interaction recognition is a challenging task because of unstable video conditions resulting from the camera wearer's movement. For human interaction recognition from a first-person viewpoint, this paper proposes a three-stream fusion network with two main parts: three-stream architecture and three-stream correlation fusion. Thre three-stream architecture captures the characteristics of the target appearance, target motion, and camera ego-motion. Meanwhile the three-stream correlation fusion combines the feature map of each of the three streams to consider the correlations among the target appearance, target motion and camera ego-motion. The fused feature vector is robust to the camera movement and compensates for the noise of the camera ego-motion. Short-term intervals are modeled using the fused feature vector, and a long short-term memory(LSTM) model considers the temporal dynamics of the video. We evaluated the proposed method on two-public benchmark datasets to validate the effectiveness of our approach. The experimental results show that the proposed fusion method successfully generated a discriminative feature vector, and our network outperformed all competing activity recognition methods in first-person videos where considerable camera ego-motion occurs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：第一人称交互识别是因为从相机穿着者的运动而产生的不稳定视频条件具有挑战性的任务。用于从第一人称视点的人类交互识别，本文提出了一种具有两个主要部分的三流融合网络：三流架构和三流相关的融合。 THRE三流架构捕获目标的外观，目标运动，和照相机自运动的特性。同时三流相关融合体组合这三个流中的特征地图考虑目标的外观，目标运动和相机自运动之间的相关性。将融合的特征向量是稳健的摄像机运动并补偿相机自运动的噪声。短期间隔使用所述稠合特征矢量建模，并且一个长短期记忆（LSTM）模型考虑了视频的时间动态。我们评估了两个公共标准数据集所提出的方法来验证我们方法的有效性。实验结果表明，所提出的融合方法成功生成的具有区分特征向量，而我们网络的表现优于在相当大的相机自运动发生的第一人称视频所有竞争行为识别方法。</font>
</div>


<hr>
<div id="paper9"> <b>9. DeFraudNet:End2End Fingerprint Spoof Detection using Patch Level  Attention</b>  <a href="https://arxiv.org/pdf/2002.08214" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Anusha%2C+B+V+S" target="_blank" rel="noopener" style="color:#0000EE;">B.V.S Anusha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Banerjee%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sayan Banerjee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaudhuri%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subhasis Chaudhuri</a><br>
<font size="3">
Abstract: In recent years, fingerprint recognition systems have made remarkable advancements in the field of biometric security as it plays an important role in personal, national and global security. In spite of all these notable advancements, the fingerprint recognition technology is still susceptible to spoof attacks which can significantly jeopardize the user security. The cross sensor and cross material spoof detection still pose a challenge with a myriad of spoof materials emerging every day, compromising sensor interoperability and robustness. This paper proposes a novel method for fingerprint spoof detection using both global and local fingerprint feature descriptors. These descriptors are extracted using DenseNet which significantly improves cross-sensor, cross-material and cross-dataset performance. A novel patch attention network is used for finding the most discriminative patches and also for network fusion. We evaluate our method on four publicly available datasets:LivDet 2011, 2013, 2015 and 2017. A set of comprehensive experiments are carried out to evaluate cross-sensor, cross-material and cross-dataset performance over these datasets. The proposed approach achieves an average accuracy of 99.52%, 99.16% and 99.72% on LivDet 2017,2015 and 2011 respectively outperforming the current state-of-the-art results by 3% and 4% for LivDet 2015 and 2011 respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，指纹识别系统在生物安全领域取得了令人瞩目的进步，因为它对个人，国家和全球安全的重要作用。尽管有这些显着的进步，指纹识别技术仍然容易受到欺骗攻击，它可以显著危及用户安全。交叉传感器和跨材料欺骗检测仍然对与恶搞材料无数，每天新出现的，影响传感器的互操作性和稳健性提出了挑战。本文提出了一种利用全局和局部的指纹特征描述指纹欺骗检测的新方法。这些描述符使用DenseNet其中显著提高跨传感器，横材料和跨数据集性能萃取。一种新的补丁关注网络用于寻找最歧视性的补丁，也为网络的融合。我们评估我们在四个可公开获得的数据集的方法：LivDet 2011年，2013年，2015年和2017年的综合性实验一组都进行了评估跨传感器，跨材料，跨数据集对这些数据集的性能。所提出的方法实现了99.52％，99.16％和上LivDet 2017,2015和2011 99.72％分别优于当前状态的最先进的结果由3％和4％分别为2015年LivDet和2011年的平均精确度。</font>
</div>


<hr>
<div id="paper10"> <b>10. Model-Agnostic Structured Sparsification with Learnable Channel Shuffle</b>  <a href="https://arxiv.org/pdf/2002.08127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin-Yu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taihong Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Ming Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Hsuan Yang</a><br>
<font size="3">
Abstract: Recent advances in convolutional neural networks (CNNs) usually come with the expense of considerable computational overhead and memory footprint. Network compression aims to alleviate this issue by training compact models with comparable performance. However, existing compression techniques either entail dedicated expert design or compromise with a moderate performance drop. To this end, we propose a model-agnostic structured sparsification method for efficient network compression. The proposed method automatically induces structurally sparse representations of the convolutional weights, thereby facilitating the implementation of the compressed model with the highly-optimized group convolution. We further address the problem of inter-group communication with a learnable channel shuffle mechanism. The proposed approach is model-agnostic and highly compressible with a negligible performance drop. Extensive experimental results and analysis demonstrate that our approach performs favorably against the state-of-the-art network pruning methods. The code will be publicly available after the review process. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）的最新进展通常会具有相当大的计算开销和内存占用为代价的。网络压缩的目的是通过培训紧凑车型相媲美的性能来缓解这个问题。然而，现有的压缩技术，无论是继承权问题专门设计的专家或妥协具有中等性能下降。为此，我们提出了高效的网络压缩模型无关的结构化方法稀疏。所提出的方法自动诱导卷积权重的结构稀疏表示，从而促进与高度优化组卷积压缩模型的实现。我们进一步用可学习信道洗牌机构解决组间通信的问题。建议的做法是模型无关，用微不足道的性能下降高度压缩。广泛的实验结果和分析表明，我们的方法进行良好地对国家的最先进的网络修剪方法。该代码将在审核之后公开。</font>
</div>


<hr>
<div id="paper11"> <b>11. Weakly-Supervised Semantic Segmentation by Iterative Affinity Learning</b>  <a href="https://arxiv.org/pdf/2002.08098" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sifei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huimin Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Hsuan Yang</a><br>
<font size="3">
Abstract: Weakly-supervised semantic segmentation is a challenging task as no pixel-wise label information is provided for training. Recent methods have exploited classification networks to localize objects by selecting regions with strong response. While such response map provides sparse information, however, there exist strong pairwise relations between pixels in natural images, which can be utilized to propagate the sparse map to a much denser one. In this paper, we propose an iterative algorithm to learn such pairwise relations, which consists of two branches, a unary segmentation network which learns the label probabilities for each pixel, and a pairwise affinity network which learns affinity matrix and refines the probability map generated from the unary network. The refined results by the pairwise network are then used as supervision to train the unary network, and the procedures are conducted iteratively to obtain better segmentation progressively. To learn reliable pixel affinity without accurate annotation, we also propose to mine confident regions. We show that iteratively training this framework is equivalent to optimizing an energy function with convergence to a local minimum. Experimental results on the PASCAL VOC 2012 and COCO datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：弱监督语义分割是一项具有挑战性的任务，因为没有逐个像素的标签信息提供了培训。最近的方法都用强烈反响选择地区利用分级网络本地化的对象。虽然这种反应图提供稀疏信息，但是，也存在于自然图像，其可以被利用来稀疏地图传播到致密得多的一个像素之间的强关系成对。在本文中，我们提出了一种迭代算法来学习这样的成对关系，它由两个分支的，一元分割网络，学习用于每个像素的标签的概率，并且其学习的亲和基质和提炼从产生的概率映射图的成对亲和网络一元网络。精制的结果通过成对的网络，然后作为监督训练一元网络，程序进行迭代以逐渐获得更好的分割。要了解可靠的像素亲和力不准确的注解，我们也建议我的自信区域。我们表明，反复训练这个框架相当于优化具有收敛的能量函数的局部最小值。在PASCAL VOC 2012和COCO数据集实验结果表明，所提出的算法进行对有利国家的最先进的方法。</font>
</div>


<hr>
<div id="paper12"> <b>12. Unsupervised Temporal Feature Aggregation for Event Detection in  Unstructured Sports Videos</b>  <a href="https://arxiv.org/pdf/2002.08097" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaudhury%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subhajit Chaudhury</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kimura%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daiki Kimura</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vinayavekhin%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phongtharin Vinayavekhin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Munawar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asim Munawar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tachibana%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryuki Tachibana</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ito%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Koji Ito</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Inaba%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuki Inaba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Matsumoto%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minoru Matsumoto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kidokoro%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuji Kidokoro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ozaki%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiroki Ozaki</a><br>
<font size="3">
Abstract: Image-based sports analytics enable automatic retrieval of key events in a game to speed up the analytics process for human experts. However, most existing methods focus on structured television broadcast video datasets with a straight and fixed camera having minimum variability in the capturing pose. In this paper, we study the case of event detection in sports videos for unstructured environments with arbitrary camera angles. The transition from structured to unstructured video analysis produces multiple challenges that we address in our paper. Specifically, we identify and solve two major problems: unsupervised identification of players in an unstructured setting and generalization of the trained models to pose variations due to arbitrary shooting angles. For the first problem, we propose a temporal feature aggregation algorithm using person re-identification features to obtain high player retrieval precision by boosting a weak heuristic scoring method. Additionally, we propose a data augmentation technique, based on multi-modal image translation model, to reduce bias in the appearance of training samples. Experimental evaluations show that our proposed method improves precision for player retrieval from 0.78 to 0.86 for obliquely angled videos. Additionally, we obtain an improvement in F1 score for rally detection in table tennis videos from 0.79 in case of global frame-level features to 0.89 using our proposed player-level features. Please see the supplementary video submission at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于图像的运动分析启用关键事件的自动检索在一场比赛中，以加快分析人类专家处理。然而，大多数现有的方法集中在与具有捕捉姿势最小变异直和固定摄像机结构电视广播​​视频数据集。在本文中，我们研究事件检测的在任意的拍摄角度非结构化环境中运动视频的情况。从结构到非结构化的视频分析的转变产生了我们在纸上应对多种挑战。具体来说，我们发现并解决两大问题：球员在训练的模型的非结构化设置和泛化带来的变化无监督的识别由于任意的拍摄角度。对于第一个问题，我们使用人重新识别特征通过提高弱启发式记分方法来获得高的球员检索精度提出了一个时间特征聚合算法。此外，我们提出了一个数据增强技术，基于多模态图像平移模式，以减少训练样本的出现偏差。试验评估表明，我们提出的方法提高精度玩家检索从0.78〜0.86的倾斜角度的视频。此外，我们获得F1在全球帧级的情况下，在乒乓球的视频分数集会检测从0.79到功能使用0.89我们建议的玩家级功能的改善。请参阅本HTTPS URL补充提交影片。</font>
</div>


<hr>
<div id="paper13"> <b>13. Meta Segmentation Network for Ultra-Resolution Medical Images</b>  <a href="https://arxiv.org/pdf/2002.08043" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanyun Qu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bicheng Dai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuxin Chen</a><br>
<font size="3">
Abstract: Despite recent progress on semantic segmentation, there still exist huge challenges in medical ultra-resolution image segmentation. The methods based on multi-branch structure can make a good balance between computational burdens and segmentation accuracy. However, the fusion structure in these methods require to be designed elaborately to achieve desirable result, which leads to model redundancy. In this paper, we propose Meta Segmentation Network (MSN) to solve this challenging problem. With the help of meta-learning, the fusion module of MSN is quite simple but effective. MSN can fast generate the weights of fusion layers through a simple meta-learner, requiring only a few training samples and epochs to converge. In addition, to avoid learning all branches from scratch, we further introduce a particular weight sharing mechanism to realize a fast knowledge adaptation and share the weights among multiple branches, resulting in the performance improvement and significant parameters reduction. The experimental results on two challenging ultra-resolution medical datasets BACH and ISIC show that MSN achieves the best performance compared with the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管在语义分割的最新进展，仍然存在着医疗超分辨率图像分割的巨大挑战。基于多分支结构的方法可以使计算负担和分割精度之间的良好平衡。然而，在这些方法中，融合结构需要被精心设计来实现期望的结果，这导致模型的冗余。在本文中，我们提出了分割元网络（MSN）来解决这个具有挑战性的问题。随着元学习的帮助下，MSN的融合模块是非常简单而有效。 MSN可以快速通过一个简单的元学习者产生熔融层的权重，只需要几个训练样本和时代收敛。此外，为了避免学习从头各分公司，我们进一步介绍，特定的加权共享机制，实现了快速的知识适应和多个分支机构之间共享的权重，从而导致性能改进和显著参数减少。两个实验结果挑战超分辨率医疗数据集巴赫和ISIC显示，MSN与国家的最先进的方法相比达到最佳的性能。</font>
</div>


<hr>
<div id="paper14"> <b>14. Feasibility of Video-based Sub-meter Localization on  Resource-constrained Platforms</b>  <a href="https://arxiv.org/pdf/2002.08039" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Musa%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abm Musa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Eriksson%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jakob Eriksson</a><br>
<font size="3">
Abstract: While the satellite-based Global Positioning System (GPS) is adequate for some outdoor applications, many other applications are held back by its multi-meter positioning errors and poor indoor coverage. In this paper, we study the feasibility of real-time video-based localization on resource-constrained platforms. Before commencing a localization task, a video-based localization system downloads an offline model of a restricted target environment, such as a set of city streets, or an indoor shopping mall. The system is then able to localize the user within the model, using only video as input. To enable such a system to run on resource-constrained embedded systems or smartphones, we (a) propose techniques for efficiently building a 3D model of a surveyed path, through frame selection and efficient feature matching, (b) substantially reduce model size by multiple compression techniques, without sacrificing localization accuracy, (c) propose efficient and concurrent techniques for feature extraction and matching to enable online localization, (d) propose a method with interleaved feature matching and optical flow based tracking to reduce the feature extraction and matching time in online localization. Based on an extensive set of both indoor and outdoor videos, manually annotated with location ground truth, we demonstrate that sub-meter accuracy, at real-time rates, is achievable on smart-phone type platforms, despite challenging video conditions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然基于卫星的全球定位系统（GPS）是否适合一些户外应用，许多其它应用也阻碍了其多米的定位精度和室内覆盖较差。在本文中，我们研究了实时视频为基础的本地化的资源约束型平台的可行性。开始本地化任务之前，基于视频的定位系统下载受限制的目标环境，离线模式等一整套城市街道，或室内购物商场。该系统然后能够在模型内定位用户，仅使用视频作为输入。为了使这样一个系统，以在资源受限的嵌入式系统或智能手机上运行，​​我们（a）中提出一种用于高效地构建调查路径的3D模型，通过帧选择和高效的特征匹配，（b）中的技术通过将多个实质上减少模型的大小压缩技术，在不牺牲定位精度，（c）中提出的特征提取和匹配高效和并行的技术，以使在线本地化，（d）提出与基于跟踪交织特征匹配和光流的方法，以减少特征提取和匹配时间在在线本地化。基于一套广泛的室内和室外的视频，与位置地面实况手动注释，我们证明了亚米级精度，在实时价格，是智能型手机平台实现的，尽管具有挑战性的视频条件。</font>
</div>


<hr>
<div id="paper15"> <b>15. On-line non-overlapping camera calibration net</b>  <a href="https://arxiv.org/pdf/2002.08005" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fangda%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhao Fangda</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tamaki%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Toru Tamaki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kurita%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takio Kurita</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Raytchev%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bisser Raytchev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaneda%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kazufumi Kaneda</a><br>
<font size="3">
Abstract: We propose an easy-to-use non-overlapping camera calibration method. First, successive images are fed to a PoseNet-based network to obtain ego-motion of cameras between frames. Next, the pose between cameras are estimated. Instead of using a batch method, we propose an on-line method of the inter-camera pose estimation. Furthermore, we implement the entire procedure on a computation graph. Experiments with simulations and the KITTI dataset show the proposed method to be effective in simulation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了一个易于使用的非重叠摄像机标定方法。首先，连续的图像被馈送到基于PoseNet的网络获得的帧之间的摄像机自运动。接下来，相机的姿势估计。而是采用间歇式方法的，我们提出了摄像装置间姿态估计的上线方法。此外，我们在执行计算图的整个过程。用模拟和实验KITTI数据集示出该方法可有效地模拟。</font>
</div>


<hr>
<div id="paper16"> <b>16. Universal Domain Adaptation through Self Supervision</b>  <a href="https://arxiv.org/pdf/2002.07953" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saito%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kuniaki Saito</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Donghyun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sclaroff%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stan Sclaroff</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saenko%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kate Saenko</a><br>
<font size="3">
Abstract: Unsupervised domain adaptation methods traditionally assume that all source categories are present in the target domain. In practice, little may be known about the category overlap between the two domains. While some methods address target settings with either partial or open-set categories, they assume that the particular setting is known a priori. We propose a more universally applicable domain adaptation approach that can handle arbitrary category shift, called Domain Adaptative Neighborhood Clustering via Entropy optimization (DANCE). DANCE combines two novel ideas: First, as we cannot fully rely on source categories to learn features discriminative for the target, we propose a novel neighborhood clustering technique to learn the structure of the target domain in a self-supervised way. Second, we use entropy-based feature alignment and rejection to align target features with the source, or reject them as unknown categories based on their entropy. We show through extensive experiments that DANCE outperforms baselines across open-set, open-partial and partial domain adaptation settings. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督域适配方法传统上假定所有源类别存在于目标域。在实践中，可以小大约两个结构域之间的重叠类是已知的。而具有部分或开组类别一些方法中地址目标设置，它们假定特定设置是先验已知的。我们提出了一个更普遍适用的领域适应性方法，可以处理任意类型的转变，通过优化熵（DANCE）称为域名适应性研究邻聚类。舞蹈结合了两种新奇的想法：首先，正如我们不能完全依靠源类别学习辨别目标，我们提出了一个新的邻里聚类技术学习目标域的结构，自我监督方式的特点。其次，我们采用基于熵的功能定位和拒绝来对准目标特征与源，或拒绝它们基于它们的熵未知的类别。我们发现，通过大量的实验跨开集，开放式部分和部分领域适应性设置DANCE性能优于基准。</font>
</div>


<hr>
<div id="paper17"> <b>17. Dataset of Segmented Nuclei in Hematoxylin and Eosin Stained  Histopathology Images of 10 Cancer Types</b>  <a href="https://arxiv.org/pdf/2002.07913" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Hou%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Le Hou</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Gupta%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rajarsi Gupta</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Van+Arnam%2C+J+S" target="_blank" rel="noopener" style="color:#0000EE;">John S. Van Arnam</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuwei Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sivalenka%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaustubh Sivalenka</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Samaras%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dimitris Samaras</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kurc%2C+T+M" target="_blank" rel="noopener" style="color:#0000EE;">Tahsin M. Kurc</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Saltz%2C+J+H" target="_blank" rel="noopener" style="color:#0000EE;">Joel H. Saltz</a><br>
<font size="3">
Abstract: The distribution and appearance of nuclei are essential markers for the diagnosis and study of cancer. Despite the importance of nuclear morphology, there is a lack of large scale, accurate, publicly accessible nucleus segmentation data. To address this, we developed an analysis pipeline that segments nuclei in whole slide tissue images from multiple cancer types with a quality control process. We have generated nucleus segmentation results in 5,060 Whole Slide Tissue images from 10 cancer types in The Cancer Genome Atlas. One key component of our work is that we carried out a multi-level quality control process (WSI-level and image patch-level), to evaluate the quality of our segmentation results. The image patch-level quality control used manual segmentation ground truth data from 1,356 sampled image patches. The datasets we publish in this work consist of roughly 5 billion quality controlled nuclei from more than 5,060 TCGA WSIs from 10 different TCGA cancer types and 1,356 manually segmented TCGA image patches from the same 10 cancer types plus additional 4 cancer types. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：分布和原子核的外观是用于诊断和癌症研究必不可少的标志。尽管核形态的重要性，但缺乏大规模，准确，公开访问的细胞核分段的数据。为了解决这个问题，我们开发了一个分析流水线段在从多种癌症类型整个幻灯片组织图像与质量控制过程的核。我们已经在10种癌症类型中的癌症基因组图谱5060个整个幻灯片组织图像生成核的分割结果。我们工作的一个重要组成部分是，我们进行了一个多层次的质量控制流程（WSI级和像块级），来评估我们的分割结果的质量。图像补丁级别的质量控制从1356个采样的图像块中使用手动分割地面实况数据。我们在这项工作中公布的数据集从超过5060次TCGA峰会从10种不同的TCGA癌症类型包括约5十亿质量控制核的和1356手动分段TCGA图像块从相同的10种癌症类型加上另外4的癌症类型。</font>
</div>


<hr>
<div id="paper18"> <b>18. Lake Ice Monitoring with Webcams and Crowd-Sourced Images</b>  <a href="https://arxiv.org/pdf/2002.07875" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Prabha%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rajanie Prabha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tom%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manu Tom</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rothermel%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mathias Rothermel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baltsavias%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emmanuel Baltsavias</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leal-Taixe%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laura Leal-Taixe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schindler%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Konrad Schindler</a><br>
<font size="3">
Abstract: Lake ice is a strong climate indicator and has been recognised as part of the Essential Climate Variables (ECV) by the Global Climate Observing System (GCOS). The dynamics of freezing and thawing, and possible shifts of freezing patterns over time, can help in understanding the local and global climate systems. One way to acquire the spatio-temporal information about lake ice formation, independent of clouds, is to analyse webcam images. This paper intends to move towards a universal model for monitoring lake ice with freely available webcam data. We demonstrate good performance, including the ability to generalise across different winters and different lakes, with a state-of-the-art Convolutional Neural Network (CNN) model for semantic image segmentation, Deeplab v3+. Moreover, we design a variant of that model, termed Deep-U-Lab, which predicts sharper, more correct segmentation boundaries. We have tested the model's ability to generalise with data from multiple camera views and two different winters. On average, it achieves intersection-over-union (IoU) values of ~71% across different cameras and ~69% across different winters, greatly outperforming prior work. Going even further, we show that the model even achieves 60% IoU on arbitrary images scraped from photo-sharing web sites. As part of the work, we introduce a new benchmark dataset of webcam images, Photi-LakeIce, from multiple cameras and two different winters, along with pixel-wise ground truth annotations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：湖冰是一个强有力的气候指标，已被确认为基本气候变量（ECV）全球气候观测系统（GCOS）的一部分。冷冻和解冻，并随着时间的推移冻结模式的可能变化，动态，可了解当地和全球气候系统的帮助。获得关于湖的冰面上形成独立云的时空信息，一种方法，是分析网络摄像头的图像。本文拟迈向通用模型移动监测湖冰与免费提供的摄像头数据。我们表现​​出良好的性能，包括在不同的冬季和不同的湖泊一概而论，与一个国家的最先进的卷积神经网络（CNN）模型语义图像分割，Deeplab V3 +的能力。此外，我们设计这个模型的变种，称为深U型实验室，它预测更清晰，更准确的分割界线。我们已经测试模型与来自多个摄像机视图和两个不同的冬天的数据归纳的能力。平均而言，实现了在不同的摄像机的〜71％相交-过联盟（IOU）值和〜69％的在不同的冬天，大大优于以前的工作。变本加厉，我们表明，该模型甚至达到60％的借条从照片共享网站刮任意图像。作为工作的一部分，我们介绍摄像头图像的新的基准数据集，Photi-LakeIce，来自多个摄像机和两种不同的冬天，用逐像素地面真相批注一起。</font>
</div>


<hr>
<div id="paper19"> <b>19. Fawkes: Protecting Personal Privacy against Unauthorized Deep Learning  Models</b>  <a href="https://arxiv.org/pdf/2002.08327" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shawn Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wenger%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emily Wenger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiayun Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huiying Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haitao Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+B+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ben Y. Zhao</a><br>
<font size="3">
Abstract: Today's proliferation of powerful facial recognition models poses a real threat to personal privacy. As this http URL demonstrated, anyone can canvas the Internet for data, and train highly accurate facial recognition models of us without our knowledge. We need tools to protect ourselves from unauthorized facial recognition systems and their numerous potential misuses. Unfortunately, work in related areas are limited in practicality and effectiveness. In this paper, we propose Fawkes, a system that allow individuals to inoculate themselves against unauthorized facial recognition models. Fawkes achieves this by helping users adding imperceptible pixel-level changes (we call them "cloaks") to their own photos before publishing them online. When collected by a third-party "tracker" and used to train facial recognition models, these "cloaked" images produce functional models that consistently misidentify the user. We experimentally prove that Fawkes provides 95+% protection against user recognition regardless of how trackers train their models. Even when clean, uncloaked images are "leaked" to the tracker and used for training, Fawkes can still maintain a 80+% protection success rate. In fact, we perform real experiments against today's state-of-the-art facial recognition services and achieve 100% success. Finally, we show that Fawkes is robust against a variety of countermeasures that try to detect or disrupt cloaks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：强大的面部识别模型今天的扩散对个人隐私构成真正的威胁。由于这个HTTP URL证明，任何人都可以帆布互联网数据，和训练我们的高度精确的面部识别模型不知情的情况下。我们需要工具来保护自己免受未经授权的面部识别系统和它们的许多潜在的滥用。不幸的是，在相关领域工作的实用性和有效性是有限的。在本文中，我们提出了福克斯，一个系统，允许个人接种自己免受未经授权的面部识别模型。福克斯通过网上发布之前，帮助用户将感觉不到的像素级的变化（我们称他们为“斗篷”），以自己的照片达到这一点。当由第三方“跟踪器”收集并用于训练面部识别模式，这些“隐形”的图像产生功能模型，始终错误识别用户。我们通过实验证明，无论怎样追踪他们的训练模式，福克斯提供了95 +％，对用户的识别保护。即使清洁，取消遮盖图像“泄露”到跟踪器和用于训练，福克斯仍能保持80 +％的保护成功率。事实上，我们对今天的国家的最先进的面部识别服务进行实时实验，达到100次％的成功。最后，我们表明，福克斯是对各种试图探测或破坏斗篷对策强劲。</font>
</div>


<hr>
<div id="paper20"> <b>20. Variational Encoder-based Reliable Classification</b>  <a href="https://arxiv.org/pdf/2002.08289" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhushan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chitresh Bhushan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhaoyuan Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Virani%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nurali Virani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Iyer%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naresh Iyer</a><br>
<font size="3">
Abstract: Machine learning models provide statistically impressive results which might be individually unreliable. To provide reliability, we propose an Epistemic Classifier (EC) that can provide justification of its belief using support from the training dataset as well as quality of reconstruction. Our approach is based on modified variational auto-encoders that can identify a semantically meaningful low-dimensional space where perceptually similar instances are close in $\ell_2$-distance too. Our results demonstrate improved reliability of predictions and robust identification of samples with adversarial attacks as compared to baseline of softmax-based thresholding. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器学习模型提供这可能是个别不可靠的统计结果令人印象深刻。为了保证系统的可靠性，我们提出了一个认知分类（EC），可以提供使用从训练数据集的支持，以及重建的质量，它相信的理由。我们的做法是基于改进变自动编码器，它可以识别语义上有意义的低维空间，感觉上类似的例子是接近$ \ $ ell_2太距离d。相比于基于SOFTMAX-阈值的基线我们的结果显示出改进的预测和与敌对攻击样品的鲁棒辨识的可靠性。</font>
</div>


<hr>
<div id="paper21"> <b>21. SYMOG: learning symmetric mixture of Gaussian modes for improved  fixed-point quantization</b>  <a href="https://arxiv.org/pdf/2002.08204" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Enderich%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lukas Enderich</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Timm%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fabian Timm</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Burgard%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wolfram Burgard</a><br>
<font size="3">
Abstract: Deep neural networks (DNNs) have been proven to outperform classical methods on several machine learning benchmarks. However, they have high computational complexity and require powerful processing units. Especially when deployed on embedded systems, model size and inference time must be significantly reduced. We propose SYMOG (symmetric mixture of Gaussian modes), which significantly decreases the complexity of DNNs through low-bit fixed-point quantization. SYMOG is a novel soft quantization method such that the learning task and the quantization are solved simultaneously. During training the weight distribution changes from an unimodal Gaussian distribution to a symmetric mixture of Gaussians, where each mean value belongs to a particular fixed-point mode. We evaluate our approach with different architectures (LeNet5, VGG7, VGG11, DenseNet) on common benchmark data sets (MNIST, CIFAR-10, CIFAR-100) and we compare with state-of-the-art quantization approaches. We achieve excellent results and outperform 2-bit state-of-the-art performance with an error rate of only 5.71% on CIFAR-10 and 27.65% on CIFAR-100. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络（DNNs）已被证明优于几种机器学习经典的基准方法。然而，他们具有很高的计算复杂度，需要强大的处理单元。尤其是在嵌入式系统部署的时候，模型的大小和推理时必须显著减少。我们建议SYMOG（高斯模式对称混合物），其显著降低DNNs的通过低位定点量化的复杂度。 SYMOG是一种新型的软量化方法，使得学习任务和量化被同时解决。期间从单峰高斯分布训练重量分布的变化，以高斯对称混合物，其中，每个平均值属于特定的定点模式。我们评估我们有共同的基准数据集不同的架构（LeNet5，VGG7，VGG11，DenseNet）（MNIST，CIFAR-10，CIFAR-100）的方式，我们与国家的最先进的量化方法进行比较。我们实现了优异的成绩，并优于国家的最先进的2位的性能仅为5.71％上CIFAR-10的错误率和27.65％的CIFAR-100。</font>
</div>


<hr>
<div id="paper22"> <b>22. Variable-Bitrate Neural Compression via Bayesian Arithmetic Coding</b>  <a href="https://arxiv.org/pdf/2002.08158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yibo Yang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bamler%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Bamler</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mandt%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephan Mandt</a><br>
<font size="3">
Abstract: Deep Bayesian latent variable models have enabled new approaches to both model and data compression. Here, we propose a new algorithm for compressing latent representations in deep probabilistic models, such as variational autoencoders, in post-processing. The approach thus separates model design and training from the compression task. Our algorithm generalizes arithmetic coding to the continuous domain, using adaptive discretization accuracy that exploits estimates of posterior uncertainty. A consequence of the "plug and play" nature of our approach is that various rate-distortion trade-offs can be achieved with a single trained model, eliminating the need to train multiple models for different bit rates. Our experimental results demonstrate the importance of taking into account posterior uncertainties, and show that image compression with the proposed algorithm outperforms JPEG over a wide range of bit rates using only a single machine learning model. Further experiments on Bayesian neural word embeddings demonstrate the versatility of the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深贝叶斯潜变量模型已经启用了新的方法来既模型和数据压缩。在这里，我们提出了压缩在深概率模型潜表示，如变自动编码，在后处理的新算法。该方法从而分离压缩任务模型的设计和培训。我们的算法推广算术编码连续域，采用自适应离散精度，它利用后不确定性的估值。我们的做法的“即插即用”特性的结果是，各率失真权衡可以用一个单一的训练模型来实现，无需训练多模型不同的比特率。我们的实验结果表明，考虑到不确定因素后的重要性，并显示在宽范围内只使用一台机器学习模型的比特率的算法性能优于JPEG是图像压缩。贝叶斯神经字的嵌入进一步的实验证明了该方法的通用性。</font>
</div>


<hr>
<div id="paper23"> <b>23. Randomized Smoothing of All Shapes and Sizes</b>  <a href="https://arxiv.org/pdf/2002.08118" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Greg Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tony Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Edward Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Salman%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hadi Salman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Razenshteyn%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ilya Razenshteyn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jerry Li</a><br>
<font size="3">
Abstract: Randomized smoothing is a recently proposed defense against adversarial attacks that has achieved state-of-the-art provable robustness against $\ell_2$ perturbations. Soon after, a number of works devised new randomized smoothing schemes for other metrics, such as $\ell_1$ or $\ell_\infty$; however, for each geometry, substantial effort was needed to derive new robustness guarantees. This begs the question: can we find a general theory for randomized smoothing? In this work we propose a novel framework for devising and analyzing randomized smoothing schemes, and validate its effectiveness in practice. Our theoretical contributions are as follows: (1) We show that for an appropriate notion of "optimal", the optimal smoothing distributions for any "nice" norm have level sets given by the *Wulff Crystal* of that norm. (2) We propose two novel and complementary methods for deriving provably robust radii for any smoothing distribution. Finally, (3) we show fundamental limits to current randomized smoothing techniques via the theory of *Banach space cotypes*. By combining (1) and (2), we significantly improve the state-of-the-art certified accuracy in $\ell_1$ on standard datasets. On the other hand, using (3), we show that, without more information than label statistics under random input perturbations, randomized smoothing cannot achieve nontrivial certified accuracy against perturbations of $\ell_\infty$-norm $\Omega(1/\sqrt d)$, when the input dimension $d$ is large. We provide code in this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随机平滑是对已取得国家的最先进的可证明可以有效抵抗$ \ $ ell_2扰动敌对攻击最近提出的辩护。不久之后，一系列的工程，设计出新的随机平滑方案的其他指标，如$ \ $ ell_1或$ \ ell_ \ infty $;然而，对于每一个几何体，大量的努力，需要推导出新的鲁棒性的保证。这引出了一个问题：我们可以找到随机平滑的一般理论？在这项工作中，我们提出了一个新的框架设计和分析随机平滑方案，并在实践中验证其有效性。我们的理论贡献如下：（1）我们表明，“最优”的适当观念，优化平滑分布的任何“好”的标准必须由*乌尔夫水晶给出水平集*是规范的。 （2）我们提出用于推导可证明坚固的半径为任何平滑分布两种新型和互补的方法。最后，（3）我们将展示通过*的Banach空间cotypes的理论，目前随机平滑技术的根本限制*。通过组合（1）和（2），我们显著改善标准数据集在$ \ $ ell_1的国家的最先进的认证精度。在另一方面，用（3），我们发现，没有比在随机输入扰动标签统计信息的详细信息，随机平滑无法实现对$ \ ell_ \ infty $范数$ \欧米茄的扰动平凡的认证精度（1 / \开方d）$，当输入尺寸$ d $大。我们在这个HTTP URL提供的代码。</font>
</div>


<hr>
<div id="paper24"> <b>24. Hierarchical Quantized Autoencoders</b>  <a href="https://arxiv.org/pdf/2002.08111" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Williams%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Will Williams</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ringer%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sam Ringer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ash%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Ash</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hughes%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Hughes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=MacLeod%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David MacLeod</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dougherty%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jamie Dougherty</a><br>
<font size="3">
Abstract: Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and high-level features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational AutoEncoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a more probabilistic framing of the VQ-VAE, of which previous work is a limiting case. Our hierarchy produces a Markovian series of latent variables that reconstruct high-quality images which retain semantically meaningful features. These latents can then be further used to generate realistic samples. We provide qualitative and quantitative evaluations of reconstructions and samples on the CelebA and MNIST datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管在训练神经网络的有损图像压缩的进步，目前的做法不能在非常低的比特率，以保持两者感知质量和高层次的特点。通过学习与矢量量化变自动编码（VQ-VAES）离散表示最近成功的鼓舞，我们鼓励使用VQ-VAES的层次达到压缩的高因素。我们表明，量化和层次潜在结构的组合有助于基于可能性的图像压缩。这使我们介绍VQ-VAE，其中以前的工作是一种极限情况的概率更取景。我们的层次产生了马氏一系列潜在变量是重构的高品质的图像保留语义上有意义的功能。然后，这些latents可以进一步用于产生逼真的样品。我们提供的CelebA和MNIST数据集重建和样品的定性和定量评估。</font>
</div>


<hr>
<div id="paper25"> <b>25. Neural Networks on Random Graphs</b>  <a href="https://arxiv.org/pdf/2002.08104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Janik%2C+R+A" target="_blank" rel="noopener" style="color:#0000EE;">Romuald A. Janik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nowak%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aleksandra Nowak</a><br>
<font size="3">
Abstract: We performed a massive evaluation of neural networks with architectures corresponding to random graphs of various types. Apart from the classical random graph families including random, scale-free and small world graphs, we introduced a novel and flexible algorithm for directly generating random directed acyclic graphs (DAG) and studied a class of graphs derived from functional resting state fMRI networks. A majority of the best performing networks were indeed in these new families. We also proposed a general procedure for turning a graph into a DAG necessary for a feed-forward neural network. We investigated various structural and numerical properties of the graphs in relation to neural network test accuracy. Since none of the classical numerical graph invariants by itself seems to allow to single out the best networks, we introduced new numerical characteristics that selected a set of quasi-1-dimensional graphs, which were the majority among the best performing networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们进行神经网络的大规模评估对应于不同类型的随机图的架构。除了经典的随机图家庭包括无规，无标度和小世界图形，我们介绍了用于直接产生随机向无环图（DAG）和研究了类从功能的静止状态的fMRI网络导出图形的一种新颖的和灵活的算法。大多数表现最好的网络确实在这些新的家庭。我们还提出了把一个图形转化为前馈神经网络所必需的DAG的一般过程。我们研究了有关神经网络的测试精度图形的各种结构和数值性质。由于没有一个经典的数值图形不变量本身似乎允许挑出最佳的网络中，我们介绍了该选择的一组准一维图形，这是大多数表现最佳的网络之间的新的数字特征。</font>
</div>


<hr>
<div id="paper26"> <b>26. Enlarging Discriminative Power by Adding an Extra Class in Unsupervised  Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2002.08041" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+H+H" target="_blank" rel="noopener" style="color:#0000EE;">Hai H. Tran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ahn%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sumyeong Ahn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taeyoung Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yung Yi</a><br>
<font size="3">
Abstract: In this paper, we study the problem of unsupervised domain adaptation that aims at obtaining a prediction model for the target domain using labeled data from the source domain and unlabeled data from the target domain. There exists an array of recent research based on the idea of extracting features that are not only invariant for both domains but also provide high discriminative power for the target domain. In this paper, we propose an idea of empowering the discriminativeness: Adding a new, artificial class and training the model on the data together with the GAN-generated samples of the new class. The trained model based on the new class samples is capable of extracting the features that are more discriminative by repositioning data of current classes in the target domain and therefore drawing the decision boundaries more effectively. Our idea is highly generic so that it is compatible with many existing methods such as DANN, VADA, and DIRT-T. We conduct various experiments for the standard data commonly used for the evaluation of unsupervised domain adaptations and demonstrate that our algorithm achieves the SOTA performance for many scenarios. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们研究了无人监管的领域适应性的问题，其目的是获得一个预测模型使用从目标域源域和未标记数据标记数据目标域。存在最近研究的基础上提取功能，不仅不变量两个领域，但也提供了目标域的高辨别力的想法的数组。在本文中，我们建议赋权discriminativeness的想法：添加一个新的，人造类和数据与新类的GAN-生成的样本一起训练模型。基于新类的样品训练的模型能够提取是通过在目标域中重新定位当前类的数据，并因此更有效地绘制决策边界更有辨别力的特征的。我们的想法是非常通用的，所以它与现有的许多方法，如DANN，VADA，和污垢-T兼容。我们进行了各种实验常用的无监督域适应的评价标准数据，并证明我们的算法实现在很多场景的SOTA性能。</font>
</div>


<hr>
<div id="paper27"> <b>27. Globally optimal point set registration by joint symmetry plane fitting</b>  <a href="https://arxiv.org/pdf/2002.07988" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lan Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haomin Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kneip%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laurent Kneip</a><br>
<font size="3">
Abstract: The present work proposes a solution to the challenging problem of registering two partial point sets of the same object with very limited overlap. We leverage the fact that most objects found in man-made environments contain a plane of symmetry. By reflecting the points of each set with respect to the plane of symmetry, we can largely increase the overlap between the sets and therefore boost the registration process. However, prior knowledge about the plane of symmetry is generally unavailable or at least very hard to find, especially with limited partial views, and finding this plane could strongly benefit from a prior alignment of the partial point sets. We solve this chicken-and-egg problem by jointly optimizing the relative pose and symmetry plane parameters, and notably do so under global optimality by employing the branch-and-bound (BnB) paradigm. Our results demonstrate a great improvement over the current state-of-the-art in globally optimal point set registration for common objects. We furthermore show an interesting application of our method to dense 3D reconstruction of scenes with repetitive objects. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本工作提出了一种解决方案，以登记所述同一物体的两个部分点集与非常有限的重叠有挑战性的问题。我们利用一个事实，即在人工环境中最对象包含对称面。通过反映每个组的点相对于对称面，我们可以在很大程度上增加集之间的重叠，并且因此提高了注册过程。然而，关于对称平面先验知识一般是不可用的，或者至少很难找到，特别是有限的局部视图，并发现这种飞机能够从强劲的部分点集的前对准受益。我们通过联合优化的相对姿态和对称平面参数解决这个鸡和蛋的问题，并通过采用分支定界（泡泡堂）范例尤其是这样做的全局最优下。我们的研究结果表明在当前国家的最先进的，共同的对象全局最优的点集注册了很大的改进。我们还表明我们的方法的一个有趣的应用程序，以密集的三维重建与重复对象的场景。</font>
</div>


<hr>
<div id="paper28"> <b>28. Block Switching: A Stochastic Approach for Deep Learning Security</b>  <a href="https://arxiv.org/pdf/2002.07920" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyue Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pin-Yu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xue Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chin%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Chin</a><br>
<font size="3">
Abstract: Recent study of adversarial attacks has revealed the vulnerability of modern deep learning models. That is, subtly crafted perturbations of the input can make a trained network with high accuracy produce arbitrary incorrect predictions, while maintain imperceptible to human vision system. In this paper, we introduce Block Switching (BS), a defense strategy against adversarial attacks based on stochasticity. BS replaces a block of model layers with multiple parallel channels, and the active channel is randomly assigned in the run time hence unpredictable to the adversary. We show empirically that BS leads to a more dispersed input gradient distribution and superior defense effectiveness compared with other stochastic defenses such as stochastic activation pruning (SAP). Compared to other defenses, BS is also characterized by the following features: (i) BS causes less test accuracy drop; (ii) BS is attack-independent and (iii) BS is compatible with other defenses and can be used jointly with others. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对抗性攻击最近的研究已经揭示现代深度学习模式的脆弱性。也就是说，输入的巧妙制作的扰动可以使高精度生产任意不正确的预测训练有素的网络，同时保持察觉不到人类视觉系统。在本文中，我们介绍了块切换（BS），针对基于随机性对抗攻击的防御策略。 BS替换具有多个平行通道的模型层的块，以及有源沟道在运行时间，因此无法预测到对手随机分配。我们表明凭经验该BS导致更分散的输入梯度分布，并与其他随机防御相比优越防御效果如随机活化修剪（SAP）。相比其他防御，BS的特征还在于以下特征：（ⅰ）BS引起较少测试精度降; （二）BS是攻击独立及（iii）BS与其他防御兼容，可以与他人共同使用。</font>
</div>


<hr>
<div id="paper29"> <b>29. LocoGAN -- Locally Convolutional GAN</b>  <a href="https://arxiv.org/pdf/2002.07897" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Struski%2C+%C5%81" target="_blank" rel="noopener" style="color:#0000EE;">Łukasz Struski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Knop%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Szymon Knop</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tabor%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jacek Tabor</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Daniec%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wiktor Daniec</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Spurek%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Przemysław Spurek</a><br>
<font size="3">
Abstract: In the paper we construct a fully convolutional GAN model: LocoGAN, which latent space is given by noise-like images of possibly different resolutions. The learning is local, i.e. we process not the whole noise-like image, but the sub-images of a fixed size. As a consequence LocoGAN can produce images of arbitrary dimensions e.g. LSUN bedroom data set. Another advantage of our approach comes from the fact that we use the position channels, which allows the generation of fully periodic (e.g. cylindrical panoramic images) or almost periodic ,,infinitely long" images (e.g. wall-papers). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们构建完全卷积GAN模型：LocoGAN，其潜在空间由下式给出噪声状的可能不同的分辨率的图像。学习是本地的，即，我们处理不整类噪声图像，但一个固定尺寸的子图像。因此LocoGAN可以产生任意的尺寸例如图像LSUN卧室的数据集。我们的方法的另一个优势来自于一个事实，我们使用的位置的通道，这使得全周期（例如圆柱形全景图像）的生成或几乎周期,,无限长”的图像（如墙纸）。</font>
</div>


<hr>
<div id="paper30"> <b>30. Towards Query-Efficient Black-Box Adversary with Zeroth-Order Natural  Gradient Descent</b>  <a href="https://arxiv.org/pdf/2002.07891" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pu Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pin-Yu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyue Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xue Lin</a><br>
<font size="3">
Abstract: Despite the great achievements of the modern deep neural networks (DNNs), the vulnerability/robustness of state-of-the-art DNNs raises security concerns in many application domains requiring high reliability. Various adversarial attacks are proposed to sabotage the learning performance of DNN models. Among those, the black-box adversarial attack methods have received special attentions owing to their practicality and simplicity. Black-box attacks usually prefer less queries in order to maintain stealthy and low costs. However, most of the current black-box attack methods adopt the first-order gradient descent method, which may come with certain deficiencies such as relatively slow convergence and high sensitivity to hyper-parameter settings. In this paper, we propose a zeroth-order natural gradient descent (ZO-NGD) method to design the adversarial attacks, which incorporates the zeroth-order gradient estimation technique catering to the black-box attack scenario and the second-order natural gradient descent to achieve higher query efficiency. The empirical evaluations on image classification datasets demonstrate that ZO-NGD can obtain significantly lower model query complexities compared with state-of-the-art attack methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管现代深层神经网络（DNNs）的巨大成就，国家的最先进的DNNs的脆弱性/稳健性提出了在许多应用领域的安全问题要求高可靠性。各种敌对攻击，提出了破坏DNN模型的学习表现。在这些，暗箱敌对攻击方法已经收到由于其实用性和简单的特殊关注。黑盒攻击通常是为了保持隐身和低成本喜欢少的查询。然而，大多数的当前黑箱攻击方法采用一阶梯度下降法，其可以配有某些缺陷，诸如相对慢的收敛和超参数设置高灵敏度。在本文中，我们提出了一个零阶的自然梯度下降（ZO-NGD）方法来设计敌对攻击，其采用了零阶梯度估计技术迎合黑盒攻击场景和第二阶固有梯度下降以实现更高的查询效率。使图像数据集分类的经验评价表明，ZO-NGD可与国家的最先进的攻击方法相比获得显著下模型查询复杂性。</font>
</div>


<hr>
<div id="paper31"> <b>31. CBIR using features derived by Deep Learning</b>  <a href="https://arxiv.org/pdf/2002.07877" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Maji%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Subhadip Maji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bose%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Smarajit Bose</a><br>
<font size="3">
Abstract: In a Content Based Image Retrieval (CBIR) System, the task is to retrieve similar images from a large database given a query image. The usual procedure is to extract some useful features from the query image, and retrieve images which have similar set of features. For this purpose, a suitable similarity measure is chosen, and images with high similarity scores are retrieved. Naturally the choice of these features play a very important role in the success of this system, and high level features are required to reduce the semantic gap. In this paper, we propose to use features derived from pre-trained network models from a deep-learning convolution network trained for a large image classification problem. This approach appears to produce vastly superior results for a variety of databases, and it outperforms many contemporary CBIR systems. We analyse the retrieval time of the method, and also propose a pre-clustering of the database based on the above-mentioned features which yields comparable results in a much shorter time in most of the cases. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于内容的图像检索（CBIR）系统，任务是从给定的查询图像的大型数据库中检索类似的图像。通常的程序是从查询图像中提取一些有用的功能，和检索具有类似的一组特征的图像。为此目的，合适的相似性度量被选择，并以高相似性分数的图像检索。当然这些功能的选择发挥该系统的成功非常重要的作用，而高层次的特点是要求减少语义鸿沟。在本文中，我们建议使用从预先训练网络模型导出的特征，从训练大图像分类问题深学习卷积网络。这种方法似乎产生远远优于结果的各种数据库，它优于许多当代CBIR系统。我们分析了该方法的检索时间，并且还提出了一种基于其产生类似的结果在更短的时间在大多数的情况下，上述特征数据库的预集群。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-19</title>
    <url>/2020/02/19/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-19/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Camera Model Anonymisation with Augmented cGANs <a href="https://arxiv.org/pdf/2002.07798" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> MAST: A Memory-Augmented Self-supervised Tracker <a href="https://arxiv.org/pdf/2002.07793" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Computational optimization of convolutional neural networks using  separated filters architecture <a href="https://arxiv.org/pdf/2002.07754" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Towards Bounding-Box Free Panoptic Segmentation <a href="https://arxiv.org/pdf/2002.07705" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Voxel-Based Indoor Reconstruction From HoloLens Triangle Meshes <a href="https://arxiv.org/pdf/2002.07689" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings <a href="https://arxiv.org/pdf/2002.07662" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Neural arbitrary style transfer for portrait images using the attention  mechanism <a href="https://arxiv.org/pdf/2002.07643" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> An interpretable classifier for high-resolution breast cancer screening  images utilizing weakly supervised localization <a href="https://arxiv.org/pdf/2002.07613" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Few-Shot Few-Shot Learning and the role of Spatial Attention <a href="https://arxiv.org/pdf/2002.07522" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> NoiseBreaker: Gradual Image Denoising Guided by Noise Analysis <a href="https://arxiv.org/pdf/2002.07487" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Motion Deblurring using Spatiotemporal Phase Aperture Coding <a href="https://arxiv.org/pdf/2002.07483" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Knowledge Integration Networks for Action Recognition <a href="https://arxiv.org/pdf/2002.07471" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Automated Cardiothoracic Ratio Calculation and Cardiomegaly Detection  using Deep Learning Approach <a href="https://arxiv.org/pdf/2002.07468" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Registration of multi-view point sets under the perspective of  expectation-maximization <a href="https://arxiv.org/pdf/2002.07464" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> V4D:4D Convolutional Neural Networks for Video-level Representation  Learning <a href="https://arxiv.org/pdf/2002.07442" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> EHSOD: CAM-Guided End-to-end Hybrid-Supervised Object Detection with  Cascade Refinement <a href="https://arxiv.org/pdf/2002.07421" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Universal-RCNN: Universal Object Detector via Transferable Graph R-CNN <a href="https://arxiv.org/pdf/2002.07417" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> DivideMix: Learning with Noisy Labels as Semi-supervised Learning <a href="https://arxiv.org/pdf/2002.07394" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> High-Order Paired-ASPP Networks for Semantic Segmenation <a href="https://arxiv.org/pdf/2002.07371" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Multi-Task Learning from Videos via Efficient Inter-Frame Attention <a href="https://arxiv.org/pdf/2002.07362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Constraining Temporal Relationship for Action Localization <a href="https://arxiv.org/pdf/2002.07358" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Restricted Structural Random Matrix for Compressive Sensing <a href="https://arxiv.org/pdf/2002.07346" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> 3D Gated Recurrent Fusion for Semantic Scene Completion <a href="https://arxiv.org/pdf/2002.07269" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Dual-Attention GAN for Large-Pose Face Frontalization <a href="https://arxiv.org/pdf/2002.07227" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Multilinear Compressive Learning with Prior Knowledge <a href="https://arxiv.org/pdf/2002.07203" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> The Tree Ensemble Layer: Differentiability meets Conditional Computation <a href="https://arxiv.org/pdf/2002.07772" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Learning Bijective Feature Maps for Linear ICA <a href="https://arxiv.org/pdf/2002.07766" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Deep Learning in Medical Ultrasound Image Segmentation: a Review <a href="https://arxiv.org/pdf/2002.07703" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Robust Quantization: One Model to Rule Them All <a href="https://arxiv.org/pdf/2002.07686" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Image Entropy for Classification and Analysis of Pathology Slides <a href="https://arxiv.org/pdf/2002.07621" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Deflecting Adversarial Attacks <a href="https://arxiv.org/pdf/2002.07405" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Picking Winning Tickets Before Training by Preserving Gradient Flow <a href="https://arxiv.org/pdf/2002.07376" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Evolutionary Optimization of Deep Learning Activation Functions <a href="https://arxiv.org/pdf/2002.07224" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> AIBench: An Agile Domain-specific Benchmarking Methodology and an AI  Benchmark Suite <a href="https://arxiv.org/pdf/2002.07162" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Camera Model Anonymisation with Augmented cGANs</b>  <a href="https://arxiv.org/pdf/2002.07798" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Andrews%2C+J+T+A" target="_blank" rel="noopener" style="color:#0000EE;">Jerone T. A. Andrews</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yidan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Griffin%2C+L+D" target="_blank" rel="noopener" style="color:#0000EE;">Lewis D. Griffin</a><br>
<font size="3">
Abstract: The model of camera that was used to capture a particular photographic image (model attribution) can be inferred from model-specific artefacts present within the image. Typically these artefacts are found in high-frequency pixel patterns, rather than image content. Model anonymisation is the process of transforming these artefacts such that the apparent capture model is changed. Improved methods for attribution and anonymisation are important for improving digital forensics, and understanding its limits. Through conditional adversarial training, we present an approach for learning these transformations. Significantly, we augment the objective with the losses from pre-trained auxiliary model attribution classifiers that constrain the generator to not only synthesise discriminative high-frequency artefacts, but also salient image-based artefacts lost during image content suppression. Quantitative comparisons against a recent representative approach demonstrate the efficacy of our framework in a non-interactive black-box setting. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：这是用于捕捉特定摄影图像（模型归属）摄像机的模型可以从图像中存在特定于模型的伪影来推断。典型地，这些伪像在高频像素图案中发现的，而不是图像内容。模型匿名化是转化这些伪影，使得表观捕获模型改变的过程。归属和匿名化的改进方法是提高数字取证，并了解其局限性重要。通过有条件的对抗性训练，我们提出了学习这些变革的方法。显著，我们扩充了客观与预训练辅助模型属性分类器限制发电机不仅合成辨别高频文物，而且图像内容抑制过程中丢失突出的基于图像的文物的损失。针对最近的代表性方法定量比较证明非交互式黑盒设置我们的框架的有效性。</font>
</div>


<hr>
<div id="paper2"> <b>2. MAST: A Memory-Augmented Self-supervised Tracker</b>  <a href="https://arxiv.org/pdf/2002.07793" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihang Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erika Lu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weidi Xie</a><br>
<font size="3">
Abstract: Recent interest in self-supervised dense tracking has yielded rapid progress, but performance still remains far from supervised methods. We propose a dense tracking model trained on videos without any annotations that surpasses previous self-supervised methods on existing benchmarks by a significant margin (+15%), and achieves performance comparable to supervised methods. In this paper, we first reassess the traditional choices used for self-supervised training and reconstruction loss by conducting thorough experiments that finally elucidate the optimal choices. Second, we further improve on existing methods by augmenting our architecture with a crucial memory component. Third, we benchmark on large-scale semi-supervised video object segmentation(aka. dense tracking), and propose a new metric: generalizability. Our first two contributions yield a self-supervised network that for the first time is competitive with supervised methods on standard evaluation metrics of dense tracking. When measuring generalizability, we show self-supervised approaches are actually superior to the majority of supervised methods. We believe this new generalizability metric can better capture the real-world use-cases for dense tracking, and will spur new interest in this research direction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在自我监督密集跟踪最近的兴趣已经取得了飞速的进步，但表现还是从监督的方法仍然远。我们提出了基于视频的培训而没有超越现有基准以前的自我监督方法，通过一个显著保证金（+ 15％）的任何注释的密集跟踪模型，并实现了性能堪比监督方法。在本文中，我们首先重新评估用于通过进行深入的实验终于阐明最佳选择，自我指导训练和重建丢失传统的选择。其次，我们进一步对现有的方法通过增加我们的架构的关键存储组件提高。第三，我们在基准大型半监督视频对象分割（又名密集跟踪），并提出了新的指标：普遍性。我们的第一个两个捐款产生自我监督的网络，第一次是与密集跟踪的标准评价指标的监督方法的竞争力。当测量普遍性，我们展现自我监督的方法实际上是优于大多数的监督方法。我们相信这个新的普遍性指标可以更好地反映真实世界的使用情况进行跟踪密集，并且将刺激这一研究方向的新兴趣。</font>
</div>


<hr>
<div id="paper3"> <b>3. Computational optimization of convolutional neural networks using  separated filters architecture</b>  <a href="https://arxiv.org/pdf/2002.07754" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Limonova%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elena Limonova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sheshkus%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Sheshkus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nikolaev%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dmitry Nikolaev</a><br>
<font size="3">
Abstract: This paper considers a convolutional neural network transformation that reduces computation complexity and thus speedups neural network processing. Usage of convolutional neural networks (CNN) is the standard approach to image recognition despite the fact they can be too computationally demanding, for example for recognition on mobile platforms or in embedded systems. In this paper we propose CNN structure transformation which expresses 2D convolution filters as a linear combination of separable filters. It allows to obtain separated convolutional filters by standard training algorithms. We study the computation efficiency of this structure transformation and suggest fast implementation easily handled by CPU or GPU. We demonstrate that CNNs designed for letter and digit recognition of proposed structure show 15% speedup without accuracy loss in industrial image recognition system. In conclusion, we discuss the question of possible accuracy decrease and the application of proposed transformation to different recognition problems. convolutional neural networks, computational optimization, separable filters, complexity reduction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文认为，降低计算复杂度卷积神经网络转型，从而加速比神经网络处理。卷积神经网络（CNN）的用法是标准的做法，以图像识别，尽管它们可以过计算要求，例如用于识别移动平台上或在嵌入式系统中的事实。在本文中，我们提出，其表达二维卷积滤波器作为可分离滤波器的线性组合CNN结构的变换。它允许通过标准训练算法，以获得分离的卷积滤波器。我们研究这种结构变换的计算效率，建议快速实现由CPU或GPU容易处理。我们证明细胞神经网络的设计文字和提出的结构的数字识别显示无工业图像识别系统的精度损失15％的速度提升。最后，我们讨论可能的精度下降的问题，并提出了改造，以不同的识别问题中的应用。卷积神经网络，计算优化，可分离过滤器，降低复杂性。</font>
</div>


<hr>
<div id="paper4"> <b>4. Towards Bounding-Box Free Panoptic Segmentation</b>  <a href="https://arxiv.org/pdf/2002.07705" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bonde%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Ujwal Bonde</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alcantarilla%2C+P+F" target="_blank" rel="noopener" style="color:#0000EE;">Pablo F. Alcantarilla</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leutenegger%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Leutenegger</a><br>
<font size="3">
Abstract: In this work we introduce a new bounding-box free network (BBFNet) for panoptic segmentation. Panoptic segmentation is an ideal problem for a bounding-box free approach as it already requires per-pixel semantic class labels. We use this observation to exploit class boundaries from an off-the-shelf semantic segmentation network and refine them to predict instance labels. Towards this goal BBFNet predicts coarse watershed levels and use it to detect large instance candidates where boundaries are well defined. For smaller instances, whose boundaries are less reliable, BBFNet also predicts instance centers by means of Hough voting followed by mean-shift to reliably detect small objects. A novel triplet loss network helps merging fragmented instances while refining boundary pixels. Our approach is distinct from previous works in panoptic segmentation that rely on a combination of a semantic segmentation network with a computationally costly instance segmentation network based on bounding boxes, such as Mask R-CNN, to guide the prediction of instance labels using a Mixture-of-Expert (MoE) approach. We benchmark our non-MoE method on Cityscapes and Microsoft COCO datasets and show competitive performance with other MoE based approaches while outperfroming exisiting non-proposal based approaches. We achieve this while been computationally more efficient in terms of number of parameters and FLOPs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们引入了全景分割新的边界框免费网络（BBFNet）。全景分割是一个包围盒免费接近理想的问题，因为它已经要求每个像素的含义类的标签。我们使用这种观察从关闭的，现成的语义分割网络攻击阶级界限和完善他们预测实例标识。为了实现这一目标BBFNet预测粗分水岭水平，并用它来检测其中的界限的明确界定大型实例的候选人。对于较小的情况下，其边界是不可靠的，BBFNet也紧跟着平均移动可靠地检测小物体霍夫投票的方式预测实例中心。一种新型的三重损失的网络可以帮助合并分散的情况下，同时优化边界像素。我们的方法是从在全景分割依赖于一个语义分割网络与基于包围盒，如面膜R-CNN，在计算上昂贵的实例分割的网络的组合之前的作品不同，以指导实例标签的使用Mixture-预测的-专家（MOE）的方法。我们的基准上风情和微软COCO数据集我们的非教育部方法，并显示与其他基于教育部的方法有竞争力的性能，同时outperfroming exisiting基于非建议的方法。我们做到这一点，而在参数和触发器数量方面已经计算更高效。</font>
</div>


<hr>
<div id="paper5"> <b>5. Voxel-Based Indoor Reconstruction From HoloLens Triangle Meshes</b>  <a href="https://arxiv.org/pdf/2002.07689" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=H%C3%BCbner%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">P. Hübner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weinmann%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">M. Weinmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wursthorn%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">S. Wursthorn</a><br>
<font size="3">
Abstract: Current mobile augmented reality devices are often equipped with range sensors. The Microsoft HoloLens for instance is equipped with a Time-Of-Flight (ToF) range camera providing coarse triangle meshes that can be used in custom applications. We suggest to use the triangle meshes for the automatic generation of indoor models that can serve as basis for augmenting their physical counterpart with location-dependent information. In this paper, we present a novel voxel-based approach for automated indoor reconstruction from unstructured three-dimensional geometries like triangle meshes. After an initial voxelization of the input data, rooms are detected in the resulting voxel grid by segmenting connected voxel components of ceiling candidates and extruding them downwards to find floor candidates. Semantic class labels like 'Wall', 'Wall Opening', 'Interior Object' and 'Empty Interior' are then assigned to the room voxels in-between ceiling and floor by a rule-based voxel sweep algorithm. Finally, the geometry of the detected walls and their openings is refined in voxel representation. The proposed approach is not restricted to Manhattan World scenarios and does not rely on room surfaces being planar. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目前的移动增强现实设备通常装备有多种传感器。例如微软HoloLens配备有越时间的飞行时间（TOF）范围照相机提供可以在定制的应用程序中使用粗三角形网格。我们建议使用三角形网格的自动生成室内模式，可以作为基础，与位置相关的信息，增强其物理副本的。在本文中，我们提出了从非结构化三维几何形状像三角形网格自动室内重建的新的基于体素的方法。所述输入数据的初始体素化后，房间在所得到的体素网格通过分割天花板候选的体素连接部件和将它们挤出向下找到地板候选检测。语义类的标签，如“长城”，“长城开”，“内政部对象”和“空”内部，然后通过基于规则的体素扫描算法分配到在中间的房间素天花板和地板。最后，将检测到的墙壁和其开口的几何形状中的体素表示精制而成。所提出的方法不限于曼哈顿的世界场景，不依赖于房间表面是平面的。</font>
</div>


<hr>
<div id="paper6"> <b>6. FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings</b>  <a href="https://arxiv.org/pdf/2002.07662" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Salscheider%2C+N+O" target="_blank" rel="noopener" style="color:#0000EE;">Niels Ole Salscheider</a><br>
<font size="3">
Abstract: Most state of the art object detectors output multiple detections per object. The duplicates are removed in a post-processing step called Non-Maximum Suppression. Classical Non-Maximum Suppression has shortcomings in scenes that contain objects with high overlap: The idea of this heuristic is that a high bounding box overlap corresponds to a high probability of having a duplicate. We propose FeatureNMS to solve this problem. FeatureNMS recognizes duplicates not only based on the intersection over union between bounding boxes, but also based on the difference of feature vectors. These feature vectors can encode more information like visual appearance. Our approach outperforms classical NMS and derived approaches and achieves state of the art performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：每个对象的技术对象检测器的输出多次检测的大多数状态。重复项以称为非最大抑制后处理步骤中除去。经典非最大抑制在包含有高度重叠对象的场景缺点：这种启发式的想法是，高边界框重叠对应于具有重复的概率高。我们建议FeatureNMS来解决这个问题。 FeatureNMS重复不仅承认基于边界框之间的交叉点上的结合，同时也基于特征向量的差异。这些特征向量可以编码更像视觉外观的信息。我们的方法优于传统的NMS和衍生方法和实现的先进的性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. Neural arbitrary style transfer for portrait images using the attention  mechanism</b>  <a href="https://arxiv.org/pdf/2002.07643" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Berezin%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">S. A. Berezin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Volkova%2C+V+M" target="_blank" rel="noopener" style="color:#0000EE;">V.M. Volkova</a><br>
<font size="3">
Abstract: Arbitrary style transfer is the task of synthesis of an image that has never been seen before, using two given images: content image and style image. The content image forms the structure, the basic geometric lines and shapes of the resulting image, while the style image sets the color and texture of the result. The word "arbitrary" in this context means the absence of any one pre-learned style. So, for example, convolutional neural networks capable of transferring a new style only after training or retraining on a new amount of data are not con-sidered to solve such a problem, while networks based on the attention mech-anism that are capable of performing such a transformation without retraining - yes. An original image can be, for example, a photograph, and a style image can be a painting of a famous artist. The resulting image in this case will be the scene depicted in the original photograph, made in the stylie of this picture. Recent arbitrary style transfer algorithms make it possible to achieve good re-sults in this task, however, in processing portrait images of people, the result of such algorithms is either unacceptable due to excessive distortion of facial features, or weakly expressed, not bearing the characteristic features of a style image. In this paper, we consider an approach to solving this problem using the combined architecture of deep neural networks with a attention mechanism that transfers style based on the contents of a particular image segment: with a clear predominance of style over the form for the background part of the im-age, and with the prevalence of content over the form in the image part con-taining directly the image of a person. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：任意的方式传输图像的合成一个从未见过的，使用两个给定图像的任务：内容形象和风格的图像。内容图像的形式的结构中，基本的几何线和所得到的图像的形状，而风格图像设置结果的颜色和质地。在这方面的手段单词“任意”没有任何一个预先得知的风格。因此，例如，能够仅培训后转移，新的样式或再上一个新的数据量的卷积神经网络是不是CON-sidered来解决这样的问题，而基于网络的关注机甲anism能够执行的没有再培训这样的转变 - 是的。原始图像可以，例如，照片，和样式的图像可以是画一个著名的艺术家的。在这种情况下，所得的图像将是在原来的照片中描绘的场景，在该画面的stylie制成。最近任意的方式传输算法使人们有可能在这个任务中，实现良好的再sults，然而，在处理人的肖像图片，这些算法的结果要么是不可接受由于五官过度扭曲，或弱阳性，不承风格图像的特征。在本文中，我们考虑一种方法来使用深层神经网络相结合的体系结构与注意机制解决这一问题，基于特定图像段的内容传送风格：一个有风格上的形式为背景的明确优势图像，和与内容在图像部分中的患病率在形式直接CON-泰宁的人的图像。</font>
</div>


<hr>
<div id="paper8"> <b>8. An interpretable classifier for high-resolution breast cancer screening  images utilizing weakly supervised localization</b>  <a href="https://arxiv.org/pdf/2002.07613" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiqiu Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Phang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Phang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jungkyu Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kangning Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tyagi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sudarshini Tyagi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Heacock%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laura Heacock</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S+G" target="_blank" rel="noopener" style="color:#0000EE;">S. Gene Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moy%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linda Moy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kyunghyun Cho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Geras%2C+K+J" target="_blank" rel="noopener" style="color:#0000EE;">Krzysztof J. Geras</a><br>
<font size="3">
Abstract: Medical images differ from natural images in significantly higher resolutions and smaller regions of interest. Because of these differences, neural network architectures that work well for natural images might not be applicable to medical image analysis. In this work, we extend the globally-aware multiple instance classifier, a framework we proposed to address these unique properties of medical images. This model first uses a low-capacity, yet memory-efficient, network on the whole image to identify the most informative regions. It then applies another higher-capacity network to collect details from chosen regions. Finally, it employs a fusion module that aggregates global and local information to make a final prediction. While existing methods often require lesion segmentation during training, our model is trained with only image-level labels and can generate pixel-level saliency maps indicating possible malignant findings. We apply the model to screening mammography interpretation: predicting the presence or absence of benign and malignant lesions. On the NYU Breast Cancer Screening Dataset, consisting of more than one million images, our model achieves an AUC of 0.93 in classifying breasts with malignant findings, outperforming ResNet-34 and Faster R-CNN. Compared to ResNet-34, our model is 4.1x faster for inference while using 78.4% less GPU memory. Furthermore, we demonstrate, in a reader study, that our model surpasses radiologist-level AUC by a margin of 0.11. The proposed model is available online: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：医学图像自然图像显著更高的分辨率和利益较小的区域不同。由于这些差异，对于自然的图像工作得很好神经网络结构可能并不适用于医学图像分析。在这项工作中，我们扩展了全球意识的多个实例的分类，我们提出，解决医学图像的这些独特性的框架。这种模式首先使用整个图像上的低容量，又节省内存，网络找出最翔实的地区。然后，它适用于另一个更高容量的网络，从选择区域收集的详细信息。最后，它采用了汇集全球和本地的信息来做出最后预测的融合模块。尽管现有方法的培训过程中经常需要病变划分，我们的模型进行训练只图像级标签，可以生成像素级的显着图指示可能的恶性结果。我们的模型适用于乳房摄影筛检解释：预测良，恶性病变的存在或不存在。在NYU乳腺癌检查数据集，包括超过一个百万的图像，我们的模型实现了0.93的AUC在乳房恶性结果进行分类，表现优于RESNET-34和更快的R-CNN。相比于RESNET-34，我们的模型是4.1倍更快的推理，同时使用更少的78.4％GPU内存。此外，我们证明，在读者学习，我们的模型由0.11余量超过放射科级AUC。该模型可在网上：此HTTPS URL。</font>
</div>


<hr>
<div id="paper9"> <b>9. Few-Shot Few-Shot Learning and the role of Spatial Attention</b>  <a href="https://arxiv.org/pdf/2002.07522" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lifchitz%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yann Lifchitz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Avrithis%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yannis Avrithis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Picard%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sylvaine Picard</a><br>
<font size="3">
Abstract: Few-shot learning is often motivated by the ability of humans to learn new tasks from few examples. However, standard few-shot classification benchmarks assume that the representation is learned on a limited amount of base class data, ignoring the amount of prior knowledge that a human may have accumulated before learning new tasks. At the same time, even if a powerful representation is available, it may happen in some domain that base class data are limited or non-existent. This motivates us to study a problem where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. We adapt the representation in two stages, namely on the few base class data if available and on the even fewer data of new tasks. In doing so, we obtain from the pre-trained classifier a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base class data are few, the network cannot learn where to focus implicitly. We also show that a pre-trained network may be easily adapted to novel classes, without meta-learning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：很少次的学习往往是由人类从几个例子学习新任务的能力动机。然而，标准的为数不多的镜头分类基准假设表示。据悉在基类数据的数量有限，忽略了人类可以学习新的任务之前所积累的先验知识量。与此同时，即使强大的表示是可用的，它可能在某些领域发生的基类的数据是有限的或不存在。这促使我们研究其中从分类器上的不同的域的大规模数据集预先训练获得的表示中的问题，假设它的训练过程中没有访问，同时基类的数据被限制为每类几个例子和它们的作用就在眼前适应代表性的域，而不是从头学起。我们采用两个阶段的表现，即上如果有少数基类的数据和新的任务，甚至更少的数据。在此过程中，我们从预先训练的分类器空间注意，允许重点对象和抑制背景杂波地图获得。这是新的问题很重要，因为当基础类数据少，网络无法学习到哪里隐式对焦。我们还表明，预训练的网络可以很容易地适应新的课程，而无需元学习。</font>
</div>


<hr>
<div id="paper10"> <b>10. NoiseBreaker: Gradual Image Denoising Guided by Noise Analysis</b>  <a href="https://arxiv.org/pdf/2002.07487" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lemarchand%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Lemarchand</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nogues%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erwan Nogues</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pelcat%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maxime Pelcat</a><br>
<font size="3">
Abstract: Fully supervised deep-learning based denoisers are currently the most performing image denoising solutions. However, they require clean reference images. When the target noise is complex, e.g. composed of an unknown mixture of primary noises with unknown intensity, fully supervised solutions are limited by the difficulty to build a suited training set for the problem. This paper proposes a gradual denoising strategy that iteratively detects the dominating noise in an image, and removes it using a tailored denoiser. The method is shown to keep up with state of the art blind denoisers on mixture noises. Moreover, noise analysis is demonstrated to guide denoisers efficiently not only on noise type, but also on noise intensity. The method provides an insight on the nature of the encountered noise, and it makes it possible to extend an existing denoiser with new noise nature. This feature makes the method adaptive to varied denoising cases. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：全面监督深学习基础denoisers是目前最进行图像去噪的解决方案。然而，他们需要干净的参考图像。当目标噪声是复杂的，例如具有未知强度一次噪声的未知混合物组成，完全监督溶液通过的困难限制建立针对该问题的合适的训练集。本文提出了一种使用定制的降噪逐渐去噪策略，反复检测图像中的主要噪声，并将其删除。该方法被示出为跟上对混合物噪声的技术盲denoisers的状态。此外，噪声分析证明有效地引导denoisers不仅对噪声的类型，而且还取决于噪声强度。该方法提供关于所遇到的噪声的性质见识，并且它使得能够延伸，以与新的噪声性质的现有降噪。这一特征使得该方法自适应于变化去噪例。</font>
</div>


<hr>
<div id="paper11"> <b>11. Motion Deblurring using Spatiotemporal Phase Aperture Coding</b>  <a href="https://arxiv.org/pdf/2002.07483" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Elmalem%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shay Elmalem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Giryes%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raja Giryes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marom%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emanuel Marom</a><br>
<font size="3">
Abstract: Motion blur is a known issue in photography, as it limits the exposure time while capturing moving objects. Extensive research has been carried to compensate for it. In this work, a computational imaging approach for motion deblurring is proposed and demonstrated. Using dynamic phase-coding in the lens aperture during the image acquisition, the trajectory of the motion is encoded in an intermediate optical image. This encoding embeds both the motion direction and extent by coloring the spatial blur of each object. The color cues serve as prior information for a blind deblurring process, implemented using a convolutional neural network (CNN) trained to utilize such coding for image restoration. We demonstrate the advantage of the proposed approach over blind-deblurring with no coding and other solutions that use coded acquisition, both in simulation and real-world experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：运动模糊是摄影中已知的问题，因为它限制了曝光时间，同时捕捉移动的物体。大量研究已经进行，以弥补它。在这项工作中，运动去模糊的计算成像方法提出并论证。使用动态相位编码的图像采集期间，在透镜孔径，运动的轨迹的中间光学图像中被编码。这种编码由着色的每个对象的空间模糊嵌入两个运动方向和程度。颜色线索作为盲去模糊处理之前的信息，使用已训练的卷积神经网络（CNN），利用这样的编码的图像复原实现。我们证明了盲去模糊所提出的方法的优点没有编码和其他的解决方案，使用编码的收购，无论是在模拟和真实世界的实验。</font>
</div>


<hr>
<div id="paper12"> <b>12. Knowledge Integration Networks for Action Recognition</b>  <a href="https://arxiv.org/pdf/2002.07471" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiwen Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sheng Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Limin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weilin Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scott%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Matthew R. Scott</a><br>
<font size="3">
Abstract: In this work, we propose Knowledge Integration Networks (referred as KINet) for video action recognition. KINet is capable of aggregating meaningful context features which are of great importance to identifying an action, such as human information and scene context. We design a three-branch architecture consisting of a main branch for action recognition, and two auxiliary branches for human parsing and scene recognition which allow the model to encode the knowledge of human and scene for action recognition. We explore two pre-trained models as teacher networks to distill the knowledge of human and scene for training the auxiliary tasks of KINet. Furthermore, we propose a two-level knowledge encoding mechanism which contains a Cross Branch Integration (CBI) module for encoding the auxiliary knowledge into medium-level convolutional features, and an Action Knowledge Graph (AKG) for effectively fusing high-level context information. This results in an end-to-end trainable framework where the three tasks can be trained collaboratively, allowing the model to compute strong context knowledge efficiently. The proposed KINet achieves the state-of-the-art performance on a large-scale action recognition benchmark Kinetics-400, with a top-1 accuracy of 77.8%. We further demonstrate that our KINet has strong capability by transferring the Kinetics-trained model to UCF-101, where it obtains 97.8% top-1 accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们提出了知识整合网络（简称KINET）视频行为识别。的Kinet是能够聚集有意义的上下文特征，其是非常重要的识别动作，如人信息和场景上下文。我们设计了一个三分支结构，包括用于动作识别一个主分区，并为人类的解析和场景识别其允许模型编码人类和场景的动作识别知识两个辅助分支。我们探讨两个预训练的模型作为教师网络提炼人类和场景的知识培训KINET的辅助任务。此外，我们提出了编码包含用于编码所述辅助知识成中等程度的卷积特征的交叉处集成（CBI）模块机制两级知识以及动作知识图（AKG），用于有效地熔合高级别上下文信息。这导致终端到终端的可训练的框架，这三个任务可以协同的训练，使模型有效计算强背景知识。所提出的Kinet实现上大规模动作识别基准动力学-400的状态的最先进的性能，具有77.8％的顶1的精度。我们进一步证明我们的KINET具有由动力学训练模式转移到UCF-101，它获得97.8％最高1精度能力强。</font>
</div>


<hr>
<div id="paper13"> <b>13. Automated Cardiothoracic Ratio Calculation and Cardiomegaly Detection  using Deep Learning Approach</b>  <a href="https://arxiv.org/pdf/2002.07468" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chamveha%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Isarun Chamveha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Promwiset%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Treethep Promwiset</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tongdee%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trongtum Tongdee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saiviroonporn%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pairash Saiviroonporn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaisangmongkon%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Warasinee Chaisangmongkon</a><br>
<font size="3">
Abstract: We propose an algorithm for calculating the cardiothoracic ratio (CTR) from chest X-ray films. Our approach applies a deep learning model based on U-Net with VGG16 encoder to extract lung and heart masks from chest X-ray images and calculate CTR from the extents of obtained masks. Human radiologists evaluated our CTR measurements, and $76.5\%$ were accepted to be included in medical reports without any need for adjustment. This result translates to a large amount of time and labor saved for radiologists using our automated tools. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出的算法，用于计算从胸部X光片的心胸比（CTR）。我们的方法适用于基于掌中从获得口罩的程度VGG16编码器提取肺和心脏口罩从胸部的X射线图像，并计算CTR了深刻的学习模式。人类放射科医师评估我们的点击率测量结果和$ 76.5 \％$被接受被列入，而无需任何调整医疗报告。这一结果转化为节省使用我们的自动化工具放射大量的时间和人力。</font>
</div>


<hr>
<div id="paper14"> <b>14. Registration of multi-view point sets under the perspective of  expectation-maximization</b>  <a href="https://arxiv.org/pdf/2002.07464" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jihua Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongyu Li</a><br>
<font size="3">
Abstract: Registration of multi-view point sets is a prerequisite for 3D model reconstruction. To solve this problem, most of previous approaches either partially explore available information or blindly utilize unnecessary information to align each point set, which may lead to the undesired results or introduce extra computation complexity. To this end, this paper consider the multi-view registration problem as a maximum likelihood estimation problem and proposes a novel multi-view registration approach under the perspective of Expectation-Maximization (EM). The basic idea of our approach is that different data points are generated by the same number of Gaussian mixture models (GMMs). For each data point in one well-aligned point set, its nearest neighbors can be searched from other well-aligned point sets to explore more available information. Then, we can suppose this data point is generated by the special GMM, which is composed of each of its nearest neighbor adhered with one Gaussian distribution. Based on this assumption, it is reasonable to define the likelihood function, which contains all rigid transformations required to be estimated for multi-view registration. Subsequently, the EM algorithm is utilized to maximize the likelihood function so as to estimate all rigid transformations. Finally, the proposed approach is tested on several bench mark data sets and compared with some state-of-the-art algorithms. Experimental results illustrate its super performance on accuracy and efficiency for the registration of multi-view point sets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多视点组的登记是3D模型重建的先决条件。为了解决这个问题，大多数以前的方法的任一部分探索可用信息或盲目利用不必要的信息来对齐各点集，这可能导致不期望的结果或引入额外的计算复杂度。为此，本文考虑的多视点登记的问题，因为最大似然估计问题，并提出了期望最大化（EM）的角度下的新的多视图配准方法。我们的方法的基本思想是由相同数量的高斯混合模型（的GMM）产生的不同的数据点。对于一个良好对准点集中每个数据点，其最近的邻居可以从其他对准良好的点集进行搜索，探索更多的可用信息。然后，我们可以假设是由特殊的GMM，它是由每一个高斯分布附着其最近邻的生成该数据点。基于这个假设，这是合理的定义似然函数，其中包含所需要估计用于多视图登记所有刚性变换。随后，EM算法来最大化似然函数，从而估计所有刚性变换。最后，所提出的方法是在多个基准点的数据集进行测试和与国家的最先进的一些算法进行比较。实验结果示出在精度和效率的多视点组的登记其超级性能。</font>
</div>


<hr>
<div id="paper15"> <b>15. V4D:4D Convolutional Neural Networks for Video-level Representation  Learning</b>  <a href="https://arxiv.org/pdf/2002.07442" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiwen Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sheng Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weilin Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scott%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Matthew R. Scott</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Limin Wang</a><br>
<font size="3">
Abstract: Most existing 3D CNNs for video representation learning are clip-based methods, and thus do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, referred as V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, and at the same time, to preserve strong 3D spatio-temporal representation with residual connections. Specifically, we design a new 4D residual block able to capture inter-clip interactions, which could enhance the representation power of the original clip-level 3D CNNs. The 4D residual blocks can be easily integrated into the existing 3D CNNs to perform long-range modeling hierarchically. We further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数现有的三维细胞神经网络的视频表示学习是基于片段的方法，因此不考虑时空特性的视频级的时间演变。在本文中，我们提出了视频级4D卷积神经网络，被称为V4D，与4D卷积长程时空表示的演化模型，并在同一时间，以保持与残烈的3D时空表示连接。具体来说，我们设计了新的4D残余块能够捕获夹具间的相互作用，这可增强原始片段级3D细胞神经网络的表现力。四维残余块可以很容易地集成到现有的3D细胞神经网络来执行远程分级建模。我们进一步介绍拟议V4D训练和推理方法。大量的实验是在三个视频识别基准，其中V4D取得了优异的成绩，大幅度超越近期3D细胞神经网络进行。</font>
</div>


<hr>
<div id="paper16"> <b>16. EHSOD: CAM-Guided End-to-end Hybrid-Supervised Object Detection with  Cascade Refinement</b>  <a href="https://arxiv.org/pdf/2002.07421" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linpu Fang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhili Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Parisot%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sarah Parisot</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenguo Li</a><br>
<font size="3">
Abstract: Object detectors trained on fully-annotated data currently yield state of the art performance but require expensive manual annotations. On the other hand, weakly-supervised detectors have much lower performance and cannot be used reliably in a realistic setting. In this paper, we study the hybrid-supervised object detection problem, aiming to train a high quality detector with only a limited amount of fullyannotated data and fully exploiting cheap data with imagelevel labels. State of the art methods typically propose an iterative approach, alternating between generating pseudo-labels and updating a detector. This paradigm requires careful manual hyper-parameter tuning for mining good pseudo labels at each round and is quite time-consuming. To address these issues, we present EHSOD, an end-to-end hybrid-supervised object detection system which can be trained in one shot on both fully and weakly-annotated data. Specifically, based on a two-stage detector, we proposed two modules to fully utilize the information from both kinds of labels: 1) CAMRPN module aims at finding foreground proposals guided by a class activation heat-map; 2) hybrid-supervised cascade module further refines the bounding-box position and classification with the help of an auxiliary head compatible with image-level data. Extensive experiments demonstrate the effectiveness of the proposed method and it achieves comparable results on multiple object detection benchmarks with only 30% fully-annotated data, e.g. 37.5% mAP on COCO. We will release the code and the trained models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：上训练完全注释的数据对象探测器目前得到的先进的性能，但需要昂贵的手动注释。在另一方面，弱监督检测器具有低得多的性能，并且不能在现实设置可靠地使用。在本文中，我们研究了混合监督对象检测的问题，目的是培养高质量的检测器，只有fullyannotated有限的数据量，并且充分利用与映像级别标签便宜的数据。的现有技术方法通常提出的迭代方法，产生伪标签和更新检测器之间交替。这种模式需要在每一轮仔细的手工超参数调整矿业良好的伪标签，是相当费时。为了解决这些问题，我们本EHSOD，端至端的混合监督物体检测系统，其能够在一杆在两个完全和弱注释的数据被训练。具体地，基于两阶段检测器上，我们提出了两个模块，以充分利用来自两个种标签的信息：1）CAMRPN模块目的是找出由类激活热图引导前景建议; 2）混合监督级联模块进一步提炼包围盒位置和分类与辅助头图像级数据兼容的帮助。广泛的实验证明了该方法的有效性，并能实现在多个对象检测基准比较的结果，只有30％的全注释的数据，例如在COCO 37.5％映像。我们将发布的代码和训练的模型。</font>
</div>


<hr>
<div id="paper17"> <b>17. Universal-RCNN: Universal Object Detector via Transferable Graph R-CNN</b>  <a href="https://arxiv.org/pdf/2002.07417" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Linpu Fang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaodan Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenxiong Kang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenguo Li</a><br>
<font size="3">
Abstract: The dominant object detection approaches treat each dataset separately and fit towards a specific domain, which cannot adapt to other domains without extensive retraining. In this paper, we address the problem of designing a universal object detection model that exploits diverse category granularity from multiple domains and predict all kinds of categories in one system. Existing works treat this problem by integrating multiple detection branches upon one shared backbone network. However, this paradigm overlooks the crucial semantic correlations between multiple domains, such as categories hierarchy, visual similarity, and linguistic relationship. To address these drawbacks, we present a novel universal object detector called Universal-RCNN that incorporates graph transfer learning for propagating relevant semantic information across multiple datasets to reach semantic coherency. Specifically, we first generate a global semantic pool by integrating all high-level semantic representation of all the categories. Then an Intra-Domain Reasoning Module learns and propagates the sparse graph representation within one dataset guided by a spatial-aware GCN. Finally, an InterDomain Transfer Module is proposed to exploit diverse transfer dependencies across all domains and enhance the regional feature representation by attending and transferring semantic contexts globally. Extensive experiments demonstrate that the proposed method significantly outperforms multiple-branch models and achieves the state-of-the-art results on multiple object detection benchmarks (mAP: 49.1% on COCO). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：占优对象检测方法分别和配合治疗每个数据集向一个特定的域，这无法适应其他域没有广泛的重新训练。在本文中，我们解决设计，它利用来自多个域不同类别的粒度通用对象检测模型的问题，并预测各种类别的一个系统。现有的工作原理是在一个共享的骨干网络整合多个检测分支看待这个问题。然而，这种模式可以观看多个域，如类别层次结构，视觉相似，和语​​言之间的关系的关键语义相关性。为了解决这些缺点，提出了所谓的通用-RCNN一种新颖的通用对象检测器并入图转印学习用于传播跨越多个数据集相关的语义信息以达到语义相干性。具体而言，我们首先通过整合所有类别中的所有高层的语义表达产生一个全球性的语义池。然后域内推理模块学习和传播通过空间感知GCN引导一个数据集内的稀疏图表示。最后，域间传输模块，提出利用在所有领域多元化转移的依赖，并通过参加和全球转移语义语境提升区域特征表示。广泛的实验表明，该方法优于显著多分支模式和实现上的多个物体检测基准（MAP：上COCO 49.1％）的状态下的最先进的结果。</font>
</div>


<hr>
<div id="paper18"> <b>18. DivideMix: Learning with Noisy Labels as Semi-supervised Learning</b>  <a href="https://arxiv.org/pdf/2002.07394" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junnan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Socher</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hoi%2C+S+C+H" target="_blank" rel="noopener" style="color:#0000EE;">Steven C.H. Hoi</a><br>
<font size="3">
Abstract: Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at this https URL . </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络被称为是注释大户。许多已经努力与深网络学习时，减少注释成本。二是突出方向包括通过利用未标记的数据与嘈杂的标签和半监督学习学习。在这项工作中，我们提出DivideMix，与嘈杂的标签，通过利用半监督学习技术，学习新的框架。特别地，DivideMix模型每样品与混合模型损失分布训练数据动态地划分为标记组用干净的样品和未标记的集合与噪声采样，和火车上以半两个标记和未标记的数据模型-supervised方式。为了避免确认偏见，我们同时培养每个网络使用的数据集划分与其他网络中的两个分歧的网络。在半监督训练阶段，我们通过改进执行标签共同细化和标签共同猜测分别标记和未标记的样本，MixMatch策略。上的多个基准数据集的实验表明在国家的最先进的方法显着改进。代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper19"> <b>19. High-Order Paired-ASPP Networks for Semantic Segmenation</b>  <a href="https://arxiv.org/pdf/2002.07371" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junyu Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Changrui Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Shen</a><br>
<font size="3">
Abstract: Current semantic segmentation models only exploit first-order statistics, while rarely exploring high-order statistics. However, common first-order statistics are insufficient to support a solid unanimous representation. In this paper, we propose High-Order Paired-ASPP Network to exploit high-order statistics from various feature levels. The network first introduces a High-Order Representation module to extract the contextual high-order information from all stages of the backbone. They can provide more semantic clues and discriminative information than the first-order ones. Besides, a Paired-ASPP module is proposed to embed high-order statistics of the early stages into the last stage. It can further preserve the boundary-related and spatial context in the low-level features for final prediction. Our experiments show that the high-order statistics significantly boost the performance on confusing objects. Our method achieves competitive performance without bells and whistles on three benchmarks, i.e, Cityscapes, ADE20K and Pascal-Context with the mIoU of 81.6%, 45.3% and 52.9%. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当前语义分割模型只利用一阶统计，而很少探讨高阶统计。然而，常见的一阶统计不足以支持了坚实的一致表示。在本文中，我们提出了高阶配对ASPP网络利用各种特征等级高阶统计。该网络首先介绍高阶呈现模块来提取主干的各个阶段的情境高阶信息。他们可以提供更多的语义线索和比第一阶的人的区别信息。此外，一个配对ASPP模块提出了早期的嵌入高阶统计进入最后阶段。它可以进一步保持在低层次功能的最终预测边界相关和空间背景。我们的实验表明，高阶统计显著提升上混淆对象的性能。我们的方法实现，而不在三个基准，即，风情，ADE20K和帕斯卡 - 语境与81.6％，45.3％和52.9％的米欧花里胡哨的竞争性优势。</font>
</div>


<hr>
<div id="paper20"> <b>20. Multi-Task Learning from Videos via Efficient Inter-Frame Attention</b>  <a href="https://arxiv.org/pdf/2002.07362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Donghyun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tian Lan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuhang Zou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Ning Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Plummer%2C+B+A" target="_blank" rel="noopener" style="color:#0000EE;">Bryan A. Plummer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sclaroff%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stan Sclaroff</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Eledath%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jayan Eledath</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Medioni%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerard Medioni</a><br>
<font size="3">
Abstract: Prior work in multi-task learning has mainly focused on predictions on a single image. In this work, we present a new approach for multi-task learning from videos. Our approach contains a novel inter-frame attention module which allows learning of task-specific attention across frames. We embed the attention module in a "slow-fast" architecture, where the slower network runs on sparsely sampled keyframes and the lightweight shallow network runs on non-key frames at a high frame rate. We further propose an effective adversarial learning strategy to encourage the slow and fast network to learn similar features. The proposed architecture ensures low-latency multi-task learning while maintaining high quality prediction. Experiments show competitive accuracy compared to state-of-the-art on two multi-task learning benchmarks while reducing the number of floating point operations (FLOPs) by 70%. Meanwhile, our attention based feature propagation outperforms other feature propagation methods in accuracy by up to 90% reduction of FLOPs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在多任务学习以前的工作主要集中在单个图像上的预测。在这项工作中，我们提出了多任务从视频中学习的新方法。我们的方法包含了允许跨帧的学习任务特别注意一个新的帧间注意模块。我们嵌入关注模块中的“慢 - 快”的架构，其中在稀疏采样关键帧的速度较慢的网络运行，并以高帧速率的非关键帧的轻质浅的网络运行。我们进一步提出了一个有效的对抗的学习策略，鼓励缓慢和快速的网络学习类似的功能。所提出的架构确保低延迟多任务学习，同时保持高品质的预测。相比于两个多任务学习的基准状态的最先进的实验表明，有竞争力的准确性，同时由70％降低浮点运算（FLOPS）的数量。同时，我们根据重视功能传播优于高达FLOPS的减少了90％的准确度等特性的传播方法。</font>
</div>


<hr>
<div id="paper21"> <b>21. Constraining Temporal Relationship for Action Localization</b>  <a href="https://arxiv.org/pdf/2002.07358" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peisen Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingxi Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ju%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Ju</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ya Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Tian</a><br>
<font size="3">
Abstract: Recently, temporal action localization (TAL), i.e., finding specific action segments in untrimmed videos, has attracted increasing attentions of the computer vision community. State-of-the-art solutions for TAL involves predicting three values at each time point, corresponding to the probabilities that the action starts, continues and ends, and post-processing these curves for the final localization. This paper delves deep into this mechanism, and argues that existing approaches mostly ignored the potential relationship of these curves, and results in low quality of action proposals. To alleviate this problem, we add extra constraints to these curves, e.g., the probability of ''action continues'' should be relatively high between probability peaks of ''action starts'' and ''action ends'', so that the entire framework is aware of these latent constraints during an end-to-end optimization process. Experiments are performed on two popular TAL datasets, THUMOS14 and ActivityNet1.3. Our approach clearly outperforms the baseline both quantitatively (in terms of the AR@AN and mAP) and qualitatively (the curves in the testing stage become much smoother). In particular, when we build our constraints beyond TSA-Net and PGCN, we achieve the state-of-the-art performance especially at strict high IoU settings. The code will be available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，颞行动本地化（TAL），即查找特定行为段的修剪视频，吸引了计算机视觉界越来越多的关注。状态的最先进的用于解决方案TAL在每个时间点包括预测三个值，对应于概率，该动作开始，继续和结束，以及后处理对最终定位这些曲线。本文深入钻研这一机制，并认为现有的方法大都忽略了这些曲线的潜在关系，并在行动建议低质量的结果。为了缓解这个问题，我们增加额外的约束这些曲线，例如，“概率”行动继续“”应该是'行动启动“”和“”行动结束'的概率峰之间相对较高，从而使整个框架是知道这些潜限制期间结束到终端的优化过程。实验是在两个流行TAL数据集，THUMOS14和ActivityNet1.3执行。我们的方法明显优于基线在数量上（在AR @ AN和地图方面）和质量（变得更加顺畅处于测试阶段的曲线）。特别是，当我们建立我们超越TSA-Net和PGCN的限制，我们实现特别是在严格的高欠条设置的国家的最先进的性能。该代码将可用。</font>
</div>


<hr>
<div id="paper22"> <b>22. Restricted Structural Random Matrix for Compressive Sensing</b>  <a href="https://arxiv.org/pdf/2002.07346" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Canh%2C+T+N" target="_blank" rel="noopener" style="color:#0000EE;">Thuong Nguyen Canh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jeon%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Byeungwoo Jeon</a><br>
<font size="3">
Abstract: Compressive sensing (CS) is well-known for its unique functionalities of sensing, compressing, and security (i.e. CS measurements are equally important). However, there is a tradeoff. Improving sensing and compressing efficiency with prior signal information tends to favor particular measurements, thus decrease the security. This work aimed to improve the sensing and compressing efficiency without compromise the security with a novel sampling matrix, named Restricted Structural Random Matrix (RSRM). RSRM unified the advantages of frame-based and block-based sensing together with the global smoothness prior (i.e. low-resolution signals are highly correlated). RSRM acquired compressive measurements with random projection (equally important) of multiple randomly sub-sampled signals, which was restricted to be the low-resolution signals (equal in energy), thereby, its observations are equally important. RSRM was proven to satisfies the Restricted Isometry Property and shows comparable reconstruction performance with recent state-of-the-art compressive sensing and deep learning-based methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：压缩感测（CS）是以其独特的感测，压缩和安全的功能众所周知的（即CS测量是同样重要的）。然而，有一个权衡。改进的感测和与现有信号信息压缩效率往往有利于特定的测量，从而降低安全性。这项工作的目的是提高传感和压缩效率不妥协的新颖采样矩阵，命名为受限结构随机矩阵（RSRM）的安全性。 RSRM统一的优点基于帧和基于块的与全局平滑度之前（即低分辨率信号是高度相关的）一起感测。 RSRM获取与随机投影压缩测量多个随机子采样的信号，将其限制为低分辨率信号（在能量相等）的（同样重要的），从而，它的观察是同样重要的。 RSRM被证明满足受限等距属性和表演相媲美重建性能与近期国家的最先进的压缩感知和深基于学习的方法。</font>
</div>


<hr>
<div id="paper23"> <b>23. 3D Gated Recurrent Fusion for Semantic Scene Completion</b>  <a href="https://arxiv.org/pdf/2002.07269" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingsen Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xia Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunxia Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ian Reid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cadena%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cesar Cadena</a><br>
<font size="3">
Abstract: This paper tackles the problem of data fusion in the semantic scene completion (SSC) task, which can simultaneously deal with semantic labeling and scene completion. RGB images contain texture details of the object(s) which are vital for semantic scene understanding. Meanwhile, depth images capture geometric clues of high relevance for shape completion. Using both RGB and depth images can further boost the accuracy of SSC over employing one modality in isolation. We propose a 3D gated recurrent fusion network (GRFNet), which learns to adaptively select and fuse the relevant information from depth and RGB by making use of the gate and memory modules. Based on the single-stage fusion, we further propose a multi-stage fusion strategy, which could model the correlations among different stages within the network. Extensive experiments on two benchmark datasets demonstrate the superior performance and the effectiveness of the proposed GRFNet for data fusion in SSC. Code will be made available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文铲球数据融合在语义现场完成（SSC）任务的问题，可以用语义标签和现场完成同时处理。 RGB图像包含的对象（S）的纹理细节这对语义理解的场景非常重要。同时，深度图像捕捉形状完成相关性高的几何线索。使用RGB和深度图像可以进一步推动南南合作的准确度采用单独一个模式。我们提出了一个3D门选复发性融合网络（GRFNet），其学会自适应地选择和熔断器从深度和RGB通过利用栅极和存储器模块的有关信息。基于单级融合，我们进一步提出了多级融合策略，可以在网络中的不同阶段之间的相关性进行建模。两个基准数据集大量的实验证明了卓越的性能和拟议GRFNet的在SSC数据融合的有效性。代码将提供。</font>
</div>


<hr>
<div id="paper24"> <b>24. Dual-Attention GAN for Large-Pose Face Frontalization</b>  <a href="https://arxiv.org/pdf/2002.07227" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Songyao Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Robinson%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Joseph P. Robinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun Fu</a><br>
<font size="3">
Abstract: Face frontalization provides an effective and efficient way for face data augmentation and further improves the face recognition performance in extreme pose scenario. Despite recent advances in deep learning-based face synthesis approaches, this problem is still challenging due to significant pose and illumination discrepancy. In this paper, we present a novel Dual-Attention Generative Adversarial Network (DA-GAN) for photo-realistic face frontalization by capturing both contextual dependencies and local consistency during GAN training. Specifically, a self-attention-based generator is introduced to integrate local features with their long-range dependencies yielding better feature representations, and hence generate faces that preserve identities better, especially for larger pose angles. Moreover, a novel face-attention-based discriminator is applied to emphasize local features of face regions, and hence reinforce the realism of synthetic frontal faces. Guided by semantic segmentation, four independent discriminators are used to distinguish between different aspects of a face (\ie skin, keypoints, hairline, and frontalized face). By introducing these two complementary attention mechanisms in generator and discriminator separately, we can learn a richer feature representation and generate identity preserving inference of frontal views with much finer details (i.e., more accurate facial appearance and textures) comparing to the state-of-the-art. Quantitative and qualitative experimental results demonstrate the effectiveness and efficiency of our DA-GAN approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面对frontalization提供了脸部数据增强的有效途径，进一步提高了在极端姿势方案中的面部识别性能。尽管深学习型脸合成方法的最新进展，这一问题仍然是由于具有挑战性的显著姿态和照度差异。在本文中，我们通过GAN训练期间捕获两者的上下文依赖关系和局部一致性呈现为照片般逼真的面frontalization一种新颖的双注意剖成对抗性网络（DA-GAN）。具体而言，基于自我关注发生器被引入到当地特色与他们的长距离依赖产生更好的特征表示整合，从而产生保护身份更好，特别是对于较大的姿势角度的面孔。另外，一种新型的基于面部注意鉴别器被施加到强调面部区域的局部特征，并因此加强合成正面人脸的真实感。通过语义分割的指导下，四个独立的鉴别器中使用的脸的不同方面（\即皮肤，关键点，发纹，和frontalized面）之间进行区分。通过分别引入发电机和鉴别这两种互补的注意力机制，我们可以学到更丰富的功能表现和产生的身份保护与更精细的细节（即，更准确的面部外形和纹理）正面看法推断比较状态的最-艺术。定量和定性实验结果表明我们的DA-GaN方法的有效性和效率。</font>
</div>


<hr>
<div id="paper25"> <b>25. Multilinear Compressive Learning with Prior Knowledge</b>  <a href="https://arxiv.org/pdf/2002.07203" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+D+T" target="_blank" rel="noopener" style="color:#0000EE;">Dat Thanh Tran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gabbouj%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Moncef Gabbouj</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Iosifidis%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandros Iosifidis</a><br>
<font size="3">
Abstract: The recently proposed Multilinear Compressive Learning (MCL) framework combines Multilinear Compressive Sensing and Machine Learning into an end-to-end system that takes into account the multidimensional structure of the signals when designing the sensing and feature synthesis components. The key idea behind MCL is the assumption of the existence of a tensor subspace which can capture the essential features from the signal for the downstream learning task. Thus, the ability to find such a discriminative tensor subspace and optimize the system to project the signals onto that data manifold plays an important role in Multilinear Compressive Learning. In this paper, we propose a novel solution to address both of the aforementioned requirements, i.e., How to find those tensor subspaces in which the signals of interest are highly separable? and How to optimize the sensing and feature synthesis components to transform the original signals to the data manifold found in the first question? In our proposal, the discovery of a high-quality data manifold is conducted by training a nonlinear compressive learning system on the inference task. Its knowledge of the data manifold of interest is then progressively transferred to the MCL components via multi-stage supervised training with the supervisory information encoding how the compressed measurements, the synthesized features, and the predictions should be like. The proposed knowledge transfer algorithm also comes with a semi-supervised adaption that enables compressive learning models to utilize unlabeled data effectively. Extensive experiments demonstrate that the proposed knowledge transfer method can effectively train MCL models to compressively sense and synthesize better features for the learning tasks with improved performances, especially when the complexity of the learning task increases. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近提出的多线性压缩学习（MCL）的框架联合多线性压缩感知和机器学习到一个终端到终端的系统设计感测和特征的合成组分时，考虑到的信号的多维结构。 MCL背后的关键思想是一个张子空间，可以从下游的学习任务，信号捕获本质特征的存在的假设。因此，要找到这样的区别张子空间，优化系统将信号投射到的数据歧管的能力起着压缩多线性学习了重要的作用。在本文中，我们提出了一个新的解决方案，以解决双方的上述要求，即如何找到那些张子空间，其中感兴趣的信号是非常可分离？和如何优化感测和特征合成组分来改造原始信号中的数据的第一个问题歧管发现？在我们的建议，高品质的数据流形的发现是通过对推理任务训练非线性压缩学习系统进行。其感兴趣的数据歧管的知识，然后逐步通过多级转移到MCL组件与监管信息编码指导训练如何压缩测量，合成特征和预测应该是这样的。所提出的知识转移算法还配备了一个半监督适应，使压缩的学习模式有效地利用无标签数据。大量的实验表明，该知识转移方法能有效地训练MCL模型来压缩感及合成更好的功能与性能改善的学习任务，尤其是当的学习任务的复杂性增加。</font>
</div>


<hr>
<div id="paper26"> <b>26. The Tree Ensemble Layer: Differentiability meets Conditional Computation</b>  <a href="https://arxiv.org/pdf/2002.07772" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hazimeh%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hussein Hazimeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ponomareva%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Natalia Ponomareva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mol%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Petros Mol</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenyu Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mazumder%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rahul Mazumder</a><br>
<font size="3">
Abstract: Neural networks and tree ensembles are state-of-the-art learners, each with its unique statistical and computational advantages. We aim to combine these advantages by introducing a new layer for neural networks, composed of an ensemble of differentiable decision trees (a.k.a. soft trees). While differentiable trees demonstrate promising results in the literature, in practice they are typically slow in training and inference as they do not support conditional computation. We mitigate this issue by introducing a new sparse activation function for sample routing, and implement true conditional computation by developing specialized forward and backward propagation algorithms that exploit sparsity. Our efficient algorithms pave the way for jointly training over deep and wide tree ensembles using first-order methods (e.g., SGD). Experiments on 23 classification datasets indicate over 10x speed-ups compared to the differentiable trees used in the literature and over 20x reduction in the number of parameters compared to gradient boosted trees, while maintaining competitive performance. Moreover, experiments on CIFAR, MNIST, and Fashion MNIST indicate that replacing dense layers in CNNs with our tree layer reduces the test loss by 7-53% and the number of parameters by 8x. We provide an open-source TensorFlow implementation with a Keras API. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经网络和树歌舞团是国家的最先进的学习者，各有其独特的统计和计算的优势。我们旨在通过引入用于神经网络新的层，可微决策树（也称为软树）的集合构成的这些优点结合起来。虽然微树分析看好文献结果，在实践中它们通常在训练和推理缓慢，因为它们不支持的条件计算。我们通过引入样本路由新稀疏激活功能缓解此问题，并通过利用稀疏开发专门的向前和向后传播算法实现真正​​的条件计算。我们高效的算法铺平道路，共同训练过使用一阶方法（例如，SGD）深而宽的树合奏。于23个分类数据集的实验表明相比，在文献中使用的并在相对于梯度的参数的数量超过20倍减少升压树，同时维持有竞争力的性能可分化树木超过10倍速度起坐。此外，在CIFAR，MNIST和时装MNIST实验表明，与我们的树层替换细胞神经网络的致密层减少了由7-53％的测试损失和参数由8倍的数量。我们提供了Keras API的开源TensorFlow实现。</font>
</div>


<hr>
<div id="paper27"> <b>27. Learning Bijective Feature Maps for Linear ICA</b>  <a href="https://arxiv.org/pdf/2002.07766" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Camuto%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Camuto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Willetts%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Willetts</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Paige%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brooks Paige</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Holmes%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chris Holmes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roberts%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Roberts</a><br>
<font size="3">
Abstract: Separating high-dimensional data like images into independent latent factors remains an open research problem. Here we develop a method that jointly learns a linear independent component analysis (ICA) model with non-linear bijective feature maps. By combining these two methods, ICA can learn interpretable latent structure for images. For non-square ICA, where we assume the number of sources is less than the dimensionality of data, we achieve better unsupervised latent factor discovery than flow-based models and linear ICA. This performance scales to large image datasets such as CelebA </font>
<br>
<font size="2" style="line-height:30px;">
摘要：分离高维数据，如图像转换成独立的潜在因素仍然是一个开放性的研究问题。在这里，我们开发共同学习线性独立成分分析（ICA）与非线性双射特征映射模型的方法。通过这两种方法相结合，ICA可以学习图像判读潜在结构。对于非方形ICA，在这里我们假设源的数量小于数据的维度，我们取得更好的监督的潜在因素发现不是基于流的模型和线性ICA。这种性能扩展到大的图像数据集，如CelebA</font>
</div>


<hr>
<div id="paper28"> <b>28. Deep Learning in Medical Ultrasound Image Segmentation: a Review</b>  <a href="https://arxiv.org/pdf/2002.07703" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziyang Wang</a><br>
<font size="3">
Abstract: Applying machine learning technologies, especially deep learning, into medical image segmentation is being widely studied because of its state-of-the-art performance and results. It can be a key step to provide a reliable basis for clinical diagnosis, such as 3D reconstruction of human tissues, image-guided interventions, image analyzing and visualization. In this review article, deep-learning-based methods for ultrasound image segmentation are categorized into six main groups according to their architectures and training at first. Secondly, for each group, several current representative algorithms are selected, introduced, analyzed and summarized in detail. In addition, common evaluation methods for image segmentation and ultrasound image segmentation datasets are summarized. Further, the performance of the current methods and their evaluations are reviewed. In the end, the challenges and potential research directions for medical ultrasound image segmentation are discussed. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：将机器学习技术，特别是深度学习，为医学图像分割是因为国家的最先进的性能和效果被广泛研究。它可以是提供用于临床诊断，如3D重建人体组织的，图像引导的介入，图像分析和可视化的可靠依据的关键步骤。在这篇综述文章中，超声图像分割深学习为基础的方法是根据在第一个自己的架构和培训分为六个主要群体。其次，对于每个组，几个当前代表性的算法被选择，引入，分析和详细总结。此外，图像分割和超声图像分割数据集共同的评价方法进行了总结。此外，目前的方法和他们的评价性能进行了综述。最后，对医学超声图像分割的挑战和潜在的研究方向进行了探讨。</font>
</div>


<hr>
<div id="paper29"> <b>29. Robust Quantization: One Model to Rule Them All</b>  <a href="https://arxiv.org/pdf/2002.07686" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shkolnik%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Moran Shkolnik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chmiel%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brian Chmiel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Banner%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ron Banner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shomron%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gil Shomron</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nahshan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Nahshan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bronstein%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alex Bronstein</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weiser%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Uri Weiser</a><br>
<font size="3">
Abstract: Neural network quantization methods often involve simulating the quantization process during training. This makes the trained model highly dependent on the precise way quantization is performed. Since low-precision accelerators differ in their quantization policies and their supported mix of data-types, a model trained for one accelerator may not be suitable for another. To address this issue, we propose KURE, a method that provides intrinsic robustness to the model against a broad range of quantization implementations. We show that KURE yields a generic model that may be deployed on numerous inference accelerators without a significant loss in accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经网络量化方法往往需要在训练中模拟量化处理。这使得训练模型高度依赖于进行量化的精确的方式。由于低精度加速器在其量化政策及其支持的数据类型的组合不同，训练了一个加速器模式未必适合另一个。为了解决这个问题，我们提出KURE，提供固有的稳健性针对广泛的量化实现的模型的方法。我们发现，KURE产生，可能在许多推论加速器部署没有精度显著损失的通用模型。</font>
</div>


<hr>
<div id="paper30"> <b>30. Image Entropy for Classification and Analysis of Pathology Slides</b>  <a href="https://arxiv.org/pdf/2002.07621" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Frank%2C+S+J" target="_blank" rel="noopener" style="color:#0000EE;">Steven J. Frank</a><br>
<font size="3">
Abstract: Pathology slides of lung malignancies are classified using the "Salient Slices" technique described in Frank et al., 2020. A four-fold cross-validation study using a small image set (42 adenocarcinoma slides and 42 squamous cell carcinoma slides) produced fully correct classifications in each fold. Probability maps enable visualization of the underlying basis for a classification. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：（42张腺癌幻灯片和42张鳞状细胞癌幻灯片）产生的肺恶性肿瘤的病理载玻片使用弗兰克描述的“凸片”技术等人，2020 A，使用一个小的图像组四个倍交叉验证研究分类。完全正确的分类在每个倍。概率图启用对分类的底层基础的可视化。</font>
</div>


<hr>
<div id="paper31"> <b>31. Deflecting Adversarial Attacks</b>  <a href="https://arxiv.org/pdf/2002.07405" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yao Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Frosst%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicholas Frosst</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Raffel%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Colin Raffel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cottrell%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Garrison Cottrell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hinton%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geoffrey Hinton</a><br>
<font size="3">
Abstract: There has been an ongoing cycle where stronger defenses against adversarial attacks are subsequently broken by a more advanced defense-aware attack. We present a new approach towards ending this cycle where we "deflect'' adversarial attacks by causing the attacker to produce an input that semantically resembles the attack's target class. To this end, we first propose a stronger defense based on Capsule Networks that combines three detection mechanisms to achieve state-of-the-art detection performance on both standard and defense-aware attacks. We then show that undetected attacks against our defense often perceptually resemble the adversarial target class by performing a human study where participants are asked to label images produced by the attack. These attack images can no longer be called "adversarial'' because our network classifies them the same way as humans do. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：已经有一个持续的周期中对敌对攻击防御强随后被更先进的国防意识的攻击破坏。我们提出争取结束这个周期的新方法，我们“打歪'对抗性通过使攻击者产生语义类似攻击的目标类的输入攻击。为此，我们首先提出了一种基于胶囊网络更强的防御结合了三种检测机制，以实现国家的最先进的检测性能的标准和国防意识的攻击。然后，我们表现出对我们的防守是未被发现的袭击进行人体研究，参与者被要求标签图像往往感知类似于敌对目标类通过攻击产生的。这些攻击的图像不能再被称为是因为我们的网络进行分类“对抗性‘’他们为人类做同样的方式。</font>
</div>


<hr>
<div id="paper32"> <b>32. Picking Winning Tickets Before Training by Preserving Gradient Flow</b>  <a href="https://arxiv.org/pdf/2002.07376" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaoqi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guodong Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grosse%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roger Grosse</a><br>
<font size="3">
Abstract: Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：Overparameterization已经显示出两者的优化和神经网络的推广中受益，但大型网络资源都在训练和测试时间饿了。网络修剪可以减少测试时间资源需求，但通常适用于训练有素的网络，因此无法避免昂贵的培训过程。我们的目标是修剪网络在初始化，从而在训练时间和节约资源。具体而言，我们认为，有效的训练需要保留通过网络梯度流动。这导致了一个简单而有效的删除准则，我们长期梯度信号保存（GRASP）。我们经验探讨CIFAR-10所提出的方法具有广泛的实验效果，CIFAR-100，微型-ImageNet和ImageNet，使用VGGNet和RESNET架构。我们的方法可以在初始化修剪上ImageNet一个VGG-16网络的权重的80％，与只在顶1精度的1.6％的降幅。此外，我们的方法实现了比在极端稀疏水平基线显著更好的性能。</font>
</div>


<hr>
<div id="paper33"> <b>33. Evolutionary Optimization of Deep Learning Activation Functions</b>  <a href="https://arxiv.org/pdf/2002.07224" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bingham%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Garrett Bingham</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Macke%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Macke</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Miikkulainen%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Risto Miikkulainen</a><br>
<font size="3">
Abstract: The choice of activation function can have a large effect on the performance of a neural network. While there have been some attempts to hand-engineer novel activation functions, the Rectified Linear Unit (ReLU) remains the most commonly-used in practice. This paper shows that evolutionary algorithms can discover novel activation functions that outperform ReLU. A tree-based search space of candidate activation functions is defined and explored with mutation, crossover, and exhaustive search. Experiments on training wide residual networks on the CIFAR-10 and CIFAR-100 image datasets show that this approach is effective. Replacing ReLU with evolved activation functions results in statistically significant increases in network accuracy. Optimal performance is achieved when evolution is allowed to customize activation functions to a particular task; however, these novel activation functions are shown to generalize, achieving high performance across tasks. Evolutionary optimization of activation functions is therefore a promising new dimension of metalearning in neural networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：激活功能的选择可能会对神经网络的性能有很大的影响。虽然已经有一些尝试来手工工程师新颖激活函数，整流线性单元（RELU）保持在实践中最常用的。本文表明，进化算法可以发现优于RELU新颖激活功能。候选激活功能的基于树的搜索空间的定义，并与变异，交叉和穷举搜索探索。上的CIFAR-10和CIFAR-100图像数据组训练宽残留网络实验表明，该方法是有效的。在网络精度统计显著上升与演变激活功能结果更换RELU。当进化允许自定义激活功能，一个特定的任务，则可获得最佳性能;然而，这些新的激活函数被示出一概而论，实现跨越任务高性能。因此，激活功能进化优化神经网络元学习的有前途的新的层面。</font>
</div>


<hr>
<div id="paper34"> <b>34. AIBench: An Agile Domain-specific Benchmarking Methodology and an AI  Benchmark Suite</b>  <a href="https://arxiv.org/pdf/2002.07162" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wanling Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Zhan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuanxin Lan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunjie Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiahui Dai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiongwang Xiong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihan Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianshu Hao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fanda Fan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xu Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunyou Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengjia Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daoyi Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoning Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhan%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kunlin Zhan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Biao Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Defei Kong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghe Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chongkang Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinhui Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yatao Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gang Lu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junchao Shao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenyu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hainan Ye</a><br>
<font size="3">
Abstract: Domain-specific software and hardware co-design is encouraging as it is much easier to achieve efficiency for fewer tasks. Agile domain-specific benchmarking speeds up the process as it provides not only relevant design inputs but also relevant metrics, and tools. Unfortunately, modern workloads like Big data, AI, and Internet services dwarf the traditional one in terms of code size, deployment scale, and execution path, and hence raise serious benchmarking challenges. This paper proposes an agile domain-specific benchmarking methodology. Together with seventeen industry partners, we identify ten important end-to-end application scenarios, among which sixteen representative AI tasks are distilled as the AI component benchmarks. We propose the permutations of essential AI and non-AI component benchmarks as end-to-end benchmarks. An end-to-end benchmark is a distillation of the essential attributes of an industry-scale application. We design and implement a highly extensible, configurable, and flexible benchmark framework, on the basis of which, we propose the guideline for building end-to-end benchmarks, and present the first end-to-end Internet service AI benchmark. The preliminary evaluation shows the value of our benchmark suite---AIBench against MLPerf and TailBench for hardware and software designers, micro-architectural researchers, and code developers. The specifications, source code, testbed, and results are publicly available from the web site \url{this http URL}. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：特定领域的软件和硬件的协同设计是令人鼓舞的，因为它是非常容易实现的任务较少的效率。敏捷特定领域的标杆速度可达的过程，因为它不仅提供了相应的设计输入，但也有相关的指标和工具。不幸的是，像大数据，人工智能和互联网服务的现代化工作负载中相形见绌代码大小，部署规模和执行路径方面的传统之一，并因此引起严重的标杆挑战。本文提出了一种灵活的特定领域的基准测试方法。加上17级行业的合作伙伴，我们确定的十大终端到终端的应用场景，其中十六个代表性的AI任务蒸馏水作为AI部件基准。我们提出重要的AI和非AI成分基准为终端到终端的基准的排列。终端到终端的基准是行业规模化应用的本质属性的升华。我们设计并实现了一个高度可扩展的，可配置的，灵活的基准框架，其中，我们提出了建立终端到终端的基准方针的基础上，并给出了第一端至端的互联网服务AI基准。初步评估显示了我们的基准测试套件的硬件和软件设计，微体系结构的研究，和代码开发者的价值---对MLPerf和TailBench AIBench。这些规范，源代码，测试平台和结果是公众可从该网站\ {URL这个HTTP URL}。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-19</title>
    <url>/2020/02/19/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-19/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> An enhanced Tree-LSTM architecture for sentence semantic modeling using  typed dependencies <a href="https://arxiv.org/pdf/2002.07775" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Learning by Semantic Similarity Makes Abstractive Summarization Better <a href="https://arxiv.org/pdf/2002.07767" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Gradient-Based Adversarial Training on Transformer Networks for  Detecting Check-Worthy Factual Claims <a href="https://arxiv.org/pdf/2002.07725" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Neural Relation Prediction for Simple Question Answering over Knowledge  Graph <a href="https://arxiv.org/pdf/2002.07715" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> A Survey of Deep Learning Techniques for Neural Machine Translation <a href="https://arxiv.org/pdf/2002.07526" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue <a href="https://arxiv.org/pdf/2002.07510" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> A New Clustering neural network for Chinese word segmentation <a href="https://arxiv.org/pdf/2002.07458" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Improving Multi-Turn Response Selection Models with Complementary  Last-Utterance Selection by Instance Weighting <a href="https://arxiv.org/pdf/2002.07397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Annotating and Extracting Synthesis Process of All-Solid-State Batteries  from Scientific Literature <a href="https://arxiv.org/pdf/2002.07339" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Conditional Self-Attention for Query-based Summarization <a href="https://arxiv.org/pdf/2002.07338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> From English To Foreign Languages: Transferring Pre-trained Language  Models <a href="https://arxiv.org/pdf/2002.07306" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Decidability of cutpoint isolation for letter-monotonic probabilistic  finite automata <a href="https://arxiv.org/pdf/2002.07660" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> A Model to Measure the Spread Power of Rumors <a href="https://arxiv.org/pdf/2002.07563" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. An enhanced Tree-LSTM architecture for sentence semantic modeling using  typed dependencies</b>  <a href="https://arxiv.org/pdf/2002.07775" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kleenankandy%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeena Kleenankandy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nazeer%2C+K+A+A" target="_blank" rel="noopener" style="color:#0000EE;">K. A. Abdul Nazeer</a><br>
<font size="3">
Abstract: Tree-based Long short term memory (LSTM) network has become state-of-the-art for modeling the meaning of language texts as they can effectively exploit the grammatical syntax and thereby non-linear dependencies among words of the sentence. However, most of these models cannot recognize the difference in meaning caused by a change in semantic roles of words or phrases because they do not acknowledge the type of grammatical relations, also known as typed dependencies, in sentence structure. This paper proposes an enhanced LSTM architecture, called relation gated LSTM, which can model the relationship between two inputs of a sequence using a control input. We also introduce a Tree-LSTM model called Typed Dependency Tree-LSTM that uses the sentence dependency parse structure as well as the dependency type to embed sentence meaning into a dense vector. The proposed model outperformed its type-unaware counterpart in two typical NLP tasks - Semantic Relatedness Scoring and Sentiment Analysis, in a lesser number of training epochs. The results were comparable or competitive with other state-of-the-art models. Qualitative analysis showed that changes in the voice of sentences had little effect on the model's predicted scores, while changes in nominal (noun) words had a more significant impact. The model recognized subtle semantic relationships in sentence pairs. The magnitudes of learned typed dependencies embeddings were also in agreement with human intuitions. The research findings imply the significance of grammatical relations in sentence modeling. The proposed models would serve as a base for future researches in this direction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于树的长短期记忆（LSTM）网络已成为国家的最先进的建模语言文本的意义，因为他们可以有效地利用句子的词之间的语法语法，从而非线性的依赖。然而，大多数这些模型不能识别在由词或短语的语义角色的变化引起的意思，因为他们不承认的语法关系的类型，也被称为类型的依赖，在句子结构上的差异。本文提出了一种增强LSTM体系结构被称为关系门控LSTM，这可以使用一个控制输入序列的两个输入之间的关系进行建模。我们还介绍了被称为类型的依赖树LSTM一树LSTM模型，使用句子的依赖解析结构以及依赖型到嵌入句义成致密的载体。该模型跑赢同类型不了解对方的两个典型的NLP任务 - 语义相关评分和情感分析，在较小数量的训练时期的。结果是可比的或竞争与国家的最先进的其他车型。定性分析表明，在句子的语音变化对模型的预测分数影响不大，而在标称（名词）字样的变化有一个更显著的影响。该模型承认句对微妙的语义关系。据悉类型的依赖的嵌入的幅度也与人类直觉的协议。研究结果意味着句子造型语法关系的重要意义。提出的模型将作为今后在这方面的研究基地。</font>
</div>


<hr>
<div id="paper2"> <b>2. Learning by Semantic Similarity Makes Abstractive Summarization Better</b>  <a href="https://arxiv.org/pdf/2002.07767" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yoon%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wonjin Yoon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yeo%2C+Y+S" target="_blank" rel="noopener" style="color:#0000EE;">Yoon Sun Yeo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jeong%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minbyul Jeong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bong-Jun Yi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaewoo Kang</a><br>
<font size="3">
Abstract: One of the obstacles of abstractive summarization is the presence of various potentially correct predictions. Widely used objective functions for supervised learning, such as cross-entropy loss, cannot handle alternative answers effectively. Rather, they act as a training noise. In this paper, we propose Semantic Similarity strategy that can consider semantic meanings of generated summaries while training. Our training objective includes maximizing semantic similarity score which is calculated by an additional layer that estimates semantic similarity between generated summary and reference summary. By leveraging pre-trained language models, our model achieves a new state-of-the-art performance, ROUGE-L score of 41.5 on CNN/DM dataset. To support automatic evaluation, we also conducted human evaluation and received higher scores relative to both baseline and reference summaries. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一个抽象总结的障碍是各种潜在的正确预测的存在。广泛用于监督学习的目标函数，如交叉熵损失，不能有效地处理备选答案。相反，他们作为一个培训噪音。在本文中，我们提出了语义相似性的策略，可以考虑在训练中产生摘要语义。我们的训练目标包括最大化其由估计生成的概要和参考摘要之间的语义相似度的附加层计算语义相似度得分。通过利用预先训练的语言模型，我们的模型实现了国家的最先进的新性能，ROUGE-L的41.5对CNN / DM数据集的分数。为了支持自动评价，我们还进行了人工评估并获得相对于基线和参考摘要更高的分数。</font>
</div>


<hr>
<div id="paper3"> <b>3. Gradient-Based Adversarial Training on Transformer Networks for  Detecting Check-Worthy Factual Claims</b>  <a href="https://arxiv.org/pdf/2002.07725" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jimenez%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Damian Jimenez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arslan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fatma Arslan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Devasier%2C+J+D" target="_blank" rel="noopener" style="color:#0000EE;">Jacob Daniel Devasier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Obembe%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Obembe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengkai Li</a><br>
<font size="3">
Abstract: We present a study on the efficacy of adversarial training on transformer neural network models, with respect to the task of detecting check-worthy claims. In this work, we introduce the first adversarially-regularized, transformer-based claim spotter model that achieves state-of-the-art results on multiple challenging benchmarks. We obtain a 4.31 point F1-score improvement and a 1.09 point mAP score improvement over current state-of-the-art models on the ClaimBuster Dataset and CLEF2019 Dataset, respectively. In the process, we propose a method to apply adversarial training to transformer models, which has the potential to be generalized to many similar text classification tasks. Along with our results, we are releasing our codebase and manually labeled datasets. We also showcase our models' real world usage via a live public API. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们对变压器神经网络模型对抗性训练，相对于检测检验，值得索赔任务的有效性提出了一项研究。在这项工作中，我们介绍了第一adversarially-转正，基于变压器的要求去污剂是实现在多个具有挑战性的基准状态的最先进成果的模型。我们获得4.31点F1-评分改善和超过国家的最先进的电流分别在ClaimBuster数据集和数据集CLEF2019，车型1.09点地图评分改善。在这个过程中，我们建议采用对抗性训练，变压器模型，其中有被推广到许多类似的文本分类任务的可能性的方法。随着我们的结果，我们发布我们的代码库和手工标注的数据集。我们还通过现场的公共API展示我们的模型在现实世界中使用。</font>
</div>


<hr>
<div id="paper4"> <b>4. Neural Relation Prediction for Simple Question Answering over Knowledge  Graph</b>  <a href="https://arxiv.org/pdf/2002.07715" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Abolghasemi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amin Abolghasemi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Momtazi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Saeedeh Momtazi</a><br>
<font size="3">
Abstract: Relation extraction from simple questions aims to capture the relation of a factoid question with one underlying relation from a set of predefined ones ina knowledge base. Most recent methods take advantage of neural networks for matching a question with all relations in order to find the best relation that is expressed by that question. In this paper, we propose an instance-based method to find similar questions of a new question, in the sense of their relations, to predict its mentioned relation. The motivation roots in the fact that a relation can be expressed with different forms of question and these forms mostly share similar terms or concepts. Our experiments on the SimpleQuestions dataset show that the proposed model achieved better accuracy compared to the state-of-the-art relation extraction models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从简单的问题，目的关系抽取捕捉事实型询问的从一组预定义的人伊娜的知识基础的一个基础关系的关系。最近方法利用神经网络进行，以便找到由这个问题所表达的最佳匹配关系的所有关系的问题。在本文中，我们提出了一种基于实例的方法来寻找新的问题类似的问题，在他们的关系的意义上，预测其提到的关系。在事实的动机根源，一个关系可以有不同形式的问题，并且这些形式来表达大多有着相似的术语或概念。我们对SimpleQuestions实验数据集表明，该模型相比，国家的最先进的关系抽取模型取得了较好的精度。</font>
</div>


<hr>
<div id="paper5"> <b>5. A Survey of Deep Learning Techniques for Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.07526" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuoheng Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaowen Chu</a><br>
<font size="3">
Abstract: In recent years, natural language processing (NLP) has got great development with deep learning techniques. In the sub-field of machine translation, a new approach named Neural Machine Translation (NMT) has emerged and got massive attention from both academia and industry. However, with a significant number of researches proposed in the past several years, there is little work in investigating the development process of this new technology trend. This literature survey traces back the origin and principal development timeline of NMT, investigates the important branches, categorizes different research orientations, and discusses some future research trends in this field. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，自然语言处理（NLP）已得到深学习技术的大发展。在机器翻译，一个名叫神经机器翻译新方法的子场（NMT）已经出现并得到大众的注意力从学术界和工业界。然而，在过去的几年中提出研究的显著号码，则在调查的这项新技术的发展趋势过程中一些工作。该文献调查追溯了起源和NMT的主要发展时间表，调查的重要分支，不同的分类研究方向，并讨论了今后的研究趋势在这一领域。</font>
</div>


<hr>
<div id="paper6"> <b>6. Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue</b>  <a href="https://arxiv.org/pdf/2002.07510" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Byeongchang Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ahn%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaewoo Ahn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gunhee Kim</a><br>
<font size="3">
Abstract: Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and subsequently the performance of utterance generation. We achieve the new state-of-the-art performance on Wizard of Wikipedia (Dinan et al., 2019) as one of the most large-scale and challenging benchmarks. We further validate the effectiveness of our model over existing conversation methods in another knowledge-based dialogue Holl-E dataset (Moghe et al., 2018). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：知识接地对话是产生基于两个话语语境和外部知识的信息响应的任务。当我们专注于多转的知识接地对话更好的建模知识的选择，我们提出了一个连续的潜变量模型作为第一种方法此事。该模型命名顺序知识变压器（SKT）可以跟踪对知识的先验和后验分布;其结果是，它不仅可以减少在谈话的知识选择的多样性造成了不确定性，也更好地利用知识的正确选择的响应信息。我们的实验结果表明，该模型提高了知识的选择准确性和随后的话语一代的性能。我们实现在维基百科上的向导新的国家的最先进的性能（迪南等，2019）作为最大型的一个，挑战基准。我们进一步验证了我们的模型了另一种以知识为基础的对话霍尔-E数据集现有对话方法的有效性（Moghe等，2018）。</font>
</div>


<hr>
<div id="paper7"> <b>7. A New Clustering neural network for Chinese word segmentation</b>  <a href="https://arxiv.org/pdf/2002.07458" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuze Zhao</a><br>
<font size="3">
Abstract: In this article I proposed a new model to achieve Chinese word segmentation(CWS),which may have the potentiality to apply in other domains in the this http URL is a new thinking in CWS compared to previous works,to consider it as a clustering problem instead of a labeling this http URL this model,LSTM and self attention structures are used to collect context also sentence level features in every layer,and after several layers,a clustering model is applied to split characters into groups,which are the final segmentation results.I call this model CLNN.This algorithm can reach 98 percent of F score (without OOV words) and 85 percent to 95 percent F score (with OOV words) in training data sets.Error analyses shows that OOV words will greatly reduce performances,which needs a deeper research in the future. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我提出了一个新的模式，实现中国的分词（CWS），这可能对其他领域应用的潜力在这个HTTP URL是CWS一种新的思维比以前的作品，将其看作一个聚类问题代替此http URL此模型中，LSTM和自关注结构用于收集上下文标记的也句子电平在每个层的功能，和经过若干层，聚类模型被应用于分割字符转换成基团，其是最终分割results.I称这种模型CLNN.This算法可以达到在训练数据sets.Error F值（无OOV单词）的98％和85％至95％的F值（与OOV单词）分析表明，OOV单词将大大减少表演，这需要在今后深入研究。</font>
</div>


<hr>
<div id="paper8"> <b>8. Improving Multi-Turn Response Selection Models with Complementary  Last-Utterance Selection by Instance Weighting</b>  <a href="https://arxiv.org/pdf/2002.07397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+W+X" target="_blank" rel="noopener" style="color:#0000EE;">Wayne Xin Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yutao Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Ji-Rong Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingsong Yu</a><br>
<font size="3">
Abstract: Open-domain retrieval-based dialogue systems require a considerable amount of training data to learn their parameters. However, in practice, the negative samples of training data are usually selected from an unannotated conversation data set at random. The generated training data is likely to contain noise and affect the performance of the response selection models. To address this difficulty, we consider utilizing the underlying correlation in the data resource itself to derive different kinds of supervision signals and reduce the influence of noisy data. More specially, we consider a main-complementary task pair. The main task (\ie our focus) selects the correct response given the last utterance and context, and the complementary task selects the last utterance given the response and context. The key point is that the output of the complementary task is used to set instance weights for the main task. We conduct extensive experiments in two public datasets and obtain significant improvement in both datasets. We also investigate the variant of our approach in multiple aspects, and the results have verified the effectiveness of our approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：开放域基于内容的检索对话系统需要大量的训练数据，了解它们的参数。然而，在实践中，训练数据的阴性样品通常是从一个未注释的对话数据集随机选择。所生成的训练数据可能含有噪声，影响反应选择车型的性能。为了解决这个困难，我们考虑利用数据资源本身的基本关系来推导不同类型的监管信号，减少噪声数据的影响。更特别，我们认为主要互补任务对。主要任务（\即我们的重点）选择最后给出的话语和语境，和互补的任务选择作出的反应和上下文的最后的声音正确的响应。关键的一点是，互补任务的输出被用于设置实例权重为主要任务。我们在两个公共数据集进行了广泛的实验，并获得两个数据集显著改善。我们还调查了在多个方面的方法的变体，结果验证了我们方法的有效性。</font>
</div>


<hr>
<div id="paper9"> <b>9. Annotating and Extracting Synthesis Process of All-Solid-State Batteries  from Scientific Literature</b>  <a href="https://arxiv.org/pdf/2002.07339" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuniyoshi%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fusataka Kuniyoshi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Makino%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kohei Makino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ozawa%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Ozawa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Miwa%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Makoto Miwa</a><br>
<font size="3">
Abstract: The synthesis process is essential for achieving computational experiment design in the field of inorganic materials chemistry. In this work, we present a novel corpus of the synthesis process for all-solid-state batteries and an automated machine reading system for extracting the synthesis processes buried in the scientific literature. We define the representation of the synthesis processes using flow graphs, and create a corpus from the experimental sections of 243 papers. The automated machine-reading system is developed by a deep learning-based sequence tagger and simple heuristic rule-based relation extractor. Our experimental results demonstrate that the sequence tagger with the optimal setting can detect the entities with a macro-averaged F1 score of 0.826, while the rule-based relation extractor can achieve high performance with a macro-averaged F1 score of 0.887. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：合成处理是用于在无机材料化学领域实现计算实验设计是至关重要的。在这项工作中，我们提出的合成过程中用于全固态电池的新颖语料库和用于提取埋在科学文献中所述的合成方法的自动化机器读取系统。我们定义使用流图的合成过程的表示，并从243篇论文的实验部分，创建的语料库。自动化机器阅读系统是由深学习型序列恶搞和简单的启发式规则为基础的关系提取开发。我们的实验结果表明，用最佳设置的顺序恶搞可以检测到实体宏平均F1得分0.826，而基于规则的关系提取可以实现与宏平均F1值0.887高性能。</font>
</div>


<hr>
<div id="paper10"> <b>10. Conditional Self-Attention for Query-based Summarization</b>  <a href="https://arxiv.org/pdf/2002.07338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yujia Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianyi Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Mao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weizhu Chen</a><br>
<font size="3">
Abstract: Self-attention mechanisms have achieved great success on a variety of NLP tasks due to its flexibility of capturing dependency between arbitrary positions in a sequence. For problems such as query-based summarization (Qsumm) and knowledge graph reasoning where each input sequence is associated with an extra query, explicitly modeling such conditional contextual dependencies can lead to a more accurate solution, which however cannot be captured by existing self-attention mechanisms. In this paper, we propose \textit{conditional self-attention} (CSA), a neural network module designed for conditional dependency modeling. CSA works by adjusting the pairwise attention between input tokens in a self-attention module with the matching score of the inputs to the given query. Thereby, the contextual dependencies modeled by CSA will be highly relevant to the query. We further studied variants of CSA defined by different types of attention. Experiments on Debatepedia and HotpotQA benchmark datasets show CSA consistently outperforms vanilla Transformer and previous models for the Qsumm problem. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自注意机制由于在连续拍摄的任意位置之间的依赖关系的灵活性实现对各种NLP任务取得圆满成功。对于诸如其中每个输入序列与一个额外的查询相关联的，显式建模这样的条件的上下文依赖可以导致更精确的解决方案，然而其不能由现有的自注意力被捕获的基于查询的总结（Qsumm）和知识图形推理问题机制。在本文中，我们提出了\ {textit有条件的自我关注}（CSA），神经网络模块设计条件依赖建模。 CSA的工作原理，通过调整输入记号之间的成对注意与匹配得分的输入，从而所述给定查询的自注意模块中。因此，由CSA模拟情境依赖将是该查询相关。我们进一步研究不同类型的注意力定义CSA的变体。在Debatepedia和HotpotQA标准数据集实验表明CSA一贯优于香草变压器和以前的型号为Qsumm问题。</font>
</div>


<hr>
<div id="paper11"> <b>11. From English To Foreign Languages: Transferring Pre-trained Language  Models</b>  <a href="https://arxiv.org/pdf/2002.07306" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Tran</a><br>
<font size="3">
Abstract: Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT base model within a day and a foreign BERT large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预先训练模式已经证明了它们在许多下游自然语言处理（NLP）任务的有效性。多语种预先训练模型的可用性允许从高资源语言的NLP任务零次转移到较低的资源的。然而，最近在改善预先训练模型的研究重点集中在英语。虽然可以从头开始培养对其他语言的最新的神经结构，这是不可取的，由于所需的计算量。在这项工作中，我们解决一个有限的计算预算下从英国转移现有的预先训练模型与其他语言的问题。随着单GPU，我们的方法可以得到一天之内外国BERT示范基地和外国BERT两天内大。此外，评估对六种语言我们的模型，我们证明了我们的模型比多种语言BERT上两个零射门的任务更好：自然语言推理和依存分析。</font>
</div>


<hr>
<div id="paper12"> <b>12. Decidability of cutpoint isolation for letter-monotonic probabilistic  finite automata</b>  <a href="https://arxiv.org/pdf/2002.07660" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bell%2C+P+C" target="_blank" rel="noopener" style="color:#0000EE;">Paul C. Bell</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Semukhin%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pavel Semukhin</a><br>
<font size="3">
Abstract: We show the surprising result that the cutpoint isolation problem is decidable for probabilistic finite automata where input words are taken from a letter-monotonic context-free language. A context-free language $L$ is letter-monotonic when $L \subseteq a_1^*a_2^* \cdots a_\ell^*$ for some finite $\ell > 0$ where each letter is distinct. A cutpoint is isolated when it cannot be approached arbitrarily closely. The decidability of this problem is in marked contrast to the situation for the (strict) emptiness problem for PFA which is undecidable under the even more severe restrictions of PFA with polynomial ambiguity, commutative matrices and input over a letter-monotonic language as well as the injectivity problem which is undecidable for PFA over letter-monotonic languages. We provide a constructive nondeterministic algorithm to solve the cutpoint isolation problem, even for exponentially ambiguous PFA, and we also show that the problem is at least NP-hard. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们展示了令人惊讶的结果是割点隔离问题是可判定的概率有限自动机，其中输入字由一个字母单调上下文无关语言考虑。甲上下文无关语言$ L $是字母单调时$ L \ subseteq A_1 ^ * A_2 ^ * \ cdots A_ \ ELL ^ * $对于一些有限$ \ ELL> 0 $其中每个字母显着。当它不能随意密切接洽切点是孤立的。这个问题的可判定是鲜明的对比为PFA的（严格）空虚问题的情况是在PFA与多项式歧义，交换矩阵和输入过一封信单调的语言以及在该更严格的限制无法判定注入的问题是不可判定为PFA在信单调的语言。我们提供了一个建设性的不确定性算法解决分割点隔离问题，甚至成倍暧昧PFA，我们还表明，这个问题至少是NP难。</font>
</div>


<hr>
<div id="paper13"> <b>13. A Model to Measure the Spread Power of Rumors</b>  <a href="https://arxiv.org/pdf/2002.07563" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jahanbakhsh-Nagadeh%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zoleikha Jahanbakhsh-Nagadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feizi-Derakhshi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad-Reza Feizi-Derakhshi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramezani%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Majid Ramezani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahkar-Farshi%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taymaz Rahkar-Farshi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Asgari-Chenaghlu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meysam Asgari-Chenaghlu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nikzad-Khasmakhi%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Narjes Nikzad-Khasmakhi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feizi-Derakhshi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali-Reza Feizi-Derakhshi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ranjbar-Khadivi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehrdad Ranjbar-Khadivi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zafarani-Moattar%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elnaz Zafarani-Moattar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Balafar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad-Ali Balafar</a><br>
<font size="3">
Abstract: Nowadays, a significant portion of daily interacted posts in social media are infected by rumors. This study investigates the problem of rumor analysis in different areas from other researches. It tackles the unaddressed problem related to calculating the Spread Power of Rumor (SPR) for the first time and seeks to examine the spread power as the function of multi-contextual features. For this purpose, the theory of Allport and Postman will be adopted. In which it claims that there are two key factors determinant to the spread power of rumors, namely importance and ambiguity. The proposed Rumor Spread Power Measurement Model (RSPMM) computes SPR by utilizing a textual-based approach which entails contextual features to compute the spread power of the rumors in two categories: False Rumor (FR) and True Rumor (TR). Totally 51 contextual features are introduced to measure SPR and their impact on classification are investigated, then 42 features in two categories "importance" (28 features) and "ambiguity" (14 features) are selected to compute SPR. The proposed RSPMM is verified on two labelled datasets, which are collected from Twitter and Telegram. The results show that (i) the proposed new features are effective and efficient to discriminate between FRs and TRs. (ii) the proposed RSPMM approach focused only on contextual features while existing techniques are based on Structure and Content features, but RSPMM achieves considerably outstanding results (F-measure=83%). (iii) The result of T-Test shows that SPR criteria can significantly distinguish between FR and TR, in addition, it can be useful as a new method to verify trueness of rumors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：如今，每天互动职位的社交媒体显著部分被传言感染。这项研究调查了来自其他研究不同领域的传闻分析的问题。它铲球首次与计算传闻（SPR）的扩展的功率未编址的问题，并寻求检查蔓延功率的多上下文特征的功能。为此，奥尔波特和邮差的理论会被采纳。在它声称，有两个关键因素决定因素的传言，即重要性和模糊性的传播力量。所提出的谣传功率测量模型（RSPMM）计算通过利用基于文本的方法这需要上下文特征来计算谣言的传播力量两类SPR：谣言（FR）和真谣言（TR）。共51上下文特征被引入到测量SPR及其对分类的影响进行了研究，然后42个特征在两个类别“重要性”（28个功能）和“模糊”（14个功能）被选择为计算SPR。所提出的RSPMM是在两个标记的数据集，这是从Twitter和电报收集核实。结果表明：（i）所述建议的新功能是有效和高效率的FR和TRS之间进行区分。 （ⅱ）所提出的方法RSPMM仅集中在上下文特征而现有技术是基于结构和内容的功能，但RSPMM达到相当优秀的结果（F值= 83％）。 （ⅲ）的T-试验表明，SPR准则可以FR和TR之间显著区分结果，此外，它可以是作为验证传言真实性的新方法是有用的。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-18</title>
    <url>/2020/02/18/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-18/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Precision Gating: Improving Neural Network Efficiency with Dynamic  Dual-Precision Activations <a href="https://arxiv.org/pdf/2002.07136" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Lake Ice Detection from Sentinel-1 SAR with Deep Learning <a href="https://arxiv.org/pdf/2002.07040" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Learning Architectures for Binary Networks <a href="https://arxiv.org/pdf/2002.06963" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Patient-Specific Finetuning of Deep Learning Models for Adaptive  Radiotherapy in Prostate CT <a href="https://arxiv.org/pdf/2002.06927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Amplifying The Uncanny <a href="https://arxiv.org/pdf/2002.06890" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Hierarchical Rule Induction Network for Abstract Visual Reasoning <a href="https://arxiv.org/pdf/2002.06838" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> DeepDualMapper: A Gated Fusion Network for Automatic Map Extraction  using Aerial Images and Trajectories <a href="https://arxiv.org/pdf/2002.06832" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting <a href="https://arxiv.org/pdf/2002.06820" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> On the Similarity of Deep Learning Representations Across Didactic and  Adversarial Examples <a href="https://arxiv.org/pdf/2002.06816" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Discernible Compressed Images via Deep Perception Consistency <a href="https://arxiv.org/pdf/2002.06810" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> CQ-VQA: Visual Question Answering on Categorized Questions <a href="https://arxiv.org/pdf/2002.06800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Deep Domain Adaptive Object Detection: a Survey <a href="https://arxiv.org/pdf/2002.06797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Unsupervised Image-generation Enhanced Adaptation for Object Detection  in Thermal images <a href="https://arxiv.org/pdf/2002.06770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Superpixel Segmentation via Convolutional Neural Networks with  Regularized Information Maximization <a href="https://arxiv.org/pdf/2002.06765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Directional Deep Embedding and Appearance Learning for Fast Video Object  Segmentation <a href="https://arxiv.org/pdf/2002.06736" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Generator From Edges: Reconstruction of Facial Images <a href="https://arxiv.org/pdf/2002.06682" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> AOL: Adaptive Online Learning for Human Trajectory Prediction in Dynamic  Video Scenes <a href="https://arxiv.org/pdf/2002.06666" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> PeelNet: Textured 3D reconstruction of human body using single view RGB  image <a href="https://arxiv.org/pdf/2002.06664" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings <a href="https://arxiv.org/pdf/2002.06661" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Block Annotation: Better Image Annotation for Semantic Segmentation with  Sub-Image Decomposition <a href="https://arxiv.org/pdf/2002.06626" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> CRL: Class Representative Learning for Image Classification <a href="https://arxiv.org/pdf/2002.06619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Key Points Estimation and Point Instance Segmentation Approach for Lane  Detection <a href="https://arxiv.org/pdf/2002.06604" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Analytic Marching: An Analytic Meshing Solution from Deep Implicit  Surface Networks <a href="https://arxiv.org/pdf/2002.06597" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Automated Labelling using an Attention model for Radiology reports of  MRI scans (ALARM) <a href="https://arxiv.org/pdf/2002.06588" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Reinforced active learning for image segmentation <a href="https://arxiv.org/pdf/2002.06583" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Topological Mapping for Manhattan-like Repetitive Environments <a href="https://arxiv.org/pdf/2002.06575" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Facial Attribute Capsules for Noise Face Super Resolution <a href="https://arxiv.org/pdf/2002.06518" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> A Real-Time Deep Network for Crowd Counting <a href="https://arxiv.org/pdf/2002.06515" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Face Recognition: Too Bias, or Not Too Bias? <a href="https://arxiv.org/pdf/2002.06483" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen  Categories <a href="https://arxiv.org/pdf/2002.06478" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> A Multiple Decoder CNN for Inverse Consistent 3D Image Registration <a href="https://arxiv.org/pdf/2002.06468" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> HighRes-net: Recursive Fusion for Multi-Frame Super-Resolution of  Satellite Imagery <a href="https://arxiv.org/pdf/2002.06460" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> An End-to-End Framework for Unsupervised Pose Estimation of Occluded  Pedestrians <a href="https://arxiv.org/pdf/2002.06429" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Scale-Invariant Multi-Oriented Text Detection in Wild Scene Images <a href="https://arxiv.org/pdf/2002.06423" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Video Face Super-Resolution with Motion-Adaptive Feedback Cell <a href="https://arxiv.org/pdf/2002.06378" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> UniViLM: A Unified Video and Language Pre-Training Model for Multimodal  Understanding and Generation <a href="https://arxiv.org/pdf/2002.06353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Cell R-CNN V3: A Novel Panoptic Paradigm for Instance Segmentation in  Biomedical Images <a href="https://arxiv.org/pdf/2002.06345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Recognizing Families In the Wild (RFIW): The 4th Edition <a href="https://arxiv.org/pdf/2002.06303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Historical Document Processing: Historical Document Processing: A Survey  of Techniques, Tools, and Trends <a href="https://arxiv.org/pdf/2002.06300" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Single Unit Status in Deep Convolutional Neural Network Codes for Face  Identification: Sparseness Redefined <a href="https://arxiv.org/pdf/2002.06274" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Layered Embeddings for Amodal Instance Segmentation <a href="https://arxiv.org/pdf/2002.06264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Why Do Line Drawings Work? A Realism Hypothesis <a href="https://arxiv.org/pdf/2002.06260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein  Graph Double-Attention Network <a href="https://arxiv.org/pdf/2002.06241" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Spectrum Translation for Cross-Spectral Ocular Matching <a href="https://arxiv.org/pdf/2002.06228" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Seeing Around Corners with Edge-Resolved Transient Imaging <a href="https://arxiv.org/pdf/2002.07118" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> Query-Efficient Physical Hard-Label Attacks on Deep Learning Visual  Classification <a href="https://arxiv.org/pdf/2002.07088" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks  for Thermal and NIR to Visible Image Transformation <a href="https://arxiv.org/pdf/2002.07082" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> Large-scale biometry with interpretable neural network regression on UK  Biobank body MRI <a href="https://arxiv.org/pdf/2002.06862" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Class-Imbalanced Semi-Supervised Learning <a href="https://arxiv.org/pdf/2002.06815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> Reinforcement learning for the manipulation of eye tracking data <a href="https://arxiv.org/pdf/2002.06806" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> Unraveling Meta-Learning: Understanding Feature Representations for  Few-Shot Tasks <a href="https://arxiv.org/pdf/2002.06753" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO  Framework <a href="https://arxiv.org/pdf/2002.06701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> Coresets for the Nearest-Neighbor Rule <a href="https://arxiv.org/pdf/2002.06650" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> Hold me tight! Influence of discriminative features on deep network  boundaries <a href="https://arxiv.org/pdf/2002.06349" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> 3D Dynamic Scene Graphs: Actionable Spatial Perception with Places,  Objects, and Humans <a href="https://arxiv.org/pdf/2002.06289" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> Learning representations of irregular particle-detector geometry with  distance-weighted graph networks <a href="https://arxiv.org/pdf/1902.07987" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Adaptive Kernel Estimation of the Spectral Density with Boundary Kernel  Analysis <a href="https://arxiv.org/pdf/1803.03906" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Precision Gating: Improving Neural Network Efficiency with Dynamic  Dual-Precision Activations</b>  <a href="https://arxiv.org/pdf/2002.07136" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yichi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ritchie Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weizhe Hua</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nayun Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Suh%2C+G+E" target="_blank" rel="noopener" style="color:#0000EE;">G. Edward Suh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiru Zhang</a><br>
<font size="3">
Abstract: We propose precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in a higher precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. Our experiments indicate that PG achieves excellent results on CNNs, including statically compressed mobile-friendly networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4$\times$ less compute on ImageNet. PG furthermore applies to RNNs. Compared to 8-bit uniform quantization, PG obtains a 1.2% improvement in perplexity per word with 2.7$\times$ computational cost reduction on LSTM on the Penn Tree Bank dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出精密门控（PG），深神经网络的终端到终端的可训练的动态双精度量化技术。 PG计算最具特色的精度低，只有重要的特征，更高的精度一小部分以保持准确性。所提出的方法是适用于各种DNN架构和显著降低DNN执行的计算成本几乎没有精度损失。我们的实验表明PG实现对细胞神经网络优异的成绩，其中包括静态压缩移动友好型网络如的ShuffleNet。的状态相比的最先进的基于预测的量化方案，PG实现了与上ImageNet 2.4 $ \倍$计算更少相同或更高的精度。 PG还适用于RNNs。相比于8位均匀量化，PG获得每字困惑1.2％的改善与LSTM $计算成本降低的宾州树库的数据集2.7 $ \倍。</font>
</div>


<hr>
<div id="paper2"> <b>2. Lake Ice Detection from Sentinel-1 SAR with Deep Learning</b>  <a href="https://arxiv.org/pdf/2002.07040" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tom%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manu Tom</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Aguilar%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roberto Aguilar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Imhof%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pascal Imhof</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leinss%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Silvan Leinss</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baltsavias%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emmanuel Baltsavias</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schindler%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Konrad Schindler</a><br>
<font size="3">
Abstract: Lake ice, as part of the Essential Climate Variable (ECV) lakes, is an important indicator to monitor climate change and global warming. The spatio-temporal extent of lake ice cover, along with the timings of key phenological events such as freeze-up and break-up, provides important cues about the local and global climate. We present a lake ice monitoring system based on the automatic analysis of Sentinel-1 Synthetic Aperture Radar (SAR) data with a deep neural network. In previous studies that used optical satellite imagery for lake ice monitoring, frequent cloud cover was a main limiting factor, which we overcome thanks to the ability of microwave sensors to penetrate clouds and observe the lakes regardless of the weather and illumination conditions. We cast ice detection as a two class (frozen, non-frozen) semantic segmentation problem and solve it using a state-of-the-art deep convolutional network (CNN). We report results on two winters ($2016-17$ and $2017-18$) and three alpine lakes in Switzerland, including cross-validation tests to assess the generalisation to unseen lakes and winters. The proposed model reaches mean Intersection-over-Union (mIoU) scores >90% on average, and >84% even for the most difficult lake. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：湖冰，作为基本气候变量（ECV）湖的一部分，监测气候变化和全球变暖的一个重要指标。湖冰覆盖的时空范围内，与关键物候事件，如冷冻起来，分手的时刻一起，提供有关本地和全球气候的重要线索。我们提出了基于哨兵-1合成孔径雷达（SAR）数据，具有深层神经网络自动分析一湖冰监测系统。在以前的研究中使用的光学卫星图像湖冰监测，频繁云盖是一个主要的限制因素，这是我们克服由于微波传感器的穿透云和无论天气和照明条件的观察湖泊的能力。我们投冰检测作为两类（冷冻的，非冷冻）语义分割问题和使用状态的最先进的深卷积网络（CNN）求解。我们报告了两个冬天（$ 2016-17 $ $和$ 2017  -  18）和瑞士三种高寒草甸湖泊，包括交叉验证测试，以评估推广到看不见的湖泊，冬季的结果。该模型河段意味着，即使是最困难的湖路口，过联盟（米欧）得分> 90％的平均值，和> 84％。</font>
</div>


<hr>
<div id="paper3"> <b>3. Learning Architectures for Binary Networks</b>  <a href="https://arxiv.org/pdf/2002.06963" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+K+P" target="_blank" rel="noopener" style="color:#0000EE;">Kunal Pratap Singh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dahyun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonghyun Choi</a><br>
<font size="3">
Abstract: Backbone architectures of most binary networks are well-known floating point architectures, such as the ResNet family. Questioning that the architectures designed for floating-point networks would not be the best for binary networks, we propose to search architectures for binary networks (BNAS). Specifically, based on the cell based search method, we define a new set of layer types, design a new cell template, and rediscover the utility of and propose to use the Zeroise layer to learn well-performing binary networks. In addition, we propose to diversify early search to learn better performing binary architectures. We show that our searched binary networks outperform state-of-the-art binary networks on CIFAR10 and ImageNet datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数二进制网络的骨干架构是众所周知的浮点架构，如RESNET家庭。质疑是专为浮点网络架构不会是最好的二进制网络，我们建议搜索二进制网络（BNAS）架构。具体而言，基于基于小区搜索方法，我们定义一组新的图层类型，设计出新的细胞模板，重新发现的效用，并提出使用Zeroise层好好学习，执行二进制网络。此外，我们建议早期的多样化搜索更好地学习执行二进制架构。我们证明了我们的搜索二进制网络性能超过CIFAR10和ImageNet数据集的国家的最先进的二进制网络。</font>
</div>


<hr>
<div id="paper4"> <b>4. Patient-Specific Finetuning of Deep Learning Models for Adaptive  Radiotherapy in Prostate CT</b>  <a href="https://arxiv.org/pdf/2002.06927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Elmahdy%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Mohamed S. Elmahdy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ahuja%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tanuj Ahuja</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=van+der+Heide%2C+U+A" target="_blank" rel="noopener" style="color:#0000EE;">U. A. van der Heide</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Staring%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marius Staring</a><br>
<font size="3">
Abstract: Contouring of the target volume and Organs-At-Risk (OARs) is a crucial step in radiotherapy treatment planning. In an adaptive radiotherapy setting, updated contours need to be generated based on daily imaging. In this work, we leverage personalized anatomical knowledge accumulated over the treatment sessions, to improve the segmentation accuracy of a pre-trained Convolution Neural Network (CNN), for a specific patient. We investigate a transfer learning approach, fine-tuning the baseline CNN model to a specific patient, based on imaging acquired in earlier treatment fractions. The baseline CNN model is trained on a prostate CT dataset from one hospital of 379 patients. This model is then fine-tuned and tested on an independent dataset of another hospital of 18 patients, each having 7 to 10 daily CT scans. For the prostate, seminal vesicles, bladder and rectum, the model fine-tuned on each specific patient achieved a Mean Surface Distance (MSD) of $1.64 \pm 0.43$ mm, $2.38 \pm 2.76$ mm, $2.30 \pm 0.96$ mm, and $1.24 \pm 0.89$ mm, respectively, which was significantly better than the baseline model. The proposed personalized model adaptation is therefore very promising for clinical implementation in the context of adaptive radiotherapy of prostate cancer. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：靶区和器官高危（桨）的轮廓加工是在放射治疗计划的关键一步。在自适应放疗设置，更新轮廓需要基于每日成像产生。在这项工作中，我们利用积累疗程个性化的解剖知识，提高预训练卷积神经网络（CNN）的分割精度，针对特定病人。我们调查的转移学习方法，微调基准CNN模型特定患者的基础上，在早期治疗分成像获取。基线CNN模型是从379例一个中院培训了前列腺CT数据集。该模型然后微调和18本例中，每个具有7-10个每日CT扫描另一医院的一个独立的数据集进行测试。对于前列腺，精囊，膀胱和直肠，模型微调每个特定患者达到$ 1.64 \下午0.43 $毫米的平均表面距离（MSD），$ 2.38 \下午2.76 $毫米，$ 2.30 \下午0.96 $毫米，和$ 1.24 \ 0.89下午$毫米，分别为，这是显著比基线模型更好。因此，提出个性化的模型适应性非常有前途的前列腺癌的放射治疗自适应的背景下临床实施。</font>
</div>


<hr>
<div id="paper5"> <b>5. Amplifying The Uncanny</b>  <a href="https://arxiv.org/pdf/2002.06890" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Broad%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Terence Broad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leymarie%2C+F+F" target="_blank" rel="noopener" style="color:#0000EE;">Frederic Fol Leymarie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grierson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mick Grierson</a><br>
<font size="3">
Abstract: Deep neural networks have become remarkably good at producing realistic deepfakes, images of people that are (to the untrained eye) indistinguishable from real images. These are produced by algorithms that learn to distinguish between real and fake images and are optimised to generate samples that the system deems realistic. This paper, and the resulting series of artworks Being Foiled explore the aesthetic outcome of inverting this process and instead optimising the system to generate images that it sees as being fake. Maximising the unlikelihood of the data and in turn, amplifying the uncanny nature of these machine hallucinations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络已经成为现实生产deepfakes，人是（未经训练的眼睛）从真实图像区分图像非常好的。这些是由学习真假图像之间进行区分，并且优化该系统认为现实的产生样品的算法产生的。本文以及由此带来的一系列作品被挫败的探索颠倒这个过程，而是优化系统生成的图像，它认为是假的审美效果。最大化又将数据的和不大可能，放大这些机器幻觉的不可思议的性质。</font>
</div>


<hr>
<div id="paper6"> <b>6. Hierarchical Rule Induction Network for Abstract Visual Reasoning</b>  <a href="https://arxiv.org/pdf/2002.06838" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sheng Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuqing Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xianglong Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanlu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shihao Bai</a><br>
<font size="3">
Abstract: Abstract reasoning refers to the ability to analyze information, discover rules at an intangible level, and solve problems in innovative ways. Raven's Progressive Matrices (RPM) test is typically used to examine the capability of abstract reasoning. In the test, the subject is asked to identify the correct choice from the answer set to fill the missing panel at the bottom right of RPM (e.g., a 3$\times$3 matrix), following the underlying rules inside the matrix. Recent studies, taking advantage of Convolutional Neural Networks (CNNs), have achieved encouraging progress to accomplish the RPM test problems. Unfortunately, simply relying on the relation extraction at the matrix level, they fail to recognize the complex attribute patterns inside or across rows/columns of RPM. To address this problem, in this paper we propose a Hierarchical Rule Induction Network (HriNet), by intimating human induction strategies. HriNet extracts multiple granularity rule embeddings at different levels and integrates them through a gated embedding fusion module. We further introduce a rule similarity metric based on the embeddings, so that HriNet can not only be trained using a tuplet loss but also infer the best answer according to the similarity score. To comprehensively evaluate HriNet, we first fix the defects contained in the very recent RAVEN dataset and generate a new one named Balanced-RAVEN. Then extensive experiments are conducted on the large-scale dataset PGM and our Balanced-RAVEN, the results of which show that HriNet outperforms the state-of-the-art models by a large margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：抽象推理是指能力，分析信息，以一种无形的层面探索规律，解决以创新的方式问题。 Raven的推理测验（RPM）测试通常用来检查抽象推理的能力。在该试验中，受试者被要求从回答集识别正确的选择在RPM的右下角，以填补缺失的面板（例如，3 $ \倍$ 3矩阵），按照矩阵内的底层规则。最近的研究，卷积神经网络（细胞神经网络）的利用，取得了可喜的进展，完成RPM测试问题。不幸的是，单纯依靠在基体层的关系提取，他们没有认识到内部或跨RPM的行/列复杂的属性模式。为了解决这个问题，在本文中，我们提出了一种层次法则归纳网（HriNet），通过威逼人类诱导策略。 HriNet提取在通过门控嵌入融合模块不同层次和整合他们多粒度规则的嵌入。我们进一步推出基于这个的嵌入规则相似性度量，这样HriNet不仅可以使用连音损失，而且推断最佳答案根据相似性得分培训。综合评价HriNet，我们首先确定包含在非常近RAVEN数据集中的缺陷，并生成一个名为平衡RAVEN新的。然后广泛的实验是在大型数据集PGM和我们的平衡RAVEN进行的，其结果表明，HriNet优于国家的最先进的车型以大比分。</font>
</div>


<hr>
<div id="paper7"> <b>7. DeepDualMapper: A Gated Fusion Network for Automatic Map Extraction  using Aerial Images and Trajectories</b>  <a href="https://arxiv.org/pdf/2002.06832" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanyuan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weiwei Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baihua Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuning Jiang</a><br>
<font size="3">
Abstract: Automatic map extraction is of great importance to urban computing and location-based services. Aerial image and GPS trajectory data refer to two different data sources that could be leveraged to generate the map, although they carry different types of information. Most previous works on data fusion between aerial images and data from auxiliary sensors do not fully utilize the information of both modalities and hence suffer from the issue of information loss. We propose a deep convolutional neural network called DeepDualMapper which fuses the aerial image and trajectory data in a more seamless manner to extract the digital map. We design a gated fusion module to explicitly control the information flows from both modalities in a complementary-aware manner. Moreover, we propose a novel densely supervised refinement decoder to generate the prediction in a coarse-to-fine way. Our comprehensive experiments demonstrate that DeepDualMapper can fuse the information of images and trajectories much more effectively than existing approaches, and is able to generate maps with higher accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：地图自动提取的城市计算和基于位置的服务具有重要意义。空间图像和GPS轨迹数据指的是可以被利用来生成地图两种不同的数据源，虽然它们携带不同类型的信息。在航拍图像和辅助传感器的数据之间的数据融合大部分以前的作品没有充分利用这两种方式的信息，因此，从信息丢失的问题受到影响。我们提出所谓DeepDualMapper了深刻的卷积神经网络，融合航拍图和轨迹数据更无缝的方式提取数字地图。我们设计了门控融合模块明确地控制从以互补感知方式两种模式中的信息流动。此外，我们提出了一个新颖的密集监督细化解码器，以生成一个粗到细的方式预测。我们全面的实验表明，DeepDualMapper可以比现有的方法更有效地融合图像和轨迹的信息，并能以更高的精度来生成地图。</font>
</div>


<hr>
<div id="paper8"> <b>8. Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting</b>  <a href="https://arxiv.org/pdf/2002.06820" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Qiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanli Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhanzhan Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunlu Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Niu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiliang Pu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a><br>
<font size="3">
Abstract: Many approaches have recently been proposed to detect irregular scene text and achieved promising results. However, their localization results may not well satisfy the following text recognition part mainly because of two reasons: 1) recognizing arbitrary shaped text is still a challenging task, and 2) prevalent non-trainable pipeline strategies between text detection and text recognition will lead to suboptimal performances. To handle this incompatibility problem, in this paper we propose an end-to-end trainable text spotting approach named Text Perceptron. Concretely, Text Perceptron first employs an efficient segmentation-based text detector that learns the latent text reading order and boundary information. Then a novel Shape Transform Module (abbr. STM) is designed to transform the detected feature regions into regular morphologies without extra parameters. It unites text detection and the following recognition part into a whole framework, and helps the whole network achieve global optimization. Experiments show that our method achieves competitive performance on two standard text benchmarks, i.e., ICDAR 2013 and ICDAR 2015, and also obviously outperforms existing methods on irregular text benchmarks SCUT-CTW1500 and Total-Text. </font>
<br>
<font size="2" style="line-height:30px;">
摘要已经提出了许多方法来检测不规则的现场文字，取得了可喜的成果：摘要。然而，他们的定位结果可能不会很好地满足，主要是因为两个原因如下文字识别部分：1）识别任意形状的文本仍然是一个艰巨的任务，和2）流行的非训练的管道战略文本检测和文字识别之间将导致次优的性能。为了解决这个不兼容的问题，在本文中，我们提出了一个名为文本感知的终端到终端的可训练的文本斑点的方法。具体地，文本感知第一采用了学习潜文本读取顺序和边界信息的高效基于分割的文本检测器。然后新颖的形状变换模块（缩写STM）被设计为变换所检测到的特征区域成规则的形态没有额外的参数。它团结文本检测和识别以下部分成一个整体的框架，并有助于整个网络实现全局优化。实验表明，我们的方法实现两种标准的文本基准竞争力的性能，即ICDAR 2013年和2015年ICDAR，也明显性能优于不规则文字的基准华南理工大学，CTW1500和总文本现有的方法。</font>
</div>


<hr>
<div id="paper9"> <b>9. On the Similarity of Deep Learning Representations Across Didactic and  Adversarial Examples</b>  <a href="https://arxiv.org/pdf/2002.06816" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Douglas%2C+P+K" target="_blank" rel="noopener" style="color:#0000EE;">Pamela K. Douglas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Farahani%2C+F+V" target="_blank" rel="noopener" style="color:#0000EE;">Farzad Vasheghani Farahani</a><br>
<font size="3">
Abstract: The increasing use of deep neural networks (DNNs) has motivated a parallel endeavor: the design of adversaries that profit from successful misclassifications. However, not all adversarial examples are crafted for malicious purposes. For example, real world systems often contain physical, temporal, and sampling variability across instrumentation. Adversarial examples in the wild may inadvertently prove deleterious for accurate predictive modeling. Conversely, naturally occurring covariance of image features may serve didactic purposes. Here, we studied the stability of deep learning representations for neuroimaging classification across didactic and adversarial conditions characteristic of MRI acquisition variability. We show that representational similarity and performance vary according to the frequency of adversarial examples in the input space. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：越来越多地使用深层神经网络（DNNs）的激励了平行的努力：对手是获利的设计从成功的错误分类。然而，并非所有敌对实例制作用于恶意目的。例如，真实世界的系统通常包含物理，时间和采样跨仪器的可变性。在野外对抗性的例子可能会在无意中证明有害准确的预测模型。相反地​​，图像特征天然存在的协方差可用于教导目的。在这里，我们研究深度学习交涉的稳定性跨MRI采集变异的特征说教和对抗性条件神经影像学分类。我们表明，代表性的相似性和性能根据在输入空间对抗性例的频率而变化。</font>
</div>


<hr>
<div id="paper10"> <b>10. Discernible Compressed Images via Deep Perception Consistency</b>  <a href="https://arxiv.org/pdf/2002.06810" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhaohui Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunhe Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chao Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chang Xu</a><br>
<font size="3">
Abstract: Image compression, as one of the fundamental low-level image processing tasks, is very essential for computer vision. Conventional image compression methods tend to obtain compressed images by minimizing their appearance discrepancy with the corresponding original images, but pay little attention to their efficacy in downstream perception tasks, e.g., image recognition and object detection. In contrast, this paper aims to produce compressed images by pursuing both appearance and perception consistency. Based on the encoder-decoder framework, we propose using a pre-trained CNN to extract features of original and compressed images. In addition, the maximum mean discrepancy (MMD) is employed to minimize the difference between feature distributions. The resulting compression network can generate images with high image quality and preserve the consistent perception in the feature domain, so that these images can be well recognized by pre-trained machine learning models. Experiments on benchmarks demonstrate the superiority of the proposed algorithm over comparison methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图像压缩，因为根本低级图像处理的任务之一，是计算机视觉非常必要的。传统图像压缩方法往往通过最小化与相应的原始图像它们的外观差异，以获得压缩的图像，但很少注意它们的功效在下游感知的任务，例如，图像识别和对象检测。相比之下，本文的目的是通过追求外观和感知的一致性，以产生压缩的图像。基于编码器，解码器的框架，我们建议使用预训练CNN提取原始图像和压缩图像的功能。此外，最大平均差异（MMD）功能，以减少特征分布之间的差异。产生的压缩网络可以产生具有高图像质量的图像和保存在功能域的一致看法，因此这些图像可以通过预先训练机器学习模型被广泛认可。在基准测试实验验证了该算法的在比较方法的优越性。</font>
</div>


<hr>
<div id="paper11"> <b>11. CQ-VQA: Visual Question Answering on Categorized Questions</b>  <a href="https://arxiv.org/pdf/2002.06800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mishra%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aakansha Mishra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anand%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashish Anand</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guha%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prithwijit Guha</a><br>
<font size="3">
Abstract: This paper proposes CQ-VQA, a novel 2-level hierarchical but end-to-end model to solve the task of visual question answering (VQA). The first level of CQ-VQA, referred to as question categorizer (QC), classifies questions to reduce the potential answer search space. The QC uses attended and fused features of the input question and image. The second level, referred to as answer predictor (AP), comprises of a set of distinct classifiers corresponding to each question category. Depending on the question category predicted by QC, only one of the classifiers of AP remains active. The loss functions of QC and AP are aggregated together to make it an end-to-end model. The proposed model (CQ-VQA) is evaluated on the TDIUC dataset and is benchmarked against state-of-the-art approaches. Results indicate competitive or better performance of CQ-VQA. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提出CQ-VQA，一种新颖的2级分层但端至端模式来解决视觉问答（VQA）的任务。 CQ-VQA的第一级，被称为问题分类（QC），分类问题，以减少潜在的答案搜索空间。该QC用途出席并融合了输入问题和图像的功能。第二个层次中，被称为应答预测器（AP），一组对应于每个问题类别不同分类器中的包括。根据通过QC预测出题类型，只有AP的分类之一仍然有效。 QC和AP的损失功能集中在一起，使其成为一个终端到高端型号。所提出的模型（CQ-VQA）上TDIUC数据集进行评估，并且对基准状态的最先进的方法。结果表明CQ-VQA的竞争性或更好的性能。</font>
</div>


<hr>
<div id="paper12"> <b>12. Deep Domain Adaptive Object Detection: a Survey</b>  <a href="https://arxiv.org/pdf/2002.06797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wanyi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fuyu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongkang Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Wang</a><br>
<font size="3">
Abstract: Deep learning (DL) based object detection has achieved great progress. These methods typically assume that large amount of labeled training data is available, and training and test data are drawn from an identical distribution. However, the two assumptions are not always hold in practice. Deep domain adaptive object detection (DDAOD) has emerged as a new learning paradigm to address the above mentioned challenges. This paper aims to review the state-of-the-art progress on deep domain adaptive object detection approaches. Firstly, we introduce briefly the basic concepts of deep domain adaptation. Secondly, the deep domain adaptive detectors are classified into four categories and detailed descriptions of representative methods in each category are provided. Finally, insights for future research trend are presented. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深学习（DL）基于对象的检测已经取得了很大的进步。这些方法通常假设大量标记的训练数据是可用的，和训练和测试数据被从同一分发绘制。然而，这两个假设并不总是保持在实践中。深域自适应对象检测（DDAOD）已经成为一种新的学习范例来解决上述挑战。本文旨在深域自适应物体检测方法，审查状态的最先进的进展。首先，我们简要地介绍了深厚的适应的基本概念。提供的每个类别的代表性方法其次，深域自适应检测器被分为四类和详细说明。最后，未来的研究发展趋势的见解呈现。</font>
</div>


<hr>
<div id="paper13"> <b>13. Unsupervised Image-generation Enhanced Adaptation for Object Detection  in Thermal images</b>  <a href="https://arxiv.org/pdf/2002.06770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wanyi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fuyu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongkang Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peng Wang</a><br>
<font size="3">
Abstract: Object detection in thermal images is an important computer vision task and has many applications such as unmanned vehicles, robotics, surveillance and night vision. Deep learning based detectors have achieved major progress, which usually need large amount of labelled training data. However, labelled data for object detection in thermal images is scarce and expensive to collect. How to take advantage of the large number labelled visible images and adapt them into thermal image domain, is expected to solve. This paper proposes an unsupervised image-generation enhanced adaptation method for object detection in thermal images. To reduce the gap between visible domain and thermal domain, the proposed method manages to generate simulated fake thermal images that are similar to the target images, and preserves the annotation information of the visible source domain. The image generation includes a CycleGAN based image-to-image translation and an intensity inversion transformation. Generated fake thermal images are used as renewed source domain. And then the off-the-shelf Domain Adaptive Faster RCNN is utilized to reduce the gap between generated intermediate domain and the thermal target domain. Experiments demonstrate the effectiveness and superiority of the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在热图像目标检测是一个重要的计算机视觉任务，有许多应用，如无人驾驶车辆，机器人，监视和夜视。基于深度学习的探测器都取得了重大进展，这通常需要大量的标记的训练数据。然而，对于在热图像对象检测标记的数据是稀缺和昂贵收集。如何利用大量用数字标记的可见光图像，并将其改编成热成像领域，有望解决。本文提出了一种在热图像物体检测无监督图像生成增强的自适应方法。为了减少可见域和热域之间的间隙，所提出的方法进行管理，以产生类似于目标图像模拟假热图像，并保留可见源域的注释信息。图像生成包括基于CycleGAN图像到图像平移和强度反转变换。产生假热图像被用作更新源域。然后关断的，现成的域自适应更快RCNN用于降低生成的中间域和热目标域之间的间隙。实验证明了该方法的有效性和优越性。</font>
</div>


<hr>
<div id="paper14"> <b>14. Superpixel Segmentation via Convolutional Neural Networks with  Regularized Information Maximization</b>  <a href="https://arxiv.org/pdf/2002.06765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Suzuki%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Teppei Suzuki</a><br>
<font size="3">
Abstract: We propose an unsupervised superpixel segmentation method by optimizing a randomly-initialized convolutional neural network (CNN) in inference time. Our method generates superpixels via CNN from a single image without any labels by minimizing a proposed objective function for superpixel segmentation in inference time. There are three advantages to our method compared with many of existing methods: (i) leverages an image prior of CNN for superpixel segmentation, (ii) adaptively changes the number of superpixels according to the given images, and (iii) controls the property of superpixels by adding an auxiliary cost to the objective function. We verify the advantages of our method quantitatively and qualitatively on BSDS500 and SBD datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出通过优化推理时间随机初始化卷积神经网络（CNN）的无监督的超像素分割方法。我们的方法通过在推理时间最小化超像素分割一个提出的目标函数生成从一个单一的图像，而没有任何标签经由CNN超像素。有三个优点我们的方法具有许多的现有方法相比：（i）在CNN的利用图像为超像素分割，（ⅱ）自适应地改变根据给定的图像中的超像素的数目，和（iii）控制的属性通过添加辅助成本对目标函数的超像素。我们定量和定性核实BSDS500和SBD数据集我们的方法的优点。</font>
</div>


<hr>
<div id="paper15"> <b>15. Directional Deep Embedding and Appearance Learning for Fast Video Object  Segmentation</b>  <a href="https://arxiv.org/pdf/2002.06736" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingjie Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">De Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xingang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Zhang</a><br>
<font size="3">
Abstract: Most recent semi-supervised video object segmentation (VOS) methods rely on fine-tuning deep convolutional neural networks online using the given mask of the first frame or predicted masks of subsequent frames. However, the online fine-tuning process is usually time-consuming, limiting the practical use of such methods. We propose a directional deep embedding and appearance learning (DDEAL) method, which is free of the online fine-tuning process, for fast VOS. First, a global directional matching module, which can be efficiently implemented by parallel convolutional operations, is proposed to learn a semantic pixel-wise embedding as an internal guidance. Second, an effective directional appearance model based statistics is proposed to represent the target and background on a spherical embedding space for VOS. Equipped with the global directional matching module and the directional appearance model learning module, DDEAL learns static cues from the labeled first frame and dynamically updates cues of the subsequent frames for object segmentation. Our method exhibits state-of-the-art VOS performance without using online fine-tuning. Specifically, it achieves a J & F mean score of 74.8% on DAVIS 2017 dataset and an overall score G of 71.3% on the large-scale YouTube-VOS dataset, while retaining a speed of 25 fps with a single NVIDIA TITAN Xp GPU. Furthermore, our faster version runs 31 fps with only a little accuracy loss. Our code and trained networks are available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的半监督视频对象分割（VOS）的方法依赖于微调深卷积神经网络的在线使用所述第一帧的所述给定掩模或后续帧的预测的掩模。然而，在线微调过程通常是耗时的，这限制了实际使用的这种方法。我们提出了一个方向性深深嵌入和外观学习（DDEAL）方法，它是免费的在线微调过程中，快速VOS。首先，全局定向匹配模块，其可以通过并行卷积操作来高效地实现，提出了学习语义逐像素嵌入作为内部引导。第二，一个有效的定向外观基于模型统计提出来表示对VOS球形嵌入空间的目标和背景。配备有全球定向匹配模块和定向外观模型学习模块，DDEAL学习来自标记的第一帧的静态提示和动态地更新为对象分割后续帧的提示。我们的方法表现出VOS性能状态的最先进的，而无需使用在线微调。具体而言，实现了74.8％的DAVIS 2017的数据集并在大型的YouTube-VOS数据集的71.3％的总成绩G为J＆F平均得分，同时保留的每秒25帧与单个NVIDIA TITAN XP中GPU的速度。此外，我们的速度更快的版本上运行31 fps的只有一点点的精度损失。我们的代码和训练有素的网络，可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper16"> <b>16. Generator From Edges: Reconstruction of Facial Images</b>  <a href="https://arxiv.org/pdf/2002.06682" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Takano%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nao Takano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alaghband%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gita Alaghband</a><br>
<font size="3">
Abstract: Applications that involve supervised training require paired images. Researchers of single image super-resolution (SISR) create such images by artificially generating blurry input images from the corresponding ground truth. Similarly we can create paired images with the canny edge. We propose Generator From Edges (GFE) [Figure 2]. Our aim is to determine the best architecture for GFE, along with reviews of perceptual loss [1, 2]. To this end, we conducted three experiments. First, we explored the effects of the adversarial loss often used in SISR. In particular, we uncovered that it is not an essential component to form a perceptual loss. Eliminating adversarial loss will lead to a more effective architecture from the perspective of hardware resource. It also means that considerations for the problems pertaining to generative adversarial network (GAN) [3], such as mode collapse, are not necessary. Second, we reexamined VGG loss and found that the mid-layers yield the best results. By extracting the full potential of VGG loss, the overall performance of perceptual loss improves significantly. Third, based on the findings of the first two experiments, we reevaluated the dense network to construct GFE. Using GFE as an intermediate process, reconstructing a facial image from a pencil sketch can become an easy task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：应用涉及指导训练需要成对的图像。单图像超分辨率（SISR）的研究人员通过人工从相应的地面实况产生模糊的输入图像创建这样的图像。同样，我们可以创建一个精明的边缘成对的图像。我们提出发电机从边缘处（GFE）[图2]。我们的目标是确定最佳结构为GFE，与视觉损失[1,2]的评论一起。为此，我们进行了三次实验。首先，我们探索了经常在SISR使用的对抗性损失的影响。特别是，我们发现，这是不形成视觉损失的必要成分。消除敌对的损失将导致从硬件资源的角度更有效的架构。这也意味着，对于关于生成对抗网络（GAN）[3]，如模式崩溃的问题的考虑，是没有必要的。其次，我们重新审视VGG损失，发现中间层产生最好的结果。通过提取VGG损失的全部潜能，视觉损失的整体性能显著提高。第三，基于前两个实验的结果，我们重新评估了密集的网络，构建GFE。使用GFE作为中间过程，重建来自铅笔素描的面部图像可以成为一个简单的任务。</font>
</div>


<hr>
<div id="paper17"> <b>17. AOL: Adaptive Online Learning for Human Trajectory Prediction in Dynamic  Video Scenes</b>  <a href="https://arxiv.org/pdf/2002.06666" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huynh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manh Huynh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alaghband%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gita Alaghband</a><br>
<font size="3">
Abstract: We present a novel adaptive online learning (AOL) framework to predict human movement trajectories in dynamic video scenes. Our framework learns and adapts to changes in the scene environment and generates best network weights for different scenarios. The framework can be applied to prediction models and improve their performance as it dynamically adjusts when it encounters changes in the scene and can apply the best training weights for predicting the next locations. We demonstrate this by integrating our framework with two existing prediction models: LSTM [3] and Future Person Location (FPL) [1]. Furthermore, we analyze the number of network weights for optimal performance and show that we can achieve real-time with a fixed number of networks using the least recently used (LRU) strategy for maintaining the most recently trained network weights. With extensive experiments, we show that our framework increases prediction accuracies of LSTM and FPL by ~17% and 28% on average, and up to ~50% for FPL on the worst case while achieving real-time (20fps). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种新的自适应在线学习（AOL）的框架来预测动态视频场景人体运动轨迹。我们的框架，学习和适应场景中的环境变化，产生不同场景的最佳网络权。该框架可以应用到预测模型和提高它们的性能，当它遇到的场景变化，并可以应用最好的训练权重来预测下一个位置，因为它动态调整。 LSTM [3]和未来的人的位置（FPL）[1]：我们通过我们的两个现有的预测模型框架集成证明这一点。此外，我们分析网络配重的数量以获得最佳性能，并表明我们可以实现实时有固定数量的使用维护最近训练好的网络权最近最少使用（LRU）策略网络。随着大量的实验，我们表明，LSTM和FPL的我们的框架增加预测精度由〜17％，平均28％，达到约50％为FPL在最坏的情况下，同时实现实时（20FPS）。</font>
</div>


<hr>
<div id="paper18"> <b>18. PeelNet: Textured 3D reconstruction of human body using single view RGB  image</b>  <a href="https://arxiv.org/pdf/2002.06664" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jinka%2C+S+S" target="_blank" rel="noopener" style="color:#0000EE;">Sai Sagar Jinka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chacko%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rohan Chacko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avinash Sharma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Narayanan%2C+P+J" target="_blank" rel="noopener" style="color:#0000EE;">P. J. Narayanan</a><br>
<font size="3">
Abstract: Reconstructing human shape and pose from a single image is a challenging problem due to issues like severe self-occlusions, clothing variations, and changes in lighting to name a few. Many applications in the entertainment industry, e-commerce, health-care (physiotherapy), and mobile-based AR/VR platforms can benefit from recovering the 3D human shape, pose, and texture. In this paper, we present PeelNet, an end-to-end generative adversarial framework to tackle the problem of textured 3D reconstruction of the human body from a single RGB image. Motivated by ray tracing for generating realistic images of a 3D scene, we tackle this problem by representing the human body as a set of peeled depth and RGB maps which are obtained by extending rays beyond the first intersection with the 3D object. This formulation allows us to handle self-occlusions efficiently. Current parametric model-based approaches fail to model loose clothing and surface-level details and are proposed for the underlying naked human body. Majority of non-parametric approaches are either computationally expensive or provide unsatisfactory results. We present a simple non-parametric solution where the peeled maps are generated from a single RGB image as input. Our proposed peeled depth maps are back-projected to 3D volume to obtain a complete 3D shape. The corresponding RGB maps provide vertex-level texture details. We compare our method against current state-of-the-art methods in 3D reconstruction and demonstrate the effectiveness of our method on BUFF and MonoPerfCap datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：重建人体的形状和姿态，从一个单一的形象是一个具有挑战性的问题，是由于其在照明仅举几例类似严重的自我闭塞，服装的变化，以及变化的问题。在娱乐行业的许多应用程序，电子商务，医疗保健（理疗），以及基于移动-AR / VR平台可以受益于恢复三维人体形状，姿态和质感。在本文中，我们提出PeelNet，端至端生成对抗框架从单个RGB图像处理纹理的3D重建人体的问题。通过射线跟踪用于生成三维场景的逼真的图像的启发，我们通过表示人体作为一组其通过延伸射线超出与3D对象的第一相交得到剥离深度和RGB图的解决这个问题。这一提法使我们能够有效地处理自我闭塞。当前参数的基于模型的方法都没有模型宽松的衣服和表面层次的细节，并提出了基本的裸体人体。的非参数方法多数要么计算昂贵的或提供不令人满意的结果。我们提出，其中剥离映射从作为输入的单个RGB图像而生成一个简单的非参数的解决方案。我们提出的去皮深度图被背投影到3D体积以获得完整的3D形状。相应的RGB地图提供顶点级纹理细节。我们将我们对国家的最先进的电流方法三维重建方法，并证明我们的BUFF和MonoPerfCap数据集方法的有效性。</font>
</div>


<hr>
<div id="paper19"> <b>19. Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings</b>  <a href="https://arxiv.org/pdf/2002.06661" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mahajan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shweta Mahajan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gurevych%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Iryna Gurevych</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roth%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Roth</a><br>
<font size="3">
Abstract: Learned joint representations of images and text form the backbone of several important cross-domain tasks such as image captioning. Prior work mostly maps both domains into a common latent representation in a purely supervised fashion. This is rather restrictive, however, as the two domains follow distinct generative processes. Therefore, we propose a novel semi-supervised framework, which models shared information between domains and domain-specific information separately. The information shared between the domains is aligned with an invertible neural network. Our model integrates normalizing flow-based priors for the domain-specific information, which allows us to learn diverse many-to-many mappings between the two domains. We demonstrate the effectiveness of our model on diverse tasks, including image captioning and text-to-image synthesis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图片和文字的教训联合代表组成的几个重要的跨域任务，例如图像字幕骨干。以前的工作两个域到一个共同的潜表示主要是在映射一个纯粹的监督方式。这是相当严格的，但是，由于两个域遵循不同的生成过程。因此，我们提出了一种新颖的半监督框架，其分别模型共享域和域特定信息之间的信息。域之间共享的信息与可逆神经网络对齐。我们的模型集成正常化基于流的先验对特定领域的信息，这使我们能够学习两个域之间不同的许多一对多的映射。我们证明在不同的任务，包括图像字幕和文本的图像合成我们的模型的有效性。</font>
</div>


<hr>
<div id="paper20"> <b>20. Block Annotation: Better Image Annotation for Semantic Segmentation with  Sub-Image Decomposition</b>  <a href="https://arxiv.org/pdf/2002.06626" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hubert Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Upchurch%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Upchurch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bala%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kavita Bala</a><br>
<font size="3">
Abstract: Image datasets with high-quality pixel-level annotations are valuable for semantic segmentation: labelling every pixel in an image ensures that rare classes and small objects are annotated. However, full-image annotations are expensive, with experts spending up to 90 minutes per image. We propose block sub-image annotation as a replacement for full-image annotation. Despite the attention cost of frequent task switching, we find that block annotations can be crowdsourced at higher quality compared to full-image annotation with equal monetary cost using existing annotation tools developed for full-image annotation. Surprisingly, we find that 50% pixels annotated with blocks allows semantic segmentation to achieve equivalent performance to 100% pixels annotated. Furthermore, as little as 12% of pixels annotated allows performance as high as 98% of the performance with dense annotation. In weakly-supervised settings, block annotation outperforms existing methods by 3-4% (absolute) given equivalent annotation time. To recover the necessary global structure for applications such as characterizing spatial context and affordance relationships, we propose an effective method to inpaint block-annotated images with high-quality labels without additional human effort. As such, fewer annotations can also be used for these applications compared to full-image annotation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提供高品质的像素级别注释图像数据组是用于语义分割有价值：在标记图像可确保每个象素的是罕见的类和小的物体进行注释。然而，全图像注释是昂贵的，与专家花费可达每图像90分钟。我们建议块子图像标注为全图像注释的替代品。尽管频繁的任务切换的关注成本，我们发现，块注释可以以更高的质量进行众包相比，使用了全图像注释开发的现有注释工具等于货币成本全图像注释。出人意料的是，我们发现，与块注释50％的像素可以让语义分割，实现相同的性能用100％的像素。此外，注释像素的少12％允许性能一样高密集注释的性能的98％。在弱监督设置，块注释性能优于由3-4％（绝对值）给出等效标注时间现有的方法。要恢复等应用特征空间环境和启示关系，必要的全球结构，我们建议无需额外的人为努力的有效方法，以高品质的标签，补绘块注释的图像。因此，更少的注释也可用于这些应用相比全图像注释。</font>
</div>


<hr>
<div id="paper21"> <b>21. CRL: Class Representative Learning for Image Classification</b>  <a href="https://arxiv.org/pdf/2002.06619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chandrashekar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mayanka Chandrashekar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yugyung Lee</a><br>
<font size="3">
Abstract: Building robust and real-time classifiers with diverse datasets are one of the most significant challenges to deep learning researchers. It is because there is a considerable gap between a model built with training (seen) data and real (unseen) data in applications. Recent works including Zero-Shot Learning (ZSL), have attempted to deal with this problem of overcoming the apparent gap through transfer learning. In this paper, we propose a novel model, called Class Representative Learning Model (CRL), that can be especially effective in image classification influenced by ZSL. In the CRL model, first, the learning step is to build class representatives to represent classes in datasets by aggregating prominent features extracted from a Convolutional Neural Network (CNN). Second, the inferencing step in CRL is to match between the class representatives and new data. The proposed CRL model demonstrated superior performance compared to the current state-of-the-art research in ZSL and mobile deep learning. The proposed CRL model has been implemented and evaluated in a parallel environment, using Apache Spark, for both distributed learning and recognition. An extensive experimental study on the benchmark datasets, ImageNet-1K, CalTech-101, CalTech-256, CIFAR-100, shows that CRL can build a class distribution model with drastic improvement in learning and recognition performance without sacrificing accuracy compared to the state-of-the-art performances in image classification. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：建立与不同的数据集强大和实时分类是深学习研究最显著的挑战之一。这是因为与培训（看到）的数据和应用程序中实（看不见的）数据建立的模型之间有相当的差距。最近的作品包括零射门学习（ZSL），试图应对克服通过转移学习的明显差距这个问题。在本文中，我们提出了一种新的模式，所谓的课代表学习模式（CRL），即可以在由ZSL影响图像分类特别有效。在CRL模型中，首先，学习步骤是建立类代表通过聚合从卷积神经网络（CNN）萃取突出的特点，以表示数据集的类。其次，在CRL的推理步骤是将集体代表和新数据之间的匹配。相比目前国家的最先进的研究ZSL和移动深度学习所提出的CRL模型表现出了超强的性能。所提出的CRL模型已经实施和评估在并行环境中，使用Apache星火，对于分布式学习和识别。基准的数据集，ImageNet-1K，加州理工学院-101，加州理工学院-256，CIFAR-100，说明CRL可以在学习建设有大幅改善的一类分布模型和识别性能的广泛的实验研究在不牺牲精度相比国有的最先进的演出图像分类。</font>
</div>


<hr>
<div id="paper22"> <b>22. Key Points Estimation and Point Instance Segmentation Approach for Lane  Detection</b>  <a href="https://arxiv.org/pdf/2002.06604" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yeongmin Ko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jun%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwon Jun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ko%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Donghwuy Ko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jeon%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Moongu Jeon</a><br>
<font size="3">
Abstract: State-of-the-art lane detection methods achieve successful performance. Despite their advantages, these methods have critical deficiencies such as the limited number of detectable lanes and high false positive. In especial, high false positive can cause wrong and dangerous control. In this paper, we propose a novel lane detection method for the arbitrary number of lanes using the deep learning method, which has the lower number of false positives than other recent lane detection methods. The architecture of the proposed method has the shared feature extraction layers and several branches for detection and embedding to cluster lanes. The proposed method can generate exact points on the lanes, and we cast a clustering problem for the generated points as a point cloud instance segmentation problem. The proposed method is more compact because it generates fewer points than the original image pixel size. Our proposed post processing method eliminates outliers successfully and increases the performance notably. Whole proposed framework achieves competitive results on the tuSimple dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：国家的最先进的车道检测方法，可以实现成功的表现。尽管它们的优点，这些方法具有严重缺陷，例如可检测的车道和高的假阳性的数量有限。在特殊的，高误报可能导致错误和危险的控制。在本文中，我们提出了用深层学习方法，它具有误报比近期其他车道检测方法的下车道数的任意数量的一种新的车道检测方法。所提出的方法的体系结构具有共享特征提取层和用于检测几个分支和嵌入到簇车道。该方法可以在车道生成精确的点，我们投一个聚类问题的产生点的点云实例分割问题。所提出的方法是更紧凑的，因为它产生比原始图像的像素尺寸的点较少。我们成功地提出了后处理方法消除异常值和显着提高性能。整个建议的架构实现了对tuSimple数据集有竞争力的结果。</font>
</div>


<hr>
<div id="paper23"> <b>23. Analytic Marching: An Analytic Meshing Solution from Deep Implicit  Surface Networks</b>  <a href="https://arxiv.org/pdf/2002.06597" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiabao Lei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jia%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kui Jia</a><br>
<font size="3">
Abstract: This paper studies a problem of learning surface mesh via implicit functions in an emerging field of deep learning surface reconstruction, where implicit functions are popularly implemented as multi-layer perceptrons (MLPs) with rectified linear units (ReLU). To achieve meshing from learned implicit functions, existing methods adopt the de-facto standard algorithm of marching cubes; while promising, they suffer from loss of precision learned in the MLPs, due to the discretization nature of marching cubes. Motivated by the knowledge that a ReLU based MLP partitions its input space into a number of linear regions, we identify from these regions analytic cells and analytic faces that are associated with zero-level isosurface of the implicit function, and characterize the theoretical conditions under which the identified analytic faces are guaranteed to connect and form a closed, piecewise planar surface. Based on our theorem, we propose a naturally parallelizable algorithm of analytic marching, which marches among analytic cells to exactly recover the mesh captured by a learned MLP. Experiments on deep learning mesh reconstruction verify the advantages of our algorithm over existing ones. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文研究经由隐函数在深度学习表面重建，其中隐函数是普遍作为多层感知器（的MLP）实现的一个新兴的领域学习表面网格的一个问题与整流线性单位（RELU）。为了实现从了解到隐函数啮合，现有的方法采用移动立方体的事实标准算法;同时承诺，从它们的精度在业主有限合伙了解到，遭受损失是由于移动立方体的离散性质。通过一个基于RELU MLP划分其输入空间为多个线性区域的知识的启发，我们从这些区域识别分析的细胞和与隐函数的零电平等值面相关的解析的面，和表征在其下理论条件所识别的解析面保证连接并形成一个封闭的，分段平坦的表面。根据我们的理论，我们提出分析行军，其分析小区间的游行正好恢复网格由获悉MLP捕获的自然并行算法。深学习网重建实验，验证了在现有的我们的算法的优点。</font>
</div>


<hr>
<div id="paper24"> <b>24. Automated Labelling using an Attention model for Radiology reports of  MRI scans (ALARM)</b>  <a href="https://arxiv.org/pdf/2002.06588" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wood%2C+D+A" target="_blank" rel="noopener" style="color:#0000EE;">David A. Wood</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lynch%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeremy Lynch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kafiabadi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sina Kafiabadi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guilhem%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emily Guilhem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Busaidi%2C+A+A" target="_blank" rel="noopener" style="color:#0000EE;">Aisha Al Busaidi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Montvila%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antanas Montvila</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Varsavsky%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Varsavsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Siddiqui%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juveria Siddiqui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gadapa%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naveen Gadapa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Townend%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Townend</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiik%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Kiik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Patel%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Keena Patel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barker%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gareth Barker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ourselin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastian Ourselin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cole%2C+J+H" target="_blank" rel="noopener" style="color:#0000EE;">James H. Cole</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Booth%2C+T+C" target="_blank" rel="noopener" style="color:#0000EE;">Thomas C. Booth</a><br>
<font size="3">
Abstract: Labelling large datasets for training high-capacity neural networks is a major obstacle to the development of deep learning-based medical imaging applications. Here we present a transformer-based network for magnetic resonance imaging (MRI) radiology report classification which automates this task by assigning image labels on the basis of free-text expert radiology reports. Our model's performance is comparable to that of an expert radiologist, and better than that of an expert physician, demonstrating the feasibility of this approach. We make code available online for researchers to label their own MRI datasets for medical imaging applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：标签大型数据集为培养高容量的神经网络是深以学习为主的医疗成像应用发展的一个主要障碍。在这里，我们提出了磁共振成像（MRI）影像报告的分类基于变压器的网络，通过自由文本放射学专家报告的基础上分配图像标签自动执行此任务。我们的模型的性能相媲美的专家放射科医师，而比专家医生的更好，证明了该方法的可行性。我们使代码可在网上为研究人员标注自己的MRI数据集用于医疗成像应用。</font>
</div>


<hr>
<div id="paper25"> <b>25. Reinforced active learning for image segmentation</b>  <a href="https://arxiv.org/pdf/2002.06583" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Casanova%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arantxa Casanova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pinheiro%2C+P+O" target="_blank" rel="noopener" style="color:#0000EE;">Pedro O. Pinheiro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rostamzadeh%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Negar Rostamzadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pal%2C+C+J" target="_blank" rel="noopener" style="color:#0000EE;">Christopher J. Pal</a><br>
<font size="3">
Abstract: Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语义分割基于学习的方法有两个内在的挑战。首先，收购逐像素的标签是昂贵和费时。其次，现实的分割数据集是高度不平衡：某些类别的人比其他人更丰富，偏置性能，以最具代表性的。在本文中，我们感兴趣的是着眼于更大的数据池的一小部分人的标签的努力，同时最大限度上保持了集分割模型的性能最大限度地减少这方面的努力。我们提出了语义分割基于深强化学习（RL）的新主动学习策略。代理学习策略选择小翔实的图像区域的一个子集 - 而不是整个图像 - 要贴上标签，从标签数据池。区域选择决定是基于细分模型正在接受训练的预测和不确定性做出。我们的方法提出了一种用于主动学习深Q-网络（DQN）制剂的新的变型中，它适应的语义分割问题大规模性质。我们测试概念证明在CamVid并提供大型数据集城市景观效果。在城市景观，我们深为基础的区域RL DQN方法需要比我们最有竞争力的基线大约30％较少的额外标记数据以达到相同的性能。此外，我们发现，我们的方法询问代表性不足的类别相比基线的多个标签，从而提高其性能，并帮助缓解类不平衡。</font>
</div>


<hr>
<div id="paper26"> <b>26. Topological Mapping for Manhattan-like Repetitive Environments</b>  <a href="https://arxiv.org/pdf/2002.06575" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Puligilla%2C+S+S" target="_blank" rel="noopener" style="color:#0000EE;">Sai Shubodh Puligilla</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tourani%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Satyajit Tourani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vaidya%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tushar Vaidya</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Parihar%2C+U+S" target="_blank" rel="noopener" style="color:#0000EE;">Udit Singh Parihar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sarvadevabhatla%2C+R+K" target="_blank" rel="noopener" style="color:#0000EE;">Ravi Kiran Sarvadevabhatla</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krishna%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">K. Madhava Krishna</a><br>
<font size="3">
Abstract: We showcase a topological mapping framework for a challenging indoor warehouse setting. At the most abstract level, the warehouse is represented as a Topological Graph where the nodes of the graph represent a particular warehouse topological construct (e.g. rackspace, corridor) and the edges denote the existence of a path between two neighbouring nodes or topologies. At the intermediate level, the map is represented as a Manhattan Graph where the nodes and edges are characterized by Manhattan properties and as a Pose Graph at the lower-most level of detail. The topological constructs are learned via a Deep Convolutional Network while the relational properties between topological instances are learnt via a Siamese-style Neural Network. In the paper, we show that maintaining abstractions such as Topological Graph and Manhattan Graph help in recovering an accurate Pose Graph starting from a highly erroneous and unoptimized Pose Graph. We show how this is achieved by embedding topological and Manhattan relations as well as Manhattan Graph aided loop closure relations as constraints in the backend Pose Graph optimization framework. The recovery of near ground-truth Pose Graph on real-world indoor warehouse scenes vindicate the efficacy of the proposed framework. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们展示一个拓扑映射框架，一个具有挑战性的室内仓库设置。在最抽象级别，仓库被表示为拓扑图，其中图的节点表示特定仓库的拓扑结构（如机架磁盘走廊）和边缘表示两个相邻节点或拓扑之间的路径的存在。在中间电平，在地图上被表示为曼哈顿图，其中所述节点和边处细节的最下一级的特征在于曼哈顿属性和作为姿态图形。拓扑结构通过一个深卷积网了解到，同时拓扑实例之间的关系特性通过连体式神经网络学会。在本文中，我们表明，保持抽象，如拓扑图，并恢复准确的姿态图形从高度是错误的，没有优化的姿态图形开始曼哈顿图表帮助。我们展示了如何通过嵌入拓扑和曼哈顿关系以及曼哈顿图形辅助环路闭合关系作为后端姿态图形优化框架的约束来实现。近地面实况姿态图形的现实世界的室内仓库场景恢复平反提出的框架的有效性。</font>
</div>


<hr>
<div id="paper27"> <b>27. Facial Attribute Capsules for Noise Face Super Resolution</b>  <a href="https://arxiv.org/pdf/2002.06518" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingwei Xin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nannan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinrui Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinbo Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhifeng Li</a><br>
<font size="3">
Abstract: Existing face super-resolution (SR) methods mainly assume the input image to be noise-free. Their performance degrades drastically when applied to real-world scenarios where the input image is always contaminated by noise. In this paper, we propose a Facial Attribute Capsules Network (FACN) to deal with the problem of high-scale super-resolution of noisy face image. Capsule is a group of neurons whose activity vector models different properties of the same entity. Inspired by the concept of capsule, we propose an integrated representation model of facial information, which named Facial Attribute Capsule (FAC). In the SR processing, we first generated a group of FACs from the input LR face, and then reconstructed the HR face from this group of FACs. Aiming to effectively improve the robustness of FAC to noise, we generate FAC in semantic, probabilistic and facial attributes manners by means of integrated learning strategy. Each FAC can be divided into two sub-capsules: Semantic Capsule (SC) and Probabilistic Capsule (PC). Them describe an explicit facial attribute in detail from two aspects of semantic representation and probability distribution. The group of FACs model an image as a combination of facial attribute information in the semantic space and probabilistic space by an attribute-disentangling way. The diverse FACs could better combine the face prior information to generate the face images with fine-grained semantic attributes. Extensive benchmark experiments show that our method achieves superior hallucination results and outperforms state-of-the-art for very low resolution (LR) noise face image super resolution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有的脸超分辨率（SR）方法主要假设输入图像是无噪音。其性能下降显着，当应用到真实世界的场景中输入图像总是被噪声污染。在本文中，我们提出了一个面部属性胶囊网（FACN）来处理高规模超分辨率嘈杂的人脸图像的问题。胶囊是一组神经元，其活性矢量模型的相同实体的不同性质。通过胶囊的概念的启发，我们提出的面部信息，其命名为面部属性胶囊（FAC）的综合表示模型。在SR处理时，我们首先生成群的FACS从输入LR面，然后从该组中的FACS重构的HR面。旨在有效提高FAC的鲁棒性的噪音，我们通过整合学习策略的手段语义，概率和面部特征的方式产生FAC。每个FAC可以被划分成两个子胶囊：胶囊的语义（SC）和概率胶囊（PC）。他们描述了从语义表示和概率分布的两个方面进行详细的显式的面部属性。该组的FACS图像建模为通过一个属性解开方式的在语义空间和概率空间的面部属性信息的组合。多样化的流式细胞仪能更好地结合起来，面对之前的信息来产生面部图像细粒度语义属性。广泛的基准测试实验表明，我们的方法达到出色的幻觉效果，优于国家的最先进的非常低的分辨率（LR）的噪音人脸图像的超分辨率。</font>
</div>


<hr>
<div id="paper28"> <b>28. A Real-Time Deep Network for Crowd Counting</b>  <a href="https://arxiv.org/pdf/2002.06515" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaowen Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Caili Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuchen Kong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang He</a><br>
<font size="3">
Abstract: Automatic analysis of highly crowded people has attracted extensive attention from computer vision research. Previous approaches for crowd counting have already achieved promising performance across various benchmarks. However, to deal with the real situation, we hope the model run as fast as possible while keeping accuracy. In this paper, we propose a compact convolutional neural network for crowd counting which learns a more efficient model with a small number of parameters. With three parallel filters executing the convolutional operation on the input image simultaneously at the front of the network, our model could achieve nearly real-time speed and save more computing resources. Experiments on two benchmarks show that our proposed method not only takes a balance between performance and efficiency which is more suitable for actual scenes but also is superior to existing light-weight models in speed. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：高度挤人的自动分析，吸引了来自计算机视觉研究的广泛关注。针对人群计数以前的方法已经取得看好在不同的基准性能。然而，为了应对实际情况，我们希望示范运行尽可能快，同时保持准确度。在本文中，我们提出了人群计数紧凑卷积神经网络，学习有少量的参数更有效的模式。随着三个平行的过滤器同时运行在网络前端的输入图像的卷积运算，我们的模型可以实现近实时的速度和节省更多的计算资源。两个基准测试实验表明，该方法不仅需要性能和效率，这是更适合实际的场景之间的平衡也优越于现有速度轻量机型。</font>
</div>


<hr>
<div id="paper29"> <b>29. Face Recognition: Too Bias, or Not Too Bias?</b>  <a href="https://arxiv.org/pdf/2002.06483" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Robinson%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Joseph P Robinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Livitz%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gennady Livitz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Henon%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yann Henon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Can Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Timoner%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samson Timoner</a><br>
<font size="3">
Abstract: We reveal critical insights into problems of bias in state-of-the-art facial recognition (FR) systems using a novel Balanced Faces In the Wild (BFW) dataset: data balanced for gender and ethnic groups. We show variations in the optimal scoring threshold for face-pairs across different subgroups. Thus, the conventional approach of learning a global threshold for all pairs resulting in performance gaps among subgroups. By learning subgroup-specific thresholds, we not only mitigate problems in performance gaps but also show a notable boost in the overall performance. Furthermore, we do a human evaluation to measure the bias in humans, which supports the hypothesis that such a bias exists in human perception. For the BFW database, source code, and more, visit this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们揭示了使用一种新的平衡的面孔在野外（BFW）数据集的关键见解在国家的最先进的面部识别（FR）系统偏压的问题：性别和族裔平衡的数据。我们显示了在不同亚群面对的最佳得分阈值的变化。因此，学习导致性能差距亚组之间的所有对一个全局阈值的传统方法。通过学习群特定的阈值，我们在性能上的差距，不仅缓解问题，而且也显示出整体性能显着提升。此外，我们做一个人的评价来衡量人类的偏见，这支持了这样的偏见在人类感知存在的假设。对于BFW数据库，源代码和详情，请访问该HTTP URL。</font>
</div>


<hr>
<div id="paper30"> <b>30. Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen  Categories</b>  <a href="https://arxiv.org/pdf/2002.06478" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tiange Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mo%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaichun Mo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiao Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiarui Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyu Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liwei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Su</a><br>
<font size="3">
Abstract: We address the problem of discovering 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learning-based agglomerative clustering framework which learns a grouping policy to progressively group small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to unseen categories. On the large-scale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four shape segmentation baselines shows that our approach achieve the state-of-the-art performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们发现解决3D零件在看不见的类对象的问题。如果能够学习部分之前的几何形状和在此之前看不见的类别转移造成对数据驱动的形状分割方法根本性的挑战。配制成上下文土匪问题，我们提出了一种基于学习的凝聚聚类框架，它学会了一分组策略，逐步一群小部分建议到一个自下而上的方式做大的。在我们的方法的核心是限制提取部件级功能，这鼓励推广到看不见的类别当地的情况。在大规模细粒度3D部分数据集，PartNet，我们证明了我们的方法可以把从3个训练类别21个看不见的测试类别了解到部分的知识，没有看到任何注释的样本。对四名形状分割基准表明，我们的方法实现国家的最先进的性能的定量比较。</font>
</div>


<hr>
<div id="paper31"> <b>31. A Multiple Decoder CNN for Inverse Consistent 3D Image Registration</b>  <a href="https://arxiv.org/pdf/2002.06468" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nazib%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abdullah Nazib</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fookes%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Clinton Fookes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Salvado%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Olivier Salvado</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Perrin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dimitri Perrin</a><br>
<font size="3">
Abstract: The recent application of deep learning technologies in medical image registration has exponentially decreased the registration time and gradually increased registration accuracy when compared to their traditional counterparts. Most of the learning-based registration approaches considers this task as a one directional problem. As a result, only correspondence from the moving image to the target image is considered. However, in some medical procedures bidirectional registration is required to be performed. Unlike other learning-based registration, we propose a registration framework with inverse consistency. The proposed method simultaneously learns forward transformation and backward transformation in an unsupervised manner. We perform training and testing of the method on the publicly available LPBA40 MRI dataset and demonstrate strong performance than baseline registration methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深度学习技术在医学图像配准的最新应用呈指数下降了报名时间，并与他们的传统的同行时逐渐增加的配准精度。大多数基于学习的登记方法认为这个任务作为一个方向性的问题。其结果是，从运动图像的对象图像对应仅考虑。然而，在一些医疗程序中，需要双向注册被执行。不同于其他基于学习的登记，我们提出用逆一致性注册框架。所提出的方法同时学习以无监督的方式正向转换和向后转换。我们进行培训和方法的公开提供的LPBA40 MRI数据集测试，并证明比基准登记方法表现强劲。</font>
</div>


<hr>
<div id="paper32"> <b>32. HighRes-net: Recursive Fusion for Multi-Frame Super-Resolution of  Satellite Imagery</b>  <a href="https://arxiv.org/pdf/2002.06460" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Deudon%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michel Deudon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kalaitzis%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alfredo Kalaitzis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goytom%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Israel Goytom</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arefin%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Md Rifat Arefin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhichao Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sankaran%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kris Sankaran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Michalski%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vincent Michalski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kahou%2C+S+E" target="_blank" rel="noopener" style="color:#0000EE;">Samira E. Kahou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cornebise%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Cornebise</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bengio%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoshua Bengio</a><br>
<font size="3">
Abstract: Generative deep learning has sparked a new wave of Super-Resolution (SR) algorithms that enhance single images with impressive aesthetic results, albeit with imaginary details. Multi-frame Super-Resolution (MFSR) offers a more grounded approach to the ill-posed problem, by conditioning on multiple low-resolution views. This is important for satellite monitoring of human impact on the planet -- from deforestation, to human rights violations -- that depend on reliable imagery. To this end, we present HighRes-net, the first deep learning approach to MFSR that learns its sub-tasks in an end-to-end fashion: (i) co-registration, (ii) fusion, (iii) up-sampling, and (iv) registration-at-the-loss. Co-registration of low-resolution views is learned implicitly through a reference-frame channel, with no explicit registration mechanism. We learn a global fusion operator that is applied recursively on an arbitrary number of low-resolution pairs. We introduce a registered loss, by learning to align the SR output to a ground-truth through ShiftNet. We show that by learning deep representations of multiple views, we can super-resolve low-resolution signals and enhance Earth Observation data at scale. Our approach recently topped the European Space Agency's MFSR competition on real-world satellite imagery. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：剖成深度学习已经引发了超分辨率（SR）增强单个图像令人印象深刻的美学效果，尽管假想的细节算法的新风潮。多帧超分辨率（MFSR）提供了一个更接地的方法来病态问题，通过对多个低分辨率意见调理。这是地球上人类的影响卫星测控重要 - 毁林，侵犯人权行为 - 依赖于可靠的图像。为此，我们本HIGHRES网，所述第一深学习方法来MFSR该学习其子任务在端至端的方式：（ⅰ）共配准，（ⅱ）融合，（ⅲ）上采样，和（iv）注册用在最损失。的低分辨率视图共配准是通过参考帧信道隐含地了解到，随着没有明确的注册机制。我们了解到，递归地应用在低分辨率对任意数量的全球融合运营商。我们引入一个注册的损失，通过学习，通过ShiftNet对准SR输出到地面实况。我们发现，通过学习的多个视图深表示，我们可以超决心低分辨率的信号，并在规模提升的地球观测数据。我们的做法最近荣登现实世界的卫星图像，欧洲航天局的MFSR竞争。</font>
</div>


<hr>
<div id="paper33"> <b>33. An End-to-End Framework for Unsupervised Pose Estimation of Occluded  Pedestrians</b>  <a href="https://arxiv.org/pdf/2002.06429" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Das%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sudip Das</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kishore%2C+P+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Perla Sai Raj Kishore</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhattacharya%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Ujjwal Bhattacharya</a><br>
<font size="3">
Abstract: Pose estimation in the wild is a challenging problem, particularly in situations of (i) occlusions of varying degrees and (ii) crowded outdoor scenes. Most of the existing studies of pose estimation did not report the performance in similar situations. Moreover, pose annotations for occluded parts of human figures have not been provided in any of the relevant standard datasets which in turn creates further difficulties to the required studies for pose estimation of the entire figure of occluded humans. Well known pedestrian detection datasets such as CityPersons contains samples of outdoor scenes but it does not include pose annotations. Here, we propose a novel multi-task framework for end-to-end training towards the entire pose estimation of pedestrians including in situations of any kind of occlusion. To tackle this problem for training the network, we make use of a pose estimation dataset, MS-COCO, and employ unsupervised adversarial instance-level domain adaptation for estimating the entire pose of occluded pedestrians. The experimental studies show that the proposed framework outperforms the SOTA results for pose estimation, instance segmentation and pedestrian detection in cases of heavy occlusions (HO) and reasonable + heavy occlusions (R + HO) on the two benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在野外姿态估计是一个具有挑战性的问题，特别是在不同程度上和（ii）拥挤室外场景的（ⅰ）闭塞的情况。大多数姿态估计现有的研究没有报告在类似情况下的性能。此外，没有被任何这反过来又创造为人类闭塞的整个人物的姿态估计所需的研究进一步的困难相关的标准数据集提供的人物遮挡零件的姿态注解。众所周知的行人探测数据集，如CityPersons包含户外场景的样本，但它不包括姿势注解。在这里，我们提出了对行人，包括任何种类的阻塞情况的整个姿态估计终端到终端的培训新型多任务框架。为了解决这个问题，为训练网络，我们使用姿势估计数据集，MS-COCO，并采用无监督敌对实例级领域适应性的估计遮挡行人的整个姿态。实验研究表明，该框架优于姿态估计，例如分割和行人探测重闭塞（HO）和合理的+重闭塞（R + HO）上的两个标准数据集的情况下，SOTA结果。</font>
</div>


<hr>
<div id="paper34"> <b>34. Scale-Invariant Multi-Oriented Text Detection in Wild Scene Images</b>  <a href="https://arxiv.org/pdf/2002.06423" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dasgupta%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kinjal Dasgupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Das%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sudip Das</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhattacharya%2C+U" target="_blank" rel="noopener" style="color:#0000EE;">Ujjwal Bhattacharya</a><br>
<font size="3">
Abstract: Automatic detection of scene texts in the wild is a challenging problem, particularly due to the difficulties in handling (i) occlusions of varying percentages, (ii) widely different scales and orientations, (iii) severe degradations in the image quality etc. In this article, we propose a fully convolutional neural network architecture consisting of a novel Feature Representation Block (FRB) capable of efficient abstraction of information. The proposed network has been trained using curriculum learning with respect to difficulties in image samples and gradual pixel-wise blurring. It is capable of detecting texts of different scales and orientations suffered by blurring from multiple possible sources, non-uniform illumination as well as partial occlusions of varying percentages. Text detection performance of the proposed framework on various benchmark sample databases including ICDAR 2015, ICDAR 2017 MLT, COCO-Text and MSRA-TD500 improves respective state-of-the-art results significantly. Source code of the proposed architecture will be made available at github. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在野外现场文本的自动检测是一个具有挑战性的问题，特别是由于在处理不同百分比的（ⅰ）闭塞的困难，（ⅱ）广泛不同尺度和方向，（ⅲ）在图像质量等严重劣化在本文中，我们提出了一种完全卷积神经网络体系结构由一种新颖的特征表示块（FRB）能够信息的高效的抽象的。所提出的网络已经采用课程学习与图像样本和逐步逐像素模糊就困难培训。它是能够检测由从多个可能的源，非均匀照明以及不同百分比的部分遮挡模糊遭受不同尺度和方向的文本。在各种基准样本数据库包括ICDAR 2015所提出的框架的文本检测的性能，ICDAR 2017 MLT，COCO  - 文本和MSRA-TD500显著提高各个国家的最先进的结果。所提出的架构的源代码将在github上提供。</font>
</div>


<hr>
<div id="paper35"> <b>35. Video Face Super-Resolution with Motion-Adaptive Feedback Cell</b>  <a href="https://arxiv.org/pdf/2002.06378" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingwei Xin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nannan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinbo Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhifeng Li</a><br>
<font size="3">
Abstract: Video super-resolution (VSR) methods have recently achieved a remarkable success due to the development of deep convolutional neural networks (CNN). Current state-of-the-art CNN methods usually treat the VSR problem as a large number of separate multi-frame super-resolution tasks, at which a batch of low resolution (LR) frames is utilized to generate a single high resolution (HR) frame, and running a slide window to select LR frames over the entire video would obtain a series of HR frames. However, duo to the complex temporal dependency between frames, with the number of LR input frames increase, the performance of the reconstructed HR frames become worse. The reason is in that these methods lack the ability to model complex temporal dependencies and hard to give an accurate motion estimation and compensation for VSR process. Which makes the performance degrade drastically when the motion in frames is complex. In this paper, we propose a Motion-Adaptive Feedback Cell (MAFC), a simple but effective block, which can efficiently capture the motion compensation and feed it back to the network in an adaptive way. Our approach efficiently utilizes the information of the inter-frame motion, the dependence of the network on motion estimation and compensation method can be avoid. In addition, benefiting from the excellent nature of MAFC, the network can achieve better performance in the case of extremely complex motion scenarios. Extensive evaluations and comparisons validate the strengths of our approach, and the experimental results demonstrated that the proposed framework is outperform the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频超分辨率（VSR）方法，最近取得了令人瞩目的成功，由于深卷积神经网络（CNN）的发展。当前状态的最先进的CNN方法通常处理VSR的问题，因为大量的单独的多帧的超分辨率任务，在该一批次低分辨率（LR）的帧被用于产生一个单一的高分辨率（HR ）帧，并运行一个滑动窗口在整个视频来选择LR帧将获得一系列HR帧。然而，二人到帧之间的复杂的时间相关性，与LR输入帧数量的增加，再生HR帧的性能变差。其原因是，这些方法缺乏复杂的时间依赖性和硬模型，给出了振动时效工艺的精确运动估计和补偿的能力。这使得性能大幅下降时，在帧的运动是复杂的。在本文中，我们提出了一个运动自适应反馈单元（MAFC），一个简单而有效的块，其可有效地捕获运动补偿和在自适应方式反馈给该网络。我们的方法有效地利用帧间运动的信息，网络上的运动估计和补偿方法的依赖性可避免的。另外，从MAFC的优良性质中受益，该网络可以实现的极其复杂的运动场景的情况下更好的性能。广泛的评估和比较验证我们的方法的优点，实验结果表明，所提出的框架是强于大盘的国家的最先进的方法。</font>
</div>


<hr>
<div id="paper36"> <b>36. UniViLM: A Unified Video and Language Pre-Training Model for Multimodal  Understanding and Generation</b>  <a href="https://arxiv.org/pdf/2002.06353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huaishao Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Botian Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoyang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianrui Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xilin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: We propose UniViLM: a Unified Video and Language pre-training Model for multimodal understanding and generation. Motivated by the recent success of BERT based pre-training technique for NLP and image-language tasks, VideoBERT and CBT are proposed to exploit BERT model for video and language pre-training using narrated instructional videos. Different from their works which only pre-train understanding task, we propose a unified video-language pre-training model for both understanding and generation tasks. Our model comprises of 4 components including two single-modal encoders, a cross encoder and a decoder with the Transformer backbone. We first pre-train our model to learn the universal representation for both video and language on a large instructional video dataset. Then we fine-tune the model on two multimodal tasks including understanding task (text-based video retrieval) and generation task (multimodal video captioning). Our extensive experiments show that our method can improve the performance of both understanding and generation tasks and achieves the state-of-the art results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出UniViLM：一个统一的视频和语言前的训练模式的多模态的理解和生成。受近期对NLP和图像语言任务，VideoBERT和CBT基于BERT前培训技术的成功激励提出了利用使用讲述了教学视频，视频和语言训练前BERT模式。从他们的作品只预列车任务的理解不同，我们提出了两种理解和生成任务统一视频语言前的训练模式。我们的模型包括4个组分，包括两个单峰编码器，一个编码器的交叉，并与变压器主链的解码器的。我们先预先训练我们的模型来学习的视频和语言上大量的教学视频数据集中的普遍代表性。然后，我们微调在两个多任务，其中包括理解任务（基于文本的视频检索）和生成任务（多视频字幕）模型。我们广泛的实验表明，我们的方法可以提高双方的理解和生成任务的性能和实现国家的艺术效果。</font>
</div>


<hr>
<div id="paper37"> <b>37. Cell R-CNN V3: A Novel Panoptic Paradigm for Instance Segmentation in  Biomedical Images</b>  <a href="https://arxiv.org/pdf/2002.06345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title37" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongnan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Donghao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heng Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weidong Cai</a><br>
<font size="3">
Abstract: Instance segmentation is an important task for biomedical image analysis. Due to the complicated background components, the high variability of object appearances, numerous overlapping objects, and ambiguous object boundaries, this task still remains challenging. Recently, deep learning based methods have been widely employed to solve these problems and can be categorized into proposal-free and proposal-based methods. However, both proposal-free and proposal-based methods suffer from information loss, as they focus on either global-level semantic or local-level instance features. To tackle this issue, we present a panoptic architecture that unifies the semantic and instance features in this work. Specifically, our proposed method contains a residual attention feature fusion mechanism to incorporate the instance prediction with the semantic features, in order to facilitate the semantic contextual information learning in the instance branch. Then, a mask quality branch is designed to align the confidence score of each object with the quality of the mask prediction. Furthermore, a consistency regularization mechanism is designed between the semantic segmentation tasks in the semantic and instance branches, for the robust learning of both tasks. Extensive experiments demonstrate the effectiveness of our proposed method, which outperforms several state-of-the-art methods on various biomedical datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：实例分割是生物医学图像分析的一项重要任务。由于复杂的背景成分，对象出场，众多重叠对象，暧昧对象边界的高可变性，这一任务仍然具有挑战性。近日，深基础的学习方法已被广泛采用，以解决这些问题，可以分成提案，免费和建议的方法。然而，无论是免费的建议和提案为基础的方法，从信息遭受损失，因为他们专注于为全局级别的语义或地方级实例特性。为了解决这个问题，我们提出了一个全景架构相结合语义和实例的功能在此工作。具体来说，我们提出的方法中包含的剩余关注特征融合机制，结合实例预测与语义特征，以便于语义上下文信息的情况下分支学习。然后，掩模质量分支被设计来对准置信度得分的每个对象的与掩模预测的质量。此外，一致性正规化机制的语义分割任务之间在语义和实例分支机构设计的，对于这两项任务的强大的学习。大量的实验证明我们提出的方法，它优于国家的最先进的几种各种生物医学数据集方法的有效性。</font>
</div>


<hr>
<div id="paper38"> <b>38. Recognizing Families In the Wild (RFIW): The 4th Edition</b>  <a href="https://arxiv.org/pdf/2002.06303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title38" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Robinson%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Joseph P. Robinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zaid Khan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Shao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyu Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stopa%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Stopa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Timoner%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samson Timoner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Turk%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Matthew A. Turk</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chellappa%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rama Chellappa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun Fu</a><br>
<font size="3">
Abstract: Recognizing Families In the Wild (RFIW): an annual large-scale, multi-track automatic kinship recognition evaluation that supports various visual kin-based problems on scales much higher than ever before. Organized in conjunction with the 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG) as a Challenge, RFIW provides a platform for publishing original work and the gathering of experts for a discussion of the next steps. This paper summarizes the supported tasks (i.e., kinship verification, tri-subject verification, and search & retrieval of missing children) in the evaluation protocols, which include the practical motivation, technical background, data splits, metrics, and benchmark results. Furthermore, top submissions (i.e., leader-board stats) are listed and reviewed as a high-level analysis on the state of the problem. In the end, the purpose of this paper is to describe the 2020 RFIW challenge, end-to-end, along with forecasts in promising future directions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：鉴于家庭在野外（RFIW）：每年的大规模，多跟踪自动识别血缘关系的评价，关于尺度支持各种基于视觉健的问题比以往任何时候都高。在第15届IEEE国际会议上的自动脸部和手势识别（FG）作为挑战共同举办，RFIW提供了发布的原创作品和专家进行的下一阶段的讨论，聚会的平台。本文总结了支持的任务（即血缘关系鉴定，三主题验证，并搜索和检索失踪儿童）在评估协议，其中包括实际的动机，技术背景，数据分片，度量和基准测试结果。此外，顶部的提交（即，前导板数据）被列出并审查作为对这个问题的状态的高层次分析。最后，本文的目的是描述2020年RFIW挑战，最终到年底，预期沿着充满希望的未来方向。</font>
</div>


<hr>
<div id="paper39"> <b>39. Historical Document Processing: Historical Document Processing: A Survey  of Techniques, Tools, and Trends</b>  <a href="https://arxiv.org/pdf/2002.06300" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title39" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Philips%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">James P. Philips</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tabrizi%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nasseh Tabrizi</a><br>
<font size="3">
Abstract: Historical Document Processing is the process of digitizing written material from the past for future use by historians and other scholars. It incorporates algorithms and software tools from various subfields of computer science, including computer vision, document analysis and recognition, natural language processing, and machine learning, to convert images of ancient manuscripts, letters, diaries, and early printed texts automatically into a digital format usable in data mining and information retrieval systems. Within the past twenty years, as libraries, museums, and other cultural heritage institutions have scanned an increasing volume of their historical document archives, the need to transcribe the full text from these collections has become acute. Since Historical Document Processing encompasses multiple sub-domains of computer science, knowledge relevant to its purpose is scattered across numerous journals and conference proceedings. This paper surveys the major phases of, standard algorithms, tools, and datasets in the field of Historical Document Processing, discusses the results of a literature review, and finally suggests directions for further research. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：历史文档处理是从过去的数字化的书面材料，为今后的历史学家和其他学者使用的过程。它采用的算法和计算机科学的不同子领域，包括计算机视觉，文档分析与识别，自然语言处理和机器学习软件工具，古代手稿，书信，日记，和早期的印刷文本的转换图像自动转换成数字格式可用在数据挖掘和信息检索系统。在过去的二十年里，图书馆，博物馆和其他文化遗产机构已经扫描他们的历史文献档案的数量不断增加，从这些藏品抄录全文的需要日益突出。由于历史文档处理包括计算机科学的多个子域，目的相关的知识分散在众多的期刊和会议论文。本文的调查，标准算法，工具和数据集的主要阶段在历史文档处理领域，讨论了文学审查的结果，最后提出了进一步研究的方向。</font>
</div>


<hr>
<div id="paper40"> <b>40. Single Unit Status in Deep Convolutional Neural Network Codes for Face  Identification: Sparseness Redefined</b>  <a href="https://arxiv.org/pdf/2002.06274" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title40" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Parde%2C+C+J" target="_blank" rel="noopener" style="color:#0000EE;">Connor J. Parde</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Col%C3%B3n%2C+Y+I" target="_blank" rel="noopener" style="color:#0000EE;">Y. Ivette Colón</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hill%2C+M+Q" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Q. Hill</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Castillo%2C+C+D" target="_blank" rel="noopener" style="color:#0000EE;">Carlos D. Castillo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dhar%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prithviraj Dhar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=O%27Toole%2C+A+J" target="_blank" rel="noopener" style="color:#0000EE;">Alice J. O'Toole</a><br>
<font size="3">
Abstract: Deep convolutional neural networks (DCNNs) trained for face identification develop representations that generalize over variable images, while retaining subject (e.g., gender) and image (e.g., viewpoint) information. Identity, gender, and viewpoint codes were studied at the "neural unit" and ensemble levels of a face-identification network. At the unit level, identification, gender classification, and viewpoint estimation were measured by deleting units to create variably-sized, randomly-sampled subspaces at the top network layer. Identification of 3,531 identities remained high (area under the ROC approximately 1.0) as dimensionality decreased from 512 units to 16 (0.95), 4 (0.80), and 2 (0.72) units. Individual identities separated statistically on every top-layer unit. Cross-unit responses were minimally correlated, indicating that units code non-redundant identity cues. This "distributed" code requires only a sparse, random sample of units to identify faces accurately. Gender classification declined gradually and viewpoint estimation fell steeply as dimensionality decreased. Individual units were weakly predictive of gender and viewpoint, but ensembles proved effective predictors. Therefore, distributed and sparse codes co-exist in the network units to represent different face attributes. At the ensemble level, principal component analysis of face representations showed that identity, gender, and viewpoint information separated into high-dimensional subspaces, ordered by explained variance. Identity, gender, and viewpoint information contributed to all individual unit responses, undercutting a neural tuning analogy for face attributes. Interpretation of neural-like codes from DCNNs, and by analogy, high-level visual codes, cannot be inferred from single unit responses. Instead, "meaning" is encoded by directions in the high-dimensional space. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：训练面部识别深层卷积神经网络（DCNNs）开发推广了可变图像表示，同时保留主题（例如，性别）和图像（例如视点）的信息。身份，性别和视点码均在“神经单元”和面部识别网络的合奏水平的研究。在单元级别，识别，性别分类，和视点估计被删除单位创建可变尺寸的，顶部网络层中随机取样的子空间测量。的3531名的身份标识（该ROC约1.0下的面积）保持高维数从512个单位减少到16（0.95），4（0.80），和2（0.72）单元。个体身份统计学分离每顶层单元上。交叉单元应答最低限度相关，这表明单元代码非冗余身份线索。这个“分布式”码只需要一个稀疏的，单位随机样本准确识别的面。性别分类的逐步下降和观点估计急剧下跌维度下降。个别单位呈弱预测性别和观点，但合奏证明是有效的预测。因此，分布式和稀疏代码并存的网络单元来代表不同的面部属性。在合奏水平，面表示的主成分分析表明，身份，性别和视点信息分离成高维子空间，通过有序解释方差。身份，性别和视点信息促成所有个别单位的响应，削弱面部属性的神经调节比喻。的神经样从DCNNs码，依此类推，高层次视觉代码解释，不能从单个单元响应来推断。相反，“意思是”由在高维空间方向上进行编码。</font>
</div>


<hr>
<div id="paper41"> <b>41. Layered Embeddings for Amodal Instance Segmentation</b>  <a href="https://arxiv.org/pdf/2002.06264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title41" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanfeng Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Psota%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Psota</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%C3%A9rez%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lance Pérez</a><br>
<font size="3">
Abstract: The proposed method extends upon the representational output of semantic instance segmentation by explicitly including both visible and occluded parts. A fully convolutional network is trained to produce consistent pixel-level embedding across two layers such that, when clustered, the results convey the full spatial extent and depth ordering of each instance. Results demonstrate that the network can accurately estimate complete masks in the presence of occlusion and outperform leading top-down bounding-box approaches. Source code available at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：所提出的方法通过明确地包括可见和遮挡部件在语义实例分割的代表性输出延伸。一个完全卷积网络进行训练，以产生横跨两层一致的像素级嵌入，使得聚集的情况下，结果传达每个实例的完整空间范围和深度排序。结果表明，该网络能够精确地估计在存在遮挡时完整面具和跑赢领先自上而下边界框接近。在此可用的源代码HTTPS URL</font>
</div>


<hr>
<div id="paper42"> <b>42. Why Do Line Drawings Work? A Realism Hypothesis</b>  <a href="https://arxiv.org/pdf/2002.06260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title42" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hertzmann%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aaron Hertzmann</a><br>
<font size="3">
Abstract: Why is it that we can recognize object identity and 3D shape from line drawings, even though they do not exist in the natural world? This paper hypothesizes that the human visual system perceives line drawings as if they were approximately realistic images. Moreover, the techniques of line drawing are chosen to accurately convey shape to a human observer. Several implications and variants of this hypothesis are explored. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为什么我们能够识别对象的身份和3D形状从线条图，即使他们没有在自然界存在吗？本文推测，人的视觉系统感知到排队附图，好像他们是约逼真的图像。此外，画线的技术被选择以准确传达形状对人类观察者。一些影响和这一假设的变种进行了探索。</font>
</div>


<hr>
<div id="paper43"> <b>43. Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein  Graph Double-Attention Network</b>  <a href="https://arxiv.org/pdf/2002.06241" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title43" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiachen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hengbo Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhihao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tomizuka%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masayoshi Tomizuka</a><br>
<font size="3">
Abstract: Effective understanding of the environment and accurate trajectory prediction of surrounding dynamic obstacles are indispensable for intelligent mobile systems (like autonomous vehicles and social robots) to achieve safe and high-quality planning when they navigate in highly interactive and crowded scenarios. Due to the existence of frequent interactions and uncertainty in the scene evolution, it is desired for the prediction system to enable relational reasoning on different entities and provide a distribution of future trajectories for each agent. In this paper, we propose a generic generative neural system (called Social-WaGDAT) for multi-agent trajectory prediction, which makes a step forward to explicit interaction modeling by incorporating relational inductive biases with a dynamic graph representation and leverages both trajectory and scene context information. We also employ an efficient kinematic constraint layer applied to vehicle trajectory prediction which not only ensures physical feasibility but also enhances model performance. The proposed system is evaluated on three public benchmark datasets for trajectory prediction, where the agents cover pedestrians, cyclists and on-road vehicles. The experimental results demonstrate that our model achieves better performance than various baseline approaches in terms of prediction accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：围绕动态障碍物的环境和准确的轨迹预测的有效了解，是智能移动系统不可缺少的（如自动驾驶汽车和社交机器人），以实现安全和高品质的规划，当他们在高度互动导航和拥挤的场景。由于频繁的互动和场景变化存在不确定性，需要为预测系统，以使不同的实体关系的推理，并提供未来轨迹的每个代理的分布。在本文中，我们提出了一个通用的生成神经系统（称为社会WaGDAT）多剂轨迹预测，其通过将关系感性的偏见与动态图表示使得向前迈进了一步，以显式交互模型，并利用双方的轨迹和场景语境信息。我们还采用了适用于车辆轨迹预测不但可以确保物理可行性的高效运动约束层，而且提高了模型的性能。三个公共基准数据集的轨迹预测，其中药剂包括行人，骑自行车和在道路上的车辆所提出的系统进行评估。实验结果表明，我们的模型获得更好的性能比基线各种在预测准确度方面接近。</font>
</div>


<hr>
<div id="paper44"> <b>44. Spectrum Translation for Cross-Spectral Ocular Matching</b>  <a href="https://arxiv.org/pdf/2002.06228" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title44" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Diaz%2C+K+H" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Hernandez Diaz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alonso-Fernandez%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fernando Alonso-Fernandez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bigun%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Josef Bigun</a><br>
<font size="3">
Abstract: Cross-spectral verification remains a big issue in biometrics, especially for the ocular area due to differences in the reflected features in the images depending on the region and spectrum used. In this paper, we investigate the use of Conditional Adversarial Networks for spectrum translation between near infra-red and visual light images for ocular biometrics. We analyze the transformation based on the overall visual quality of the transformed images and the accuracy drop of the identification system when trained with opposing data. We use the PolyU database and propose two different systems for biometric verification, the first one based on Siamese Networks trained with Softmax and Cross-Entropy loss, and the second one a Triplet Loss network. We achieved an EER of 1\% when using a Triplet Loss network trained for NIR and finding the Euclidean distance between the real NIR images and the fake ones translated from the visible spectrum. We also outperform previous results using baseline algorithms. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：互谱验证仍然是生物识别技术的一个大问题，尤其是对眼部区域由于在取决于所使用的区域和光谱图像的反射特性的差异。在本文中，我们探讨眼部生物识别近红外和可见光图像之间的频谱转换使用条件对抗性网络。我们分析基于变换图像的整体视觉质量，并且当与反对数据训练识别系统的准确度下降的转变。我们用理大数据库，并提出了生物特征识别两个不同的系统，第一个基于网络的连体与SOFTMAX和交叉熵损失的训练，而第二个三重损失的网络。我们使用训练NIR三重损失的网络，并找到真正的NIR图像和假的来自于可见光谱转换之间的欧氏距离时获得的1 \％的能效比。我们也优于使用基线算法以前的结果。</font>
</div>


<hr>
<div id="paper45"> <b>45. Seeing Around Corners with Edge-Resolved Transient Imaging</b>  <a href="https://arxiv.org/pdf/2002.07118" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title45" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Rapp%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joshua Rapp</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Saunders%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Charles Saunders</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tachella%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julián Tachella</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Murray-Bruce%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Murray-Bruce</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Altmann%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoann Altmann</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tourneret%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-Yves Tourneret</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=McLaughlin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen McLaughlin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dawson%2C+R+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Robin M. A. Dawson</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wong%2C+F+N+C" target="_blank" rel="noopener" style="color:#0000EE;">Franco N. C. Wong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Goyal%2C+V+K" target="_blank" rel="noopener" style="color:#0000EE;">Vivek K Goyal</a><br>
<font size="3">
Abstract: Non-line-of-sight (NLOS) imaging is a rapidly growing field seeking to form images of objects outside the field of view, with potential applications in search and rescue, reconnaissance, and even medical imaging. The critical challenge of NLOS imaging is that diffuse reflections scatter light in all directions, resulting in weak signals and a loss of directional information. To address this problem, we propose a method for seeing around corners that derives angular resolution from vertical edges and longitudinal resolution from the temporal response to a pulsed light source. We introduce an acquisition strategy, scene response model, and reconstruction algorithm that enable the formation of 2.5-dimensional representations -- a plan view plus heights -- and a 180$^{\circ}$ field of view (FOV) for large-scale scenes. Our experiments demonstrate accurate reconstructions of hidden rooms up to 3 meters in each dimension. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：非视距视距（NLOS）成像是一个快速增长的领域寻求形成物体的图像的视野之外，在搜救，侦察，甚至是医疗成像应用潜力。 NLOS成像的关键的挑战是，漫反射在所有方向上散射光，从而导致弱信号和方向信息的损失。为了解决这个问题，我们提出了看到周围导出从垂直边缘，并从以一个脉冲光源的时间响应纵向分辨率的角分辨率的角的方法。我们引入一个采集策略，场景响应模型，和重建算法，使2.5维表示的形成 - 的平面图加高度 - 和180 $ ^ {\ CIRC}视场（FOV）的$字段适用于大大规模的场面。我们的实验证明隐藏客房精确重建达在每个维度3米。</font>
</div>


<hr>
<div id="paper46"> <b>46. Query-Efficient Physical Hard-Label Attacks on Deep Learning Visual  Classification</b>  <a href="https://arxiv.org/pdf/2002.07088" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title46" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryan Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiefeng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manohar%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nelson Manohar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fernandes%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Earlence Fernandes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jha%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Somesh Jha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prakash%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Atul Prakash</a><br>
<font size="3">
Abstract: We present Survival-OPT, a physical adversarial example algorithm in the black-box hard-label setting where the attacker only has access to the model prediction class label. Assuming such limited access to the model is more relevant for settings such as proprietary cyber-physical and cloud systems than the whitebox setting assumed by prior work. By leveraging the properties of physical attacks, we create a novel approach based on the survivability of perturbations corresponding to physical transformations. Through simply querying the model for hard-label predictions, we optimize perturbations to survive in many different physical conditions and show that adversarial examples remain a security risk to cyber-physical systems (CPSs) even in the hard-label threat model. We show that Survival-OPT is query-efficient and robust: using fewer than 200K queries, we successfully attack a stop sign to be misclassified as a speed limit 30 km/hr sign in 98.5% of video frames in a drive-by setting. Survival-OPT also outperforms our baseline combination of existing hard-label and physical approaches, which required over 10x more queries for less robust results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前生存-OPT，在黑盒硬标签的设置下，攻击者只能访问模型预测类别标签物理对抗性的示例算法。该模型假定，例如有限的访问是用于设置，例如不是由以前的工作假定白盒设置专有网络物理和云系统更相关。通过利用物理攻击的特性，我们创建基于对应于物理转换扰动的生存能力的新方法。通过简单的查询硬标签的预测模型，我们优化扰动在许多不同的物理条件和显示的生存是对抗性的例子仍然是一个安全隐患，即使在硬标签威胁模型的网络物理系统（CPS的）。我们证明了生存-OPT是查询效率和稳健的：使用不到200K的查询，我们成功地攻击停止的迹象在驱动设置被错误归类为一个速度下限的30公里/小时的标志在视频帧的98.5％。生存-OPT也优于我们的基准现有的硬标签和物理的方法，这需要对不太可靠的结果超过10倍的查询组合。</font>
</div>


<hr>
<div id="paper47"> <b>47. PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks  for Thermal and NIR to Visible Image Transformation</b>  <a href="https://arxiv.org/pdf/2002.07082" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title47" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Babu%2C+K+K" target="_blank" rel="noopener" style="color:#0000EE;">Kancharagunta Kishan Babu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dubey%2C+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Shiv Ram Dubey</a><br>
<font size="3">
Abstract: In many real world scenarios, it is difficult to capture the images in the visible light spectrum (VIS) due to bad lighting conditions. However, the images can be captured in such scenarios using Near-Infrared (NIR) and Thermal (THM) cameras. The NIR and THM images contain the limited details. Thus, there is a need to transform the images from THM/NIR to VIS for better understanding. However, it is non-trivial task due to the large domain discrepancies and lack of abundant datasets. Nowadays, Generative Adversarial Network (GAN) is able to transform the images from one domain to another domain. Most of the available GAN based methods use the combination of the adversarial and the pixel-wise losses (like L1 or L2) as the objective function for training. The quality of transformed images in case of THM/NIR to VIS transformation is still not up to the mark using such objective function. Thus, better objective functions are needed to improve the quality, fine details and realism of the transformed images. A new model for THM/NIR to VIS image transformation called Perceptual Cyclic-Synthesized Generative Adversarial Network (PCSGAN) is introduced to address these issues. The PCSGAN uses the combination of the perceptual (i.e., feature based) losses along with the pixel-wise and the adversarial losses. Both the quantitative and qualitative measures are used to judge the performance of the PCSGAN model over the WHU-IIP face and the RGB-NIR scene datasets. The proposed PCSGAN outperforms the state-of-the-art image transformation models, including Pix2pix, DualGAN, CycleGAN, PS2GAN, and PAN in terms of the SSIM, MSE, PSNR and LPIPS evaluation measures. The code is available at: \url{this https URL}. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在许多现实世界的场景，很难捕捉可见光光谱（VIS）的图像由于恶劣的照明条件。然而，该图像可在使用近红外（NIR）和热（THM）摄像机这样的场景被捕获。该NIR和THM图像包含有限的详细信息。因此，有必要从THM / NIR到VIS变换图像更好地理解。然而，这是不平凡的任务，由于大域差异和缺乏丰富的数据集。如今，剖成对抗性网络（GAN）是能够从一个域变换图像到另一个域。大多数可用的GaN系的方法使用对抗和逐像素损失（如L1或L2）作为用于训练的目标函数的组合。变换图像在THM / NIR到VIS转化的情况下的质量仍然没有达到使用这样的目标函数的标志。因此，需要更好的目标函数，以提高质量，精致的细节和逼真的变换图像。对于THM / NIR新模式，VIS图像变换称为感知循环合成剖成对抗性网络（PCSGAN）引入来解决这些问题。所述PCSGAN使用感知的组合（即，特征为基础）与逐像素和对抗损失沿损失。无论是定量和定性指标来判断PCSGAN模型在WHU-IIP面和RGB-NIR现场数据集的性能。所提出的PCSGAN优于国家的最先进的图像变换模型，包括Pix2pix，DualGAN，CycleGAN，PS2GAN和PAN在SSIM，MSE，PSNR的条款和LPIPS评估措施。该代码可在：\ {URL这HTTPS URL}。</font>
</div>


<hr>
<div id="paper48"> <b>48. Large-scale biometry with interpretable neural network regression on UK  Biobank body MRI</b>  <a href="https://arxiv.org/pdf/2002.06862" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title48" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Langner%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taro Langner</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ahlstr%C3%B6m%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Håkan Ahlström</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kullberg%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joel Kullberg</a><br>
<font size="3">
Abstract: The UK Biobank study has successfully imaged more than 32,000 volunteer participants with neck-to-knee body MRI. Each scan is linked to extensive metadata, providing a comprehensive survey of imaged anatomy and related health states. Despite its potential for research, this vast amount of data presents a challenge to established methods of evaluation, which often rely on manual input. To date, the range of reference values for cardiovascular and metabolic risk factors is therefore incomplete. In this work, neural networks were trained for regression to infer various biological metrics from the neck-to-knee body MRI automatically. The approach requires no manual intervention or ground truth segmentations for training. The examined fields span 64 variables derived from anthropometric measurements, dual-energy X-ray absorptiometry (DXA), atlas-based segmentations, and dedicated liver scans. The standardized framework achieved a close fit to the target values (median R^2 > 0.97) in 7-fold cross-validation with the ResNet50. Interpretation of aggregated saliency maps suggests that the network correctly targets specific body regions and limbs, and learned to emulate different modalities. On several body composition metrics, the quality of the predictions is within the range of variability observed between established gold standard techniques. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：英国生物库研究已经成功地成像与颈部到膝盖的人体MRI 32000名多名志愿者参加。每个扫描链接到大量的元数据，提供成像解剖学及相关健康状况的全面调查。尽管其研究潜力，这个庞大的数据量的礼物，以评估建立的方法，这往往依靠手工输入一个挑战。迄今为止，用于治疗心血管和代谢风险因素的参考值的范围，因此是不完整的。在这项工作中，神经网络是从颈部到膝盖的身体MRI自动训练回归推断各种生物指标。该方法需要对培训没有人工干预或地面实况分割。所检查的领域跨越从人体测量，双能X射线吸收法（DXA），基于图谱-分割，和专用肝脏扫描得到64个变量。标准化框架实现了紧密配合到所述目标的值（中值R ^ 2> 0.97）在7倍交叉验证与ResNet50。聚集的显着性的解释映射表明，网络正确针对特定的身体部位和四肢，并学会了模仿不同的方式。在几个身体组成指标，预测的质量是建立金标准技术之间观察到的变异性的范围内。</font>
</div>


<hr>
<div id="paper49"> <b>49. Class-Imbalanced Semi-Supervised Learning</b>  <a href="https://arxiv.org/pdf/2002.06815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title49" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hyun%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minsung Hyun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jeong%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jisoo Jeong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kwak%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nojun Kwak</a><br>
<font size="3">
Abstract: Semi-Supervised Learning (SSL) has achieved great success in overcoming the difficulties of labeling and making full use of unlabeled data. However, SSL has a limited assumption that the numbers of samples in different classes are balanced, and many SSL algorithms show lower performance for the datasets with the imbalanced class distribution. In this paper, we introduce a task of class-imbalanced semi-supervised learning (CISSL), which refers to semi-supervised learning with class-imbalanced data. In doing so, we consider class imbalance in both labeled and unlabeled sets. First, we analyze existing SSL methods in imbalanced environments and examine how the class imbalance affects SSL methods. Then we propose Suppressed Consistency Loss (SCL), a regularization method robust to class imbalance. Our method shows better performance than the conventional methods in the CISSL environment. In particular, the more severe the class imbalance and the smaller the size of the labeled data, the better our method performs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：半监督学习（SSL）在克服标签的困难，充分利用未标记数据的取得了巨大成功。然而，SSL具有有限的假设，在不同类型的样品的数量是平衡的，许多SSL算法显示与不平衡类分布的数据集较低的性能。在本文中，我们介绍了类不平衡半监督学习（CISSL），它指的是半监督学习与类不平衡数据的任务。在此过程中，我们考虑在这两个标记和未标记套类不平衡。首先，我们分析了不平衡的环境中现有的SSL方法和检验类失衡是如何影响SSL的方法。然后，我们建议禁止一致性损失（SCL），正则化方法稳健类不平衡。我们的方法显示出比CISSL环境的传统方法更好的性能。特别地，更严重的类不平衡和标记数据的大小越小越好我们的方法进行。</font>
</div>


<hr>
<div id="paper50"> <b>50. Reinforcement learning for the manipulation of eye tracking data</b>  <a href="https://arxiv.org/pdf/2002.06806" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title50" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fuhl%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wolfgang Fuhl</a><br>
<font size="3">
Abstract: In this paper, we present an approach based on reinforcement learning for eye tracking data manipulation. It is based on two opposing agents, where one tries to classify the data correctly and the second agent looks for patterns in the data, which get manipulated to hide specific information. We show that our approach is successfully applicable to preserve the privacy of a subject. In addition, our approach allows to evaluate the importance of temporal, as well as spatial, information of eye tracking data for specific classification goals. In general, this approach can also be used for stimuli manipulation, making it interesting for gaze guidance. For this purpose, this work provides the theoretical basis, which is why we have also integrated a section on how to apply this method for gaze guidance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种基于强化学习的眼动追踪数据操作的方法。它是基于两个对立的代理，其中一个尝试将数据正确分类和第二剂会在数据当中去操纵隐藏特定信息的模式。我们表明，我们的做法是成功适用于保护受试者的隐私。此外，我们的方法可以评估的具体分类目标眼动追踪数据的时间，以及空间，信息的重要性。在一般情况下，这种方法也可以用于刺激操纵，使得它有趣的凝视指导。为此，这项工作提供了理论依据，这就是为什么我们还集成了对如何申请视线引导这种方法的部分。</font>
</div>


<hr>
<div id="paper51"> <b>51. Unraveling Meta-Learning: Understanding Feature Representations for  Few-Shot Tasks</b>  <a href="https://arxiv.org/pdf/2002.06753" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title51" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldblum%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Micah Goldblum</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reich%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steven Reich</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fowl%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liam Fowl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Renkun Ni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cherepanova%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Valeriia Cherepanova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldstein%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Goldstein</a><br>
<font size="3">
Abstract: Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification. While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well. We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically. In doing so, we develop several hypotheses for why meta-learned models perform better. In addition to visualizations, we design several regularizers inspired by our hypotheses which improve performance on few-shot classification. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：元学习算法产生其实现上为数不多的镜头分类的国家的最先进的性能特征提取。虽然文学是丰富的元学习方法，鲜为人知的是，为什么得到的特征提取如此上佳表现。我们更好地了解元学习和模型之间的差异的底层机制的使用已训练元学习这些经典训练模式。在此过程中，我们开发了为什么元学模型进行更好的几种假说。除了可视化，我们设计我们假设这改善为数不多的镜头分类性能的启发几个regularizers。</font>
</div>


<hr>
<div id="paper52"> <b>52. Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO  Framework</b>  <a href="https://arxiv.org/pdf/2002.06701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title52" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sur%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chiranjib Sur</a><br>
<font size="3">
Abstract: In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF) for Better Semantic Selection for Indian regional language-based image captioning and introduced a procedure where we used the existing translation and English crowd-sourced sentences for training. We have shown that this architecture is a promising alternative source, where there is a crunch in resources. Our main contribution of this work is the development of deep learning architectures for the Bengali language (is the fifth widely spoken language in the world) with a completely different grammar and language attributes. We have shown that these are working well for complex applications like language generation from image contexts and can diversify the representation through introducing constraints, more extensive features, and unique feature spaces. We also established that we could achieve absolute precision and diversity when we use smoothened semantic tensor with the traditional LSTM and feature decomposition networks. With better learning architecture, we succeeded in establishing an automated algorithm and assessment procedure that can help in the evaluation of competent applications without the requirement for expertise and human intervention. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们已经介绍了高斯平滑语义特征（GSSF）为更好的语义选择对印度地方语言基础的图像和字幕介绍我们用于训练现有的翻译和英语人群来源的句子的过程。我们已经证明这种体系结构是一个有前途的替代来源，那里是在资源危机。我们这项工作的主要贡献是为孟加拉语深度学习架构（是世界第五广泛的语言）使用完全不同的语法和语言特性的发展。我们已经表明，这些从图像背景复杂的应用程序，如语言生成运作良好，可以通过引入约束，更丰富的功能，以及独特的功能空间多样化的表现。我们还建立了，当我们使用平滑的语义张量与传统LSTM和功能分解的网络，我们可以实现绝对精度和多样性。有了更好的学习结构，我们成功地建立自动算法和评估程序，在主管应用的评价可以帮助没有专业知识和人为干预的要求。</font>
</div>


<hr>
<div id="paper53"> <b>53. Coresets for the Nearest-Neighbor Rule</b>  <a href="https://arxiv.org/pdf/2002.06650" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title53" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Velazco%2C+A+F" target="_blank" rel="noopener" style="color:#0000EE;">Alejandro Flores Velazco</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mount%2C+D+M" target="_blank" rel="noopener" style="color:#0000EE;">David M. Mount</a><br>
<font size="3">
Abstract: The problem of nearest-neighbor condensation deals with finding a subset R from a set of labeled points P such that for every point p in R the nearest-neighbor of p in R has the same label as p. This is motivated by applications in classification, where the nearest-neighbor rule assigns to an unlabeled query point the label of its nearest-neighbor in the point set. In this context, condensation aims to reduce the size of the set needed to classify new points. However, finding such subsets of minimum cardinality is NP-hard, and most research has focused on practical heuristics without performance guarantees. Additionally, the use of exact nearest-neighbors is always assumed, ignoring the effect of condensation in the classification accuracy when nearest-neighbors are computed approximately. In this paper, we address these shortcomings by proposing new approximation-sensitive criteria for the nearest-neighbor condensation problem, along with practical algorithms with provable performance guarantees. We characterize sufficient conditions to guarantee correct classification of unlabeled points using approximate nearest-neighbor queries on these subsets, which introduces the notion of coresets for classification with the nearest-neighbor rule. Moreover, we prove that it is NP-hard to compute subsets with these characteristics, whose cardinality approximates that of the minimum cardinality subset. Additionally, we propose new algorithms for computing such subsets, with tight approximation factors in general metrics, and improved factors for doubling metrics and l_p metrics with p >= 2. Finally, we show an alternative implementation scheme that reduces the worst-case time complexity of one of these algorithms, becoming the first truly subquadratic approximation algorithm for the nearest-neighbor condensation problem. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近邻冷凝涉及从一组标记的点P使得对R中的每个点p中的R p的近邻具有相同的标签为P的找到一个子集R本问题。这是通过分类，其中最近邻规则受让人向无查询点的最近邻居的标签点集中的应用程序的动机。在此背景下，冷凝旨在减少所需的分类新点集的大小。然而，寻找最小基数的这种子集是NP难，大部分研究都集中在实用的启发式没有性能保证。此外，使用精确的近邻的总是假设，忽略缩合而当近邻近似计算的分类精度的影响。在本文中，我们通过提出新的逼近敏感的标准，最近邻凝结问题，与可证明的性能保证的实用算法一起克服这些缺点。我们描述了充分条件，以保证使用近似最近邻查询在这些子集，它引入了coresets的与最近邻规则分类的概念未标记点的正确分类。此外，我们证明它是NP-难以计算的子集具有这些特征，其基数近似于最小基数子集。此外，我们提出了新的算法，用于计算这种子集，紧张因素近似一般指标，和改进的因素有p倍增度量和L_P度量> = 2。最后，我们表明，减少了最坏情况下的时间复杂度的可选的实施方案对这些算法之一，成为最近邻结露问题的第一个真正的次二次近似算法。</font>
</div>


<hr>
<div id="paper54"> <b>54. Hold me tight! Influence of discriminative features on deep network  boundaries</b>  <a href="https://arxiv.org/pdf/2002.06349" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title54" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ortiz-Jimenez%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillermo Ortiz-Jimenez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Modas%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Apostolos Modas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moosavi-Dezfooli%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seyed-Mohsen Moosavi-Dezfooli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Frossard%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pascal Frossard</a><br>
<font size="3">
Abstract: Important insights towards the explainability of neural networks and their properties reside in the formation of their decision boundaries. In this work, we borrow tools from the field of adversarial robustness and propose a new framework that permits to relate the features of the dataset with the distance of data samples to the decision boundary along specific directions. We demonstrate that the inductive bias of deep learning has the tendency to generate classification functions that are invariant along non-discriminative directions of the dataset. More surprisingly, we further show that training on small perturbations of the data samples are sufficient to completely change the decision boundary. This is actually the characteristic exploited by the so-called adversarial training to produce robust classifiers. Our general framework can be used to reveal the effect of specific dataset features on the macroscopic properties of deep models and to develop a better understanding of the successes and limitations of deep learning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对神经网络及其属性的explainability重要见解驻留在他们的决策边界的形成。在这项工作中，我们从对抗鲁棒性领域借用工具，并提出一个新的框架，以允许相关数据集的功能与数据样本沿特定方向的决策边界的距离。我们证明了深度学习的归纳偏置具有生成沿着数据集的非歧视的方向不变分类功能的倾向。更令人惊讶的，我们进一步显示出对数据样本的微小扰动培训足以彻底改变决策边界。这实际上是由所谓的对抗训练利用来产生强大的分类特征。我们的总体框架可用于揭示的特定数据集的功能深模型的宏观性质的影响，并开发出更好的成绩和深度学习的局限性的理解。</font>
</div>


<hr>
<div id="paper55"> <b>55. 3D Dynamic Scene Graphs: Actionable Spatial Perception with Places,  Objects, and Humans</b>  <a href="https://arxiv.org/pdf/2002.06289" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title55" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosinol%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antoni Rosinol</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arjun Gupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Abate%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marcus Abate</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingnan Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carlone%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luca Carlone</a><br>
<font size="3">
Abstract: We present a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g. objects, walls, rooms), and edges represent relations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g. humans, robots), and to include actionable information that supports planning and decision-making (e.g. spatio-temporal relations, topology at different levels of abstraction). Our second contribution is to provide the first fully automatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial data. We integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. To the best of our knowledge, this is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, we provide algorithms to obtain hierarchical representations of indoor environments (e.g. places, structures, rooms) and their relations. Our third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where we assess its robustness and expressiveness. Finally, we discuss the implications of our proposal on modern robotics applications. 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction. A video abstract is available at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了可操作的空间知觉的统一表示：3D动态场景图。场景图的有向图，其中节点代表场景中的实体（例如对象，墙壁，房间），并且边表示节点之间的关系（例如包含，邻接）。动态场景图（DSGs）扩展这个概念来表示与移动代理的动态场景（例如人类，机器人），并包括可操作的信息，支持规划和决策（例如时空关系，拓扑结构，在不同的抽象层次）。我们的第二贡献是提供第一全自动空间感知发动机（SPIN）建立从视觉惯性数据的DSG。我们整合国家的最先进的技术目标和人类检测和姿态估计，我们描述了如何稳健地推断物体，机器人，并在拥挤的场景人类节点。据我们所知，这是和解的视觉惯性SLAM和密集的人力网跟踪的第一篇论文。此外，我们提供的算法来获得室内环境的分层表示（例如地方，结构，室）和他们的关系。我们的第三个贡献是验证了空间感知发动机在照片般逼真的基于统一的仿真，我们评估其耐用性和表现力。最后，我们讨论我们的建议对现代机器人技术应用的影响。 3D动态场景图可以对规划和决策，人机交互，长期的自主权，并现场预测了深远的影响。视频摘要可在此HTTPS URL</font>
</div>


<hr>
<div id="paper56"> <b>56. Learning representations of irregular particle-detector geometry with  distance-weighted graph networks</b>  <a href="https://arxiv.org/pdf/1902.07987" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title56" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/physics?searchtype=author&query=Qasim%2C+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Shah Rukh Qasim</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Kieseler%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Kieseler</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Iiyama%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yutaro Iiyama</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Pierini%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maurizio Pierini</a><br>
<font size="3">
Abstract: We explore the use of graph networks to deal with irregular-geometry detectors in the context of particle reconstruction. Thanks to their representation-learning capabilities, graph networks can exploit the full detector granularity, while natively managing the event sparsity and arbitrarily complex detector geometries. We introduce two distance-weighted graph network architectures, dubbed GarNet and GravNet layers, and apply them to a typical particle reconstruction task. The performance of the new architectures is evaluated on a data set of simulated particle interactions on a toy model of a highly granular calorimeter, loosely inspired by the endcap calorimeter to be installed in the CMS detector for the High-Luminosity LHC phase. We study the clustering of energy depositions, which is the basis for calorimetric particle reconstruction, and provide a quantitative comparison to alternative approaches. The proposed algorithms provide an interesting alternative to existing methods, offering equally performing or less resource-demanding solutions with less underlying assumptions on the detector geometry and, consequently, the possibility to generalize to other detectors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：探索利用图表网络来处理在粒子重建过程中不规则几何形状的检测器。由于其表示学习能力，图形网络可以利用完全探测器粒度，而本地管理事件稀疏和任意复杂的检测器几何形状。我们引入两个距离加权图的网络架构，被称为石榴石和GravNet层，并将其应用到一个典型的颗粒重建任务。新的体系结构的性能上非常细微的量热计的玩具模型模拟粒子相互作用，通过端盖量热松弛地启发，被安装在CMS探测器的高辉度LHC相位的数据组进行评估。我们研究的能量沉积的聚类，这是量热粒子重建的基础上，并且提供一个定量的比较来替代方法。所提出的算法提供了一个有趣的替代现有的方法，将提供同样具有较少潜在的假设上的检测器几何形状执行以下需要资源的解决方案，因此，的可能性推广到其他检测器。</font>
</div>


<hr>
<div id="paper57"> <b>57. Adaptive Kernel Estimation of the Spectral Density with Boundary Kernel  Analysis</b>  <a href="https://arxiv.org/pdf/1803.03906" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title57" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/stat?searchtype=author&query=Sidorenko%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Sidorenko</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&query=Riedel%2C+K+S" target="_blank" rel="noopener" style="color:#0000EE;">Kurt S. Riedel</a><br>
<font size="3">
Abstract: A hybrid estimator of the log-spectral density of a stationary time series is proposed. First, a multiple taper estimate is performed, followed by kernel smoothing the log-multitaper estimate. This procedure reduces the expected mean square error by $({\pi^2 \over 4})^{.8}$ over simply smoothing the log tapered periodogram. The optimal number of tapers is $O(N^{8/15})$. A data adaptive implementation of a variable bandwidth kernel smoother is given. When the spectral density is discontinuous, one sided smoothing estimates are used. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一个固定的时间序列的对数谱密度的混合估计算法。首先，将多个锥形估算被执行，随后内核平滑对数多窗口估计。此过程通过$降低了预期均方误差（{\ PI ^ 2 \超过4}）^ {8} $过简单地平滑日志锥形周期图。锥度的最佳数量是$ O（N ^ {8/15}）$。可变带宽内核平滑的数据自适应实现中给出。当谱密度是不连续的，使用一种双面平滑估计。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-18</title>
    <url>/2020/02/18/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-18/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset <a href="https://arxiv.org/pdf/2002.06854" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> GameWikiSum: a Novel Large Multi-Document Summarization Dataset <a href="https://arxiv.org/pdf/2002.06851" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Incorporating BERT into Neural Machine Translation <a href="https://arxiv.org/pdf/2002.06823" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Multi-layer Representation Fusion for Neural Machine Translation <a href="https://arxiv.org/pdf/2002.06714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO  Framework <a href="https://arxiv.org/pdf/2002.06701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for  Ainu Language <a href="https://arxiv.org/pdf/2002.06675" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> The Utility of General Domain Transfer Learning for Medical Language  Tasks <a href="https://arxiv.org/pdf/2002.06670" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word  Models <a href="https://arxiv.org/pdf/2002.06652" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Towards Detection of Subjective Bias using Contextualized Word  Embeddings <a href="https://arxiv.org/pdf/2002.06644" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Neural Machine Translation with Joint Representation <a href="https://arxiv.org/pdf/2002.06546" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Exploring Neural Models for Parsing Natural Language into First-Order  Logic <a href="https://arxiv.org/pdf/2002.06544" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Learning to Generate Multiple Style Transfer Outputs for an Input  Sentence <a href="https://arxiv.org/pdf/2002.06525" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> A Multimodal Dialogue System for Conversational Image Editing <a href="https://arxiv.org/pdf/2002.06484" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Deeper Task-Specificity Improves Joint Entity and Relation Extraction <a href="https://arxiv.org/pdf/2002.06424" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Fine-Tuning Pretrained Language Models: Weight Initializations, Data  Orders, and Early Stopping <a href="https://arxiv.org/pdf/2002.06305" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Controlling Computation versus Quality for Neural Sequence Models <a href="https://arxiv.org/pdf/2002.07106" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Computing rank-revealing factorizations of matrices stored out-of-core <a href="https://arxiv.org/pdf/2002.06960" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Supervised Phrase-boundary Embeddings <a href="https://arxiv.org/pdf/2002.06450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Open Knowledge Enrichment for Long-tail Entities <a href="https://arxiv.org/pdf/2002.06397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> UniViLM: A Unified Video and Language Pre-Training Model for Multimodal  Understanding and Generation <a href="https://arxiv.org/pdf/2002.06353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Semantic Relatedness and Taxonomic Word Embeddings <a href="https://arxiv.org/pdf/2002.06235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset</b>  <a href="https://arxiv.org/pdf/2002.06854" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Antognini%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diego Antognini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Faltings%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Boi Faltings</a><br>
<font size="3">
Abstract: Today, recommender systems are an inevitable part of everyone's daily digital routine and are present on most internet platforms. State-of-the-art deep learning-based models require a large number of data to achieve their best performance. Many datasets fulfilling this criterion have been proposed for multiple domains, such as Amazon products, restaurants, or beers. However, works and datasets in the hotel domain are limited: the largest hotel review dataset is below the million samples. Additionally, the hotel domain suffers from a higher data sparsity than traditional recommendation datasets and therefore, traditional collaborative-filtering approaches cannot be applied to such data. In this paper, we propose HotelRec, a very large-scale hotel recommendation dataset, based on TripAdvisor, containing 50 million reviews. To the best of our knowledge, HotelRec is the largest publicly available dataset in the hotel domain (50M versus 0.9M) and additionally, the largest recommendation dataset in a single domain and with textual reviews (50M versus 22M). We release HotelRec for further research: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：今天，推荐系统是每个人的日常数字常规不可避免的一部分，并且存在于大多数互联网平台。国家的最先进的深基础的学习的模型需要大量的数据来实现其最佳性能。许多数据集满足这个标准已经被提出了多个域名，如Amazon的产品，餐馆或啤酒。然而，工作和数据集在酒店领域是有限的：最大的三星级酒店的数据集是万个样本下方。另外，从比传统的建议的数据集，并且因此更高的数据稀疏的酒店域患有，传统的协作过滤方法不能被应用到这样的数据。在本文中，我们提出HotelRec，一个很大规模的酒店推荐数据集的基础上，TripAdvisor的，含有5000万条评论。据我们所知，HotelRec是在酒店领域最大的可公开获得的数据集（50M与0.9M），另外，在一个域中，并用文字评论（50M与22M）最大的推荐数据集。我们发布HotelRec进一步研究：此HTTPS URL。</font>
</div>


<hr>
<div id="paper2"> <b>2. GameWikiSum: a Novel Large Multi-Document Summarization Dataset</b>  <a href="https://arxiv.org/pdf/2002.06851" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Antognini%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diego Antognini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Faltings%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Boi Faltings</a><br>
<font size="3">
Abstract: Today's research progress in the field of multi-document summarization is obstructed by the small number of available datasets. Since the acquisition of reference summaries is costly, existing datasets contain only hundreds of samples at most, resulting in heavy reliance on hand-crafted features or necessitating additional, manually annotated data. The lack of large corpora therefore hinders the development of sophisticated models. Additionally, most publicly available multi-document summarization corpora are in the news domain, and no analogous dataset exists in the video game domain. In this paper, we propose GameWikiSum, a new domain-specific dataset for multi-document summarization, which is one hundred times larger than commonly used datasets, and in another domain than news. Input documents consist of long professional video game reviews as well as references of their gameplay sections in Wikipedia pages. We analyze the proposed dataset and show that both abstractive and extractive models can be trained on it. We release GameWikiSum for further research: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：今天的多文档文摘领域的研究进展是由少数可用的数据集的阻碍。由于参考摘要的获取是昂贵的，现有数据集仅包含数百个样品至多，导致在手工制作的特征或迫使额外的，手动注释的数据重依赖。因此，缺乏大型语料库的阻碍复杂的模型的开发。此外，大多数公开可用的多文档文摘语料库在新闻领域，并没有类似的数据集在视频游戏领域的存在。在本文中，我们提出GameWikiSum，一个新的域特定的数据集的多文档文摘，这比通常使用的数据集较大的一个百倍，而在另一个领域不是新闻。输入文件包括长期专业视频游戏评论以及在维基百科网页游戏的部分的引用。我们分析所提出的数据集，并表明，无论是抽象和采掘模型可以在其上进行训练。我们发布GameWikiSum进一步研究：此HTTPS URL。</font>
</div>


<hr>
<div id="paper3"> <b>3. Incorporating BERT into Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.06823" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinhua Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingce Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lijun Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Di He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wengang Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Houqiang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tie-Yan Liu</a><br>
<font size="3">
Abstract: The recently proposed BERT has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then the representations are fused with each layer of the encoder and decoder of the NMT model through attention mechanisms. We conduct experiments on supervised (including sentence-level and document-level translations), semi-supervised and unsupervised machine translation, and achieve state-of-the-art results on seven benchmark datasets. Our code is available at \url{this https URL}. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近提出BERT已经在各种自然语言理解任务，如文本分类，阅读理解等。然而，如何有效地应用BERT神经机器翻译（NMT）缺乏足够的勘探显示出巨大的威力。虽然BERT是比较常用的微调，而不是下游语言理解任务的上下文嵌入在NMT，我们作为背景嵌入比使用微调更好地使用BERT的初步探索。这促使我们思考如何更好地利用BERT为NMT沿着这个方向发展。我们提出了一个名为BERT融合模式新的算法，在此我们首先使用BERT提取表示对于输入序列，然后表示是通过关注机制NMT模型的编码器和解码器的每一层融合。我们在开展监督实验（包括句子级和文件级转换），半监督和无人监督的机器翻译，实现国家的最先进的七个标准数据集的结果。我们的代码是可以在\ {URL这HTTPS URL}。</font>
</div>


<hr>
<div id="paper4"> <b>4. Multi-layer Representation Fusion for Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.06714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fuxue Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanyang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yinqiao Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingbo Zhu</a><br>
<font size="3">
Abstract: Neural machine translation systems require a number of stacked layers for deep models. But the prediction depends on the sentence representation of the top-most layer with no access to low-level representations. This makes it more difficult to train the model and poses a risk of information loss to prediction. In this paper, we propose a multi-layer representation fusion (MLRF) approach to fusing stacked layers. In particular, we design three fusion functions to learn a better representation from the stack. Experimental results show that our approach yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-of-the-art in German-English translation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经机器翻译系统需要大量的深模型叠层。但预测依赖于最顶层的，没有获得低级别表示句子表示。这使得它更难以培养模式，造成信息损失预测的风险。在本文中，我们提出了一种多层表示融合（MLRF）的方法来熔合叠层。特别是，我们设计了三种融合功能，学习从堆栈中一个更好的代表性。实验结果表明，分别，我们在上IWSLT德文 - 英语和NIST中国英语MT任务的强大的变压器基线的0.92和0.56 BLEU点的方法产生的改进。其结果是新的国家的最先进的德国英文翻译。</font>
</div>


<hr>
<div id="paper5"> <b>5. Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO  Framework</b>  <a href="https://arxiv.org/pdf/2002.06701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sur%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chiranjib Sur</a><br>
<font size="3">
Abstract: In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF) for Better Semantic Selection for Indian regional language-based image captioning and introduced a procedure where we used the existing translation and English crowd-sourced sentences for training. We have shown that this architecture is a promising alternative source, where there is a crunch in resources. Our main contribution of this work is the development of deep learning architectures for the Bengali language (is the fifth widely spoken language in the world) with a completely different grammar and language attributes. We have shown that these are working well for complex applications like language generation from image contexts and can diversify the representation through introducing constraints, more extensive features, and unique feature spaces. We also established that we could achieve absolute precision and diversity when we use smoothened semantic tensor with the traditional LSTM and feature decomposition networks. With better learning architecture, we succeeded in establishing an automated algorithm and assessment procedure that can help in the evaluation of competent applications without the requirement for expertise and human intervention. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们已经介绍了高斯平滑语义特征（GSSF）为更好的语义选择对印度地方语言基础的图像和字幕介绍我们用于训练现有的翻译和英语人群来源的句子的过程。我们已经证明这种体系结构是一个有前途的替代来源，那里是在资源危机。我们这项工作的主要贡献是为孟加拉语深度学习架构（是世界第五广泛的语言）使用完全不同的语法和语言特性的发展。我们已经表明，这些从图像背景复杂的应用程序，如语言生成运作良好，可以通过引入约束，更丰富的功能，以及独特的功能空间多样化的表现。我们还建立了，当我们使用平滑的语义张量与传统LSTM和功能分解的网络，我们可以实现绝对精度和多样性。有了更好的学习结构，我们成功地建立自动算法和评估程序，在主管应用的评价可以帮助没有专业知识和人为干预的要求。</font>
</div>


<hr>
<div id="paper6"> <b>6. Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for  Ainu Language</b>  <a href="https://arxiv.org/pdf/2002.06675" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Matsuura%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kohei Matsuura</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ueno%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sei Ueno</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mimura%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masato Mimura</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sakai%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shinsuke Sakai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kawahara%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tatsuya Kawahara</a><br>
<font size="3">
Abstract: Ainu is an unwritten language that has been spoken by Ainu people who are one of the ethnic groups in Japan. It is recognized as critically endangered by UNESCO and archiving and documentation of its language heritage is of paramount importance. Although a considerable amount of voice recordings of Ainu folklore has been produced and accumulated to save their culture, only a quite limited parts of them are transcribed so far. Thus, we started a project of automatic speech recognition (ASR) for the Ainu language in order to contribute to the development of annotated language archives. In this paper, we report speech corpus development and the structure and performance of end-to-end ASR for Ainu. We investigated four modeling units (phone, syllable, word piece, and word) and found that the syllable-based model performed best in terms of both word and phone recognition accuracy, which were about 60% and over 85% respectively in speaker-open condition. Furthermore, word and phone accuracy of 80% and 90% has been achieved in a speaker-closed setting. We also found out that a multilingual ASR training with additional speech corpora of English and Japanese further improves the speaker-open test accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：阿伊努人是已被阿伊努人谁是日本的民族之一说出一个不成文的语言。如暴联合国教科文组织濒危和归档和语言遗产的文档是非常重要的它是公认的。尽管相当数量的阿伊努民间传说的录音已产生和积累，以挽救他们的文化，只有他们的十分有限的部分至今转录。因此，我们为了促进注释文档案的开发开始自动语音识别（ASR）的阿伊努语的项目。在本文中，我们报道了语料库发展的结构和终端到终端的ASR为阿伊努人的表现。我们查处4个建模单位（电话，音节，字块，和字），发现基于音节的模型字和电话识别准确度，其分别约为60％和85％以上的条件进行最好的扬声器开条件。此外，字和80％的手机准确性和90％已在扬声器关闭的设置来实现的。我们还发现，多语种ASR训练，英语和日语的附加语音语料库进一步提高了扬声器，开放测试的准确性。</font>
</div>


<hr>
<div id="paper7"> <b>7. The Utility of General Domain Transfer Learning for Medical Language  Tasks</b>  <a href="https://arxiv.org/pdf/2002.06670" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ranti%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Ranti</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hanss%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Katie Hanss</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shan Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arvind%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Varun Arvind</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Titano%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joseph Titano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Costa%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anthony Costa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oermann%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eric Oermann</a><br>
<font size="3">
Abstract: The purpose of this study is to analyze the efficacy of transfer learning techniques and transformer-based models as applied to medical natural language processing (NLP) tasks, specifically radiological text classification. We used 1,977 labeled head CT reports, from a corpus of 96,303 total reports, to evaluate the efficacy of pretraining using general domain corpora and a combined general and medical domain corpus with a bidirectional representations from transformers (BERT) model for the purpose of radiological text classification. Model performance was benchmarked to a logistic regression using bag-of-words vectorization and a long short-term memory (LSTM) multi-label multi-class classification model, and compared to the published literature in medical text classification. The BERT models using either set of pretrained checkpoints outperformed the logistic regression model, achieving sample-weighted average F1-scores of 0.87 and 0.87 for the general domain model and the combined general and biomedical-domain model. General text transfer learning may be a viable technique to generate state-of-the-art results within medical NLP tasks on radiological corpora, outperforming other deep models such as LSTMs. The efficacy of pretraining and transformer-based models could serve to facilitate the creation of groundbreaking NLP models in the uniquely challenging data environment of medical text. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本研究的目的是应用于医疗自然语言处理（NLP）的任务，特别是放射性文本分类分析的转移学习技术和基于变压器模型的功效。我们使用1977份标头CT报告，从96303个总报告文集，以评估使用通用领域语料库和与双向交涉组合一般和医疗领域的语料库从变压器（BERT）模型放射文本的目的训练前的功效分类。模型的性能用袋的词矢量和长短期记忆（LSTM）多品牌多类分类模型的基准测试在回归，并与医疗文本分类出版的文献。使用任一组预训练的检查点的BERT模型优于逻辑回归模型，实现0.87和0.87的样品加权平均值F1-分数一般域模型，并将合并的一般和生物医学域模型。一般文本传输学习可能是一个可行的技术产生内的放射性语料库医疗NLP任务，优于其他车型深如LSTMs国家的先进成果。训练前和基于变压器模型的功效可以有助于促进医疗文本的独特挑战数据环境开创性的NLP模型的创建。</font>
</div>


<hr>
<div id="paper8"> <b>8. SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word  Models</b>  <a href="https://arxiv.org/pdf/2002.06652" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuo%2C+C+-+J" target="_blank" rel="noopener" style="color:#0000EE;">C.-C. Jay Kuo</a><br>
<font size="3">
Abstract: Sentence embedding is an important research topic in natural language processing (NLP) since it can transfer knowledge to downstream tasks. Meanwhile, a contextualized word representation, called BERT, achieves the state-of-the-art performance in quite a few NLP tasks. Yet, it is an open problem to generate a high quality sentence representation from BERT-based word models. It was shown in previous study that different layers of BERT capture different linguistic properties. This allows us to fusion information across layers to find better sentence representation. In this work, we study the layer-wise pattern of the word representation of deep contextualized models. Then, we propose a new sentence embedding method by dissecting BERT-based word models through geometric analysis of the space spanned by the word representation. It is called the SBERT-WK method. No further training is required in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and downstream supervised tasks. Furthermore, ten sentence-level probing tasks are presented for detailed linguistic analysis. Experiments show that SBERT-WK achieves the state-of-the-art performance. Our codes are publicly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：句子嵌入在自然语言处理（NLP）的重要研究课题，因为它可以的知识转移给下游任务。同时，情境化的单词表示，所谓的BERT，实现了相当多的NLP任务的国家的最先进的性能。然而，这是生成基于BERT字款高品质的句子表达一个开放的问题。结果表明在以前的研究认为BERT捕捉不同的语言特性的不同层。这使我们能够跨层融合的信息，以找到更好的句子表示。在这项工作中，我们研究了深情境模型的字表示的逐层模式。然后，我们提出了一个新的句子通过用单词表示围成的空间的几何分析解剖基础BERT组字模式嵌入方法。这就是所谓的SBERT-WK方法。没有进一步的培训是必需的SBERT-WK。我们对语义文本相似性和下游监督任务评估SBERT-WK。此外，十句级探测任务提出了详细的语言分析。实验表明，SBERT-WK实现国家的最先进的性能。我们的代码是公开的。</font>
</div>


<hr>
<div id="paper9"> <b>9. Towards Detection of Subjective Bias using Contextualized Word  Embeddings</b>  <a href="https://arxiv.org/pdf/2002.06644" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dadu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tanvi Dadu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pant%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kartikey Pant</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mamidi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Radhika Mamidi</a><br>
<font size="3">
Abstract: Subjective bias detection is critical for applications like propaganda detection, content recommendation, sentiment analysis, and bias neutralization. This bias is introduced in natural language via inflammatory words and phrases, casting doubt over facts, and presupposing the truth. In this work, we perform comprehensive experiments for detecting subjective bias using BERT-based models on the Wiki Neutrality Corpus(WNC). The dataset consists of $360k$ labeled instances, from Wikipedia edits that remove various instances of the bias. We further propose BERT-based ensembles that outperform state-of-the-art methods like $BERT_{large}$ by a margin of $5.6$ F1 score. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：主观偏见检测是像宣传的检测，内容推荐，情感分析和偏置中和应用的关键。这种偏见在自然语言通过炎症的单词和短语介绍，让人怀疑过的事实，并预先假定的真相。在这项工作中，我们进行全面的实验检测使用Wiki上中立语料库（WNC）基于BERT的模型主观偏见。该数据集包括$ 360K $标记的情况下，维基百科的编辑是去除偏见的各种实例。我们进一步建议，超越国家的最先进的方法，如$ BERT_基于BERT-合奏{}大$由$ 5.6 $ F1得分的余量。</font>
</div>


<hr>
<div id="paper10"> <b>10. Neural Machine Translation with Joint Representation</b>  <a href="https://arxiv.org/pdf/2002.06546" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">YanYang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tongran Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingbo Zhu</a><br>
<font size="3">
Abstract: Though early successes of Statistical Machine Translation (SMT) systems are attributed in part to the explicit modelling of the interaction between any two source and target units, e.g., alignment, the recent Neural Machine Translation (NMT) systems resort to the attention which partially encodes the interaction for efficiency. In this paper, we employ Joint Representation that fully accounts for each possible interaction. We sidestep the inefficiency issue by refining representations with the proposed efficient attention operation. The resulting Reformer models offer a new Sequence-to- Sequence modelling paradigm besides the Encoder-Decoder framework and outperform the Transformer baseline in either the small scale IWSLT14 German-English, English-German and IWSLT15 Vietnamese-English or the large scale NIST12 Chinese-English translation tasks by about 1 BLEU point.We also propose a systematic model scaling approach, allowing the Reformer model to beat the state-of-the-art Transformer in IWSLT14 German-English and NIST12 Chinese-English with about 50% fewer parameters. The code is publicly available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管统计机器翻译的早期成功（SMT）系统在部分归因于任何两个源和目标单位之间相互作用的明确建模，例如，对齐，近期神经机器翻译（NMT）系统诉诸注意哪些部分编码效率的相互作用。在本文中，我们采用联合代表，充分占每个可能的交互。我们精炼后所提出的高效操作注意回避交涉的低效率问题。得到的改革者模型提供了一个新的序列-TO-序列建模范例除了编码器，解码器框架，无论是在小规模IWSLT14德语英语，英语，德语和IWSLT15越南英文或大规模NIST12华裔跑赢基准变压器约1 BLEU一点。我们还提出了一个系统模型缩小方法，使改革者模型击败国家的最先进的变压器在IWSLT14德文 - 英语和NIST12中国英语用更少的约50％的参数英语翻译任务。该代码是公开的，在此HTTPS URL。</font>
</div>


<hr>
<div id="paper11"> <b>11. Exploring Neural Models for Parsing Natural Language into First-Order  Logic</b>  <a href="https://arxiv.org/pdf/2002.06544" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hrituraj Singh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Aggrawal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Milan Aggrawal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krishnamurthy%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Balaji Krishnamurthy</a><br>
<font size="3">
Abstract: Semantic parsing is the task of obtaining machine-interpretable representations from natural language text. We consider one such formal representation - First-Order Logic (FOL) and explore the capability of neural models in parsing English sentences to FOL. We model FOL parsing as a sequence to sequence mapping task where given a natural language sentence, it is encoded into an intermediate representation using an LSTM followed by a decoder which sequentially generates the predicates in the corresponding FOL formula. We improve the standard encoder-decoder model by introducing a variable alignment mechanism that enables it to align variables across predicates in the predicted FOL. We further show the effectiveness of predicting the category of FOL entity - Unary, Binary, Variables and Scoped Entities, at each decoder step as an auxiliary task on improving the consistency of generated FOL. We perform rigorous evaluations and extensive ablations. We also aim to release our code as well as large scale FOL dataset along with models to aid further research in logic-based parsing and inference in NLP. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语义分析是获得从自然语言文本的机器可解释表示的任务。我们认为这样的一个正式代表 - 一阶逻辑（FOL），并探讨分析英语句子FOL神经模型的能力。我们模型FOL解析作为序列到序列映射任务，其中给定的自然语言句子，它被编码为使用LSTM随后由相应的式FOL其中依次生成的谓词的解码器的中间表示。我们通过引入一个可变对准机构，它能够使在预测FOL跨越谓词对准变量改进的标准编码器 - 解码器模型。我们进一步显示预测FOL实体的类别的有效性 - 一元，二元，变量和实体作用域，在每个解码器步骤，作为有关改善产生FOL的一致性的辅助任务。我们执行严格的评估和广泛的消融。我们还致力于与模型，以帮助进一步研究在NLP基于逻辑分析和推理沿着释放我们的代码，以及大规模数据集FOL。</font>
</div>


<hr>
<div id="paper12"> <b>12. Learning to Generate Multiple Style Transfer Outputs for an Input  Sentence</b>  <a href="https://arxiv.org/pdf/2002.06525" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Yu Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming-Ting Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kautz%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Kautz</a><br>
<font size="3">
Abstract: Text style transfer refers to the task of rephrasing a given text in a different style. While various methods have been proposed to advance the state of the art, they often assume the transfer output follows a delta distribution, and thus their models cannot generate different style transfer results for a given input text. To address the limitation, we propose a one-to-many text style transfer framework. In contrast to prior works that learn a one-to-one mapping that converts an input sentence to one output sentence, our approach learns a one-to-many mapping that can convert an input sentence to multiple different output sentences, while preserving the input content. This is achieved by applying adversarial training with a latent decomposition scheme. Specifically, we decompose the latent representation of the input sentence to a style code that captures the language style variation and a content code that encodes the language style-independent content. We then combine the content code with the style code for generating a style transfer output. By combining the same content code with a different style code, we generate a different style transfer output. Extensive experimental results with comparisons to several text style transfer approaches on multiple public datasets using a diverse set of performance metrics validate effectiveness of the proposed approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文本样式转移是指在重新表述不同的风格给定文本的任务。虽然各种方法被提出推进技术状态，他们往往承担转移输出遵循增量分布，因而他们的模型无法为给定的输入文本不同的风格转移的结果。为了解决这个限制，我们提出了一个一对多的文本样式转让框架。在对比的是学习一到一个映射之前的作品是将输入句子一个输出一句话，我们的方法学习一个一对多的映射，可以输入句子转换成多个不同的输出语句，同时保留输入内容。这是通过与潜在分解方案将对抗训练来实现的。具体而言，我们分解输入句子的潜在代表性的样式代码捕获的语言风格变化和内容代码编码语言风格无关的内容。然后，我们有用于产生一种风格转移输出样式代码相结合的内容的代码。由于同样的内容代码不同的代码风格结合起来，我们产生不同的风格转移输出。相比较之下几个文本样式转移大量的实验结果上使用一组不同的所提出的方法的性能指标验证有效性的多个公共数据集的方法。</font>
</div>


<hr>
<div id="paper13"> <b>13. A Multimodal Dialogue System for Conversational Image Editing</b>  <a href="https://arxiv.org/pdf/2002.06484" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tzu-Hsiang Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trung Bui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D+S" target="_blank" rel="noopener" style="color:#0000EE;">Doo Soon Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean Oh</a><br>
<font size="3">
Abstract: In this paper, we present a multimodal dialogue system for Conversational Image Editing. We formulate our multimodal dialogue system as a Partially Observed Markov Decision Process (POMDP) and trained it with Deep Q-Network (DQN) and a user simulator. Our evaluation shows that the DQN policy outperforms a rule-based baseline policy, achieving 90\% success rate under high error rates. We also conducted a real user study and analyzed real user behavior. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个对话图像编辑的多模式对话系统。我们制定了多式联运对话系统的部分可观测马尔可夫决策过程（POMDP），并与深Q-网络（DQN）和用户培训的模拟器它。我们的评估显示，DQN政策优于基于规则的基准策略，实现在高错误率90 \％的成功率。我们也进行了真实的用户研究和分析真实的用户行为。</font>
</div>


<hr>
<div id="paper14"> <b>14. Deeper Task-Specificity Improves Joint Entity and Relation Extraction</b>  <a href="https://arxiv.org/pdf/2002.06424" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Crone%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phil Crone</a><br>
<font size="3">
Abstract: Multi-task learning (MTL) is an effective method for learning related tasks, but designing MTL models necessitates deciding which and how many parameters should be task-specific, as opposed to shared between tasks. We investigate this issue for the problem of jointly learning named entity recognition (NER) and relation extraction (RE) and propose a novel neural architecture that allows for deeper task-specificity than does prior work. In particular, we introduce additional task-specific bidirectional RNN layers for both the NER and RE tasks and tune the number of shared and task-specific layers separately for different datasets. We achieve state-of-the-art (SOTA) results for both tasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA results on the NER task and competitive results on the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggests that previous solutions to joint NER and RE undervalue task-specificity and demonstrates the importance of correctly balancing the number of shared and task-specific parameters for MTL approaches in general. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多任务学习（MTL）是学习相关的任务，但设计MTL模型就必须决定哪些许多参数应该如何针对特定任务的，而不是任务共享的有效方法。我们正在调查这个问题的共同学习命名实体识别（NER）和关系抽取（RE）的问题，并提出了一种新的神经结构，允许更深的任务特异性比以前做的工作。特别是，我们介绍了NER两者和RE任务和调共享和任务特定层的分别为不同的数据集的数量的附加任务特定双向RNN层。我们实现在ADE数据集两个任务的国家的最先进的（SOTA）的结果;在CoNLL04数据集，我们在使用的数量较少的可训练的参数比现行SOTA架构的订单实现对NER任务，并在RE任务竞争的结果SOTA结果。消融研究证实为实现这些成果的额外任务的特定层的重要性。我们的研究表明联合NER以前的解决方案和RE价值低估任务的特殊性和展示的正确平衡共享和任务特定参数的数量MTL一般方法的重要性。</font>
</div>


<hr>
<div id="paper15"> <b>15. Fine-Tuning Pretrained Language Models: Weight Initializations, Data  Orders, and Early Stopping</b>  <a href="https://arxiv.org/pdf/2002.06305" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dodge%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jesse Dodge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ilharco%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabriel Ilharco</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schwartz%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roy Schwartz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Farhadi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Farhadi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hajishirzi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hannaneh Hajishirzi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Noah Smith</a><br>
<font size="3">
Abstract: Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：微调预训练的情景字嵌入模式，以监督下游任务已经成为自然语言处理司空见惯。该方法中，然而，通常是脆：即使是相同的值超参数，不同的随机种子可以导致显着不同的结果。为了更好地理解这一现象，我们尝试从胶基准四个数据集，微调BERT数百次每个而仅改变随机种子。我们比较了先前报道的结果发现显着的性能增长，我们量化最好的发现模型的性能如何作为微调的试验数量的变化而变化。此外，我们审查随机种子的选择的影响两个因素：重量初始化和训练数据的顺序。我们发现，这两种同等于外的样本表现的方差贡献，以及一些重初始化所有任务表现良好的探讨。在小的数据集，我们观察到许多微调试验通过培训发散的方式的一部分，我们停止训练提供从业者的最佳做法不太看好初期运行。我们公开发布的所有我们的实验数据，包括2,100试验训练和验证成绩，鼓励的微调过程中培训力度进一步分析。</font>
</div>


<hr>
<div id="paper16"> <b>16. Controlling Computation versus Quality for Neural Sequence Models</b>  <a href="https://arxiv.org/pdf/2002.07106" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankur Bapna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arivazhagan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naveen Arivazhagan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Orhan Firat</a><br>
<font size="3">
Abstract: Most neural networks utilize the same amount of compute for every example independent of the inherent complexity of the input. Further, methods that adapt the amount of computation to the example focus on finding a fixed inference-time computational graph per example, ignoring any external computational budgets or varying inference time limitations. In this work, we utilize conditional computation to make neural sequence models (Transformer) more efficient and computation-aware during inference. We first modify the Transformer architecture, making each set of operations conditionally executable depending on the output of a learned control network. We then train this model in a multi-task setting, where each task corresponds to a particular computation budget. This allows us to train a single model that can be controlled to operate on different points of the computation-quality trade-off curve, depending on the available computation budget at inference time. We evaluate our approach on two tasks: (i) WMT English-French Translation and (ii) Unsupervised representation learning (BERT). Our experiments demonstrate that the proposed Conditional Computation Transformer (CCT) is competitive with vanilla Transformers when allowed to utilize its full computational budget, while improving significantly over computationally equivalent baselines when operating on smaller computational budgets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数神经网络利用计算的相同量的每一个独立于输入的固有的复杂性的一个例子。此外，该方法适应的计算量的示例集中在寻找每例的固定推理时间的计算曲线图，忽略任何外部计算预算或变化的推理时间的限制。在这项工作中，我们利用的条件计算，使神经序列模型（变压器）更有效率和计算感知的推理过程。我们首先修改变压器的架构，使各组条件执行取决于学习型控制网络的输出操作。然后，我们培养这种模式在多任务环境，在这里每个任务对应一个特定的计算预算。这让我们训练，可以控制对计算质量的权衡曲线的不同点进行操作，这取决于在推理时的可用计算预算的单一模式。我们评估我们的两个任务的方法：（I）WMT英语法语翻译及（ii）无监督学习的表示（BERT）。我们的实验证明，允许利用其全面的计算预算的时候，而在较小的计算预算工作时提高显著在计算等效基准所提出的条件计算变压器（CCT）是香草变形金刚竞争力。</font>
</div>


<hr>
<div id="paper17"> <b>17. Computing rank-revealing factorizations of matrices stored out-of-core</b>  <a href="https://arxiv.org/pdf/2002.06960" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Heavner%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nathan Heavner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martinsson%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Per-Gunnar Martinsson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Quintana-Ort%C3%AD%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gregorio Quintana-Ortí</a><br>
<font size="3">
Abstract: This paper describes efficient algorithms for computing rank-revealing factorizations of matrices that are too large to fit in RAM, and must instead be stored on slow external memory devices such as solid-state or spinning disk hard drives (out-of-core or out-of-memory). Traditional algorithms for computing rank revealing factorizations, such as the column pivoted QR factorization, or techniques for computing a full singular value decomposition of a matrix, are very communication intensive. They are naturally expressed as a sequence of matrix-vector operations, which become prohibitively expensive when data is not available in main memory. Randomization allows these methods to be reformulated so that large contiguous blocks of the matrix can be processed in bulk. The paper describes two distinct methods. The first is a blocked version of column pivoted Householder QR, organized as a ``left-looking'' method to minimize the number of write operations (which are more expensive than read operations on a spinning disk drive). The second method results in a so called UTV factorization which expresses a matrix $A$ as $A = U T V^*$ where $U$ and $V$ are unitary, and $T$ is triangular. This method is organized as an algorithm-by-blocks, in which floating point operations overlap read and write operations. The second method incorporates power iterations, and is exceptionally good at revealing the numerical rank; it can often be used as a substitute for a full singular value decomposition. Numerical experiments demonstrate that the new algorithms are almost as fast when processing data stored on a hard drive as traditional algorithms are for data stored in main memory. To be precise, the computational time for fully factorizing an $n\times n$ matrix scales as $cn^{3}$, with a scaling constant $c$ that is only marginally larger when the matrix is stored out of core. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文描述了一种用于计算太大，以适应在RAM矩阵的秩揭示因式分解有效的算法，而必须被存储在慢速外部存储设备，诸如固态或旋转磁盘的硬盘驱动器（外的芯或外的存储器）。用于计算秩揭示因式分解，如柱枢转QR分解，或技术来计算矩阵的完整奇异值分解传统算法，都非常通信密集型的。它们天然表达为矩阵向量运算，当数据不在主存储器中可用而变得昂贵的序列。随机化允许重新这些方法使得矩阵的大的连续块可以批量进行处理。本文介绍了两种截然不同的方法。首先是柱的封端的枢转版本的Householder QR，组织为``左视“”的方法以最小化的写入操作的数目（其比纺丝磁盘驱动器上的读操作更贵）。第二种方法导致所谓的UTV因式分解其表达矩阵$ A $为$ A = UŤV ^ * $其中$ U $ $和V $是酉，和$ T $为三角形。该方法被组织为一个算法逐块，其中浮点操作重叠读取和写入操作。第二种方法采用了功率的迭代，并且是在揭示数值秩特别好;它通常可以用来作为一个完整的奇异值分解的替代品。数值试验表明，新的算法几乎一样快，传统的算法用于数据存储在主存储器对存储在硬盘驱动器上的数据时。确切地说，对于完全因式分解的$ N \ n次$矩阵尺度为$ CN ^ {3} $，具有缩放常数$ C $当基质芯的存储指出，只是勉强越大计算时间。</font>
</div>


<hr>
<div id="paper18"> <b>18. Supervised Phrase-boundary Embeddings</b>  <a href="https://arxiv.org/pdf/2002.06450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manni Singh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weston%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Weston</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Levene%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mark Levene</a><br>
<font size="3">
Abstract: We propose a new word embedding model, called SPhrase, that incorporates supervised phrase information. Our method modifies traditional word embeddings by ensuring that all target words in a phrase have exactly the same context. We demonstrate that including this information within a context window produces superior embeddings for both intrinsic evaluation tasks and downstream extrinsic tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了一个新词嵌入模型，称为SPhrase，并入监督短语信息。我们的方法通过确保在一个短语中所有目标词具有完全相同的情况下修改了传统的文字的嵌入。我们证明包括上下文窗口内的这些信息包括内在评价任务和下游外在任务产生高质量的嵌入。</font>
</div>


<hr>
<div id="paper19"> <b>19. Open Knowledge Enrichment for Long-tail Entities</b>  <a href="https://arxiv.org/pdf/2002.06397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ermei Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Difeng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiacheng Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Hu</a><br>
<font size="3">
Abstract: Knowledge bases (KBs) have gradually become a valuable asset for many AI applications. While many current KBs are quite large, they are widely acknowledged as incomplete, especially lacking facts of long-tail entities, e.g., less famous persons. Existing approaches enrich KBs mainly on completing missing links or filling missing values. However, they only tackle a part of the enrichment problem and lack specific considerations regarding long-tail entities. In this paper, we propose a full-fledged approach to knowledge enrichment, which predicts missing properties and infers true facts of long-tail entities from the open Web. Prior knowledge from popular entities is leveraged to improve every enrichment step. Our experiments on the synthetic and real-world datasets and comparison with related work demonstrate the feasibility and superiority of the approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：知识库（KBS）已逐渐成为许多AI应用的宝贵财富。虽然目前许多KB的都相当大，他们被公认为是不完整的，特别是缺乏长尾实体的事实，例如，名气不大的人。现有的方法主要富集在完成缺少的环节或填充缺失值KB的。然而，他们只是解决问题的富集的一部分，并没有关于长尾实体的具体考虑。在本文中，我们提出了一个全面的方法来充实知识，这预示缺少的属性和推断从开放的网络长尾实体的事实真相。从流行实体的先验知识利用，以提高每个富集步骤。我们对合成和真实世界的数据集，并与相关工作的比较实验证明了该方法的可行性和优越性。</font>
</div>


<hr>
<div id="paper20"> <b>20. UniViLM: A Unified Video and Language Pre-Training Model for Multimodal  Understanding and Generation</b>  <a href="https://arxiv.org/pdf/2002.06353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huaishao Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Botian Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoyang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianrui Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xilin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: We propose UniViLM: a Unified Video and Language pre-training Model for multimodal understanding and generation. Motivated by the recent success of BERT based pre-training technique for NLP and image-language tasks, VideoBERT and CBT are proposed to exploit BERT model for video and language pre-training using narrated instructional videos. Different from their works which only pre-train understanding task, we propose a unified video-language pre-training model for both understanding and generation tasks. Our model comprises of 4 components including two single-modal encoders, a cross encoder and a decoder with the Transformer backbone. We first pre-train our model to learn the universal representation for both video and language on a large instructional video dataset. Then we fine-tune the model on two multimodal tasks including understanding task (text-based video retrieval) and generation task (multimodal video captioning). Our extensive experiments show that our method can improve the performance of both understanding and generation tasks and achieves the state-of-the art results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出UniViLM：一个统一的视频和语言前的训练模式的多模态的理解和生成。受近期对NLP和图像语言任务，VideoBERT和CBT基于BERT前培训技术的成功激励提出了利用使用讲述了教学视频，视频和语言训练前BERT模式。从他们的作品只预列车任务的理解不同，我们提出了两种理解和生成任务统一视频语言前的训练模式。我们的模型包括4个组分，包括两个单峰编码器，一个编码器的交叉，并与变压器主链的解码器的。我们先预先训练我们的模型来学习的视频和语言上大量的教学视频数据集中的普遍代表性。然后，我们微调在两个多任务，其中包括理解任务（基于文本的视频检索）和生成任务（多视频字幕）模型。我们广泛的实验表明，我们的方法可以提高双方的理解和生成任务的性能和实现国家的艺术效果。</font>
</div>


<hr>
<div id="paper21"> <b>21. Semantic Relatedness and Taxonomic Word Embeddings</b>  <a href="https://arxiv.org/pdf/2002.06235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kacmajor%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Magdalena Kacmajor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kelleher%2C+J+D" target="_blank" rel="noopener" style="color:#0000EE;">John D. Kelleher</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Klubicka%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Filip Klubicka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Maldonado%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alfredo Maldonado</a><br>
<font size="3">
Abstract: This paper connects a series of papers dealing with taxonomic word embeddings. It begins by noting that there are different types of semantic relatedness and that different lexical representations encode different forms of relatedness. A particularly important distinction within semantic relatedness is that of thematic versus taxonomic relatedness. Next, we present a number of experiments that analyse taxonomic embeddings that have been trained on a synthetic corpus that has been generated via a random walk over a taxonomy. These experiments demonstrate how the properties of the synthetic corpus, such as the percentage of rare words, are affected by the shape of the knowledge graph the corpus is generated from. Finally, we explore the interactions between the relative sizes of natural and synthetic corpora on the performance of embeddings when taxonomic and thematic embeddings are combined. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文连接一系列的处理分类字的嵌入文件。它首先指出，有不同类型的语义关联的，并且不同的词汇表示编码不同形式的相关性。语义关联中特别重要的区别是，专题与分类相关性。接下来，我们提出了一定数量的分析已经培训了已经通过了一个分类随机游走产生的合成语料库分类的嵌入实验。这些实验证明如何合成语料库的性能，如稀有字的百分比，通过从生成的语料库的知识库图形的形状的影响。最后，我们探讨的嵌入性能的天然和合成语料库的相对大小之间的相互作用时，分类和专题的嵌入结合。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-17</title>
    <url>/2020/02/17/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-17/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Purifying Real Images with an Attention-guided Style Transfer Network  for Gaze Estimation <a href="https://arxiv.org/pdf/2002.06145" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Combining Visual and Textual Features for Semantic Segmentation of  Historical Newspapers <a href="https://arxiv.org/pdf/2002.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Layer-wise Pruning and Auto-tuning of Layer-wise Learning Rates in  Fine-tuning of Deep Networks <a href="https://arxiv.org/pdf/2002.06048" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Constrained Dominant sets and Its applications in computer vision <a href="https://arxiv.org/pdf/2002.06028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Building Networks for Image Segmentation using Particle Competition and  Cooperation <a href="https://arxiv.org/pdf/2002.06001" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> A Hybrid 3DCNN and 3DC-LSTM based model for 4D Spatio-temporal fMRI  data: An ABIDE Autism Classification study <a href="https://arxiv.org/pdf/2002.05981" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Context Conditional Variational Autoencoder for Predicting Multi-Path  Trajectories in Mixed Traffic <a href="https://arxiv.org/pdf/2002.05966" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Multi-Level Feature Fusion Mechanism for Single Image Super-Resolution <a href="https://arxiv.org/pdf/2002.05962" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Counting dense objects in remote sensing images <a href="https://arxiv.org/pdf/2002.05928" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> End-to-end Learning of Object Motion Estimation from Retinal Events for  Event-based Object Tracking <a href="https://arxiv.org/pdf/2002.05911" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> A Survey on 3D Skeleton-Based Action Recognition Using Learning Method <a href="https://arxiv.org/pdf/2002.05907" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Liver Segmentation in Abdominal CT Images via Auto-Context Neural  Network and Self-Supervised Contour Attention <a href="https://arxiv.org/pdf/2002.05895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset <a href="https://arxiv.org/pdf/2002.05878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Skip Connections Matter: On the Transferability of Adversarial Examples  Generated with ResNets <a href="https://arxiv.org/pdf/2002.05990" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> SemI2I: Semantically Consistent Image-to-Image Translation for Domain  Adaptation of Remote Sensing Data <a href="https://arxiv.org/pdf/2002.05925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Remove Appearance Shift for Ultrasound Image Segmentation via Fast and  Universal Style Transfer <a href="https://arxiv.org/pdf/2002.05844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Variational Conditional-Dependence Hidden Markov Models for Human Action  Recognition <a href="https://arxiv.org/pdf/2002.05809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> ACEnet: Anatomical Context-Encoding Network for Neuroanatomy  Segmentation <a href="https://arxiv.org/pdf/2002.05773" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Purifying Real Images with an Attention-guided Style Transfer Network  for Gaze Estimation</b>  <a href="https://arxiv.org/pdf/2002.06145" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxiao Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinjia Peng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huibing Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xianping Fu</a><br>
<font size="3">
Abstract: Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distribution of synthetic images compared to real images, the desired performance cannot be achieved. Real images consist of multiple forms of light orientation, while synthetic images consist of a uniform light orientation. These features are considered to be characteristic of outdoor and indoor scenes, respectively. To solve this problem, the previous method learned a model to improve the realism of the synthetic image. Different from the previous methods, this paper try to purify real image by extracting discriminative and robust features to convert outdoor real images to indoor synthetic images. In this paper, we first introduce the segmentation masks to construct RGB-mask pairs as inputs, then we design a attention-guided style transfer network to learn style features separately from the attention and bkgd(background) region , learn content features from full and attention region. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using mixed studies (qualitative and quantitative) methods to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on three public datasets, including LPW, COCO and MPIIGaze. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，边学边合成的进步提出了合成图像的人才培养模式，可有效降低人力和物力成本。然而，由于不同的分布相比，实际图像的合成图像，无法实现期望的性能。真实图像包括的光取向多种形式，而合成图像由一个均匀的光取向的。这些特性被认为是室内和室外场景的特点，分别。为了解决这个问题，以前的方法学到了模式，提高合成图像的真实感。从以前的方法不同，本文尝试通过提取歧视和强大的功能，真正的户外图像转换为室内合成图像净化真实图像。在本文中，我们首先介绍的分割掩码构建RGB-掩模对作为输入，然后我们设计了注意力引导式传送网络学习风格从关注和BKGD（背景）区分开的特征，从全学习内容的特征和关注区域。此外，我们提出了一个新的区域级任务引导损失抑制从风格和内容学习的特点。实验使用混合研究（定性和定量）方法来证明在复杂的方向净化真实图像的可能性进行。我们评估三个公共数据集，包括LPW，COCO和MPIIGaze所提出的方法。广泛的实验结果表明，所提出的方法是有效的和达到状态的最先进的结果。</font>
</div>


<hr>
<div id="paper2"> <b>2. Combining Visual and Textual Features for Semantic Segmentation of  Historical Newspapers</b>  <a href="https://arxiv.org/pdf/2002.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Barman%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raphaël Barman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ehrmann%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maud Ehrmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clematide%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Clematide</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oliveira%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Sofia Ares Oliveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaplan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Frédéric Kaplan</a><br>
<font size="3">
Abstract: The massive amounts of digitized historical documents acquired over the last decades naturally lend themselves to automatic processing and exploration. Research work seeking to automatically process facsimiles and extract information thereby are multiplying with, as a first essential step, document layout analysis. If the identification and categorization of segments of interest in document images have seen significant progress over the last years thanks to deep learning techniques, many challenges remain with, among others, the use of finer-grained segmentation typologies and the consideration of complex, heterogeneous documents such as historical newspapers. Besides, most approaches consider visual features only, ignoring textual signal. In this context, we introduce a multimodal approach for the semantic segmentation of historical newspapers that combines visual and textual features. Based on a series of experiments on diachronic Swiss and Luxembourgish newspapers, we investigate, among others, the predictive power of visual and textual features and their capacity to generalize across time and sources. Results show consistent improvement of multimodal models in comparison to a strong visual baseline, as well as better robustness to high material variance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在过去的十年中取得数字化的历史记录了大量的自然借给自己自动处理和探索。研究工作寻求自动处理传真和提取信息，从而与倍增，作为第一个重要步骤，文档布局分析。如果识别和文件图像的兴趣细分的分类已经看到过去几年中由于深学习技术了显著的进步，许多挑战仍然存在，等等，使用细粒度分割类型学和考虑复杂的异构文件如历史报纸。此外，大多数的方法只考虑视觉特征，忽略文本信号。在这方面，我们引入历史报纸的语义分割，结合视觉和文本特征的多模态的方法。基于一系列关于历时瑞士和卢森堡报纸实验，调查，除其他外，视觉和文字特征的预测能力和他们的能力，以跨越时间和来源一概而论。结果表明，比较多车型的持续改进，以强烈的视觉底线，以及更好的鲁棒性高的材料差异。</font>
</div>


<hr>
<div id="paper3"> <b>3. Layer-wise Pruning and Auto-tuning of Layer-wise Learning Rates in  Fine-tuning of Deep Networks</b>  <a href="https://arxiv.org/pdf/2002.06048" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ro%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youngmin Ro</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Jin Young Choi</a><br>
<font size="3">
Abstract: Existing fine-tuning methods use a single learning rate over all layers. In this paper, first, we discuss that trends of layer-wise weight variations by fine-tuning using a single learning rate do not match the well-known notion that lower-level layers extract general features and higher-level layers extract specific features. Based on our discussion, we propose an algorithm that improves fine-tuning performance and reduces network complexity through layer-wise pruning and auto-tuning of layer-wise learning rates. Through in-depth experiments on image retrieval (CUB-200-2011, Stanford online products, and Inshop) and fine-grained classification (Stanford cars, Aircraft) datasets, the effectiveness of the proposed algorithm is verified. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有的微调方法使用在所有层的单个学习率。在本文中，第一，我们使用单个学习速率不匹配的公知的概念，即较低级的层提取一般特征和更高级别的层提取特定特征讨论逐层重量变化通过微调该趋势。根据我们的讨论，我们提出改进微调性能，并通过逐层修剪和逐层学习速率自动调整降低了网络复杂性的算法。通过对图像检索进行了深入的实验（CUB-200-2011，斯坦福在线产品和Inshop）和细粒度分类（斯坦福汽车，飞机）的数据集，该算法的有效性进行验证。</font>
</div>


<hr>
<div id="paper4"> <b>4. Constrained Dominant sets and Its applications in computer vision</b>  <a href="https://arxiv.org/pdf/2002.06028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tesfaye%2C+A+L" target="_blank" rel="noopener" style="color:#0000EE;">Alemu Leulseged Tesfaye</a><br>
<font size="3">
Abstract: In this thesis, we present new schemes which leverage a constrained clustering method to solve several computer vision tasks ranging from image retrieval, image segmentation and co-segmentation, to person re-identification. In the last decades clustering methods have played a vital role in computer vision applications; herein, we focus on the extension, reformulation, and integration of a well-known graph and game theoretic clustering method known as Dominant Sets. Thus, we have demonstrated the validity of the proposed methods with extensive experiments which are conducted on several benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出新的方案，其杠杆率约束的聚类方法来解决几个计算机视觉任务，从图像检索，图像分割和共同分割，以人重新鉴定。在聚类方法在过去几十年都起到了计算机视觉应用至关重要的作用;本文中，我们侧重于延伸，再形成，以及一个公知的图形和博弈论聚类方法称为显性集的整合。因此，我们已经证明了广泛的实验所提出的方法被几个基准数据集进行的有效性。</font>
</div>


<hr>
<div id="paper5"> <b>5. Building Networks for Image Segmentation using Particle Competition and  Cooperation</b>  <a href="https://arxiv.org/pdf/2002.06001" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Breve%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fabricio Breve</a><br>
<font size="3">
Abstract: Particle competition and cooperation (PCC) is a graph-based semi-supervised learning approach. When PCC is applied to interactive image segmentation tasks, pixels are converted into network nodes, and each node is connected to its k-nearest neighbors, according to the distance between a set of features extracted from the image. Building a proper network to feed PCC is crucial to achieve good segmentation results. However, some features may be more important than others to identify the segments, depending on the characteristics of the image to be segmented. In this paper, an index to evaluate candidate networks is proposed. Thus, building the network becomes a problem of optimizing some feature weights based on the proposed index. Computer simulations are performed on some real-world images from the Microsoft GrabCut database, and the segmentation results related in this paper show the effectiveness of the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：粒子竞争与合作（PCC）是一种基于图的半监督学习方法。当PCC被施加到交互式图像分割任务，像素被转换成的网络节点，并且每个节点被连接到它的k-最近邻，根据一组从图像中提取特征之间的距离。建立一个适当的网络饲料PCC是实现良好的分割效果的关键。然而，比其他人来识别段，根据图像的特性来被分段一些特征可能是更重要的。在本文中，来评估候选网络的指数建议。因此，建设网络成为优化基础上，提出了指数一些要素权重的问题。计算机模拟从微软GrabCut数据库中的一些真实世界的影像进行，在本文中涉及的分割结果证明了该方法的有效性。</font>
</div>


<hr>
<div id="paper6"> <b>6. A Hybrid 3DCNN and 3DC-LSTM based model for 4D Spatio-temporal fMRI  data: An ABIDE Autism Classification study</b>  <a href="https://arxiv.org/pdf/2002.05981" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=El-Gazzar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmed El-Gazzar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Quaak%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mirjam Quaak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cerliani%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leonardo Cerliani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bloem%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Bloem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=van+Wingen%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guido van Wingen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+R+M" target="_blank" rel="noopener" style="color:#0000EE;">Rajat Mani Thomas</a><br>
<font size="3">
Abstract: Functional Magnetic Resonance Imaging (fMRI) captures the temporal dynamics of neural activity as a function of spatial location in the brain. Thus, fMRI scans are represented as 4-Dimensional (3-space + 1-time) tensors. And it is widely believed that the spatio-temporal patterns in fMRI manifests as behaviour and clinical symptoms. Because of the high dimensionality ($\sim$ 1 Million) of fMRI, and the added constraints of limited cardinality of data sets, extracting such patterns are challenging. A standard approach to overcome these hurdles is to reduce the dimensionality of the data by either summarizing activation over time or space at the expense of possible loss of useful information. Here, we introduce an end-to-end algorithm capable of extracting spatiotemporal features from the full 4-D data using 3-D CNNs and 3-D Convolutional LSTMs. We evaluate our proposed model on the publicly available ABIDE dataset to demonstrate the capability of our model to classify Autism Spectrum Disorder (ASD) from resting-state fMRI data. Our results show that the proposed model achieves state of the art results on single sites with F1-scores of 0.78 and 0.7 on NYU and UM sites, respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：功能性磁共振成像（fMRI）技术捕获的空间位置在大脑功能的神经活动的时间动态。因此，功能磁共振成像扫描被表示为4维（3-空间+ 1时）张量。而人们普遍认为，在功能磁共振成像表现为行为和临床症状的时空格局。因为高维数（$ \ SIM $ 100万）的fMRI的，和数据集的基数有限的附加约束，提取这种图案是具有挑战性。克服这些障碍的标准方法是通过任一总结激活随时间或空间，以减少数据的维度在有用的信息可能丢失的费用。在这里，介绍一种能够使用3-d细胞神经网络和3-d卷积LSTMs全4-d数据提取时空特征的端至端的算法。我们评估的可公开获得的数据集遵守我们提出的模型，以展示我们的模型，以泛自闭症障碍（ASD），从静止状态fMRI数据进行分类的能力。我们的研究结果表明，该模型实现了对单个站点，分别为0.78和0.7的纽约大学和UM网站，F1分数艺术成果的状态。</font>
</div>


<hr>
<div id="paper7"> <b>7. Context Conditional Variational Autoencoder for Predicting Multi-Path  Trajectories in Mixed Traffic</b>  <a href="https://arxiv.org/pdf/2002.05966" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W+L+M+Y" target="_blank" rel="noopener" style="color:#0000EE;">Wentong Liao. Michael Ying Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sester%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Monica Sester</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosenhahn%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bodo Rosenhahn</a><br>
<font size="3">
Abstract: Trajectory prediction in urban mixed-traffic zones is critical for many AI systems, such as traffic management, social robots and autonomous driving.However, there are many challenges to predict the trajectories of heterogeneous road agents (pedestrians, cyclists and vehicles) at a microscopic-level. For example, an agent might be able to choose multiple plausible paths in complex interactions with other agents in varying environments. To this end, we propose an approach named Context Conditional Variational Autoencoder (CCVAE) that encodes both past and future scene context, interaction context and motion information to capture the variations of the future trajectories using a set of stochastic latent variables. We predict multi-path trajectories conditioned on past information of the target agent by sampling the latent variable multiple times. Through experiments on several datasets of varying scenes, our method outperforms the recent state-of-the-art methods for mixed traffic trajectory prediction by a large margin and more robust in a very challenging environment. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在城市混合交通区轨迹预测是许多AI系统，如交通管理，社会机器人和自主driving.However，有预测异质道路剂（行人，自行车和汽车）的的轨迹许多挑战的关键微观层面。例如，一个代理可能能够选择与在不同环境中的其他代理复杂交互的多个合理的路径。为此，我们提出了一个名为语境条件变自动编码器（CCVAE）编码过去和将来的景物情境，互动情境和运动信息采集使用一组随机潜在变量的未来轨迹的变化的方法。我们预测多路径轨迹通过潜变量多次取样空调以往的目标代理的信息。通过对不同场景的几个数据集实验，我们的方法优于近期的大幅度和一个非常具有挑战性的环境中更稳健的国家的最先进的方法混合交通轨迹预测。</font>
</div>


<hr>
<div id="paper8"> <b>8. Multi-Level Feature Fusion Mechanism for Single Image Super-Resolution</b>  <a href="https://arxiv.org/pdf/2002.05962" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lyn%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiawen Lyn</a><br>
<font size="3">
Abstract: Convolution neural network (CNN) has been widely used in Single Image Super Resolution (SISR) so that SISR has been a great success recently. As the network deepens, the learning ability of network becomes more and more powerful. However, most SISR methods based on CNN do not make full use of hierarchical feature and the learning ability of network. These features cannot be extracted directly by subsequent layers, so the previous layer hierarchical information has little impact on the output and performance of subsequent layers relatively poor. To solve above problem, a novel Multi-Level Feature Fusion network (MLRN) is proposed, which can take full use of global intermediate features. We also introduce Feature Skip Fusion Block (FSFblock) as basic module. Each block can be extracted directly to the raw multiscale feature and fusion multi-level feature, then learn feature spatial correlation. The correlation among the features of the holistic approach leads to a continuous global memory of information mechanism. Extensive experiments on public datasets show that the method proposed by MLRN can be implemented, which is favorable performance for the most advanced methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（CNN）已被广泛应用于单图像超分辨率（SISR），以便SISR取得了很大的成功最近。随着网络的深入，网络的学习能力变得越来越强大。然而，根据CNN最SISR方法没有充分利用分层特征和网络的学习能力。这些功能不能由随后的层直接提取，所以先前层的分层信息对输出和后续层相对较差的性能的影响很小。为了解决上述问题，一种新颖的多级特征融合网络（MLRN）提出了一种能充分利用全球中间特性。我们还引进功能跳过融合块（FSFblock）为基本模块。每个块可以被直接提取到的原始多尺度特征和融合多层次特征，然后学习特征空间相关性。整体性方法导致的功能当中的信息机制，连续全局内存的相关性。公共数据集大量的实验表明，MLRN提出的方法可以实现，这是最先进的方法良好的性能。</font>
</div>


<hr>
<div id="paper9"> <b>9. Counting dense objects in remote sensing images</b>  <a href="https://arxiv.org/pdf/2002.05928" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guangshuai Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingjie Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunhong Wang</a><br>
<font size="3">
Abstract: Estimating accurate number of interested objects from a given image is a challenging yet important task. Significant efforts have been made to address this problem and achieve great progress, yet counting number of ground objects from remote sensing images is barely studied. In this paper, we are interested in counting dense objects from remote sensing images. Compared with object counting in natural scene, this task is challenging in following factors: large scale variation, complex cluttered background and orientation arbitrariness. More importantly, the scarcity of data severely limits the development of research in this field. To address these issues, we first construct a large-scale object counting dataset based on remote sensing images, which contains four kinds of objects: buildings, crowded ships in harbor, large-vehicles and small-vehicles in parking lot. We then benchmark the dataset by designing a novel neural network which can generate density map of an input image. The proposed network consists of three parts namely convolution block attention module (CBAM), scale pyramid module (SPM) and deformable convolution module (DCM). Experiments on the proposed dataset and comparisons with state of the art methods demonstrate the challenging of the proposed dataset, and superiority and effectiveness of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从给定的图像估计感兴趣对象的准确数字是一个挑战而重要的任务。显著已作出努力来解决这个问题，并取得了长足的进步，但是从遥感影像地面物体的计数值几乎没有影响。在本文中，我们感兴趣的是从遥感图像计数密集的对象。大规模的变化，复杂的复杂背景和方向随意性：在自然场景对象计数相比，这个任务是在以下因素的挑战。更重要的是，数据的缺乏严重限制了该领域研究的发展。为了解决这些问题，我们首先建立基于遥感影像，其包含四个类型的对象的大型对象计数数据集：建筑，拥挤的船只在港口，大型车和小型车的停车场。然后，我们的基准通过设计其可产生输入图像的密度图的新的神经网络的数据集。所提出的网络由三个部分组成，即卷积块注意模块（CBAM），规模金字塔模块（SPM）和可变形卷积模块（DCM）的。上提出的数据集实验和比较用的现有技术的方法证明了该数据集的挑战，而我们的方法的优越性和有效性。</font>
</div>


<hr>
<div id="paper10"> <b>10. End-to-end Learning of Object Motion Estimation from Retinal Events for  Event-based Object Tracking</b>  <a href="https://arxiv.org/pdf/2002.05911" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haosheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Suter%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Suter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiangqiang Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanzi Wang</a><br>
<font size="3">
Abstract: Event cameras, which are asynchronous bio-inspired vision sensors, have shown great potential in computer vision and artificial intelligence. However, the application of event cameras to object-level motion estimation or tracking is still in its infancy. The main idea behind this work is to propose a novel deep neural network to learn and regress a parametric object-level motion/transform model for event-based object tracking. To achieve this goal, we propose a synchronous Time-Surface with Linear Time Decay (TSLTD) representation, which effectively encodes the spatio-temporal information of asynchronous retinal events into TSLTD frames with clear motion patterns. We feed the sequence of TSLTD frames to a novel Retinal Motion Regression Network (RMRNet) to perform an end-to-end 5-DoF object motion regression. Our method is compared with state-of-the-art object tracking methods, that are based on conventional cameras or event cameras. The experimental results show the superiority of our method in handling various challenging environments such as fast motion and low illumination conditions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事件相机，这是异步的仿生视觉传感器，显示在计算机视觉和人工智能的巨大潜力。然而，事件摄像机对象级运动估计或跟踪的应用仍处于初级阶段。这背后工作的主要思想是提出一种新颖的深层神经网络学习和退步的参数对象级运动/变换模型基于事件的对象跟踪。为了实现这个目标，提出了一种同步时间曲面的线性时间衰减（TSLTD）表示，这有效地编码异步事件视网膜与清晰的运动模式TSLTD帧的时空信息。我们从进料TSLTD帧序列，以一种新颖的视网膜运动回归网络（RMRNet）来执行一个终端到终端的五自由度对象运动消退。我们的方法是与国家的最先进的物体跟踪方法中，是基于传统摄像机或事件相机相比。实验结果表明我们在处理各种挑战性的环境，诸如快速运动和低光照条件的方法的优越性。</font>
</div>


<hr>
<div id="paper11"> <b>11. A Survey on 3D Skeleton-Based Action Recognition Using Learning Method</b>  <a href="https://arxiv.org/pdf/2002.05907" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mengyuan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Runwei Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hong Liu</a><br>
<font size="3">
Abstract: 3D skeleton-based action recognition, owing to the latent advantages of skeleton, has been an active topic in computer vision. As a consequence, there are lots of impressive works including conventional handcraft feature based and learned feature based have been done over the years. However, previous surveys about action recognition mostly focus on the video or RGB data dominated methods, and the scanty existing reviews related to skeleton data mainly indicate the representation of skeleton data or performance of some classic techniques on a certain dataset. Besides, though deep learning methods has been applied to this field for years, there is no related reserach concern about an introduction or review from the perspective of deep learning architectures. To break those limitations, this survey firstly highlight the necessity of action recognition and the significance of 3D-skeleton data. Then a comprehensive introduction about Recurrent Neural Network(RNN)-based, Convolutional Neural Network(CNN)-based and Graph Convolutional Network(GCN)-based main stream action recognition techniques are illustrated in a data-driven manner. Finally, we give a brief talk about the biggest 3D skeleton dataset NTU-RGB+D and its new edition called NTU-RGB+D 120, accompanied with several existing top rank algorithms within those two datasets. To our best knowledge, this is the first research which give an overall discussion over deep learning-based action recognitin using 3D skeleton data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于骨架的3D动作识别，由于骨骼的潜在优势，一直是计算机视觉活跃的课题。因此，有很多令人印象深刻的作品，包括和基于学习的特征已多年来做传统手工功能。然而，关于动作识别之前的调查主要集中在视频和RGB数据为主的方法，和现有相关的骨架数据的评论寥寥无几，主要显示骨架数据或对某一数据集一些经典的技术性能的表现。此外，虽然深学习方法已应用到这个领域里，有来自深学习架构的角度介绍或审查没有相关启发式算法关注。为了打破这些限制，本次调查首先突出动作识别的必要性和3D骨架数据的意义。然后将约回归神经网络（RNN）为主，卷积神经网络（CNN）的全面介绍的基于图形和卷积网络（GCN）基主流动作识别技术在数据驱动的方式示出。最后，我们给出一个简单说说最大的三维骨骼数据集NTU-RGB + d和它的新版本称为NTU-RGB + d 120，伴随着这两个数据集内的多个现有热门排名算法。据我们所知，这是这给使用的三维骨骼数据在深基础的学习行动recognitin全面讨论第一个研究。</font>
</div>


<hr>
<div id="paper12"> <b>12. Liver Segmentation in Abdominal CT Images via Auto-Context Neural  Network and Self-Supervised Contour Attention</b>  <a href="https://arxiv.org/pdf/2002.05895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chung%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minyoung Chung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingyu Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeongjin Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yeong-Gil Shin</a><br>
<font size="3">
Abstract: Accurate image segmentation of the liver is a challenging problem owing to its large shape variability and unclear boundaries. Although the applications of fully convolutional neural networks (CNNs) have shown groundbreaking results, limited studies have focused on the performance of generalization. In this study, we introduce a CNN for liver segmentation on abdominal computed tomography (CT) images that shows high generalization performance and accuracy. To improve the generalization performance, we initially propose an auto-context algorithm in a single CNN. The proposed auto-context neural network exploits an effective high-level residual estimation to obtain the shape prior. Identical dual paths are effectively trained to represent mutual complementary features for an accurate posterior analysis of a liver. Further, we extend our network by employing a self-supervised contour scheme. We trained sparse contour features by penalizing the ground-truth contour to focus more contour attentions on the failures. The experimental results show that the proposed network results in better accuracy when compared to the state-of-the-art networks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CT images for training and validation. Two-fold cross-validation is presented for a comparison with the state-of-the-art neural networks. Novel multiple N-fold cross-validations are conducted to verify the performance of generalization. The proposed network showed the best generalization performance among the networks. Additionally, we present a series of ablation experiments that comprehensively support the importance of the underlying concepts. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：肝脏精确的图像分割是由于其较大的形状变化和界限不清一个具有挑战性的问题。虽然完全卷积神经网络（细胞神经网络）的应用已经显示开创性成果，有限的研究都集中在泛化的表现。在这项研究中，我们对腹部电脑断层扫描（CT）图像引入CNN肝分割昭示着高泛化性能和精度。为了提高推广能力，我们初步提出了在一个单一的CNN自动背景算法。所提出的自动上下文神经网络利用一个有效的高层次的剩余估计之前获得的形状。相同的双路径被有效地训练以表示用于肝脏的精确分析后相互互补的特征。此外，我们通过采用自监督轮廓方案我们的网络扩展。我们通过惩罚地面真轮廓专注于故障的详细轮廓重视培训的稀疏轮廓特征。实验结果表明，在更好的准确度所提出的网络的结果通过减少Hausdorff距离的10.31％，较先进的最先进的网络时。我们使用180幅腹部CT图像进行训练和验证。两折交叉验证呈现，用于与状态的最先进的神经网络的比较。新颖倍数N倍交叉验证是为了验证泛化的性能。所提出的网络显示网络中最好的泛化性能。此外，我们提出了一系列的消融实验证明，全面支持基本概念的重要性。</font>
</div>


<hr>
<div id="paper13"> <b>13. An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset</b>  <a href="https://arxiv.org/pdf/2002.05878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhicheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhihao Gu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Di%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuan Di</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rongye Shi</a><br>
<font size="3">
Abstract: The Waymo Open Dataset has been released recently, providing a platform to crowdsource some fundamental challenges for automated vehicles (AVs), such as 3D detection and tracking. While the dataset provides a large amount of high-quality and multi-source driving information, people in academia are more interested in the underlying driving policy programmed in Waymo self-driving cars, which is inaccessible due to AV manufacturers' proprietary protection. Accordingly, academic researchers have to make various assumptions to implement AV components in their models or simulations, which may not represent the realistic interactions in real-world traffic. Thus, this paper introduces an approach to learn an long short-term memory (LSTM)-based model for imitating the behavior of Waymo's self-driving model. The proposed model has been evaluated based on Mean Absolute Error (MAE). The experimental results show that our model outperforms several baseline models in driving action prediction. Also, a visualization tool is presented for verifying the performance of the model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：Waymo打开的数据集已经发布最近，提供一个平台，以众包的自动车（AVS），如3D检测和跟踪一些根本性的挑战。而该数据集提供了大量的高品质和多源驾驶信息，学术界人士更关心的是Waymo自动驾驶汽车编程的基本推动政策，这是人迹罕至，由于AV厂商的专有保护对策研究。因此，学术研究人员不得不做出各种假设以实现他们的模型或模拟AV设备，这可能不能代表现实世界的交通现实的互动。因此，本文介绍了学习的长短期记忆（LSTM）为基础的模型模仿Waymo的自驾车模型的行为的方法。基于平均绝对误差（MAE），该模型已被评估。实验结果表明，该模型优于几个基本模式，在驾驶行动的预测。此外，可视化工具提出了用于验证模型的性能。</font>
</div>


<hr>
<div id="paper14"> <b>14. Skip Connections Matter: On the Transferability of Adversarial Examples  Generated with ResNets</b>  <a href="https://arxiv.org/pdf/2002.05990" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongxian Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yisen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shu-Tao Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bailey%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Bailey</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xingjun Ma</a><br>
<font size="3">
Abstract: Skip connections are an essential component of current state-of-the-art deep neural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt. Despite their huge success in building deeper and more powerful DNNs, we identify a surprising security weakness of skip connections in this paper. Use of skip connections allows easier generation of highly transferable adversarial examples. Specifically, in ResNet-like (with skip connections) neural networks, gradients can backpropagate through either skip connections or residual modules. We find that using more gradients from the skip connections rather than the residual modules according to a decay factor, allows one to craft adversarial examples with high transferability. Our method is termed Skip Gradient Method(SGM). We conduct comprehensive transfer attacks against state-of-the-art DNNs including ResNets, DenseNets, Inceptions, Inception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trained DNNs. We show that employing SGM on the gradient flow can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, SGM can be easily combined with existing black-box attack techniques, and obtain high improvements over state-of-the-art transferability methods. Our findings not only motivate new research into the architectural vulnerability of DNNs, but also open up further challenges for the design of secure DNN architectures. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跳过连接是国家的最先进的电流深神经网络（DNNs）如RESNET，WideResNet，DenseNet，和ResNeXt的一个基本组成部分。尽管他们在建立更深入和更强大的DNNs巨大的成功，我们确定在本文中跳跃连接的一个令人惊讶的安全弱点。跳过连接的使用使得更容易产生高度对抗性转让的例子。具体而言，在RESNET状（具有跳跃连接）神经网络，梯度可backpropagate通过任跳过连接或残余模块。我们发现，使用更多的梯度从跳过连接，而不是根据衰减因子残留的模块，允许人们与手艺高转印对抗性例子。我们的方法被称为跳过梯度法（SGM）。我们开展反对国家的最先进的DNNs全面转让攻击，包括ResNets，DenseNets，Inceptions，成立之初，RESNET，挤压和激励网络（SENET）和稳健的培训DNNs。我们发现，采用SGM的梯度流动可以大大提高制作的攻击转让在几乎所有情况。此外，SGM能够容易地与现有的黑盒攻击技术组合，并且获得对国家的最先进的转印方法高的改进。我们的研究结果不仅激发新的研究DNNs的建筑的脆弱性，同时也开辟安全DNN架构的设计进一步的挑战。</font>
</div>


<hr>
<div id="paper15"> <b>15. SemI2I: Semantically Consistent Image-to-Image Translation for Domain  Adaptation of Remote Sensing Data</b>  <a href="https://arxiv.org/pdf/2002.05925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Tasar%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Onur Tasar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Happy%2C+S+L" target="_blank" rel="noopener" style="color:#0000EE;">S L Happy</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tarabalka%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuliya Tarabalka</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Alliez%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre Alliez</a><br>
<font size="3">
Abstract: Although convolutional neural networks have been proven to be an effective tool to generate high quality maps from remote sensing images, their performance significantly deteriorates when there exists a large domain shift between training and test data. To address this issue, we propose a new data augmentation approach that transfers the style of test data to training data using generative adversarial networks. Our semantic segmentation framework consists in first training a U-net from the real training data and then fine-tuning it on the test stylized fake training data generated by the proposed approach. Our experimental results prove that our framework outperforms the existing domain adaptation methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然卷积神经网络已经被证明是产生从遥感影像的高质量地图的有效工具，当存在训练和测试数据之间存在较大的领域转变他们的表现显著恶化。为了解决这个问题，我们提出了转会的测试数据的样式训练数据使用生成对抗性的网络新的数据增强方法。我们的语义分割框架由在第一次训练从真正的训练数据的U形网，然后进行精细调整，通过该方法生成的测试程式化假的训练数据。我们的实验结果证明我们的架构优于现有的域自适应方法。</font>
</div>


<hr>
<div id="paper16"> <b>16. Remove Appearance Shift for Ultrasound Image Segmentation via Fast and  Universal Style Transfer</b>  <a href="https://arxiv.org/pdf/2002.05844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhendong Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Yang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Gao%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Gao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shengfeng Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dou%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoran Dou</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=He%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuangchi He</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Huang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhao Huang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Huang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yankai Huang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Luo%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huanjia Luo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanji Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Xiong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi Xiong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ni%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dong Ni</a><br>
<font size="3">
Abstract: Deep Neural Networks (DNNs) suffer from the performance degradation when image appearance shift occurs, especially in ultrasound (US) image segmentation. In this paper, we propose a novel and intuitive framework to remove the appearance shift, and hence improve the generalization ability of DNNs. Our work has three highlights. First, we follow the spirit of universal style transfer to remove appearance shifts, which was not explored before for US images. Without sacrificing image structure details, it enables the arbitrary style-content transfer. Second, accelerated with Adaptive Instance Normalization block, our framework achieved real-time speed required in the clinical US scanning. Third, an efficient and effective style image selection strategy is proposed to ensure the target-style US image and testing content US image properly match each other. Experiments on two large US datasets demonstrate that our methods are superior to state-of-the-art methods on making DNNs robust against various appearance shifts. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络（DNNs）从性能下降时遭受图像外观发生偏移，尤其是在超声（US）图像分割。在本文中，我们提出了一种新颖的和直观的架构，消除外观移位，并因此提高DNNs的泛化能力。我们的工作有三大亮点。首先，我们按照通用的风格传递的精神，以除去外观的变化，这是以前没有的超声图像研究。在不牺牲图像结构的详细信息，它使任意的方式，内容传输。其次，与Adaptive实例标准化框加快，我们的框架实现在美国临床扫描所需的实时速度。第三，高效和有效的风格形象的选择策略，提出了确保目标式的美国形象和测试内容美国的形象正确地相互匹配。两个大型数据集美国实验证明我们的方法是优于国家的最先进的方法上做出DNNs对各种外观的变化稳健。</font>
</div>


<hr>
<div id="paper17"> <b>17. Variational Conditional-Dependence Hidden Markov Models for Human Action  Recognition</b>  <a href="https://arxiv.org/pdf/2002.05809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Panousis%2C+K+P" target="_blank" rel="noopener" style="color:#0000EE;">Konstantinos P. Panousis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chatzis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sotirios Chatzis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Theodoridis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergios Theodoridis</a><br>
<font size="3">
Abstract: Hidden Markov Models (HMMs) are a powerful generative approach for modeling sequential data and time-series in general. However, the commonly employed assumption of the dependence of the current time frame to a single or multiple immediately preceding frames is unrealistic; more complicated dynamics potentially exist in real world scenarios. Human Action Recognition constitutes such a scenario, and has attracted increased attention with the advent of low-cost 3D sensors. The naturally arising variations and complex temporal dependencies have established this task as a challenging problem in the community. This paper revisits conventional sequential modeling approaches, aiming to address the problem of capturing time-varying temporal dependency patterns. To this end, we propose a different formulation of HMMs, whereby the dependence on past frames is dynamically inferred from the data. Specifically, we introduce a hierarchical extension by postulating an additional latent variable layer; therein, the (time-varying) temporal dependence patterns are treated as latent variables over which inference is performed. We leverage solid arguments from the Variational Bayes framework and derive a tractable inference algorithm based on the forward-backward algorithm. As we experimentally show using benchmark datasets, our approach yields competitive recognition accuracy and can effectively handle data with missing values. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：隐马尔可夫模型（HMM）是用于一般模拟连续数据和时间序列的一个强大的生成方法。然而，当前时间帧的单个或多个紧接在前的帧的依赖性的通常使用的假设是不现实的;更复杂的动态潜在存在于真实世界的场景。人类行为识别构成这样的情景，并吸引具有低成本的3D传感器的出现越来越多的关注。自然产生的变化和复杂的时序依赖已经建立了这个任务，因为在社会上具有挑战性的问题。本文回访传统的顺序建模方法，旨在解决捕捉时间变化的时间依赖性模式的问题。为此，我们提出的HMM的不同的制剂，由此在过去的帧的依赖性被动态地从数据推断。具体来说，我们介绍通过假定一个附加潜变量层的分层扩展;在其中，所述（随时间变化）时间依赖性模式将被视为在其上执行推理潜变量。我们从变贝叶斯框架充分利用了坚实的论据并且基于向前向后的算法易处理推理算法。正如我们通过实验证明使用标准数据集，我们的方法产生有竞争力的识别精度和能有效地缺失值处理数据。</font>
</div>


<hr>
<div id="paper18"> <b>18. ACEnet: Anatomical Context-Encoding Network for Neuroanatomy  Segmentation</b>  <a href="https://arxiv.org/pdf/2002.05773" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuemeng Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongming Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yong Fan</a><br>
<font size="3">
Abstract: Segmentation of brain structures from magnetic resonance (MR) scans plays an important role in the quantification of brain morphology. Since 3D deep learning models suffer from high computational cost, 2D deep learning methods are favored for their computational efficiency. However, existing 2D deep learning methods are not equipped to effectively capture 3D spatial contextual information that is needed to achieve accurate brain structure segmentation. In order to overcome this limitation, we develop an Anatomical Context-Encoding Network (ACEnet) to incorporate 3D spatial and anatomical contexts in 2D convolutional neural networks (CNNs) for efficient and accurate segmentation of brain structures from MR scans, consisting of 1) an anatomical context encoding module to incorporate anatomical information in 2D CNNs, 2) a spatial context encoding module to integrate 3D image information in 2D CNNs, and 3) a skull stripping module to guide 2D CNNs to attend to the brain. Extensive experiments on three benchmark datasets have demonstrated that our method outperforms state-of-the-art alternative methods for brain structure segmentation in terms of both computational efficiency and segmentation accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从分割磁共振脑结构（MR）扫描起着大脑形态的量化具有重要作用。由于3D深度学习模型从高计算成本受到影响，2D深学习方法有利于他们的计算效率。然而，现有的2D深学习方法不具备有效地捕捉所需要实现精确的大脑结构分割三维空间的上下文信息。为了克服这种限制，我们开发的解剖上下文编码网络（ACENET）掺入三维空间和解剖上下文在2D卷积神经网络（细胞神经网络），用于从MR扫描脑结构的有效和准确的分割，包括1）一个解剖上下文编码模块纳入在2D细胞神经网络，2）空间上下文编码模块到3D图像信息中的2D细胞神经网络整合，以及3）一个头骨汽提模块引导2D细胞神经网络参加到大脑的解剖信息。三个基准数据集大量的实验已经证明，我们的方法优于国家的最先进的脑结构分割的替代方法在两个计算效率和分割准确度方面。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-17</title>
    <url>/2020/02/17/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-17/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Transformer on a Diet <a href="https://arxiv.org/pdf/2002.06170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base <a href="https://arxiv.org/pdf/2002.06115" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> FQuAD: French Question Answering Dataset <a href="https://arxiv.org/pdf/2002.06071" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Dialogue history integration into end-to-end signal-to-concept spoken  language understanding systems <a href="https://arxiv.org/pdf/2002.06012" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Integrating Discrete and Neural Features via Mixed-feature  Trans-dimensional Random Field Language Models <a href="https://arxiv.org/pdf/2002.05967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> A Data Efficient End-To-End Spoken Language Understanding Architecture <a href="https://arxiv.org/pdf/2002.05955" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Zero-Resource Cross-Domain Named Entity Recognition <a href="https://arxiv.org/pdf/2002.05923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Understanding patient complaint characteristics using contextual  clinical BERT embeddings <a href="https://arxiv.org/pdf/2002.05902" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Transformers as Soft Reasoners over Language <a href="https://arxiv.org/pdf/2002.05867" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> HULK: An Energy Efficiency Benchmark Platform for Responsible Natural  Language Processing <a href="https://arxiv.org/pdf/2002.05829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Unsupervised Speaker Adaptation using Attention-based Speaker Memory for  End-to-End ASR <a href="https://arxiv.org/pdf/2002.06165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Combining Visual and Textual Features for Semantic Segmentation of  Historical Newspapers <a href="https://arxiv.org/pdf/2002.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Exploring Chemical Space using Natural Language Processing Methodologies  for Drug Discovery <a href="https://arxiv.org/pdf/2002.06053" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Deep Speaker Embeddings for Far-Field Speaker Recognition on Short  Utterances <a href="https://arxiv.org/pdf/2002.06033" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Query2box: Reasoning over Knowledge Graphs in Vector Space using Box  Embeddings <a href="https://arxiv.org/pdf/2002.05969" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Transformer on a Diet</b>  <a href="https://arxiv.org/pdf/2002.06170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenguang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ye%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihao Ye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aston Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Smola%2C+A+J" target="_blank" rel="noopener" style="color:#0000EE;">Alexander J. Smola</a><br>
<font size="3">
Abstract: Transformer has been widely used thanks to its ability to capture sequence information in an efficient way. However, recent developments, such as BERT and GPT-2, deliver only heavy architectures with a focus on effectiveness. In this paper, we explore three carefully-designed light Transformer architectures to figure out whether the Transformer with less computations could produce competitive results. Experimental results on language model benchmark datasets hint that such trade-off is promising, and the light Transformer reduces 70% parameters at best, while obtains competitive perplexity compared to standard Transformer. The source code is publicly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：变压器已广泛应用于得益于其能否捕获序列信息的有效途径。然而，最近的事态发展，如BERT和GPT-2，重点放在有效性只提供重的体系结构。在本文中，我们将探讨3精心设计的灯变压器架构弄清楚用更少的计算，Transformer是否可以产生竞争的结果。在语言模型标准数据集实验结果提示，这种权衡是有希望的，虽然取得竞争的困惑与标准变压器，光变压器充其量减少70％的参数。源代码是公开的。</font>
</div>


<hr>
<div id="paper2"> <b>2. Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base</b>  <a href="https://arxiv.org/pdf/2002.06115" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+W+W" target="_blank" rel="noopener" style="color:#0000EE;">William W. Cohen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haitian Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hofer%2C+R+A" target="_blank" rel="noopener" style="color:#0000EE;">R. Alex Hofer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Siegler%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthew Siegler</a><br>
<font size="3">
Abstract: We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB. This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations. The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们描述代表称为稀疏矩阵物化KB一个象征性的知识基础（KB）的新方法。这表示使得神经模块完全区分的，忠实于KB，表现足够多跳推理模型，可扩展性足以与使用大现实KB的原始语义。稀疏矩阵物化KB可以跨多个GPU分布，可以扩展到数以千万计的实体和事实的，是数量级比幼稚稀疏矩阵实现更快。在物化的KB能够非常简单的终端到终端的架构就代表任务的两个家庭几个基准获得有竞争力的性能：KB完成，并从denotations学习语义解析器。</font>
</div>


<hr>
<div id="paper3"> <b>3. FQuAD: French Question Answering Dataset</b>  <a href="https://arxiv.org/pdf/2002.06071" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=d%27Hoffschmidt%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin d'Hoffschmidt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vidal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maxime Vidal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Belblidia%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wacim Belblidia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brendl%C3%A9%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Brendlé</a><br>
<font size="3">
Abstract: Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, the Machine Reading Comprehension task has made significant progress. However, most of the results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is French Native Reading Comprehension dataset that consists of 25,000+ questions on a set of Wikipedia articles. A baseline model is trained which achieves an F1 score of 88.0% and an exact match ratio of 77.9% on the test set. The dataset is made freely available at https://fquad.illuin.tech. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在语言建模领域的最新进展已改善许多自然语言处理任务的国家的最先进的成果。其中，机器阅读理解任务，取得了显著的进步。然而，大部分的结果都报道了英语，因为在其他语言，如法语标注的资源，仍然十分匮乏。在目前的工作中，我们引进法国问答集（FQuAD）。 FQuAD是法语为母语阅读理解数据集包括一组的维基百科文章的25000个问题。基线模型被训练其实现的88.0％的F1分数和在测试组的77.9％的精确匹配比。该数据集是在https://fquad.illuin.tech免费提供。</font>
</div>


<hr>
<div id="paper4"> <b>4. Dialogue history integration into end-to-end signal-to-concept spoken  language understanding systems</b>  <a href="https://arxiv.org/pdf/2002.06012" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tomashenko%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Natalia Tomashenko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Raymond%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Raymond</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caubriere%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antoine Caubriere</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=De+Mori%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Renato De Mori</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Esteve%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yannick Esteve</a><br>
<font size="3">
Abstract: This work investigates the embeddings for representing dialog history in spoken language understanding (SLU) systems. We focus on the scenario when the semantic information is extracted directly from the speech signal by means of a single end-to-end neural network model. We proposed to integrate dialogue history into an end-to-end signal-to-concept SLU system. The dialog history is represented in the form of dialog history embedding vectors (so-called h-vectors) and is provided as an additional information to end-to-end SLU models in order to improve the system performance. Three following types of h-vectors are proposed and experimentally evaluated in this paper: (1) supervised-all embeddings predicting bag-of-concepts expected in the answer of the user from the last dialog system response; (2) supervised-freq embeddings focusing on predicting only a selected set of semantic concept (corresponding to the most frequent errors in our experiments); and (3) unsupervised embeddings. Experiments on the MEDIA corpus for the semantic slot filling task demonstrate that the proposed h-vectors improve the model performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：该作品探讨了在口语理解（SLU）系统代表对话历史的嵌入物。我们专注于场景时，直接从语音信号通过一个单端至端的神经网络模型的装置所提取的语义信息。我们提出了对话的历史融入一个终端到终端的信号 - 概念SLU系统。对话历史被在对话历史嵌入矢量的形式表示（所谓的H-载体）和以提高系统的性能被设置为附加信息，以端 - 端SLU模型。三个以下类型的h-向量提出和实验本文评价：（1）监督-所有的嵌入预测袋的的概念预计在从最后一个对话系统响应所述用户的答案; （2）监督-FREQ的嵌入集中于预测只有选定的一组语义概念（对应于在我们的实验中最频繁的误差）的;和（3）无监督的嵌入。对媒体语料库语义槽分配任务，实验结果表明，所提出的H-载体提高模型的性能。</font>
</div>


<hr>
<div id="paper5"> <b>5. Integrating Discrete and Neural Features via Mixed-feature  Trans-dimensional Random Field Language Models</b>  <a href="https://arxiv.org/pdf/2002.05967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Silin Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ou%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhijian Ou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huifang Xu</a><br>
<font size="3">
Abstract: There has been a long recognition that discrete features (n-gram features) and neural network based features have complementary strengths for language models (LMs). Improved performance can be obtained by model interpolation, which is, however, a suboptimal two-step integration of discrete and neural features. The trans-dimensional random field (TRF) framework has the potential advantage of being able to flexibly integrate a richer set of features. However, either discrete or neural features are used alone in previous TRF LMs. This paper develops a mixed-feature TRF LM and demonstrates its advantage in integrating discrete and neural features. Various LMs are trained over PTB and Google one-billion-word datasets, and evaluated in N-best list rescoring experiments for speech recognition. Among all single LMs (i.e. without model interpolation), the mixed-feature TRF LMs perform the best, improving over both discrete TRF LMs and neural TRF LMs alone, and also being significantly better than LSTM LMs. Compared to interpolating two separately trained models with discrete and neural features respectively, the performance of mixed-feature TRF LMs matches the best interpolated model, and with simplified one-step training process and reduced training time. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：已经有很长的承认，离散特征（正语法特征）和基于神经网络的特点对语言模型（LMS）的互补优势。改进的性能可通过模型内插，然而其​​是，离散和神经功能次优的两个步骤的积分而获得。反式维随机场（TRF）框架具有能够灵活地集成更丰富的功能的潜在优势。然而，无论是分立或神经功能在以前的TRF LM的单独使用。本文开发的混合特征TRF LM并演示了在整合离散和神经功能的优势。各种LM的是培养了PTB和谷歌一十亿字的数据集，并评估了N最佳列表再评分实验语音识别。在所有单个的LM（即没有模型内插），混合特征TRF的LM执行最好的，改善优于分立的TRF LMS和神经TRF的LM单独，也被显著优于LSTM的LM。相比分别内插两个可单独训练的模型具有离散和神经功能，混合特征TRF的LM的性能最好的内插模型，并用简化的一步法训练过程和减少训练时间相匹配。</font>
</div>


<hr>
<div id="paper6"> <b>6. A Data Efficient End-To-End Spoken Language Understanding Architecture</b>  <a href="https://arxiv.org/pdf/2002.05955" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dinarelli%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marco Dinarelli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kapoor%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikita Kapoor</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jabaian%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bassam Jabaian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Besacier%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laurent Besacier</a><br>
<font size="3">
Abstract: End-to-end architectures have been recently proposed for spoken language understanding (SLU) and semantic parsing. Based on a large amount of data, those models learn jointly acoustic and linguistic-sequential features. Such architectures give very good results in the context of domain, intent and slot detection, their application in a more complex semantic chunking and tagging task is less easy. For that, in many cases, models are combined with an external language model to enhance their performance. In this paper we introduce a data efficient system which is trained end-to-end, with no additional, pre-trained external module. One key feature of our approach is an incremental training procedure where acoustic, language and semantic models are trained sequentially one after the other. The proposed model has a reasonable size and achieves competitive results with respect to state-of-the-art while using a small training dataset. In particular, we reach 24.02% Concept Error Rate (CER) on MEDIA/test while training on MEDIA/train without any additional data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结束到终端的架构最近已提出了口语理解（SLU）和语义分析。基于大量的数据，这些模型学习共同声音和语言顺序功能。这种架构给域，意图和插槽检测，他们在更复杂的语义分块的应用程序的情况下非常好的效果和标记的任务是不容易。对于这一点，在许多情况下，模型与外部的语言模型相结合，以提高它们的性能。在本文中，我们介绍该训练端至端，没有额外的，预先训练外部模块数据有效的系统。我们的做法的一个重要功能就是声学，语言和语义模型依次经过培训的一前一后的增量训练过程。该模型有一个合理的规模，并实现了相对于同时使用一个小的训练数据集的国家的最先进的具有竞争力的结果。特别是，我们达到24.02％，概念错误率上的媒体（CER）/测试，而在媒体/火车训练没有任何额外的数据。</font>
</div>


<hr>
<div id="paper7"> <b>7. Zero-Resource Cross-Domain Named Entity Recognition</b>  <a href="https://arxiv.org/pdf/2002.05923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihan Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Winata%2C+G+I" target="_blank" rel="noopener" style="color:#0000EE;">Genta Indra Winata</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fung%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pascale Fung</a><br>
<font size="3">
Abstract: Existing models for cross-domain named entity recognition (NER) rely on numerous unlabeled corpus or labeled NER training data in target domains. However, collecting data for low-resource target domains is not only expensive but also time-consuming. Hence, we propose a cross-domain NER model that does not use any external resources. We first introduce Multi-Task Learning (MTL) by adding a new objective function to detect whether tokens are named entities or not. We then introduce a framework called Mixture of Entity Experts (MoEE) to improve the robustness for zero-resource domain adaptation. Finally, experimental results show that our model outperforms strong unsupervised cross-domain sequence labeling models, and the performance of our model is close to that of the state-of-the-art model which leverages extensive resources. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨域命名实体识别（NER）现有模型依赖于大量的未标记的语料库或目标域标记NER的训练数据。然而，对于低资源目标域收集数据不仅昂贵而且耗时。因此，我们建议不使用任何外部资源跨域NER模型。我们首先通过添加一个新的目标函数，以检测是否令牌被命名为实体或不引入多任务学习（MTL）。然后，我们引入了一个名为实体专家（MoEE）的混合物来改善零资源领域适应性的鲁棒性框架。最后，实验结果表明，我们的模型优于强监督的跨域序列标注模型，我们的模型的性能接近国家的最先进的模型，利用广泛的资源。</font>
</div>


<hr>
<div id="paper8"> <b>8. Understanding patient complaint characteristics using contextual  clinical BERT embeddings</b>  <a href="https://arxiv.org/pdf/2002.05902" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saha%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Budhaditya Saha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lisboa%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanal Lisboa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghosh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shameek Ghosh</a><br>
<font size="3">
Abstract: In clinical conversational applications, extracted entities tend to capture the main subject of a patient's complaint, namely symptoms or diseases. However, they mostly fail to recognize the characterizations of a complaint such as the time, the onset, and the severity. For example, if the input is "I have a headache and it is extreme", state-of-the-art models only recognize the main symptom entity - headache, but ignore the severity factor of "extreme", that characterizes headache. In this paper, we design a two-stage approach to detect the characterizations of entities like symptoms presented by general users in contexts where they would describe their symptoms to a clinician. We use Word2Vec and BERT to encode clinical text given by the patients. We transform the output and re-frame the task as multi-label classification problem. Finally, we combine the processed encodings with the Linear Discriminant Analysis (LDA) algorithm to classify the characterizations of the main entity. Experimental results demonstrate that our method achieves 40-50% improvement on the accuracy over the state-of-the-art models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在临床应用会话，提取的实体往往抓住患者的投诉，即症状或疾病的主要议题。然而，他们大多没有认识到投诉的刻画，如时间，发病和严重程度。例如，如果输入的是“我头疼，这是极端的”，国家的最先进的机型只承认主要症状实体 - 头痛，却忽略了“极端”的严重性因素，表征头痛。在本文中，我们设计了一个两阶段的方法来检测类似的环境中一般用户呈现症状实体的表征，他们会描述自己的症状给临床医生。我们使用Word2Vec和BERT通过给予患者临床文本进行编码。我们变换输出和重制帧任务的多标签分类问题。最后，我们用线性判别分析（LDA）算法相结合的处理编码的主要实体的表征进行分类。实验结果表明，我们的方法实现对精度超过国家的最先进的机型40-50％的改善。</font>
</div>


<hr>
<div id="paper9"> <b>9. Transformers as Soft Reasoners over Language</b>  <a href="https://arxiv.org/pdf/2002.05867" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Clark%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Clark</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tafjord%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oyvind Tafjord</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Richardson%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kyle Richardson</a><br>
<font size="3">
Abstract: AI has long pursued the goal of having systems reason over *explicitly provided* knowledge, but building suitable representations has proved challenging. Here we explore whether transformers can similarly learn to reason (or emulate reasoning), but using rules expressed in language, thus bypassing a formal representation. We provide the first demonstration that this is possible, and characterize the extent of this capability. To do this, we use a collection of synthetic datasets that test increasing levels of reasoning complexity (number of rules, presence of negation, and depth of chaining). We find transformers appear to learn rule-based reasoning with high (99%) accuracy on these datasets, and in a way that generalizes to test data requiring substantially deeper chaining than in the training data (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as a limited "soft theorem prover" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering. All datasets and a live demo are available at this http URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：AI一直在寻求具有系统原因，目标明确* *提供的知识，更适合建设表示已被证明具有挑战性。这里，我们探讨是否变压器同样可以学习的原因（或模拟推理），但使用在语言表达的规则，从而绕过了正式代表。我们提供首次证明这是可能的，而表征这种能力的程度。要做到这一点，我们使用合成的数据集的集合测试增加复杂的推理（规则数，否定的存在，和链接的深度）的水平。我们发现变压器出现学习规则推理高（99％）的精度对这些数据集，并在推广到测试数据需要大幅更深的链接比在训练数据（95％+）分的方式。我们还表明，该模型转移阱两个手创作的规则库，并转述成更自然的语言规则库。因为它表明变压器一个新角色，即作为一个有限的“软定理证明”工作语言在明确的理论这些发现显著。这又提出了explainability，可纠，并在问题回答反推理新的可能性。所有的数据集和现场演示都可以在这个HTTP URL</font>
</div>


<hr>
<div id="paper10"> <b>10. HULK: An Energy Efficiency Benchmark Platform for Responsible Natural  Language Processing</b>  <a href="https://arxiv.org/pdf/2002.05829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiyou Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyu Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyong Jin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W+Y" target="_blank" rel="noopener" style="color:#0000EE;">William Yang Wang</a><br>
<font size="3">
Abstract: Computation-intensive pretrained models have been taking the lead of many natural language processing benchmarks such as GLUE. However, energy efficiency in the process of model training and inference becomes a critical bottleneck. We introduce HULK, a multi-task energy efficiency benchmarking platform for responsible natural language processing. With HULK, we compare pretrained models' energy efficiency from the perspectives of time and cost. Baseline benchmarking results are provided for further analysis. The fine-tuning efficiency of different pretrained models can differ a lot among different tasks and fewer parameter number does not necessarily imply better efficiency. We analyzed such phenomenon and demonstrate the method of comparing the multi-task efficiency of pretrained models. Our platform is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：计算密集型的预训练模式已经采取了许多自然语言处理基准测试中领先胶水等。然而，在模型训练和推理过程中的能源效率成为一个关键瓶颈。我们介绍HULK，多任务的能源效率基准平台负责自然语言处理。随着HULK，我们比较从时间和成本的角度预训练模型的能源效率。基线基准测试结果提供了进一步的分析。不同预训练模型的微调效率不同，不同的任务和更少的参数号中有很多并不一定意味着更好的效率。我们分析了这种现象，并证明比较预先训练模式的多任务效率的方法。我们的平台可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper11"> <b>11. Unsupervised Speaker Adaptation using Attention-based Speaker Memory for  End-to-End ASR</b>  <a href="https://arxiv.org/pdf/2002.06165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Sar%C4%B1%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leda Sarı</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Moritz%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Niko Moritz</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Hori%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Takaaki Hori</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Roux%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Le Roux</a><br>
<font size="3">
Abstract: We propose an unsupervised speaker adaptation method inspired by the neural Turing machine for end-to-end (E2E) automatic speech recognition (ASR). The proposed model contains a memory block that holds speaker i-vectors extracted from the training data and reads relevant i-vectors from the memory through an attention mechanism. The resulting memory vector (M-vector) is concatenated to the acoustic features or to the hidden layer activations of an E2E neural network model. The E2E ASR system is based on the joint connectionist temporal classification and attention-based encoder-decoder architecture. M-vector and i-vector results are compared for inserting them at different layers of the encoder neural network using the WSJ and TED-LIUM2 ASR benchmarks. We show that M-vectors, which do not require an auxiliary speaker embedding extraction system at test time, achieve similar word error rates (WERs) compared to i-vectors for single speaker utterances and significantly lower WERs for utterances in which there are speaker changes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议由神经图灵机的端至端（E2E）的启发无监督说话人自适应方法自动语音识别（ASR）。该模型包含一个保持从训练数据中提取扬声器的i-矢量，并通过关注机构从存储器读出有关的i-矢量的存储器块。将得到的存储器向量（M-矢量）级联到声学特征或到E2E神经网络模型的隐藏层的激活。该E2E ASR系统是基于基于共同的关注联结时间分类和编码器，解码器架构。 M-矢量和i-矢量结果在使用WSJ和TED-LIUM2 ASR基准编码器神经网络的不同层将它们插入比较。我们证明了M-载体，不需要辅助扬声器在测试时嵌入提取系统，相比于我的载体为单喇叭话语和显著降低WERS的言论，其中有扬声器的变化实现类似的字错误率（WERS） 。</font>
</div>


<hr>
<div id="paper12"> <b>12. Combining Visual and Textual Features for Semantic Segmentation of  Historical Newspapers</b>  <a href="https://arxiv.org/pdf/2002.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Barman%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raphaël Barman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ehrmann%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maud Ehrmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clematide%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Clematide</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oliveira%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Sofia Ares Oliveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaplan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Frédéric Kaplan</a><br>
<font size="3">
Abstract: The massive amounts of digitized historical documents acquired over the last decades naturally lend themselves to automatic processing and exploration. Research work seeking to automatically process facsimiles and extract information thereby are multiplying with, as a first essential step, document layout analysis. If the identification and categorization of segments of interest in document images have seen significant progress over the last years thanks to deep learning techniques, many challenges remain with, among others, the use of finer-grained segmentation typologies and the consideration of complex, heterogeneous documents such as historical newspapers. Besides, most approaches consider visual features only, ignoring textual signal. In this context, we introduce a multimodal approach for the semantic segmentation of historical newspapers that combines visual and textual features. Based on a series of experiments on diachronic Swiss and Luxembourgish newspapers, we investigate, among others, the predictive power of visual and textual features and their capacity to generalize across time and sources. Results show consistent improvement of multimodal models in comparison to a strong visual baseline, as well as better robustness to high material variance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在过去的十年中取得数字化的历史记录了大量的自然借给自己自动处理和探索。研究工作寻求自动处理传真和提取信息，从而与倍增，作为第一个重要步骤，文档布局分析。如果识别和文件图像的兴趣细分的分类已经看到过去几年中由于深学习技术了显著的进步，许多挑战仍然存在，等等，使用细粒度分割类型学和考虑复杂的异构文件如历史报纸。此外，大多数的方法只考虑视觉特征，忽略文本信号。在这方面，我们引入历史报纸的语义分割，结合视觉和文本特征的多模态的方法。基于一系列关于历时瑞士和卢森堡报纸实验，调查，除其他外，视觉和文字特征的预测能力和他们的能力，以跨越时间和来源一概而论。结果表明，比较多车型的持续改进，以强烈的视觉底线，以及更好的鲁棒性高的材料差异。</font>
</div>


<hr>
<div id="paper13"> <b>13. Exploring Chemical Space using Natural Language Processing Methodologies  for Drug Discovery</b>  <a href="https://arxiv.org/pdf/2002.06053" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/q-bio?searchtype=author&query=%C3%96zt%C3%BCrk%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hakime Öztürk</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=%C3%96zg%C3%BCr%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arzucan Özgür</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Schwaller%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philippe Schwaller</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Laino%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Teodoro Laino</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&query=Ozkirimli%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elif Ozkirimli</a><br>
<font size="3">
Abstract: Text-based representations of chemicals and proteins can be thought of as unstructured languages codified by humans to describe domain-specific knowledge. Advances in natural language processing (NLP) methodologies in the processing of spoken languages accelerated the application of NLP to elucidate hidden knowledge in textual representations of these biochemical entities and then use it to construct models to predict molecular properties or to design novel molecules. This review outlines the impact made by these advances on drug discovery and aims to further the dialogue between medicinal chemists and computer scientists. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：化学物质和蛋白质的基于文本的表示可以被认为是人类编纂非结构化的语言来描述特定领域的知识。进展自然语言处理（NLP）的方法在语言的处理加速NLP的应用，阐明这些生化实体的文本表示隐含的知识，然后用它来构建模型来预测分子性质或设计新分子。本次审查概述了对药物发现和宗旨这些进步，以推动药物化学家和计算机科学家之间的对话的影响。</font>
</div>


<hr>
<div id="paper14"> <b>14. Deep Speaker Embeddings for Far-Field Speaker Recognition on Short  Utterances</b>  <a href="https://arxiv.org/pdf/2002.06033" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gusev%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aleksei Gusev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Volokhov%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vladimir Volokhov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Andzhukaev%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tseren Andzhukaev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Novoselov%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sergey Novoselov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lavrentyeva%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Galina Lavrentyeva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Volkova%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marina Volkova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gazizullina%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alice Gazizullina</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shulipa%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrey Shulipa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gorlanov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Artem Gorlanov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Avdeeva%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anastasia Avdeeva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ivanov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Artem Ivanov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kozlov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Kozlov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pekhovsky%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timur Pekhovsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Matveev%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Matveev</a><br>
<font size="3">
Abstract: Speaker recognition systems based on deep speaker embeddings have achieved significant performance in controlled conditions according to the results obtained for early NIST SRE (Speaker Recognition Evaluation) datasets. From the practical point of view, taking into account the increased interest in virtual assistants (such as Amazon Alexa, Google Home, AppleSiri, etc.), speaker verification on short utterances in uncontrolled noisy environment conditions is one of the most challenging and highly demanded tasks. This paper presents approaches aimed to achieve two goals: a) improve the quality of far-field speaker verification systems in the presence of environmental noise, reverberation and b) reduce the system qualitydegradation for short utterances. For these purposes, we considered deep neural network architectures based on TDNN (TimeDelay Neural Network) and ResNet (Residual Neural Network) blocks. We experimented with state-of-the-art embedding extractors and their training procedures. Obtained results confirm that ResNet architectures outperform the standard x-vector approach in terms of speaker verification quality for both long-duration and short-duration utterances. We also investigate the impact of speech activity detector, different scoring models, adaptation and score normalization techniques. The experimental results are presented for publicly available data and verification protocols for the VoxCeleb1, VoxCeleb2, and VOiCES datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于深扬声器的嵌入说话人识别系统已经根据早期NIST SRE（说话人识别评价）的数据集所获得的结果在控制的条件下实现显著性能。从实用的角度出发，考虑到虚拟助理（如Alexa的亚马逊，谷歌主页，AppleSiri等），在不受控制嘈杂的环境条件下短话语说话人确认的兴趣增加是一个最具挑战性和高要求任务。本文礼物办法旨在实现两个目标：1）提高远场扬声器验证系统的环境噪声，混响和b的存在质量）减少短话语系统qualitydegradation。为了这些目的，我们认为是基于TDNN（纯滞后神经网络）和RESNET（残余神经网络）块深层神经网络结构。我们尝试与国家的最先进的嵌入提取和他们的训练程序。得到的结果证实，RESNET架构要优于标准的x向量方法在两个长持续时间和短持续时间的话语的说话者验证质量方面。我们还调查语音活动检测器，不同的评分模型，适应和分数标准化技术的影响。实验结果提出了可公开获得的数据和验证协议的VoxCeleb1，VoxCeleb2和声音的数据集。</font>
</div>


<hr>
<div id="paper15"> <b>15. Query2box: Reasoning over Knowledge Graphs in Vector Space using Box  Embeddings</b>  <a href="https://arxiv.org/pdf/2002.05969" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongyu Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weihua Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leskovec%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jure Leskovec</a><br>
<font size="3">
Abstract: Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\wedge$) and existential quantifiers ($\exists$). Handling queries with logical disjunctions ($\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\wedge$, $\vee$, and $\exists$ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with $\wedge$, $\vee$, $\exists$ in a scalable manner. We demonstrate the effectiveness of query2box on three large KGs and show that query2box achieves up to 25% relative improvement over the state of the art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型不全知识图（KGS）回答复杂的逻辑查询是一项基本而具有挑战性的任务。近日，有前途的方法这个问题已经嵌入KG实体以及查询到向量空间，使得回答查询实体嵌入接近查询。然而，以前的工作模式查询，在向量空间，这是问题，因为一个复杂的查询表示一个潜在的大集的答案实体，但目前还不清楚一套怎么这么可以表示为一个单点单点。此外，以前的工作只能处理的查询使用连词（$ \ $楔）和存在量词（$ \ $存在）。处理查询与逻辑或（$ \ $ V型）仍然是一个悬而未决的问题。在这里我们建议query2box，与$ \ $楔形，$ \ $ V型推理在任意查询基于嵌入的框架和$ \大规模和不完整的幼儿园存在$运营商。我们的主要观点是，查询可以嵌入为框（即超矩形），其中一组框对应内部点的一组查询的答案实体。我们表明，连词可以自然地表示为方框交叉，也证明了一个否定结果处理析取需要与尺寸比例嵌入到KG实体的数量。然而，我们表明，通过将查询到的析取范式，query2box能够处理任意逻辑查询与$ \ $楔形，$ \ $ V型，$ \以可扩展的方式存在$。我们证明query2box的三个大型幼儿园的有效性，并表明query2box在技术状态具有高达25％的相对改善。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>screen后台运行进程</title>
    <url>/2020/02/16/screen%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<p>我们常常需要将进程挂在后台运行，防止因关闭窗口或断开连接导致进程被杀掉。screen可以实现进程与当前窗口分离，即使断开连接了，进行仍可以继续运行；并且当我们重新连接后，仍可读取当前进程。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install screen</span><br></pre></td></tr></table></figure><h1 id="新建窗口"><a href="#新建窗口" class="headerlink" title="新建窗口"></a>新建窗口</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 方法一</span></span><br><span class="line">screen # 新建一个无名窗口，断开连接后仍可以后台运行，但是无法重新连接</span><br><span class="line"><span class="meta">#</span><span class="bash"> 方法二</span></span><br><span class="line">screen -S &lt;screen_name&gt; # 新建一个窗口并进入该窗口</span><br></pre></td></tr></table></figure><a id="more"></a>




<h1 id="运行后台程序"><a href="#运行后台程序" class="headerlink" title="运行后台程序"></a>运行后台程序</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">screen &lt;your_command&gt; # 在无名窗口执行命令 &lt;your_command&gt;</span><br></pre></td></tr></table></figure>
<p>或者在新建<code>&lt;screen_name&gt;</code>窗口后，直接运行相应程序就好</p>
<h1 id="会话分离"><a href="#会话分离" class="headerlink" title="会话分离"></a>会话分离</h1><p>退出该screen，让进程在后台运行，按住快捷键<strong><em>Ctrl + A + D</em></strong></p>
<h1 id="查看所有窗口"><a href="#查看所有窗口" class="headerlink" title="查看所有窗口"></a>查看所有窗口</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">screen -ls</span><br></pre></td></tr></table></figure>
<h1 id="恢复窗口"><a href="#恢复窗口" class="headerlink" title="恢复窗口"></a>恢复窗口</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 方法一</span></span><br><span class="line">screen -r &lt;PID_to_screen&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 方法二</span></span><br><span class="line">screen -r &lt;screen_name&gt;</span><br></pre></td></tr></table></figure>
<h1 id="杀死会话"><a href="#杀死会话" class="headerlink" title="杀死会话"></a>杀死会话</h1><h2 id="杀死当前会话窗口"><a href="#杀死当前会话窗口" class="headerlink" title="杀死当前会话窗口"></a>杀死当前会话窗口</h2><p>按住快捷键<strong><em>Ctrl + A + K</em></strong></p>
<h2 id="杀死指定会话窗口"><a href="#杀死指定会话窗口" class="headerlink" title="杀死指定会话窗口"></a>杀死指定会话窗口</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kill -9 &lt;PID_to_screen&gt;</span><br></pre></td></tr></table></figure>
<h1 id="清除僵尸窗口"><a href="#清除僵尸窗口" class="headerlink" title="清除僵尸窗口"></a>清除僵尸窗口</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">screen -wipe</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>screen</tag>
        <tag>后台</tag>
      </tags>
  </entry>
  <entry>
    <title>tgz文件压缩&amp;解压</title>
    <url>/2020/02/15/tgz%E6%96%87%E4%BB%B6%E5%8E%8B%E7%BC%A9-%E8%A7%A3%E5%8E%8B/</url>
    <content><![CDATA[<h1 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h1><h2 id="压缩指定文件夹"><a href="#压缩指定文件夹" class="headerlink" title="压缩指定文件夹"></a>压缩指定文件夹</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar zcvf &lt;filename&gt;.tgz &lt;path_to_dir&gt;</span><br></pre></td></tr></table></figure><h1 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h1><h2 id="解压到当前文件夹"><a href="#解压到当前文件夹" class="headerlink" title="解压到当前文件夹"></a>解压到当前文件夹</h2><h3 id="保留原始压缩文件"><a href="#保留原始压缩文件" class="headerlink" title="保留原始压缩文件"></a>保留原始压缩文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar zxvf &lt;filename&gt;.tgz -C .</span><br></pre></td></tr></table></figure><h2 id="解压到指定文件夹"><a href="#解压到指定文件夹" class="headerlink" title="解压到指定文件夹"></a>解压到指定文件夹</h2><h3 id="保留原始压缩文件-1"><a href="#保留原始压缩文件-1" class="headerlink" title="保留原始压缩文件"></a>保留原始压缩文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar zxvf &lt;filename&gt;.tgz -C &lt;path_to_dir&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>tgz</tag>
        <tag>压缩</tag>
        <tag>解压</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-14</title>
    <url>/2020/02/15/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-14/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Looking Enhances Listening: Recovering Missing Speech Using Images <a href="https://arxiv.org/pdf/2002.05639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Pre-Training for Query Rewriting in A Spoken Language Understanding  System <a href="https://arxiv.org/pdf/2002.05607" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Sentiment Analysis Using Averaged Weighted Word Vector Features <a href="https://arxiv.org/pdf/2002.05606" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Sparse and Structured Visual Attention <a href="https://arxiv.org/pdf/2002.05556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Unsupervised Separation of Native and Loanwords for Malayalam and Telugu <a href="https://arxiv.org/pdf/2002.05527" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Comparison of Turkish Word Representations Trained on Different  Morphological Forms <a href="https://arxiv.org/pdf/2002.05417" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Keyphrase Extraction with Span-based Feature Representations <a href="https://arxiv.org/pdf/2002.05407" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Exploiting the Matching Information in the Support Set for Few Shot  Event Classification <a href="https://arxiv.org/pdf/2002.05295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> What Would You Ask the Machine Learning Model? Identification of User  Needs for Model Explanations Based on Human-Model Conversations <a href="https://arxiv.org/pdf/2002.05674" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Image-to-Image Translation with Text Guidance <a href="https://arxiv.org/pdf/2002.05235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Deep compositional robotic planners that follow natural language  commands <a href="https://arxiv.org/pdf/2002.05201" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> A Combined Stochastic and Physical Framework for Modeling Indoor 5G  Millimeter Wave Propagation <a href="https://arxiv.org/pdf/2002.05162" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Looking Enhances Listening: Recovering Missing Speech Using Images</b>  <a href="https://arxiv.org/pdf/2002.05639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Srinivasan%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tejas Srinivasan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sanabria%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ramon Sanabria</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Metze%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Metze</a><br>
<font size="3">
Abstract: Speech is understood better by using visual context; for this reason, there have been many attempts to use images to adapt automatic speech recognition (ASR) systems. Current work, however, has shown that visually adapted ASR models only use images as a regularization signal, while completely ignoring their semantic content. In this paper, we present a set of experiments where we show the utility of the visual modality under noisy conditions. Our results show that multimodal ASR models can recover words which are masked in the input acoustic signal, by grounding its transcriptions using the visual representations. We observe that integrating visual context can result in up to 35% relative improvement in masked word recovery. These results demonstrate that end-to-end multimodal ASR systems can become more robust to noise by leveraging the visual context. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：言语理解通过视觉环境更好;因为这个原因，已经有许多尝试使用图片来适应自动语音识别（ASR）系统。目前的工作，但是，已经表明，在视觉上适应ASR机型只能使用图片作为正规化信号，而全然不顾自己的语义内容。在本文中，我们提出了一套，我们展示的视觉方式的噪声条件下的实用试验。我们的研究结果表明，多ASR模式可以恢复被掩盖在输入声音信号，通过使用可视化表示接地其改编的话。我们观察到，整合的视觉环境可以导致高达屏蔽字恢复35％的相对改善。这些结果表明，端至端多峰ASR系统可通过利用可视上下文成为噪声更为鲁棒。</font>
</div>


<hr>
<div id="paper2"> <b>2. Pre-Training for Query Rewriting in A Spoken Language Understanding  System</b>  <a href="https://arxiv.org/pdf/2002.05607" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xing Fan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan Ling</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mathias%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lambert Mathias</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenlei Guo</a><br>
<font size="3">
Abstract: Query rewriting (QR) is an increasingly important technique to reduce customer friction caused by errors in a spoken language understanding pipeline, where the errors originate from various sources such as speech recognition errors, language understanding errors or entity resolution errors. In this work, we first propose a neural-retrieval based approach for query rewriting. Then, inspired by the wide success of pre-trained contextual language embeddings, and also as a way to compensate for insufficient QR training data, we propose a language-modeling (LM) based approach to pre-train query embeddings on historical user conversation data with a voice assistant. In addition, we propose to use the NLU hypotheses generated by the language understanding system to augment the pre-training. Our experiments show pre-training provides rich prior information and help the QR task achieve strong performance. We also show joint pre-training with NLU hypotheses has further benefit. Finally, after pre-training, we find a small set of rewrite pairs is enough to fine-tune the QR model to outperform a strong baseline by full training on all QR training data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：查询重写（QR）是减少在口语理解管道，其中的误差来自各种来源，如语音识别错误，语言理解错误或实体解析错误源于错误造成客户的摩擦日益重要的技术。在这项工作中，我们首先提出了查询重写神经检索基础的方法。然后，通过预先训练情境语言的嵌入的广泛成功的启发，并以此来弥补不足QR训练数据，我们提出了一个语言模型（LM）为基础的方法预火车上的历史用户会话数据查询的嵌入与语音助手。此外，我们建议使用通过了解系统，以加强前培训语言产生的NLU假设。我们的实验显示前培训提供了丰富的先验信息和帮助的QR任务实现强劲性能。我们还表明联合前培训NLU假设有另一个好处。最后，经过岗前培训，我们发现了一个小套重写对足以微调QR模型通过对所有QR训练数据全员培训跑赢强大的基线。</font>
</div>


<hr>
<div id="paper3"> <b>3. Sentiment Analysis Using Averaged Weighted Word Vector Features</b>  <a href="https://arxiv.org/pdf/2002.05606" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Erkan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Erkan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gungor%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tunga Gungor</a><br>
<font size="3">
Abstract: People use the world wide web heavily to share their experience with entities such as products, services, or travel destinations. Texts that provide online feedback in the form of reviews and comments are essential to make consumer decisions. These comments create a valuable source that may be used to measure satisfaction related to products or services. Sentiment analysis is the task of identifying opinions expressed in such text fragments. In this work, we develop two methods that combine different types of word vectors to learn and estimate polarity of reviews. We develop average review vectors from word vectors and add weights to this review vectors using word frequencies in positive and negative sensitivity-tagged reviews. We applied the methods to several datasets from different domains that are used as standard benchmarks for sentiment analysis. We ensemble the techniques with each other and existing methods, and we make a comparison with the approaches in the literature. The results show that the performances of our approaches outperform the state-of-the-art success rates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人们使用万维网巨资分享他们的实体，如产品，服务或旅游目的地体验。提供的评论和意见的形式在线反馈文本是必须要做出决定消费。这些意见创建可用于测量有关的产品或服务的满意度的重要来源。情感分析是识别这样的文本片段表达意见的任务。在这项工作中，我们开发了两个方法，结合不同类型的词矢量的学习和评估审查的极性。我们开发从词矢量的平均评价载体，并添加砝码使用在正面和负面的敏感性标记评论词频本次审查的载体。我们使用的方法从不同的域中的多个数据集，它们作为标准的基准情感分析。我们合奏彼此之间以及现有方法的技术，和我们做与对比文献的方法。结果表明，我们的方法的性能优于国家的最先进的成功率。</font>
</div>


<hr>
<div id="paper4"> <b>4. Sparse and Structured Visual Attention</b>  <a href="https://arxiv.org/pdf/2002.05556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P+H" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Henrique Martins</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niculae%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vlad Niculae</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marinho%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zita Marinho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">André Martins</a><br>
<font size="3">
Abstract: Visual attention mechanisms are widely used in multimodal tasks, such as image captioning and visual question answering (VQA). One drawback of softmax-based attention mechanisms is that they assign probability mass to all image regions, regardless of their adjacency structure and of their relevance to the text. In this paper, to better link the image structure with the text, we replace the traditional softmax attention mechanism with two alternative sparsity-promoting transformations: sparsemax, which is able to select the relevant regions only (assigning zero weight to the rest), and a newly proposed Total-Variation Sparse Attention (TVmax), which further encourages the joint selection of adjacent spatial locations. Experiments in image captioning and VQA, using both LSTM and Transformer architectures, show gains in terms of human-rated caption quality, attention relevance, and VQA accuracy, with improved interpretability. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉注意机制被广泛应用于多任务，如图像字幕和视觉问答（VQA）。基于SOFTMAX注意力机制的一个缺点是它们分配概率质量到所有的图像区域，无论其邻接结构及其相关的文字。在本文中，以更好地链接与文本的图像结构，我们更换两种可供选择的稀疏性，促进转变传统的SOFTMAX注意机制：sparsemax，这是能够选择相关区域中仅仅（分配权重为零的其余部分），和新提出的总的变化率稀疏注意（TVmax），其进一步鼓励相邻的空间位置的联合选择。在图像字幕和VQA，同时使用LSTM和变压器的架构实验，显示人类额定字幕质量，重视相关性，准确性VQA，具有完善的可解释性方面的收益。</font>
</div>


<hr>
<div id="paper5"> <b>5. Unsupervised Separation of Native and Loanwords for Malayalam and Telugu</b>  <a href="https://arxiv.org/pdf/2002.05527" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Prakhya%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sridhama Prakhya</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deepak P</a><br>
<font size="3">
Abstract: Quite often, words from one language are adopted within a different language without translation; these words appear in transliterated form in text written in the latter language. This phenomenon is particularly widespread within Indian languages where many words are loaned from English. In this paper, we address the task of identifying loanwords automatically and in an unsupervised manner, from large datasets of words from agglutinative Dravidian languages. We target two specific languages from the Dravidian family, viz., Malayalam and Telugu. Based on familiarity with the languages, we outline an observation that native words in both these languages tend to be characterized by a much more versatile stem - stem being a shorthand to denote the subword sequence formed by the first few characters of the word - than words that are loaned from other languages. We harness this observation to build an objective function and an iterative optimization formulation to optimize for it, yielding a scoring of each word's nativeness in the process. Through an extensive empirical analysis over real-world datasets from both Malayalam and Telugu, we illustrate the effectiveness of our method in quantifying nativeness effectively over available baselines for the task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：很多时候，从一种语言单词不用翻译不同语言中通过;这些词出现在写在后面的语言文本音译形式。这种现象是许多词是从英语借给印度语中特别普遍。在本文中，我们要解决自动在无人监督的方式识别外来词，从粘着达罗毗荼语系的单词大型数据集的任务。我们的目标从德拉威家庭两个特定的语言，即，马拉雅拉姆语和泰卢固语。根据与语言的熟悉程度，我们从整体上观察，在这两种语言的本地话往往被表征一个更通用的干 - 干是表示由单词的前几个字符构成的子字序列的速记 - 比的话从其他语言贷款。我们利用这些观测建立一个目标函数和迭代优化配方，以优化它，产生过程中的每个字的本土性的进球。通过以上来自马来亚和泰卢固语真实世界的数据集丰富的实证分析，说明我们在全球为任务提供基准有效量化本土化方法的有效性。</font>
</div>


<hr>
<div id="paper6"> <b>6. Comparison of Turkish Word Representations Trained on Different  Morphological Forms</b>  <a href="https://arxiv.org/pdf/2002.05417" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%BCler%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gökhan Güler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tantu%C4%9F%2C+A+C" target="_blank" rel="noopener" style="color:#0000EE;">A. Cüneyd Tantuğ</a><br>
<font size="3">
Abstract: Increased popularity of different text representations has also brought many improvements in Natural Language Processing (NLP) tasks. Without need of supervised data, embeddings trained on large corpora provide us meaningful relations to be used on different NLP tasks. Even though training these vectors is relatively easy with recent methods, information gained from the data heavily depends on the structure of the corpus language. Since the popularly researched languages have a similar morphological structure, problems occurring for morphologically rich languages are mainly disregarded in studies. For morphologically rich languages, context-free word vectors ignore morphological structure of languages. In this study, we prepared texts in morphologically different forms in a morphologically rich language, Turkish, and compared the results on different intrinsic and extrinsic tasks. To see the effect of morphological structure, we trained word2vec model on texts which lemma and suffixes are treated differently. We also trained subword model fastText and compared the embeddings on word analogy, text classification, sentimental analysis, and language model tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：增加不同的文字表述的人气也自然语言处理（NLP）任务带来了许多改进。而不需要监督的数据，培训了大量语料的嵌入提供我们要在不同的NLP任务时使用有意义的关系。虽然训练这些载体是相对容易与最近的方法，从数据中获得的信息很大程度上取决于语料库的语言结构。由于普遍研究的语言也有类似的形态结构，发生了形态丰富的语言主要是忽略在研究的问题。对于形态丰富的语言，上下文词矢量忽视的语言形态结构。在这项研究中，我们准备了形态不同形式的文本在形态丰富的语言，土耳其语和比较不同的内在和外在的任务的结果。看形态结构的影响，我们训练上引理和后缀区别对待文本word2vec模型。我们还培养了子字模型fastText和比较了字类比，文本分类，感性的分析和语言模型任务的嵌入。</font>
</div>


<hr>
<div id="paper7"> <b>7. Keyphrase Extraction with Span-based Feature Representations</b>  <a href="https://arxiv.org/pdf/2002.05407" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Funan Mu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenting Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">LiFeng Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yequan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingyu Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yibo Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liqun Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Teng Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xing Zhou</a><br>
<font size="3">
Abstract: Keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. Since keyphrase extraction is able to facilitate the management, categorization, and retrieval of information, it has received much attention in recent years. There are three approaches to address keyphrase extraction: (i) traditional two-step ranking method, (ii) sequence labeling and (iii) generation using neural networks. Two-step ranking approach is based on feature engineering, which is labor intensive and domain dependent. Sequence labeling is not able to tackle overlapping phrases. Generation methods (i.e., Sequence-to-sequence neural network models) overcome those shortcomings, so they have been widely studied and gain state-of-the-art performance. However, generation methods can not utilize context information effectively. In this paper, we propose a novelty Span Keyphrase Extraction model that extracts span-based feature representation of keyphrase directly from all the content tokens. In this way, our model obtains representation for each keyphrase and further learns to capture the interaction between keyphrases in one document to get better ranking results. In addition, with the help of tokens, our model is able to extract overlapped keyphrases. Experimental results on the benchmark datasets show that our proposed model outperforms the existing methods by a large margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：关键字句能够提供语义元数据特征文件和产生文件的内容的概述的。由于关键词的提取是能够方便管理，分类和检索的信息，它受到很多关注在最近几年。有三种方法来解决的关键词提取：（ⅰ）传统的两步骤排序方法，（ⅱ）序列标签和（iii）使用神经网络的产生。两步排序方法是基于特征的工程，这是劳动密集和域依赖。序列标注是不能够解决重叠短语。产生方法（即，序列到序列神经网络模型）克服这些缺点，所以它们已经被广泛地研究和国家的最先进的增益性能。然而，代方法不能有效地利用上下文信息。在本文中，我们提出了一个新颖的跨度的关键词提取模型，提取跨度基于关键词短语特征表示直接从所有内容令牌。这样一来，我们的模型获得每个关键词的进一步获悉代表性捕捉关键短语之间的相互作用一个文档中获得更好的排名结果。此外，与令牌的帮助下，我们的模型是能够提取重叠的关键字句。基准的数据集实验结果表明，该模型优于大幅度现有的方法。</font>
</div>


<hr>
<div id="paper8"> <b>8. Exploiting the Matching Information in the Support Set for Few Shot  Event Classification</b>  <a href="https://arxiv.org/pdf/2002.05295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+V+D" target="_blank" rel="noopener" style="color:#0000EE;">Viet Dac Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dernoncourt%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Franck Dernoncourt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+T+H" target="_blank" rel="noopener" style="color:#0000EE;">Thien Huu Nguyen</a><br>
<font size="3">
Abstract: The existing event classification (EC) work primarily focuseson the traditional supervised learning setting in which models are unableto extract event mentions of new/unseen event types. Few-shot learninghas not been investigated in this area although it enables EC models toextend their operation to unobserved event types. To fill in this gap, inthis work, we investigate event classification under the few-shot learningsetting. We propose a novel training method for this problem that exten-sively exploit the support set during the training process of a few-shotlearning model. In particular, in addition to matching the query exam-ple with those in the support set for training, we seek to further matchthe examples within the support set themselves. This method providesmore training signals for the models and can be applied to every metric-learning-based few-shot learning methods. Our extensive experiments ontwo benchmark EC datasets show that the proposed method can improvethe best reported few-shot learning models by up to 10% on accuracyfor event classification </font>
<br>
<font size="2" style="line-height:30px;">
摘要：新的/看不见的事件类型的现有的事件分类（EC）的工作主要focuseson传统的监督式学习环境中，模型unableto提取物事件中提到。很少拍learninghas没有在这方面进行了研究，虽然它使EC车型toextend其操作未观察到的事件类型。为了填补这一空白，inthis工作中，我们很少拍learningsetting下调查事件分类。我们提出这个问题的新的训练方法EXTEN-sively开发过程中的几个-shotlearning模型的训练过程中的支集。特别是，除了查询考试-PLE与那些在训练支持组匹配，我们寻求进一步小组赛的例子中支持自己设定。为模特这个方法providesmore训练信号，并且可以应用于所有的基于度量学习几拍的学习方法。我们广泛的实验ontwo基准EC数据集表明，该方法可以improvethe最好的报道很少拍学习高达模型10％accuracyfor事件分类</font>
</div>


<hr>
<div id="paper9"> <b>9. What Would You Ask the Machine Learning Model? Identification of User  Needs for Model Explanations Based on Human-Model Conversations</b>  <a href="https://arxiv.org/pdf/2002.05674" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ku%C5%BAba%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michał Kuźba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Biecek%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Przemysław Biecek</a><br>
<font size="3">
Abstract: Recently we see a rising number of methods in the field of eXplainable Artificial Intelligence. To our surprise, their development is driven by model developers rather than a study of needs for human end users. To answer the question "What would a human operator like to ask the ML model?" we propose a conversational system explaining decisions of the predictive model. In this experiment, we implement a chatbot called dr_ant and train a model predicting survival odds on Titanic. People can talk to dr_ant about the model to understand the rationale behind its predictions. Having collected a corpus of 1000+ dialogues, we analyse the most common types of questions that users would like to ask. To our knowledge, it is the first study of needs for human operators in the context of conversations with an ML model. It is also a first study which uses a conversational system for interactive exploration of a predictive model trained on tabular data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近，我们看到了越来越多的在解释的人工智能领域的方法。令我们惊讶的是，他们的发展是由开发商模式，而不是对人类最终用户需求的研究驱动。要回答这个问题：“什么想人类操作员问ML模式？”我们提出了一个对讲系统解释预测模型的决定。在这个实验中，我们实施了一个名为dr_ant聊天机器人和培养模式上的泰坦尼克号预测生存概率。人们可以跟dr_ant有关的模型，以了解它的预测背后的基本原理。在收集的1000多个对话语料库，我们分析了最常见的问题是用户想请教。据我们所知，它是在与ML模型对话的背景下人工操作需要先学习。它也是使用对话系统，训练有素的表格数据的预测模型的互动探索第一个研究。</font>
</div>


<hr>
<div id="paper10"> <b>10. Image-to-Image Translation with Text Guidance</b>  <a href="https://arxiv.org/pdf/2002.05235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bowen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojuan Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torr%2C+P+H+S" target="_blank" rel="noopener" style="color:#0000EE;">Philip H. S. Torr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lukasiewicz%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Lukasiewicz</a><br>
<font size="3">
Abstract: The goal of this paper is to embed controllable factors, i.e., natural language descriptions, into image-to-image translation with generative adversarial networks, which allows text descriptions to determine the visual attributes of synthetic images. We propose four key components: (1) the implementation of part-of-speech tagging to filter out non-semantic words in the given description, (2) the adoption of an affine combination module to effectively fuse different modality text and image features, (3) a novel refined multi-stage architecture to strengthen the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to further improve discriminators to better distinguish real and synthetic images. Extensive experiments on the COCO dataset demonstrate that our method has a superior performance on both visual realism and semantic consistency with given descriptions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文的目的是嵌入可控因素，即，自然语言描述成图像到图像的平移与生成对抗性的网络，它允许文本描述，以确定合成图像的视觉属性。我们提出四个主要组成部分：（1）部分的词性标注的实施，以过滤出在给定的描述非语义字，（2）通过仿射组合模块的有效熔丝不同模态的文本和图像的特征， （3）一种新的改进的多级结构，以加强鉴别器的差动能力和发电机的整流能力，和（4）的新结构的损失，进一步提高鉴别器，以更好地分辨实际的和合成的图像。在COCO大量的实验数据集表明，我们的方法有两个逼真视觉效果，并与给定的描述语义一致性优越的性能。</font>
</div>


<hr>
<div id="paper11"> <b>11. Deep compositional robotic planners that follow natural language  commands</b>  <a href="https://arxiv.org/pdf/2002.05201" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yen-Ling Kuo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Katz%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Boris Katz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barbu%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrei Barbu</a><br>
<font size="3">
Abstract: We demonstrate how a sampling-based robotic planner can be augmented to learn to understand a sequence of natural language commands in a continuous configuration space to move and manipulate objects. Our approach combines a deep network structured according to the parse of a complex command that includes objects, verbs, spatial relations, and attributes, with a sampling-based planner, RRT. A recurrent hierarchical deep network controls how the planner explores the environment, determines when a planned path is likely to achieve a goal, and estimates the confidence of each move to trade off exploitation and exploration between the network and the planner. Planners are designed to have near-optimal behavior when information about the task is missing, while networks learn to exploit observations which are available from the environment, making the two naturally complementary. Combining the two enables generalization to new maps, new kinds of obstacles, and more complex sentences that do not occur in the training set. Little data is required to train the model despite it jointly acquiring a CNN that extracts features from the environment as it learns the meanings of words. The model provides a level of interpretability through the use of attention maps allowing users to see its reasoning steps despite being an end-to-end model. This end-to-end model allows robots to learn to follow natural language commands in challenging continuous environments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们展示了一个基于采样的机器人计划者可以如何增强学习理解自然语言指令在连续配置空间的顺序移动和操作物体。我们的方法结合了根据一个复杂的命令，其中包括对象，动词，空间关系和属性，具有基于采样的规划师，RRT的解析构成的深网络。规划者如何探索环境的反复出现的深层次的网络控制，确定何时有计划的路径是有可能实现一个目标，并估计每一个举动权衡网络和规划者之间的开采和勘探的信心。规划者被设计成具有当有关任务的信息丢失接近最优的行为，而网络学习利用观察其可从环境，使两个自然补充。两者结合能够推广到不训练集中出现新的地图，新的各种障碍，和更复杂的句子。小数据需要火车模型，尽管它共同取得CNN说，从提取的环境特征，因为它学习单词的含义。该模型提供通过使用注意地图让用户看到它的推理步骤，尽管是一个终端到高端机型的一个解释性的水平。这端至端模型允许机器人学会跟随在挑战不断的环境中的自然语言命令。</font>
</div>


<hr>
<div id="paper12"> <b>12. A Combined Stochastic and Physical Framework for Modeling Indoor 5G  Millimeter Wave Propagation</b>  <a href="https://arxiv.org/pdf/2002.05162" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nassif%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Georges Nassif</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gloaguen%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Catherine Gloaguen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philippe Martins</a><br>
<font size="3">
Abstract: Indoor coverage is a major challenge for 5G millimeter waves (mmWaves). In this paper, we address this problem through a novel theoretical framework that combines stochastic indoor environment modeling with advanced physical propagation simulation. This approach is particularly adapted to investigate indoor-to-indoor 5G mmWave propagation. Its system implementation, so-called iGeoStat, generates parameterized typical environments that account for the indoor spatial variations, then simulates radio propagation based on the physical interaction between electromagnetic waves and material properties. This framework is not dedicated to a particular environment, material, frequency or use case and aims to statistically understand the influence of indoor environment parameters on mmWave propagation properties, especially coverage and path loss. Its implementation raises numerous computational challenges that we solve by formulating an adapted link budget and designing new memory optimization algorithms. The first simulation results for two major 5G applications are validated with measurement data and show the efficiency of iGeoStat to simulate multiple diffusion in realistic environments, within a reasonable amount of time and memory resources. Generated output maps confirm that diffusion has a critical impact on indoor mmWave propagation and that proper physical modeling is of the utmost importance to generate relevant propagation models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：室内覆盖是5G毫米波（mmWaves）的一个重大挑战。在本文中，我们通过一个新的理论框架，解决这个问题，结合随机与先进的物理传播模拟室内环境建模。这种方法特别适合于调查室内至室内5G毫米波传播。其系统实施，所谓iGeoStat，生成参数即占室内空间变化典型的环境中，然后模拟基于电磁波和材料性能之间的物理相互作用的无线电传播。该框架不专用于特定的环境中，物质，频率或使用情况，其目的在于统计学理解对毫米波传输特性，特别是覆盖和路径损耗的室​​内环境参数的影响。它的实施，提高了我们通过制定适当链路预算和设计新的内存优化算法，解决了许多计算挑战。第一仿真结果两大5G应用程序验证用的测量数据，并显示iGeoStat的效率，以模拟真实的环境的多个扩散，时间和内存资源的合理量内。生成的输出贴图证实，扩散，对室内毫米波传播和适当的物理建模是极为重要的，以生成相关的传播模型具有关键性的影响。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-14</title>
    <url>/2020/02/15/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-14/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Automatically Discovering and Learning New Visual Categories with  Ranking Statistics <a href="https://arxiv.org/pdf/2002.05714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Classifying the classifier: dissecting the weight space of neural  networks <a href="https://arxiv.org/pdf/2002.05688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Summarizing the performances of a background subtraction algorithm  measured on several videos <a href="https://arxiv.org/pdf/2002.05654" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> GANILLA: Generative Adversarial Networks for Image to Illustration  Translation <a href="https://arxiv.org/pdf/2002.05638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Asynchronous Tracking-by-Detection on Adaptive Time Surfaces for  Event-based Object Tracking <a href="https://arxiv.org/pdf/2002.05583" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> SpotNet: Self-Attention Multi-Task Network for Object Detection <a href="https://arxiv.org/pdf/2002.05540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Replacing Mobile Camera ISP with a Single Deep Learning Model <a href="https://arxiv.org/pdf/2002.05509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Chaotic Phase Synchronization and Desynchronization in an Oscillator  Network for Object Selection <a href="https://arxiv.org/pdf/2002.05493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> EndoL2H: Deep Super-Resolution for Capsule Endoscopy <a href="https://arxiv.org/pdf/2002.05459" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Emotion Recognition for In-the-wild Videos <a href="https://arxiv.org/pdf/2002.05447" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Recurrent Attention Model with Log-Polar Mapping is Robust against  Adversarial Attacks <a href="https://arxiv.org/pdf/2002.05388" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Hypergraph Optimization for Multi-structural Geometric Model Fitting <a href="https://arxiv.org/pdf/2002.05350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Object Detection on Single Monocular Images through Canonical  Correlation Analysis <a href="https://arxiv.org/pdf/2002.05349" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Continual Universal Object Detection <a href="https://arxiv.org/pdf/2002.05347" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D  Vehicle Detection from Point Cloud <a href="https://arxiv.org/pdf/2002.05316" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Improving Efficiency in Neural Network Accelerator Using Operands  Hamming Distance optimization <a href="https://arxiv.org/pdf/2002.05293" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Solving Missing-Annotation Object Detection with Background  Recalibration Loss <a href="https://arxiv.org/pdf/2002.05274" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Leveraging Affect Transfer Learning for Behavior Prediction in an  Intelligent Tutoring System <a href="https://arxiv.org/pdf/2002.05242" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Image-to-Image Translation with Text Guidance <a href="https://arxiv.org/pdf/2002.05235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Cross-Iteration Batch Normalization <a href="https://arxiv.org/pdf/2002.05712" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> A Simple Framework for Contrastive Learning of Visual Representations <a href="https://arxiv.org/pdf/2002.05709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Generative-based Airway and Vessel Morphology Quantification on Chest CT  Images <a href="https://arxiv.org/pdf/2002.05702" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Neuromorphologicaly-preserving Volumetric data encoding using VQ-VAE <a href="https://arxiv.org/pdf/2002.05692" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains <a href="https://arxiv.org/pdf/2002.05665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Machines Learn Appearance Bias in Face Recognition <a href="https://arxiv.org/pdf/2002.05636" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Sparse and Structured Visual Attention <a href="https://arxiv.org/pdf/2002.05556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Superpixel Image Classification with Graph Attention Networks <a href="https://arxiv.org/pdf/2002.05544" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Deep Learning-based End-to-end Diagnosis System for Avascular Necrosis  of Femoral Head <a href="https://arxiv.org/pdf/2002.05536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Abnormal respiratory patterns classifier may contribute to large-scale  screening of people infected with COVID-19 in an accurate and unobtrusive  manner <a href="https://arxiv.org/pdf/2002.05534" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Real or Not Real, that is the Question <a href="https://arxiv.org/pdf/2002.05512" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> MLFcGAN: Multi-level Feature Fusion based Conditional GAN for Underwater  Image Color Correction <a href="https://arxiv.org/pdf/2002.05333" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral  Segmentation of Rock micro-CT Images <a href="https://arxiv.org/pdf/2002.05322" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> A Provably Robust Multiple Rotation Averaging Scheme for SO(2) <a href="https://arxiv.org/pdf/2002.05299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Geom-GCN: Geometric Graph Convolutional Networks <a href="https://arxiv.org/pdf/2002.05287" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine  Learning Models <a href="https://arxiv.org/pdf/2002.05271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> Graph Similarity Using PageRank and Persistent Homology <a href="https://arxiv.org/pdf/2002.05158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Automatically Discovering and Learning New Visual Categories with  Ranking Statistics</b>  <a href="https://arxiv.org/pdf/2002.05714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rebuffi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sylvestre-Alvise Rebuffi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ehrhardt%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastien Ehrhardt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vedaldi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Vedaldi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zisserman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Zisserman</a><br>
<font size="3">
Abstract: We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. This setting is similar to semi-supervised learning, but significantly harder because there are no labelled examples for the new classes. The challenge, then, is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. In this work we address this problem by combining three ideas: (1) we suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) we use rank statistics to transfer the model's knowledge of the labelled classes to the problem of clustering the unlabelled images; and, (3) we train the data representation by optimizing a joint objective function on the labelled and unlabelled subsets of the data, improving both the supervised classification of the labelled data, and the clustering of the unlabelled data. We evaluate our approach on standard classification benchmarks and outperform current methods for novel category discovery by a significant margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们给出解决其他类的标识样本的图像集合中发现新类的问题。因为有新的类没有标识样本此设置类似于半监督学习，而是显著更难。我们面临的挑战，那么，是利用以学习通用的集群模式，并使用后者来识别未标记的数据的新类包含在标记图像的信息。在这项工作中，我们结合三个想法解决这个问题：（1）我们建议使用标记的数据自举图像表示的共同方法只引入了不必要的偏见，而这可以通过自我监督学习到火车避免从标记的和未标记的数据的联合划伤表示; （2）我们使用排名统计标记的类模型的知识传递给聚类未标记的图像的问题;和，（3），我们通过优化上的数据的标记和未标记的子集的联合目标函数，提高了标记的数据的两个监督分类，和未标记数据的聚类训练数据表示。我们评估的标准分类的基准方法，并超越由显著裕新类别发现目前的方法。</font>
</div>


<hr>
<div id="paper2"> <b>2. Classifying the classifier: dissecting the weight space of neural  networks</b>  <a href="https://arxiv.org/pdf/2002.05688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Eilertsen%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabriel Eilertsen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=J%C3%B6nsson%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Jönsson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ropinski%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timo Ropinski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Unger%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonas Unger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ynnerman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anders Ynnerman</a><br>
<font size="3">
Abstract: This paper presents an empirical study on the weights of neural networks, where we interpret each model as a point in a high-dimensional space -- the neural weight space. To explore the complex structure of this space, we sample from a diverse selection of training variations (dataset, optimization procedure, architecture, etc.) of neural network classifiers, and train a large number of models to represent the weight space. Then, we use a machine learning approach for analyzing and extracting information from this space. Most centrally, we train a number of novel deep meta-classifiers with the objective of classifying different properties of the training setup by identifying their footprints in the weight space. Thus, the meta-classifiers probe for patterns induced by hyper-parameters, so that we can quantify how much, where, and when these are encoded through the optimization process. This provides a novel and complementary view for explainable AI, and we show how meta-classifiers can reveal a great deal of information about the training setup and optimization, by only considering a small subset of randomly selected consecutive weights. To promote further research on the weight space, we release the neural weight space (NWS) dataset -- a collection of 320K weight snapshots from 16K individually trained deep neural networks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了关于神经网络，在那里我们解释每个模型为高维空间中的点的权重进行了实证研究 - 神经权空间。为了探究这种空间的结构复杂，从我们的神经网络分类器的训练变化（数据集，优化过程，建筑等）的多样选择采样，并培养了大量的模型来表示重量的空间。然后，我们用从这个空间分析和提取信息的机器学习方法。最集中，我们通过鉴定权空间他们的足迹培养出一批新的深荟萃分类与客观的培训设置的不同性质进行分类的。因此，元分类探测由超参数引起的模式，让我们多少可以量化，在那里，当这些通过优化过程进行编码。这为解释的AI一种新颖的和互补的观点，我们展示荟萃分类如何揭示的有关训练的设置和优化的大量信息，只考虑随机选择的连续权重的一小部分。为了促进对重空间的进一步研究，我们释放神经权空间（NWS）数据集 - 的320K重量快照从16K集合单独训练深层神经网络。</font>
</div>


<hr>
<div id="paper3"> <b>3. Summarizing the performances of a background subtraction algorithm  measured on several videos</b>  <a href="https://arxiv.org/pdf/2002.05654" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pi%C3%A9rard%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sébastien Piérard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Droogenbroeck%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marc Van Droogenbroeck</a><br>
<font size="3">
Abstract: There exist many background subtraction algorithms to detect motion in videos. To help comparing them, datasets with ground-truth data such as CDNET or LASIESTA have been proposed. These datasets organize videos in categories that represent typical challenges for background subtraction. The evaluation procedure promoted by their authors consists in measuring performance indicators for each video separately and to average them hierarchically, within a category first, then between categories, a procedure which we name "summarization". While the summarization by averaging performance indicators is a valuable effort to standardize the evaluation procedure, it has no theoretical justification and it breaks the intrinsic relationships between summarized indicators. This leads to interpretation inconsistencies. In this paper, we present a theoretical approach to summarize the performances for multiple videos that preserves the relationships between performance indicators. In addition, we give formulas and an algorithm to calculate summarized performances. Finally, we showcase our observations on CDNET 2014. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：存在很多背景减除算法来检测视频中的运动。要比较它们的帮助下，与地面实况数据，如CDNET或LASIESTA数据集已经被提出。这些数据集在组织代表背景扣除的典型挑战类视频。它们的作者所倡导的评估过程包括分别测量性能指标为每个视频和他们的平均分层次，类别之内，然后再分类，这是我们的名字“汇总”的程序之间。虽然通过平均业绩指标汇总是一种宝贵的努力，以规范的评估程序，它没有理论依据和它打破了总结指标之间的内在关系。这导致解释不一致。在本文中，我们提出了一个理论方法总结为保留性能指标之间的关系多部影片的演出。另外，我们给出的公式和算法来计算总结演出。最后，我们上展示CDNET 2014我们的观察。</font>
</div>


<hr>
<div id="paper4"> <b>4. GANILLA: Generative Adversarial Networks for Image to Illustration  Translation</b>  <a href="https://arxiv.org/pdf/2002.05638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hicsonmez%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samet Hicsonmez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Samet%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nermin Samet</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Akbas%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emre Akbas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duygulu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pinar Duygulu</a><br>
<font size="3">
Abstract: In this paper, we explore illustrations in children's books as a new domain in unpaired image-to-image translation. We show that although the current state-of-the-art image-to-image translation models successfully transfer either the style or the content, they fail to transfer both at the same time. We propose a new generator network to address this issue and show that the resulting network strikes a better balance between style and content. There are no well-defined or agreed-upon evaluation metrics for unpaired image-to-image translation. So far, the success of image translation models has been based on subjective, qualitative visual comparison on a limited number of images. To address this problem, we propose a new framework for the quantitative evaluation of image-to-illustration models, where both content and style are taken into account using separate classifiers. In this new evaluation framework, our proposed model performs better than the current state-of-the-art models on the illustrations dataset. Our code and pretrained models can be found at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们将探讨在儿童读物插图不成对图像 - 图像转换一个新的领域。我们发现，虽然目前国家的最先进的图像到图像的翻译模式成功传输的样式或内容，他们不能在同一时间传送两者。我们提出了一个新的发电机网络，以解决这一问题，并表明，导致网络罢工的风格和内容之间实现更好的平衡。有没有明确的或商定的评估指标不成对图像 - 图像转换。到目前为止，图像平移模式的成功是基于图像的数量有限，主观的，定性的视觉比较。为了解决这个问题，我们提出了图像到图模型的定量评价，在内容和风格都使用单独的分类考虑到了新的框架。在这个新的评估框架，我们提出的模型比对说明当前国家的最先进的机型更好的数据集。我们的代码和预训练的模型可以在此HTTPS URL中找到。</font>
</div>


<hr>
<div id="paper5"> <b>5. Asynchronous Tracking-by-Detection on Adaptive Time Surfaces for  Event-based Object Tracking</b>  <a href="https://arxiv.org/pdf/2002.05583" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haosheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiangqiang Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanjie Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinbo Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanzi Wang</a><br>
<font size="3">
Abstract: Event cameras, which are asynchronous bio-inspired vision sensors, have shown great potential in a variety of situations, such as fast motion and low illumination scenes. However, most of the event-based object tracking methods are designed for scenarios with untextured objects and uncluttered backgrounds. There are few event-based object tracking methods that support bounding box-based object tracking. The main idea behind this work is to propose an asynchronous Event-based Tracking-by-Detection (ETD) method for generic bounding box-based object tracking. To achieve this goal, we present an Adaptive Time-Surface with Linear Time Decay (ATSLTD) event-to-frame conversion algorithm, which asynchronously and effectively warps the spatio-temporal information of asynchronous retinal events to a sequence of ATSLTD frames with clear object contours. We feed the sequence of ATSLTD frames to the proposed ETD method to perform accurate and efficient object tracking, which leverages the high temporal resolution property of event cameras. We compare the proposed ETD method with seven popular object tracking methods, that are based on conventional cameras or event cameras, and two variants of ETD. The experimental results show the superiority of the proposed ETD method in handling various challenging environments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事件相机，其是异步仿生视觉传感器，已在各种情况下，如快动作和低照明场景示出巨大的潜力。然而，大多数的基于事件的对象跟踪方法设计用于无网纹对象和整洁的背景场景。很少有基于事件的对象跟踪方法，借助现成支持边界对象跟踪。这背后工作的主要思想是提出了基于框包围仿制对象跟踪基于异步事件跟踪 - 通过检测（ETD）方法。为了实现这个目标，提出了一种自适应时表面与线性时间衰减（ATSLTD）事件到帧转换算法，它异步地和有效地经线异步视网膜事件的时空信息来ATSLTD帧的清晰目的的序列轮廓。我们从进料ATSLTD帧序列所提出的ETD方法来执行精确和高效的对象跟踪，它利用的事件摄像机的高时间分辨率特性。我们比较建议的ETD方法有七个流行的对象跟踪方法，是基于传统相机或事件相机和ETD的两个变种。实验结果表明，在处理各种复杂的环境下提出的ETD方法的优越性。</font>
</div>


<hr>
<div id="paper6"> <b>6. SpotNet: Self-Attention Multi-Task Network for Object Detection</b>  <a href="https://arxiv.org/pdf/2002.05540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Perreault%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hughes Perreault</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bilodeau%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillaume-Alexandre Bilodeau</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saunier%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Saunier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=H%C3%A9ritier%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maguelonne Héritier</a><br>
<font size="3">
Abstract: Humans are very good at directing their visual attention toward relevant areas when they search for different types of objects. For instance, when we search for cars, we will look at the streets, not at the top of buildings. The motivation of this paper is to train a network to do the same via a multi-task learning approach. To train visual attention, we produce foreground/background segmentation labels in a semi-supervised way, using background subtraction or optical flow. Using these labels, we train an object detection model to produce foreground/background segmentation maps as well as bounding boxes while sharing most model parameters. We use those segmentation maps inside the network as a self-attention mechanism to weight the feature map used to produce the bounding boxes, decreasing the signal of non-relevant areas. We show that by using this method, we obtain a significant mAP improvement on two traffic surveillance datasets, with state-of-the-art results on both UA-DETRAC and UAVDT. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类是在指导他们的视觉注意力转向当他们搜索不同类型的对象的相关领域的非常好。例如，当我们搜索汽车，我们将着眼于街道，而不是在建筑物的顶部。这篇文章的动机是培养网络通过多任务学习的方法来这样做。培养视觉注意，我们产生一个半监督方式前景/背景分割的标签，使用背景减除或光流。使用这些标签，我们培养的目标检测模型生成前景/背景分割映射以及边界框，而分享最模型参数。我们使用这些分割网络作为自注意机制加权用于生产边界框的功能地图内的地图，减小非相关领域的信号。我们表明，采用这种方法，我们获得显著改善地图上的两个交通监控的数据集，用两个UA-DETRAC和UAVDT国家的先进成果。</font>
</div>


<hr>
<div id="paper7"> <b>7. Replacing Mobile Camera ISP with a Single Deep Learning Model</b>  <a href="https://arxiv.org/pdf/2002.05509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ignatov%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrey Ignatov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Gool%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luc Van Gool</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Timofte%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Radu Timofte</a><br>
<font size="3">
Abstract: As the popularity of mobile photography is growing constantly, lots of efforts are being invested now into building complex hand-crafted camera ISP solutions. In this work, we demonstrate that even the most sophisticated ISP pipelines can be replaced with a single end-to-end deep learning model trained without any prior knowledge about the sensor and optics used in a particular device. For this, we present PyNET, a novel pyramidal CNN architecture designed for fine-grained image restoration that implicitly learns to perform all ISP steps such as image demosaicing, denoising, white balancing, color and contrast correction, demoireing, etc. The model is trained to convert RAW Bayer data obtained directly from mobile camera sensor into photos captured with a professional high-end DSLR camera, making the solution independent of any particular mobile ISP implementation. To validate the proposed approach on the real data, we collected a large-scale dataset consisting of 10 thousand full-resolution RAW-RGB image pairs captured in the wild with the Huawei P20 cameraphone (12.3 MP Sony Exmor IMX380 sensor) and Canon 5D Mark IV DSLR. The experiments demonstrate that the proposed solution can easily get to the level of the embedded P20's ISP pipeline that, unlike our approach, is combining the data from two (RGB + B/W) camera sensors. The dataset, pre-trained models and codes used in this paper are available on the project website. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着移动摄影的普及在不断增加，大量的努力，现在正在投入到构建复杂的手工制作的相机ISP解决方案。在这项工作中，我们证明，即使是最先进的ISP管道可以用单端至端深学习模型没有关于特定设备中使用的传感器和光学任何先验知识培训的进行更换。对于这一点，我们现在PyNET，一种新颖的锥体CNN架构设计用于细粒度图像恢复隐式学习执行所有ISP作为图像去马赛克，去噪，白平衡，颜色和对比度校正，demoireing等模型被训练步骤，例如转换直接从移动照相机传感器获得的与一个专业高端数码单反相机拍摄的照片的RAW拜尔数据，使得溶液独立于任何特定的移动ISP实现的。为了验证对实际数据所提出的方法，我们收集了大规模的数据集，包括与华为P20拍照手机（12.3 MP的索尼Exmor IMX380传感器）和佳能5D Mark野外捕获10000全分辨率的RAW-RGB图像对IV数码单反相机。实验结果表明，所提出的解决方案可以轻松搞定嵌入式P20的ISP管线的水平，不像我们的做法，是结合两个（RGB + B / W）相机传感器的数据。该数据集，预先训练模式，本文使用的代码都可以在项目网站上。</font>
</div>


<hr>
<div id="paper8"> <b>8. Chaotic Phase Synchronization and Desynchronization in an Oscillator  Network for Object Selection</b>  <a href="https://arxiv.org/pdf/2002.05493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Breve%2C+F+A" target="_blank" rel="noopener" style="color:#0000EE;">Fabricio A Breve</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Quiles%2C+M+G" target="_blank" rel="noopener" style="color:#0000EE;">Marcos G Quiles</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Macau%2C+E+E+N" target="_blank" rel="noopener" style="color:#0000EE;">Elbert E. N. Macau</a><br>
<font size="3">
Abstract: Object selection refers to the mechanism of extracting objects of interest while ignoring other objects and background in a given visual scene. It is a fundamental issue for many computer vision and image analysis techniques and it is still a challenging task to artificial visual systems. Chaotic phase synchronization takes place in cases involving almost identical dynamical systems and it means that the phase difference between the systems is kept bounded over the time, while their amplitudes remain chaotic and may be uncorrelated. Instead of complete synchronization, phase synchronization is believed to be a mechanism for neural integration in brain. In this paper, an object selection model is proposed. Oscillators in the network representing the salient object in a given scene are phase synchronized, while no phase synchronization occurs for background objects. In this way, the salient object can be extracted. In this model, a shift mechanism is also introduced to change attention from one object to another. Computer simulations show that the model produces some results similar to those observed in natural vision systems. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对象选择是指提取关注对象，而忽略了在给定的视觉场景中其他对象和背景的机制。这是许多计算机视觉和图像分析技术的一个基本问题，它仍然是一个艰巨的任务，以人工视觉系统。乱相同步需要在涉及几乎相同的动力系统的情况下的地方，这意味着该系统之间的相位差被保持为界在时间，而它们的幅度保持混乱，并且可以是不相关的。相反，完全同步，相位同步被认为是对脑神经一体化的机制。在本文中，对象选择模型。在网络中的振荡器表示在给定的场景中的显着对象的相位同步，而没有相位同步发生为背景对象。通过这种方式，显着对象可以提取。在这种模式下，换档机构也被引入到变化的注意力从一个对象到另一个。计算机模拟表明，该模型产生相似于在自然视觉系统观察到了一定的成效。</font>
</div>


<hr>
<div id="paper9"> <b>9. EndoL2H: Deep Super-Resolution for Capsule Endoscopy</b>  <a href="https://arxiv.org/pdf/2002.05459" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Almalioglu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yasin Almalioglu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gokce%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abdulkadir Gokce</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Incetan%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kagan Incetan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Simsek%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Muhammed Ali Simsek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ararat%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kivanc Ararat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+R+J" target="_blank" rel="noopener" style="color:#0000EE;">Richard J. Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Durr%2C+N+J" target="_blank" rel="noopener" style="color:#0000EE;">Nichalos J. Durr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mahmood%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Faisal Mahmood</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Turan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehmet Turan</a><br>
<font size="3">
Abstract: Wireless capsule endoscopy is the preferred modality for diagnosis and assessment of small bowel disease. However, the poor resolution is a limitation for both subjective and automated diagnostics. Enhanced-resolution endoscopy has shown to improve adenoma detection rate for conventional endoscopy and is likely to do the same for capsule endoscopy. In this work, we propose and quantitatively validate a novel framework to learn a mapping from low-to-high resolution endoscopic images. We use conditional adversarial networks and spatial attention to improve the resolution by up to a factor of 8x. Our quantitative study demonstrates the superiority of our proposed approach over Super-Resolution Generative Adversarial Network (SRGAN) and bicubic interpolation. For qualitative analysis, visual Turing tests were performed by 16 gastroenterologists to confirm the clinical utility of the proposed approach. Our approach is generally applicable to any endoscopic capsule system and has the potential to improve diagnosis and better harness computational approaches for polyp detection and characterization. Our code and trained models are available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无线胶囊内窥镜是用于小肠疾病的诊断和评估的优选的形态。然而，可怜的分辨率是主观和自动诊断的限制。增强的分辨率内镜已经显示出改善常规胃镜腺瘤检出率，并有可能对胶囊内镜做同样的。在这项工作中，我们提出并定量验证新框架，从低到高清晰度内窥镜图像学的映射。我们使用条件对抗网络和空间注意改善了分辨率8X的一个因素。我们的定量研究表明我们提出的方法在超分辨率剖成对抗性网络（SRGAN）和双三次插值的优越性。对于定性分析，视觉图灵测试是由16名胃肠病来证实了该方法的临床应用。我们的做法是普遍适用于任何的胶囊内窥镜系统，并具有提高诊断和息肉检测和表征更好地利用计算方法的潜力。我们的代码和训练的模型可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper10"> <b>10. Emotion Recognition for In-the-wild Videos</b>  <a href="https://arxiv.org/pdf/2002.05447" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanyu Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiabei Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiguang Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xilin Chen</a><br>
<font size="3">
Abstract: This paper is a brief introduction to our submission to the seven basic expression classification track of Affective Behavior Analysis in-the-wild Competition held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. Our method combines Deep Residual Network (ResNet) and Bidirectional Long Short-Term Memory Network (BLSTM), achieving 64.3% accuracy and 43.4% final metric on the validation set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文简要介绍我们提交情感行为分析的七种基本表情分类轨道在最狂野结合的自动面部和手势识别（FG）2020年，我们的方法结合了IEEE国际会议举行比赛深剩余网络（RESNET）和双向长短时记忆网络（BLSTM），实现了64.3％的准确率和43.4％的验证集的最终指标。</font>
</div>


<hr>
<div id="paper11"> <b>11. Recurrent Attention Model with Log-Polar Mapping is Robust against  Adversarial Attacks</b>  <a href="https://arxiv.org/pdf/2002.05388" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiritani%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taro Kiritani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ono%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Koji Ono</a><br>
<font size="3">
Abstract: Convolutional neural networks are vulnerable to small $\ell^p$ adversarial attacks, while the human visual system is not. Inspired by neural networks in the eye and the brain, we developed a novel artificial neural network model that recurrently collects data with a log-polar field of view that is controlled by attention. We demonstrate the effectiveness of this design as a defense against SPSA and PGD adversarial attacks. It also has beneficial properties observed in the animal visual system, such as reflex-like pathways for low-latency inference, fixed amount of computation independent of image size, and rotation and scale invariance. The code for experiments is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络很容易受到小$ \ ELL ^ P $敌对攻击，而人的视觉系统是没有的。通过在眼睛和大脑的神经网络的启发，我们开发了具有反复的观点，即由关注控制的数极场收集数据的新型人工神经网络模型。我们证明这种设计的对抗SPSA和PGD敌对攻击防御的有效性。它还具有在动物的视觉系统观察到的有益的性质，例如反射般途径低延迟推断，计算独立的图像尺寸的固定量，并且旋转和尺度不变性。用于实验的代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper12"> <b>12. Hypergraph Optimization for Multi-structural Geometric Model Fitting</b>  <a href="https://arxiv.org/pdf/2002.05350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuyuan Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guobao Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Suter%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Suter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanzi Wang</a><br>
<font size="3">
Abstract: Recently, some hypergraph-based methods have been proposed to deal with the problem of model fitting in computer vision, mainly due to the superior capability of hypergraph to represent the complex relationship between data points. However, a hypergraph becomes extremely complicated when the input data include a large number of data points (usually contaminated with noises and outliers), which will significantly increase the computational burden. In order to overcome the above problem, we propose a novel hypergraph optimization based model fitting (HOMF) method to construct a simple but effective hypergraph. Specifically, HOMF includes two main parts: an adaptive inlier estimation algorithm for vertex optimization and an iterative hyperedge optimization algorithm for hyperedge optimization. The proposed method is highly efficient, and it can obtain accurate model fitting results within a few iterations. Moreover, HOMF can then directly apply spectral clustering, to achieve good fitting performance. Extensive experimental results show that HOMF outperforms several state-of-the-art model fitting methods on both synthetic data and real images, especially in sampling efficiency and in handling data with severe outliers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近来，一些基于超图的方法已经被提出来处理在计算机视觉模型拟合的问题，主要是由于超图来表示数据点之间的复杂关系的卓越能力。然而，当输入数据包括大量的数据点（通常沾染噪声和异常值），这将增加显著的计算负担的超图变得极其复杂。为了克服上述问题，我们提出了一种新颖的超图优化的基于模型拟合（HOMF）方法来构造一个简单但有效的超图。具体而言，HOMF包括两个主要部分：用于顶点优化的自适应内点估计算法和用于超边优化迭代超边优化算法。所提出的方法是高效的，并且它可以在几次迭代中获得准确的模型拟合的结果。此外，HOMF就可以直接申请谱聚类，以达到良好的装配性能。广泛的实验结果表明，HOMF性能优于几个国家的最先进的模型拟合在两个合成数据和真实图像的方法，尤其是在采样效率，并与严重的异常值处理数据。</font>
</div>


<hr>
<div id="paper13"> <b>13. Object Detection on Single Monocular Images through Canonical  Correlation Analysis</b>  <a href="https://arxiv.org/pdf/2002.05349" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zifan Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=You%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suya You</a><br>
<font size="3">
Abstract: Without using extra 3-D data like points cloud or depth images for providing 3-D information, we retrieve the 3-D object information from single monocular images. The high-quality predicted depth images are recovered from single monocular images, and it is fed into the 2-D object proposal network with corresponding monocular images. Most existing deep learning frameworks with two-streams input data always fuse separate data by concatenating or adding, which views every part of a feature map can contribute equally to the whole task. However, when data are noisy, and too much information is redundant, these methods no longer produce predictions or classifications efficiently. In this report, we propose a two-dimensional CCA(canonical correlation analysis) framework to fuse monocular images and corresponding predicted depth images for basic computer vision tasks like image classification and object detection. Firstly, we implemented different structures with one-dimensional CCA and Alexnet to test the performance on the image classification task. And then, we applied one of these structures with 2D-CCA for object detection. During these experiments, we found that our proposed framework behaves better when taking predicted depth images as inputs with the model trained from ground truth depth. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：不使用额外的3-d数据，如点云或提供3 d信息的深度图像，我们从中检索单一单眼图像的3-d对象的信息。高品质的预测深度图像是从单个单目图像恢复，并且它被送入2-d对象提案网络与对应单眼图像。与大多数现有的深度学习框架，通过连接或添加，其浏览量特征图的每一个部分可以对整个任务同样有助于总是两流输入数据熔丝单独的数据。然而，当数据是嘈杂的，和太多的信息是多余的，这些方法不再生产预测或分类有效。在本报告中，我们提出了一种二维CCA（典型相关分析）框架，保险丝单眼图像和基本计算机视觉任务，如图像分类和物体检测对应的预测深度图像。首先，我们实施了不同的结构与一维CCA和Alexnet测试在图像分类任务性能。然后，我们应用这些结构的2D-CCA为对象检测中的一个。在这些实验中，我们发现，服用预测深度图像与地面真相的深度训练模型输入时，我们提出的框架的行为更好。</font>
</div>


<hr>
<div id="paper14"> <b>14. Continual Universal Object Detection</b>  <a href="https://arxiv.org/pdf/2002.05347" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xialei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ravichandran%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avinash Ravichandran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhotika%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rahul Bhotika</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soatto%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefano Soatto</a><br>
<font size="3">
Abstract: Object detection has improved significantly in recent years on multiple challenging benchmarks. However, most existing detectors are still domain-specific, where the models are trained and tested on a single domain. When adapting these detectors to new domains, they often suffer from catastrophic forgetting of previous knowledge. In this paper, we propose a continual object detector that can learn sequentially from different domains without forgetting. First, we explore learning the object detector continually in different scenarios across various domains and categories. Learning from the analysis, we propose attentive feature distillation leveraging both bottom-up and top-down attentions to mitigate forgetting. It takes advantage of attention to ignore the noisy background information and feature distillation to provide strong supervision. Finally, for the most challenging scenarios, we propose an adaptive exemplar sampling method to leverage exemplars from previous tasks for less forgetting effectively. The experimental results show the excellent performance of our proposed method in three different scenarios across seven different object detection datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目的检测已在多个具有挑战性的基准近年来显著改善。然而，大多数现有的探测器依然特定领域，其中模型训练和在单个域进行测试。当采用这些探测器新的领域，但常常会出现以前的知识的灾难性遗忘。在本文中，我们提出一种可从不同的域顺序地学习没有忘记一个连续的物体检测装置。首先，我们探究的学习对象检测器不断地在不同领域和类别不同的​​场景。从分析中学习，我们提出了周到的功能，利用蒸馏既自下而上和自上而下的注意力，以减轻遗忘。这需要关注的优势，忽略了嘈杂的背景信息和功能蒸馏提供强有力的监督。最后，最具挑战性的场景中，我们提出了有效的少遗忘的自适应典范抽样的方法，从以前的任务杠杆典范。实验结果表明，在三个不同的场景在七个不同的物体探测数据集我们提出的方法的优良性能。</font>
</div>


<hr>
<div id="paper15"> <b>15. SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D  Vehicle Detection from Point Cloud</b>  <a href="https://arxiv.org/pdf/2002.05316" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongwei Yi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shaoshuai Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingyu Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiankai Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kui Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sheng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoping Wang</a><br>
<font size="3">
Abstract: 3D vehicle detection based on point cloud is a challenging task in real-world applications such as autonomous driving. Despite significant progress has been made, we observe two aspects to be further improved. First, the semantic context information in LiDAR is seldom explored in previous works, which may help identify ambiguous vehicles. Second, the distribution of point cloud on vehicles varies continuously with increasing depths, which may not be well modeled by a single model. In this work, we propose a unified model SegVoxelNet to address the above two problems. A semantic context encoder is proposed to leverage the free-of-charge semantic segmentation masks in the bird's eye view. Suspicious regions could be highlighted while noisy regions are suppressed by this module. To better deal with vehicles at different depths, a novel depth-aware head is designed to explicitly model the distribution differences and each part of the depth-aware head is made to focus on its own target detection range. Extensive experiments on the KITTI dataset show that the proposed method outperforms the state-of-the-art alternatives in both accuracy and efficiency with point cloud as input only. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于点云3D车辆检测是在现实世界的应用，如自动驾驶一个具有挑战性的任务。尽管显著已经取得了进展，我们观察到两个方面有待进一步提高。首先，在激光雷达语义上下文信息在以前的作品中，这可能有助于识别模糊的车辆很少探讨。其次，点云上的车辆分布随深度，这可能不是一个单一的模式很好地模拟连续变化。在这项工作中，我们提出了一个统一的模型SegVoxelNet解决上述两个问题。语义上下文编码器，提出了利用自由充电语义分割口罩的鸟瞰视图。而嘈杂的区域由该模块抑制可疑的区域可以高亮显示。为了更好地应对在不同深度的车辆，新颖的深度感知的头被设计成分布差异和深度感知头部的每个部分是由专注于自己的目标的探测距离清晰的模型。对数据集KITTI表明，该方法优于国家的最先进的替代品在精度和效率与点云作为仅输入了广泛的实验。</font>
</div>


<hr>
<div id="paper16"> <b>16. Improving Efficiency in Neural Network Accelerator Using Operands  Hamming Distance optimization</b>  <a href="https://arxiv.org/pdf/2002.05293" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Meng Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yilei Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chuang%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierce Chuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liangzhen Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chandra%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vikas Chandra</a><br>
<font size="3">
Abstract: Neural network accelerator is a key enabler for the on-device AI inference, for which energy efficiency is an important metric. The data-path energy, including the computation energy and the data movement energy among the arithmetic units, claims a significant part of the total accelerator energy. By revisiting the basic physics of the arithmetic logic circuits, we show that the data-path energy is highly correlated with the bit flips when streaming the input operands into the arithmetic units, defined as the hamming distance of the input operand matrices. Based on the insight, we propose a post-training optimization algorithm and a hamming-distance-aware training algorithm to co-design and co-optimize the accelerator and the network synergistically. The experimental results based on post-layout simulation with MobileNetV2 demonstrate on average 2.85X data-path energy reduction and up to 8.51X data-path energy reduction for certain layers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经网络加速器是用于在设备上的AI推理的一个关键因素，为此，能量效率是一个重要的度量。数据通路的能量，包括计算能量和运算单元之间的数据移动的能量，要求总能量加速器的显著一部分。通过重新审视的算术逻辑电路的基本物理中，我们表明，与流式输入操作数到运算单元，其定义为输入操作数矩阵的汉明距离，当位翻转数据通路能量高度相关。基于这样的认识，我们提出了一个培训后的优化算法和汉明距离感知训练算法协同设计和协同优化的加速器和网络协同。基于后布局仿真MobileNetV2实验结果表明，平均2.85X数据路径能量减少且至多为8.51X某些层数据路径能量削减。</font>
</div>


<hr>
<div id="paper17"> <b>17. Solving Missing-Annotation Object Detection with Background  Recalibration Loss</b>  <a href="https://arxiv.org/pdf/2002.05274" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fangyi Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiqiang Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hao%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiqi Hao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenchen Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Savvides%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marios Savvides</a><br>
<font size="3">
Abstract: This paper focuses on a novel and challenging detection scenario: A majority of true objects/instances is unlabeled in the datasets, so these missing-labeled areas will be regarded as the background during training. Previous art on this problem has proposed to use soft sampling to re-weight the gradients of RoIs based on the overlaps with positive instances, while their method is mainly based on the two-stage detector (i.e. Faster RCNN) which is more robust and friendly for the missing label scenario. In this paper, we introduce a superior solution called Background Recalibration Loss (BRL) that can automatically re-calibrate the loss signals according to the pre-defined IoU threshold and input image. Our design is built on the one-stage detector which is faster and lighter. Inspired by the Focal Loss formulation, we make several significant modifications to fit on the missing-annotation circumstance. We conduct extensive experiments on the curated PASCAL VOC and MS COCO datasets. The results demonstrate that our proposed method outperforms the baseline and other state-of-the-arts by a large margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文着重于新颖和具有挑战性的检测情况：多数真正的对象/实例中的数据集未标记的，所以这些丢失的标记区将被视为训练期间的背景。对这个问题以前的技术已提出了使用软采样重新重量基于与正实例的重叠ROI的梯度，而他们的方法主要是基于两阶段检测器上（即，更快的RCNN），这是更健壮的和友好的失踪标签的情况。在本文中，我们引入称为背景重新校准损失（BRL）优越的解决方案根据所述预定义的阈值IOU和输入图像，可以自动重新校准损失信号。我们的设计是建立在一个阶段的检测速度更快，更轻的。由焦点损失配方的启发，我们做几个显著的修改，以适应失踪的注释情况。我们进行的策划PASCAL VOC和MS COCO数据集大量的实验。结果表明，我们提出的方法优于大幅度基线和其他国家的的美术馆。</font>
</div>


<hr>
<div id="paper18"> <b>18. Leveraging Affect Transfer Learning for Behavior Prediction in an  Intelligent Tutoring System</b>  <a href="https://arxiv.org/pdf/2002.05242" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruiz%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nataniel Ruiz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jalal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mona Jalal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ablavsky%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vitaly Ablavsky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Allessio%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Danielle Allessio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Magee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Magee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Whitehill%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jacob Whitehill</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arroyo%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ivon Arroyo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Woolf%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Beverly Woolf</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sclaroff%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stan Sclaroff</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Betke%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Margrit Betke</a><br>
<font size="3">
Abstract: In the context of building an intelligent tutoring system (ITS), which improves student learning outcomes by intervention, we set out to improve prediction of student problem outcome. In essence, we want to predict the outcome of a student answering a problem in an ITS from a video feed by analyzing their face and gestures. For this, we present a novel transfer learning facial affect representation and a user-personalized training scheme that unlocks the potential of this representation. We model the temporal structure of video sequences of students solving math problems using a recurrent neural network architecture. Additionally, we extend the largest dataset of student interactions with an intelligent online math tutor by a factor of two. Our final model, coined ATL-BP (Affect Transfer Learning for Behavior Prediction) achieves an increase in mean F-score over state-of-the-art of 45% on this new dataset in the general case and 50% in a more challenging leave-users-out experimental setting when we use a user-personalized training scheme. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在建设智能教学系统（ITS），它通过干预提高了学生的学习成果的背景下，我们着手提高学生的问题结果的预测。从本质上说，我们希望通过分析他们的脸和手势来预测一个学生从视频源的ITS回答问题的结果。为此，我们提出了一个新的转移学习的面部影响表现和解锁此表示的潜在用户个性化的培训方案。我们的学生解决使用递归神经网络结构的数学题的视频序列的时间结构建模。此外，我们通过两个因素具有智能在线数学家教延长学生互动的最大的数据集。我们的最终模型，创造了ATL-BP（影响对行为预测迁移学习）达到平均F-得分超过国家的最先进的45％，在这个新的数据集在一般情况下增加，在50％以上挑战假用户出实验设置，当我们使用用户个性化的培训方案。</font>
</div>


<hr>
<div id="paper19"> <b>19. Image-to-Image Translation with Text Guidance</b>  <a href="https://arxiv.org/pdf/2002.05235" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bowen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojuan Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torr%2C+P+H+S" target="_blank" rel="noopener" style="color:#0000EE;">Philip H. S. Torr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lukasiewicz%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Lukasiewicz</a><br>
<font size="3">
Abstract: The goal of this paper is to embed controllable factors, i.e., natural language descriptions, into image-to-image translation with generative adversarial networks, which allows text descriptions to determine the visual attributes of synthetic images. We propose four key components: (1) the implementation of part-of-speech tagging to filter out non-semantic words in the given description, (2) the adoption of an affine combination module to effectively fuse different modality text and image features, (3) a novel refined multi-stage architecture to strengthen the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to further improve discriminators to better distinguish real and synthetic images. Extensive experiments on the COCO dataset demonstrate that our method has a superior performance on both visual realism and semantic consistency with given descriptions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文的目的是嵌入可控因素，即，自然语言描述成图像到图像的平移与生成对抗性的网络，它允许文本描述，以确定合成图像的视觉属性。我们提出四个主要组成部分：（1）部分的词性标注的实施，以过滤出在给定的描述非语义字，（2）通过仿射组合模块的有效熔丝不同模态的文本和图像的特征， （3）一种新的改进的多级结构，以加强鉴别器的差动能力和发电机的整流能力，和（4）的新结构的损失，进一步提高鉴别器，以更好地分辨实际的和合成的图像。在COCO大量的实验数据集表明，我们的方法有两个逼真视觉效果，并与给定的描述语义一致性优越的性能。</font>
</div>


<hr>
<div id="paper20"> <b>20. Cross-Iteration Batch Normalization</b>  <a href="https://arxiv.org/pdf/2002.05712" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhuliang Yao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yue Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuxin Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gao Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Lin</a><br>
<font size="3">
Abstract: A well-known issue of Batch Normalization is its significantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statistics upon which the normalization is defined cannot be reliably estimated from it during a training iteration. To address this problem, we present Cross-Iteration Batch Normalization (CBN), in which examples from multiple recent iterations are jointly utilized to enhance estimation quality. A challenge of computing statistics over multiple iterations is that the network activations from different iterations are not comparable to each other due to changes in network weights. We thus compensate for the network weight changes via a proposed technique based on Taylor polynomials, so that the statistics can be accurately estimated and batch normalization can be effectively applied. On object detection and image classification with small mini-batch sizes, CBN is found to outperform the original batch normalization and a direct calculation of statistics over previous iterations without the proposed compensation technique. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：知名批标准化的问题是迷你小批量的情况下，其显著降低效果。当小批量包含几个例子，在其上归一化定义的统计数据不能可靠地从它训练迭代期间估计。为了解决这个问题，我们提出了交叉迭代批标准化（CBN），其最近多次迭代的例子共同利用，以提高估计质量。在多次迭代计算统计数据的一个挑战是，从不同的迭代中的网络激活没有可比性彼此由于在网络权的变化。因此，我们补偿通过基于泰勒多项式一个提出的技术的网络的重量变化，从而使统计数据可以精确地估计和批量标准化可以有效的应用。关于物体检测及图像分类与迷你小批量大小，CBN发现优于原始批标准化并在先前迭代统计而不拟议补偿技术直接计算。</font>
</div>


<hr>
<div id="paper21"> <b>21. A Simple Framework for Contrastive Learning of Visual Representations</b>  <a href="https://arxiv.org/pdf/2002.05709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Ting Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kornblith%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Kornblith</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Norouzi%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Norouzi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hinton%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geoffrey Hinton</a><br>
<font size="3">
Abstract: This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍SimCLR：对于视觉表现的对比学习一个简单的框架。我们简化了最近提出的对比自我监督学习算法，而不需要专门的架构或存储库。为了了解什么能使对比预测任务学习用的表现，我们系统地研究我们的框架的主要组成部分。我们显示数据增扩的：（1）组合物在定义有效的预测任务（2）将所述表示和所述对比损耗基本上之间的可以学习的非线性变换关键作用，改善了学习表示的质量，和（3）对比学习从大批量和更多的培训措施的好处相比，监督学习。通过结合这些研究结果，我们能够显着跑赢上ImageNet自我监督和半监督学习以前的方法。训练由SimCLR了解到自监督表示的线性分类器达到76.5％顶-1精度，这比以前的国家的最先进的7％的相对改善，匹配的性能监督RESNET-50。当只有1％的标签的微调，我们达到85.8％，排名前五的准确性，跑赢AlexNet与100X较少的标签。</font>
</div>


<hr>
<div id="paper22"> <b>22. Generative-based Airway and Vessel Morphology Quantification on Chest CT  Images</b>  <a href="https://arxiv.org/pdf/2002.05702" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Nardelli%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pietro Nardelli</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ross%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">James C. Ross</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Est%C3%A9par%2C+R+S+J" target="_blank" rel="noopener" style="color:#0000EE;">Raúl San José Estépar</a><br>
<font size="3">
Abstract: Accurately and precisely characterizing the morphology of small pulmonary structures from Computed Tomography (CT) images, such as airways and vessels, is becoming of great importance for diagnosis of pulmonary diseases. The smaller conducting airways are the major site of increased airflow resistance in chronic obstructive pulmonary disease (COPD), while accurately sizing vessels can help identify arterial and venous changes in lung regions that may determine future disorders. However, traditional methods are often limited due to image resolution and artifacts. We propose a Convolutional Neural Regressor (CNR) that provides cross-sectional measurement of airway lumen, airway wall thickness, and vessel radius. CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth. For validation, we first use synthetically generated airways and vessels produced by the proposed generative model to compute the relative error and directly evaluate the accuracy of CNR in comparison with traditional methods. Then, in-vivo validation is performed by analyzing the association between the percentage of the predicted forced expiratory volume in one second (FEV1\%) and the value of the Pi10 parameter, two well-known measures of lung function and airway disease, for airways. For vessels, we assess the correlation between our estimate of the small-vessel blood volume and the lungs' diffusing capacity for carbon monoxide (DLCO). The results demonstrate that Convolutional Neural Networks (CNNs) provide a promising direction for accurately measuring vessels and airways on chest CT images with physiological correlates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：准确且精确地从计算机断层摄影术表征小肺结构的形态（CT）图像，如气道和血管中，对肺部疾病的诊断成为非常重要的。较小的传导气道中是慢性阻塞性肺疾病（COPD）增加的气流阻力的主要部位，而准确地定径容器可以帮助识别在肺部区域的动脉和静脉的变化可确定未来失调。然而，传统的方法往往是有限的，由于图像分辨率和文物。我们提出了一个卷积神经回归（CNR），其提供气道内腔，气道壁的厚度，和容器半径的横截面测量。 CNR进行训练由在与模拟和无监督剖成对抗式网络（SimGAN）来创建仿真和精制气道和与已知的地面实况容器组合使用的合成结构的生成模型创建的数据。进行验证，我们首先使用由所提出的生成模型产生合成产生的气道和血管以计算的相对误差，并直接评价与传统的方法相比CNR的精度。然后，在体内验证是通过分析所预测的用力呼气体积的百分比之间的关联在一秒钟（FEV1 \％）和PI10参数的值中，两个肺功能的公知的措施及气道疾病的药物，进行气道。对于容器，我们评估我们的小血管血液体积的估计和所述肺的一氧化碳弥散（弥散）容量之间的相关性。结果表明，卷积神经网络（细胞神经网络）提供用于准确测量与生理相关因素胸部CT图像的血管和气道有希望的方向。</font>
</div>


<hr>
<div id="paper23"> <b>23. Neuromorphologicaly-preserving Volumetric data encoding using VQ-VAE</b>  <a href="https://arxiv.org/pdf/2002.05692" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Tudosiu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Petru-Daniel Tudosiu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Varsavsky%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Varsavsky</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Shaw%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Shaw</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Graham%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mark Graham</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Nachev%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Parashkev Nachev</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ourselin%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastien Ourselin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sudre%2C+C+H" target="_blank" rel="noopener" style="color:#0000EE;">Carole H. Sudre</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cardoso%2C+M+J" target="_blank" rel="noopener" style="color:#0000EE;">M. Jorge Cardoso</a><br>
<font size="3">
Abstract: The increasing efficiency and compactness of deep learning architectures, together with hardware improvements, have enabled the complex and high-dimensional modelling of medical volumetric data at higher resolutions. Recently, Vector-Quantised Variational Autoencoders (VQ-VAE) have been proposed as an efficient generative unsupervised learning approach that can encode images to a small percentage of their initial size, while preserving their decoded fidelity. Here, we show a VQ-VAE inspired network can efficiently encode a full-resolution 3D brain volume, compressing the data to $0.825\%$ of the original size while maintaining image fidelity, and significantly outperforming the previous state-of-the-art. We then demonstrate that VQ-VAE decoded images preserve the morphological characteristics of the original data through voxel-based morphology and segmentation experiments. Lastly, we show that such models can be pre-trained and then fine-tuned on different datasets without the introduction of bias. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：提高效率和深度学习体系结构紧凑，与硬件的改进在一起，已经使医疗容积数据的复杂性和高维模型在更高的分辨率。近日，矢量量化变自动编码（VQ-VAE）已被提议作为一种高效生成无监督的学习方法，可以对图像进行编码，以他们的初始大小的一小部分，同时保留其解码的保真度。在这里，我们展示了一个VQ-VAE启发网络可以有效地编码全分辨率3D脑容量，数据压缩至$在0.825 \％的原始大小的$同时保持图像保真度，并显著超越以前的状态的最先进的。然后，我们证明了VQ-VAE解码图像通过基于体素的形态学和分割实验保留原始数据的形态特征。最后，我们表明，这种模式可以预先培训，并在不同的数据集，然后微调不引入偏见。</font>
</div>


<hr>
<div id="paper24"> <b>24. FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains</b>  <a href="https://arxiv.org/pdf/2002.05665" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Harb%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeanine Harb</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=R%C3%A9b%C3%A9na%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Rébéna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chosidow%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raphaël Chosidow</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roblin%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Grégoire Roblin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Potarusov%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roman Potarusov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hajri%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hatem Hajri</a><br>
<font size="3">
Abstract: In the realm of autonomous transportation, there have been many initiatives for open-sourcing self-driving cars datasets, but much less for alternative methods of transportation such as trains. In this paper, we aim to bridge the gap by introducing FRSign, a large-scale and accurate dataset for vision-based railway traffic light detection and recognition. Our recordings were made on selected running trains in France and benefited from carefully hand-labeled annotations. An illustrative dataset which corresponds to ten percent of the acquired data to date is published in open source with the paper. It contains more than 100,000 images illustrating six types of French railway traffic lights and their possible color combinations, together with the relevant information regarding their acquisition such as date, time, sensor parameters, and bounding boxes. This dataset is published in open-source at the address \url{this https URL}. We compare, analyze various properties of the dataset and provide metrics to express its variability. We also discuss specific challenges and particularities related to autonomous trains in comparison to autonomous cars. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在自治区交通运输领域，已经出现了开放式采购自动驾驶汽车的数据集诸多举措，但对于运输的替代方法，如火车要少得多。在本文中，我们的目标是通过引入FRSign，一个大型和准确的数据集用于基于视觉的铁路交通灯检测和识别，以缩小差距。我们的记录作了在法国选择运行列车和精心手工标记注释中受益。其对应于所获取的数据的最新的百分之十的示例性数据集发表在开源与纸。它包含了超过10万个图像，说明六类法国铁路交通信号灯和他们可能的颜色组合，连同有关的信息对他们的收购，如日期，时间，传感器参数，和边框。该数据集是在地址\ {URL这HTTPS URL}刊登在开源。我们比较，分析数据集的各种属性和提供指标来表达它的可变性。我们还讨论具体的挑战，相较于自主车与自主列车特殊性。</font>
</div>


<hr>
<div id="paper25"> <b>25. Machines Learn Appearance Bias in Face Recognition</b>  <a href="https://arxiv.org/pdf/2002.05636" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Steed%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryan Steed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caliskan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aylin Caliskan</a><br>
<font size="3">
Abstract: We seek to determine whether state-of-the-art, black box face recognition techniques can learn first-impression appearance bias from human annotations. With FaceNet, a popular face recognition architecture, we train a transfer learning model on human subjects' first impressions of personality traits in other faces. We measure the extent to which this appearance bias is embedded and benchmark learning performance for six different perceived traits. In particular, we find that our model is better at judging a person's dominance based on their face than other traits like trustworthiness or likeability, even for emotionally neutral faces. We also find that our model tends to predict emotions for deliberately manipulated faces with higher accuracy than for randomly generated faces, just like a human subject. Our results lend insight into the manner in which appearance biases may be propagated by standard face recognition models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们试图确定是否国家的最先进的，黑盒脸部识别技术可以借鉴人类注释的第一印象，外观偏向。随着FaceNet，一个流行的脸部识别架构，我们培养对人的在其他面性格特征的第一印象是一个转移的学习模式。我们测量到这次出现偏差被嵌入的程度和基准学习表现为六个不同的感知特性。特别是，我们发现，我们的模型是基于判断自己的脸比其他性状一样可信性或喜爱程度一个人的主导地位，甚至情绪中性面孔更好。我们还发现，我们的模型往往会预测故意操纵面孔的情绪比为随机生成的面部更高的精确度，就像一个人的问题。我们的结果借洞察其外观偏见可以通过标准的面部识别模型来传播的方式。</font>
</div>


<hr>
<div id="paper26"> <b>26. Sparse and Structured Visual Attention</b>  <a href="https://arxiv.org/pdf/2002.05556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+P+H" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Henrique Martins</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Niculae%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vlad Niculae</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marinho%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zita Marinho</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">André Martins</a><br>
<font size="3">
Abstract: Visual attention mechanisms are widely used in multimodal tasks, such as image captioning and visual question answering (VQA). One drawback of softmax-based attention mechanisms is that they assign probability mass to all image regions, regardless of their adjacency structure and of their relevance to the text. In this paper, to better link the image structure with the text, we replace the traditional softmax attention mechanism with two alternative sparsity-promoting transformations: sparsemax, which is able to select the relevant regions only (assigning zero weight to the rest), and a newly proposed Total-Variation Sparse Attention (TVmax), which further encourages the joint selection of adjacent spatial locations. Experiments in image captioning and VQA, using both LSTM and Transformer architectures, show gains in terms of human-rated caption quality, attention relevance, and VQA accuracy, with improved interpretability. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉注意机制被广泛应用于多任务，如图像字幕和视觉问答（VQA）。基于SOFTMAX注意力机制的一个缺点是它们分配概率质量到所有的图像区域，无论其邻接结构及其相关的文字。在本文中，以更好地链接与文本的图像结构，我们更换两种可供选择的稀疏性，促进转变传统的SOFTMAX注意机制：sparsemax，这是能够选择相关区域中仅仅（分配权重为零的其余部分），和新提出的总的变化率稀疏注意（TVmax），其进一步鼓励相邻的空间位置的联合选择。在图像字幕和VQA，同时使用LSTM和变压器的架构实验，显示人类额定字幕质量，重视相关性，准确性VQA，具有完善的可解释性方面的收益。</font>
</div>


<hr>
<div id="paper27"> <b>27. Superpixel Image Classification with Graph Attention Networks</b>  <a href="https://arxiv.org/pdf/2002.05544" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Avelar%2C+P+H+C" target="_blank" rel="noopener" style="color:#0000EE;">Pedro H. C. Avelar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tavares%2C+A+R" target="_blank" rel="noopener" style="color:#0000EE;">Anderson R. Tavares</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=da+Silveira%2C+T+L+T" target="_blank" rel="noopener" style="color:#0000EE;">Thiago L. T. da Silveira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+C+R" target="_blank" rel="noopener" style="color:#0000EE;">Cláudio R. Jung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lamb%2C+L+C" target="_blank" rel="noopener" style="color:#0000EE;">Luís C. Lamb</a><br>
<font size="3">
Abstract: This document reports the use of Graph Attention Networks for classifying oversegmented images, as well as a general procedure for generating oversegmented versions of image-based datasets. The code and learnt models for/from the experiments are available on github. The experiments were ran from June 2019 until December 2019. We obtained better results than the baseline models that uses geometric distance-based attention by using instead self attention, in a more sparsely connected graph network. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文报道了使用图形注意网络用于生成基于图像的数据集oversegmented版本oversegmented图像，以及作为一般程序进行分类。用于/代码，学习模型从实验都可以在GitHub上。该实验是跑到离2019年6月至2019年12月，我们获得比使用，而不是自我的关注，更稀疏连通图网络采用基于几何距离，注意基线模型更好的效果。</font>
</div>


<hr>
<div id="paper28"> <b>28. Deep Learning-based End-to-end Diagnosis System for Avascular Necrosis  of Femoral Head</b>  <a href="https://arxiv.org/pdf/2002.05536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Li</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tian%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hua Tian</a><br>
<font size="3">
Abstract: As the first diagnostic imaging modality of avascular necrosis of the femoral head (AVNFH), accurately staging AVNFH from a plain radiograph is critical and challenging for orthopedists. Thus, we propose a deep learning-based AVNFH diagnosis system (AVN-net). The proposed AVN-net reads plain radiographs of the pelvis, conducts diagnosis, and visualizes results automatically. Deep convolutional neural networks are trained to provide an end-to-end diagnosis solution, covering femoral head detection, exam-view/sides identification, AVNFH diagnosis, and key clinical note generation subtasks. AVN-net is able to obtain state-of-the-art testing AUC of 0.95 (95% CI: 0.92-0.98) in AVNFH detection and significantly greater F1 scores (p<0.01) 1 4 than less-to-moderately experienced orthopedists in all diagnostic tests. furthermore, two real-world pilot studies were conducted for diagnosis support and education assistance, respectively, to assess the utility of avn-net. experimental results are promising. with avn-net as a reference, accuracy consistency considerably improved while requiring only time. students self-studying avnfh using can learn better faster control group. best our knowledge, this study is first research on prospective use deep learning-based system by conducting representing application scenarios. we have demonstrated that proposed achieves expert-level performance, provides efficient clinical decision-making, effectively passes experience students. < font>
<br>
<font size="2" style="line-height:30px;">
摘要：股骨头缺血性坏死（AVNFH）的所述第一诊断成像模态，准确地从平片分级AVNFH是关键的，并且对骨科挑战。因此，我们提出了一个深刻的学习型AVNFH诊断系统（AVN-网）。所提出的AVN网自动读取骨盆，行为诊断的平片，以及可视化的结果。深卷积神经网络被训练，以提供端至端诊断溶液，覆盖股骨头检测，考试视点/边识别，AVNFH诊断，和关键临床音符生成的子任务。 AVN网能够获得国家的最先进的测试0.95的AUC（95％CI：0.92-0.98）中AVNFH检测和显著更大F1分数（P <0.01）小于到适度在所有经验骨科诊断测试。此外，两个真实世界的试点研究，诊断支持和教育协助下进行，分别评估avn网的效用。实验结果是有希望的。与avn网诊断，因为所有的骨科的基准，诊断的准确性和一致性，同时仅需要的时间的1 4大幅度地改善。学生自主学习使用avn网能够更好地学习和速度比对照组avnfh诊断。据我们所知，这研究是开展代表现实世界的应用场景的两个试点研究，在未来的使用了avnfh深基础的学习诊断系统的第一个研究。我们已经证明，所提出的avn网达到专家级avnfh诊断性能，提供了在临床决策的有效支持，并有效地传递临床经验的学生。< font>
</0.01）小于到适度在所有经验骨科诊断测试。此外，两个真实世界的试点研究，诊断支持和教育协助下进行，分别评估avn网的效用。实验结果是有希望的。与avn网诊断，因为所有的骨科的基准，诊断的准确性和一致性，同时仅需要的时间的1></font></0.01)></font></div>


<hr>
<div id="paper29"> <b>29. Abnormal respiratory patterns classifier may contribute to large-scale  screening of people infected with COVID-19 in an accurate and unobtrusive  manner</b>  <a href="https://arxiv.org/pdf/2002.05534" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunlu Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Menghan Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingli Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao-Ping Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhai%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guangtao Zhai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Yao</a><br>
<font size="3">
Abstract: Research significance: During the epidemic prevention and control period, our study can be helpful in prognosis, diagnosis and screening for the patients infected with COVID-19 (the novel coronavirus) based on breathing characteristics. According to the latest clinical research, the respiratory pattern of COVID-19 is different from the respiratory patterns of flu and the common cold. One significant symptom that occurs in the COVID-19 is Tachypnea. People infected with COVID-19 have more rapid respiration. Our study can be utilized to distinguish various respiratory patterns and our device can be preliminarily put to practical use. Demo videos of this method working in situations of one subject and two subjects can be downloaded online. Research details: Accurate detection of the unexpected abnormal respiratory pattern of people in a remote and unobtrusive manner has great significance. In this work, we innovatively capitalize on depth camera and deep learning to achieve this goal. The challenges in this task are twofold: the amount of real-world data is not enough for training to get the deep model; and the intra-class variation of different types of respiratory patterns is large and the outer-class variation is small. In this paper, considering the characteristics of actual respiratory signals, a novel and efficient Respiratory Simulation Model (RSM) is first proposed to fill the gap between the large amount of training data and scarce real-world data. Subsequently, we first apply a GRU neural network with bidirectional and attentional mechanisms (BI-AT-GRU) to classify 6 clinically significant respiratory patterns (Eupnea, Tachypnea, Bradypnea, Biots, Cheyne-Stokes and Central-Apnea). The proposed deep model and the modeling ideas have the great potential to be extended to large scale applications such as public places, sleep scenario, and office environment. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：研究意义：在疫情防控期间，我们的研究可以在预后，有助于诊断和筛查感染基于呼吸特性COVID-19（该新型冠状病毒）的患者。根据最新的临床研究，COVID-19的呼吸模式是由流感的呼吸模式和普通感冒不同。发生在COVID-19的一个显著的症状是呼吸急促。感染COVID-19的人有更多的呼吸急促。我们的研究可以用来区分不同的呼吸模式和我们的设备可以预先投入实际使用。这种方法在一个主体和两个科目的情况下工作的演示视频可以在网上下载。研究细节：人在一个偏僻的和不显眼的方式意想不到的异常呼吸模式的准确的检测具有重要的意义。在这项工作中，我们创新性地利用深度相机和深度学习到实现这一目标。此任务中的挑战是双重的：真实世界的数据量是不够的训练得到深层模型;和不同类型的呼吸模式的类内变化较大和外级变化小。在本文中，考虑到实际的呼吸信号，一种新颖且有效的呼吸仿真模型（RSM）的特性被首次提出以填充大量的训练数据和稀缺真实世界的数据之间的间隙。随后，我们首先应用具有双向和注意力机制（BI-AT-GRU）一GRU神经网络分类6种临床显著的呼吸模式（正常呼吸，呼吸急促，Bradypnea，Biots，陈 - 施氏及中环呼吸暂停）。所提出的深层模型和建模的思想有很大的潜力可扩展到大规模应用，如公共场所，睡眠情况，以及办公环境。</font>
</div>


<hr>
<div id="paper30"> <b>30. Real or Not Real, that is the Question</b>  <a href="https://arxiv.org/pdf/2002.05512" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiangli%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanbo Xiangli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yubin Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Dai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Loy%2C+C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Change Loy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dahua Lin</a><br>
<font size="3">
Abstract: While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper we generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. Compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN architecture to generate realistic images at 1024*1024 resolution when trained from scratch. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虽然生成对抗网络（GAN）已经在各种主题被广泛采用，在本文中，我们通过治疗真实性为可以从多个角度来估计一个随机变量概括的标准GAN到一个新的视角。在此广义框架中，被称为RealnessGAN，鉴别器输出一个分发真实性的量度。虽然RealnessGAN分享相似的理论保证与标准甘，它提供了对抗的学习更多的见解。相比于多基线，RealnessGAN提供了更强的指导发电机，实现对合成和真实世界的数据集的改进。此外，它使基本DCGAN架构在1024 * 1024分辨率从头开始训练的时候，产生逼真的图像。</font>
</div>


<hr>
<div id="paper31"> <b>31. MLFcGAN: Multi-level Feature Fusion based Conditional GAN for Underwater  Image Color Correction</b>  <a href="https://arxiv.org/pdf/2002.05333" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaodong Liu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Gao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhi Gao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chen%2C+B+M" target="_blank" rel="noopener" style="color:#0000EE;">Ben M. Chen</a><br>
<font size="3">
Abstract: Color correction for underwater images has received increasing interests, due to its critical role in facilitating available mature vision algorithms for underwater scenarios. Inspired by the stunning success of deep convolutional neural networks (DCNNs) techniques in many vision tasks, especially the strength in extracting features in multiple scales, we propose a deep multi-scale feature fusion net based on the conditional generative adversarial network (GAN) for underwater image color correction. In our network, multi-scale features are extracted first, followed by augmenting local features on each scale with global features. This design was verified to facilitate more effective and faster network learning, resulting in better performance in both color correction and detail preservation. We conducted extensive experiments and compared with the state-of-the-art approaches quantitatively and qualitatively, showing that our method achieves significant improvements. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：颜色校正水下图像已经受到越来越多的利益，由于在促进现有成熟的视觉算法用于水下场景中的关键作用。深卷积神经网络（DCNNs）在许多视觉任务的技术，尤其是在多个尺度提取特征的实力令人惊叹的成功的启发，我们提出了一个深刻的多尺度特征融合基础条件生成对抗网络（GAN）的净水下图像颜色校正。在我们的网络，多尺度特征首先提取，然后在全球各功能扩充规模的局部特征。这种设计进行了验证，以促进更有效和更快的网络学习，导致这两个色彩校正和细节保持更好的性能。我们进行了广泛的实验，相比定量和定性的方法，这表明我们的方法实现显著的改善最先进的国家的的。</font>
</div>


<hr>
<div id="paper32"> <b>32. Physical Accuracy of Deep Neural Networks for 2D and 3D Multi-Mineral  Segmentation of Rock micro-CT Images</b>  <a href="https://arxiv.org/pdf/2002.05322" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Da+Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Ying Da Wang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Shabaninejad%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehdi Shabaninejad</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Armstrong%2C+R+T" target="_blank" rel="noopener" style="color:#0000EE;">Ryan T. Armstrong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mostaghimi%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peyman Mostaghimi</a><br>
<font size="3">
Abstract: Segmentation of 3D micro-Computed Tomographic uCT) images of rock samples is essential for further Digital Rock Physics (DRP) analysis, however, conventional methods such as thresholding, watershed segmentation, and converging active contours are susceptible to user-bias. Deep Convolutional Neural Networks (CNNs) have produced accurate pixelwise semantic segmentation results with natural images and $\mu$CT rock images, however, physical accuracy is not well documented. The performance of 4 CNN architectures is tested for 2D and 3D cases in 10 configurations. Manually segmented uCT images of Mt. Simon Sandstone are treated as ground truth and used as training and validation data, with a high voxelwise accuracy (over 99%) achieved. Downstream analysis is then used to validate physical accuracy. The topology of each segmented phase is calculated, and the absolute permeability and multiphase flow is modelled with direct simulation in single and mixed wetting cases. These physical measures of connectivity, and flow characteristics show high variance and uncertainty, with models that achieve 95\%+ in voxelwise accuracy possessing permeabilities and connectivities orders of magnitude off. A new network architecture is also introduced as a hybrid fusion of U-net and ResNet, combining short and long skip connections in a Network-in-Network configuration. The 3D implementation outperforms all other tested models in voxelwise and physical accuracy measures. The network architecture and the volume fraction in the dataset (and associated weighting), are factors that not only influence the accuracy trade-off in the voxelwise case, but is especially important in training a physically accurate model for segmentation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：3D的分割微计算机断层UCT）岩石样品的图像是用于进一步数字岩石物理（DRP）分析必不可少的，然而，常规方法如阈值，分水岭分割，并会聚主动轮廓很容易受到用户偏置。深卷积神经网络（细胞神经网络）已经产生与自然图像和$ \亩$ CT岩石的图像，然而，物理精度不会有据可查的准确按像素语义分割结果。的4 CNN架构性能为2D和3D的情况下在10个配置测试。人工分割山UCT图片西蒙砂岩被视为基础事实和用作训练和验证数据，以实现高的精度voxelwise（超过99％）。然后向下游分析用于验证物理精度。每个分段的相位的拓扑计算，并且绝对渗透率和多相流建模与单一和混合润湿的情况下直接模拟。连接的这些物理措施和流动特性表现出较大差异性和不确定性，与在voxelwise准确性拥有幅度的渗透性和连通性关闭订单达到95 \％+车型。一个新的网络架构也被引入作为U型网和RESNET，结合短期和长期跳过在以网络为在网络配置连接的杂合融合。三维实现优于所有其他测试车型voxelwise和物理精度的措施。的网络体系结构和在数据集（和相关联的权重）的体积分数，是不仅影响精度的折衷在voxelwise情况下因素，但是在训练物理上精确模型分割尤其重要。</font>
</div>


<hr>
<div id="paper33"> <b>33. A Provably Robust Multiple Rotation Averaging Scheme for SO(2)</b>  <a href="https://arxiv.org/pdf/2002.05299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/math?searchtype=author&query=Maunu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tyler Maunu</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Lerman%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gilad Lerman</a><br>
<font size="3">
Abstract: We give adversarial robustness results for synchronization on the rotation group over $\mathbb{R}^2$, $\mathrm{SO}(2)$. In particular, we consider an adversarial corruption setting, where an adversary can choose which measurements to corrupt as well as what to corrupt them to. In this setting, we first show that some common nonconvex formulations, which are categorized as "multiple rotation averaging", may fail. We then discuss a new fast algorithm, called Trimmed Averaging Synchronization, which has exact recovery and linear convergence up to an outlier percentage of $1/4$. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：给予超过$ \ mathbb {R} ^ 2，$ \ mathrm {SO}（2）$上旋转组同步对抗性鲁棒性的结果。特别是，我们考虑一个对抗性腐败的设置，其中一个对手可以选择测量腐败是什么，以及腐败他们。在这种背景下，我们首先表明，一些常见的非凸制剂，其被归类为“多回转平均”，可能会失败。然后，我们讨论了一个新的快速算法，称为修剪平均化同步，其中有确切的恢复和线性收敛高达$ 1/4 $离群值百分比。</font>
</div>


<hr>
<div id="paper34"> <b>34. Geom-GCN: Geometric Graph Convolutional Networks</b>  <a href="https://arxiv.org/pdf/2002.05287" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pei%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongbin Pei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bingzhe Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K+C" target="_blank" rel="noopener" style="color:#0000EE;">Kevin Chen-Chuan Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lei%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Lei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Yang</a><br>
<font size="3">
Abstract: Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed GeomGCN, to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：消息传递神经网络（MPNNs）已经在各种实际应用中已成功地应用于表示学习上的图表。然而，MPNNs'聚合的两个根本性的弱点限制了他们的代表图结构数据的能力：失去街区节点的结构信息，缺乏捕捉到远距离的依赖于异配图的能力。很少有研究发现从不同的角度弱点。从经典的神经网络和网络上的几何形状的观察结果，我们提出了图形神经网络克服了两个弱点新颖的几何集成方案。后面的基本思想是在曲线图上的聚合可以从一个连续的空间图形底层受益。建议的聚合方案是排列不变和由三个模块组成，节点嵌入，结构附近，以及两级聚集。我们还提出在图形卷积网络计划的实施，被称为GeomGCN，对图形进行式学习。实验结果表明，所提出的Geom-GCN上大量的图形的开放数据集的实现状态的最先进的性能。代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper35"> <b>35. HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine  Learning Models</b>  <a href="https://arxiv.org/pdf/2002.05271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title35" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianwen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alexander%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Alexander</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pegg%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jack Pegg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huamin Qu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Min Chen</a><br>
<font size="3">
Abstract: In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a "concept" or "feature" may benefit or hinder a ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing data is transformed to a visual representation for rapid observation of the conclusions and the logical flow between the testing data and hypotheses.We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了实现的机器学习（ML）模型基于假设的评估可视化分析工具。我们描述了一种新ML-测试框架，结合有关的多个假设的结论，逻辑推理传统的统计假设检验（在实证研究常用）。该框架定义了用于测试多个假设是否以及如何对一个“概念”或“功能”一些额外的信息可能会受益或阻碍ML模型控制的配置。因为推理多个假设并不总是简单的，我们提供HypoML作为视觉分析工具，与其中，多线程测试数据被变换为结论的快速观察和测试数据和假设之间的逻辑流程的可视化表示。我们应用HypoML到多个虚拟的概念，展示了可视化分析的直观解释的性质。</font>
</div>


<hr>
<div id="paper36"> <b>36. Graph Similarity Using PageRank and Persistent Homology</b>  <a href="https://arxiv.org/pdf/2002.05158" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title36" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hajij%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mustafa Hajij</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Munch%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elizabeth Munch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosen%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paul Rosen</a><br>
<font size="3">
Abstract: The PageRank of a graph is a scalar function defined on the node set of the graph which encodes nodes centrality information of the graph. In this work, we utilize the PageRank function on the lower-star filtration of the graph as input to persistent homology to study the problem of graph similarity. By representing each graph as a persistence diagram, we can then compare outputs using the bottleneck distance. We show the effectiveness of our method by utilizing it on two shape mesh datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一个图的PageRank是在节点集的图表，其编码节点的曲线图的中心性信息的定义的标量函数。在这项工作中，我们利用图表上的输入，持续的同源性研究图形的相似问题的低星级过滤的PageRank功能。由表示每个图形作为持久图，我们可以然后比较输出使用所述瓶颈距离。我们利用这两个状的网数据集显示了该方法的有效性。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-13</title>
    <url>/2020/02/14/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-13/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Rembrandts and Robots: Using Neural Networks to Explore Authorship in  Painting <a href="https://arxiv.org/pdf/2002.05107" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Component Analysis for Visual Question Answering Architectures <a href="https://arxiv.org/pdf/2002.05104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> AlignNet: A Unifying Approach to Audio-Visual Alignment <a href="https://arxiv.org/pdf/2002.05070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Detect and Correct Bias in Multi-Site Neuroimaging Datasets <a href="https://arxiv.org/pdf/2002.05049" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Intra-Camera Supervised Person Re-Identification <a href="https://arxiv.org/pdf/2002.05046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Learning light field synthesis with Multi-Plane Images: scene encoding  as a recurrent segmentation task <a href="https://arxiv.org/pdf/2002.05028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Hi-Net: Hybrid-fusion Network for Multi-modal MR Image Synthesis <a href="https://arxiv.org/pdf/2002.05000" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Real-Time Semantic Background Subtraction <a href="https://arxiv.org/pdf/2002.04993" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Hierarchical Auto-Regressive Model for Image Compression Incorporating  Object Saliency and a Deep Perceptual Loss <a href="https://arxiv.org/pdf/2002.04988" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Towards Precise Intra-camera Supervised Person Re-identification <a href="https://arxiv.org/pdf/2002.04932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> A Zero-Shot based Fingerprint Presentation Attack Detection System <a href="https://arxiv.org/pdf/2002.04908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Bi-Directional Generation for Unsupervised Domain Adaptation <a href="https://arxiv.org/pdf/2002.04869" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Analysis Of Multi Field Of View Cnn And Attention Cnn On H&amp;E Stained  Whole-slide Images On Hepatocellular Carcinoma <a href="https://arxiv.org/pdf/2002.04836" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> End-to-End Face Parsing via Interlinked Convolutional Neural Networks <a href="https://arxiv.org/pdf/2002.04831" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Uniform Interpolation Constrained Geodesic Learning on Data Manifold <a href="https://arxiv.org/pdf/2002.04829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Deep-HR: Fast Heart Rate Estimation from Face Video Under Realistic  Conditions <a href="https://arxiv.org/pdf/2002.04821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> A Visual-inertial Navigation Method for High-Speed Unmanned Aerial  Vehicles <a href="https://arxiv.org/pdf/2002.04791" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> MFFW: A new dataset for multi-focus image fusion <a href="https://arxiv.org/pdf/2002.04780" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Efficient Training of Deep Convolutional Neural Networks by Augmentation  in Embedding Space <a href="https://arxiv.org/pdf/2002.04776" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Progressive Object Transfer Detection <a href="https://arxiv.org/pdf/2002.04741" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Improving Place Recognition Using Dynamic Object Detection <a href="https://arxiv.org/pdf/2002.04698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Learning spatio-temporal representations with temporal squeeze pooling <a href="https://arxiv.org/pdf/2002.04685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Object Detection as a Positive-Unlabeled Problem <a href="https://arxiv.org/pdf/2002.04672" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Validating uncertainty in medical image translation <a href="https://arxiv.org/pdf/2002.04639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Finding novelty with uncertainty <a href="https://arxiv.org/pdf/2002.04626" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Patternless Adversarial Attacks on Video Recognition Networks <a href="https://arxiv.org/pdf/2002.05123" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> From IC Layout to Die Photo: A CNN-Based Data-Driven Approach <a href="https://arxiv.org/pdf/2002.04967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Synaptic Integration of Spatiotemporal Features with a Dynamic  Neuromorphic Processor <a href="https://arxiv.org/pdf/2002.04924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Machine-Learning-Based Multiple Abnormality Prediction with Large-Scale  Chest Computed Tomography Volumes <a href="https://arxiv.org/pdf/2002.04752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for  Healthcare <a href="https://arxiv.org/pdf/2002.04700" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> fastai: A Layered API for Deep Learning <a href="https://arxiv.org/pdf/2002.04688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> A Non-Intrusive Correction Algorithm for Classification Problems with  Corrupted Data <a href="https://arxiv.org/pdf/2002.04658" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Neuroevolution of Neural Network Architectures Using CoDeepNEAT and  Keras <a href="https://arxiv.org/pdf/2002.04634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Rembrandts and Robots: Using Neural Networks to Explore Authorship in  Painting</b>  <a href="https://arxiv.org/pdf/2002.05107" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+S+J" target="_blank" rel="noopener" style="color:#0000EE;">Steven J. Frank</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+A+M" target="_blank" rel="noopener" style="color:#0000EE;">Andrea M. Frank</a><br>
<font size="3">
Abstract: We use convolutional neural networks to analyze authorship questions surrounding works of representational art. Trained on the works of an artist under study and visually comparable works of other artists, our system can identify forgeries and provide attributions. Our system can also assign classification probabilities within a painting, revealing mixed authorship and identifying regions painted by different hands. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们使用卷积神经网络来分析周围的代表性艺术作品的著作权问题。培训了一个艺术家的所研究的作品和其他艺术家的视觉作品相媲美，我们的系统可以识别伪造并提供归属。我们的系统可以画内还可以指派分类概率，揭示了混合署名权，并确定由不同的双手涂上地区。</font>
</div>


<hr>
<div id="paper2"> <b>2. Component Analysis for Visual Question Answering Architectures</b>  <a href="https://arxiv.org/pdf/2002.05104" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kolling%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Camila Kolling</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wehrmann%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jônatas Wehrmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barros%2C+R+C" target="_blank" rel="noopener" style="color:#0000EE;">Rodrigo C. Barros</a><br>
<font size="3">
Abstract: Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在计算机视觉和自然语言处理的最新研究进展介绍，铺平解决AI完全问题的方式新颖任务。其中的一个任务被称为视觉答疑（VQA）。一个VQA系统必须采取的图像和关于图像的自由形式的，开放式的自然语言问题，而产生的自然语言回答作为输出。这样的任务已经从科学界，其产生的旨在提高VQA预测精度接近过多的高度关注。他们中的大多数包括三个主要组成部分：（一）独立表示学习图像和问题; （ⅱ）特征融合因此模型可以使用来自两个源的信息来回答问题视觉;及（iii）在自然语言的正确答案的产生。有了这么多的方法被新近推出的，它变得不明朗的各成分的模型的最终性能的真正的贡献。本文的主要目的是提供关于每个组件的VQA模型的影响进行全面分析。我们广泛组实验涵盖视觉和文本元素，以及这些表象的融合，注重机制的形式组合。我们的主要贡献是确定培训VQA车型的核心部件，以最大限度地提高其预测性能。</font>
</div>


<hr>
<div id="paper3"> <b>3. AlignNet: A Unifying Approach to Audio-Visual Alignment</b>  <a href="https://arxiv.org/pdf/2002.05070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianren Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhaoyuan Fang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Zhao</a><br>
<font size="3">
Abstract: We present AlignNet, a model that synchronizes videos with reference audios under non-uniform and irregular misalignments. AlignNet learns the end-to-end dense correspondence between each frame of a video and an audio. Our method is designed according to simple and well-established principles: attention, pyramidal processing, warping, and affinity function. Together with the model, we release a dancing dataset Dance50 for training and evaluation. Qualitative, quantitative and subjective evaluation results on dance-music alignment and speech-lip alignment demonstrate that our method far outperforms the state-of-the-art methods. Project video and code are available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前AlignNet，即同步与下不均匀和不规则的错位参考音频视频的模式。 AlignNet得知一个视频的每个帧和音频之间的端至端致密的对应关系。注意，金字塔形处理，翘曲和亲和功能：我们的方法是根据简单的和行之有效的原则设计的。连同模型，我们发布一个跳舞数据集Dance50的培训和考核。定性，舞蹈，音乐对准和语音唇对准定量和主观评价结果表明，我们的方法远优于国家的最先进的方法。项目视频和代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper4"> <b>4. Detect and Correct Bias in Multi-Site Neuroimaging Datasets</b>  <a href="https://arxiv.org/pdf/2002.05049" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wachinger%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Wachinger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rieckmann%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anna Rieckmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=P%C3%B6lsterl%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sebastian Pölsterl</a><br>
<font size="3">
Abstract: The desire to train complex machine learning algorithms and to increase the statistical power in association studies drives neuroimaging research to use ever-larger datasets. The most obvious way to increase sample size is by pooling scans from independent studies. However, simple pooling is often ill-advised as selection, measurement, and confounding biases may creep in and yield spurious correlations. In this work, we combine 35,320 magnetic resonance images of the brain from 17 studies to examine bias in neuroimaging. In the first experiment, Name That Dataset, we provide empirical evidence for the presence of bias by showing that scans can be correctly assigned to their respective dataset with 71.5% accuracy. Given such evidence, we take a closer look at confounding bias, which is often viewed as the main shortcoming in observational studies. In practice, we neither know all potential confounders nor do we have data on them. Hence, we model confounders as unknown, latent variables. Kolmogorov complexity is then used to decide whether the confounded or the causal model provides the simplest factorization of the graphical model. Finally, we present methods for dataset harmonization and study their ability to remove bias in imaging features. In particular, we propose an extension of the recently introduced ComBat algorithm to control for global variation across image features, inspired by adjusting for population stratification in genetics. Our results demonstrate that harmonization can reduce dataset-specific information in image features. Further, confounding bias can be reduced and even turned into a causal relationship. However, harmonziation also requires caution as it can easily remove relevant subject-specific information. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：培养复杂的机器学习算法，提高了统计功率关联研究神经影像学驱动器研究使用越来越大的数据集的愿望。增加样本规模最明显的方法是由独立的研究集中扫描。然而，简单的池通常不明智作为选择，测量和混杂偏差可能在蠕变和屈服虚假相关。在这项工作中，我们结合大脑的35320个磁共振图像从17项研究审查神经影像学的偏见。在第一个实验中，名称该数据集，我们提供了由表示扫描可以正确地分配给它们各自的数据集与71.5％的准确度存在偏差的经验证据。鉴于这些证据，我们就在混杂的偏见，这通常被视为观察性研究的主要缺点一探究竟。在实践中，我们不知道，所有的潜在混杂因素我们也没有对他们的数据。因此，我们的模型混杂因素未知的，潜在变量。然后Kolmogorov复杂被用来决定是否混淆或因果模型提供图形模型的最简单的因式分解。最后，我们对数据集协调本发明的方法，并研究其去除影像学特征偏差的能力。特别是，我们提出了最近推出的打击算法的扩展来控制整个图像的功能，通过调整遗传学群体分层的启发全球变化。我们的研究结果表明，统一可以降低图像特征数据集的特定信息。另外，混杂偏压可以减少，甚至变成了因果关系。然而，harmonziation也需要谨慎，因为它可以很容易地删除相关对象特定信息。</font>
</div>


<hr>
<div id="paper5"> <b>5. Intra-Camera Supervised Person Re-Identification</b>  <a href="https://arxiv.org/pdf/2002.05046" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangping Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiatian Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minxian Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morerio%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pietro Morerio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Murino%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vittorio Murino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shaogang Gong</a><br>
<font size="3">
Abstract: Existing person re-identification (re-id) methods mostly exploit a large set of cross-camera identity labelled training data. This requires a tedious data collection and annotation process, leading to poor scalability in practical re-id applications. On the other hand unsupervised re-id methods do not need identity label information, but they usually suffer from much inferior and insufficient model performance. To overcome these fundamental limitations, we propose a novel person re-identification paradigm based on an idea of independent per-camera identity annotation. This eliminates the most time-consuming and tedious inter-camera identity labelling process, significantly reducing the amount of human annotation efforts. Consequently, it gives rise to a more scalable and more feasible setting, which we call Intra-Camera Supervised (ICS) person re-id, for which we formulate a Multi-tAsk mulTi-labEl (MATE) deep learning method. Specifically, MATE is designed for self-discovering the cross-camera identity correspondence in a per-camera multi-task inference framework. Extensive experiments demonstrate the cost-effectiveness superiority of our method over the alternative approaches on three large person re-id datasets. For example, MATE yields 88.7% rank-1 score on Market-1501 in the proposed ICS person re-id setting, significantly outperforming unsupervised learning models and closely approaching conventional fully supervised learning competitors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现有人员重新鉴定（重新编号）方法主要是利用大集交叉的摄像机标识标记的训练数据。这需要繁琐的数据收集和注释过程，从而导致实际的再ID应用程序可扩展性差。在另一方面监督的重新编号方法不需要身份标签信息，但它们通常是从远不如和不足模型的性能受到影响。为了克服这些基本的限制，提出了一种基于独立的每个摄像机的身份注解的想法新颖的人重新鉴定的范例。这消除了最耗时和繁琐的摄像装置间的身份标记过程，显著减少人为批注努力的量。因此，产生了更多的可扩展性和更可行的设置，我们称之为摄像机内监督（ICS）的人重新编号，为此我们制定一个多任务多标签（MATE）深的学习方法。具体而言，MATE被设计用于在每个摄像机多任务推理框架自发现横相机身份对应。大量的实验证明我们的方法超过三个大的人重新编号数据集替代方法的成本效益优势。例如，MATE产生在建议ICS人再ID设置，以市场为1501 88.7％秩1的比分，显著跑赢无监督学习模式，并密切接近传统的完全监督学习的竞争对手。</font>
</div>


<hr>
<div id="paper6"> <b>6. Learning light field synthesis with Multi-Plane Images: scene encoding  as a recurrent segmentation task</b>  <a href="https://arxiv.org/pdf/2002.05028" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=V%C3%B6lker%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomás Völker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Boisson%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guillaume Boisson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chupeau%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bertrand Chupeau</a><br>
<font size="3">
Abstract: In this paper we address the problem of view synthesis from large baseline light fields, by turning a sparse set of input views into a Multi-plane Image (MPI). Because available datasets are scarce, we propose a lightweight network that does not require extensive training. Unlike latest approaches, our model does not learn to estimate RGB layers but only encodes the scene geometry within MPI alpha layers, which comes down to a segmentation task. A Learned Gradient Descent (LGD) framework is used to cascade the same convolutional network in a recurrent fashion in order to refine the volumetric representation obtained. Thanks to its low number of parameters, our model trains successfully on a small light field video dataset and provides visually appealing results. It also exhibits convenient generalization properties regarding both the number of input views, the number of depth planes in the MPI, and the number of refinement iterations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们解决视图合成的问题从大基线光场，通过转动稀疏集合的输入视图到多平面图像（MPI）。由于可用的数据集是稀缺的，我们建议，不需要大量的培训一个轻量级的网络。与最新的方法，我们的模型不学习估算RGB层，但仅编码MPI阿尔法层内的场景几何，这可以归结为一个细分任务。习得梯度下降（LGD）框架用于级联中一个反复出现的方式相同的卷积网络，以便改进所获得的体积表示。由于其数量少的参数，我们的模型成功列车小亮场图像数据集，并提供视觉吸引力的结果。这也显示出关于输入两种意见的数量，深度平面中的MPI的数量，和改进的迭代次数方便的泛化性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. Hi-Net: Hybrid-fusion Network for Multi-modal MR Image Synthesis</b>  <a href="https://arxiv.org/pdf/2002.05000" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huazhu Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianbing Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Ling Shao</a><br>
<font size="3">
Abstract: Magnetic resonance imaging (MRI) is a widely used neuroimaging technique that can provide images of different contrasts (i.e., modalities). Fusing this multi-modal data has proven particularly effective for boosting model performance in many tasks. However, due to poor data quality and frequent patient dropout, collecting all modalities for every patient remains a challenge. Medical image synthesis has been proposed as an effective solution to this, where any missing modalities are synthesized from the existing ones. In this paper, we propose a novel Hybrid-fusion Network (Hi-Net) for multi-modal MR image synthesis, which learns a mapping from multi-modal source images (i.e., existing modalities) to target images (i.e., missing modalities). In our Hi-Net, a modality-specific network is utilized to learn representations for each individual modality, and a fusion network is employed to learn the common latent representation of multi-modal data. Then, a multi-modal synthesis network is designed to densely combine the latent representation with hierarchical features from each modality, acting as a generator to synthesize the target images. Moreover, a layer-wise multi-modal fusion strategy is presented to effectively exploit the correlations among multiple modalities, in which a Mixed Fusion Block (MFB) is proposed to adaptively weight different fusion strategies (i.e., element-wise summation, product, and maximization). Extensive experiments demonstrate that the proposed model outperforms other state-of-the-art medical image synthesis methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：磁共振成像（MRI）是一种广泛使用的神经成像技术，其可以提供不同的对比度（即，模式）的图像。这个融合多模态数据已经证明，在许多任务提高模型的性能特别有效。然而，由于不良的数据质量和频繁的患者差，收集所有方式为每位患者仍然是一个挑战。医用图像合成已经被提出作为一种有效的解决方案，这一点，其中任何缺失的方式从现有的合成。在本文中，我们提出一种用于多模态MR图像合成的新型混合融合网络（高净），该学习到的多模态的源图像的映射（即，现有的模式）到目标图像（即，丢失的方式） 。在我们的Hi-网，一个特定的模态网络被用于学习的表示为每个单独的模式，并且采用的融合网络学习多模态数据的共同潜表示。然后，多模式合成网被设计成密集地结合具有分级特性的潜表示从每个模态，作为发电机来合成目标图像。此外，逐层多模态融合策略被呈现给有效地利用其中混合融合块（MFB）提出了自适应重量不同融合策略（即，逐元素求和，产品和多个模态之间的相关性，最大化）。广泛的实验表明，该模型优于其他国家的最先进的医用图像的合成方法。</font>
</div>


<hr>
<div id="paper8"> <b>8. Real-Time Semantic Background Subtraction</b>  <a href="https://arxiv.org/pdf/2002.04993" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cioppa%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anthony Cioppa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Droogenbroeck%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marc Van Droogenbroeck</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Braham%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marc Braham</a><br>
<font size="3">
Abstract: Semantic background subtraction SBS has been shown to improve the performance of most background subtraction algorithms by combining them with semantic information, derived from a semantic segmentation network. However, SBS requires high-quality semantic segmentation masks for all frames, which are slow to compute. In addition, most state-of-the-art background subtraction algorithms are not real-time, which makes them unsuitable for real-world applications. In this paper, we present a novel background subtraction algorithm called Real-Time Semantic Background Subtraction (denoted RT-SBS) which extends SBS for real-time constrained applications while keeping similar performances. RT-SBS effectively combines a real-time background subtraction algorithm with high-quality semantic information which can be provided at a slower pace, independently for each pixel. We show that RT-SBS coupled with ViBe sets a new state of the art for real-time background subtraction algorithms and even competes with the non real-time state-of-the-art ones. Note that python CPU and GPU implementations of RT-SBS will be released soon. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语义背景减除SBS已经显示出通过将它们与语义信息，从一个语义分割网络衍生组合以改善的最背景减除算法的性能。然而，SBS需要对所有帧，这是缓慢的计算高质量的语义分割口罩。此外，国家的最先进最背景减除算法不是实时的，这使得它们不适合于现实世界的应用。在本文中，我们提出了所谓的实时语义背景减法（表示RT-SBS）一种新型的背景减除算法延伸SBS实时受限的应用，同时保持性能相似。 RT-SBS有效地结合有能够以较慢的速度被提供，独立地对每个像素的高品质的语义信息的实时背景减除算法。我们表明，RT-SBS加上盛传将艺术进行实时背景减除算法的一个新的状态，甚至与非实时状态的最先进的人竞争。需要注意的是RT-SBS的蟒蛇CPU和GPU的实现将很快被释放。</font>
</div>


<hr>
<div id="paper9"> <b>9. Hierarchical Auto-Regressive Model for Image Compression Incorporating  Object Saliency and a Deep Perceptual Loss</b>  <a href="https://arxiv.org/pdf/2002.04988" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Patel%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yash Patel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Appalaraju%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Srikar Appalaraju</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manmatha%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">R. Manmatha</a><br>
<font size="3">
Abstract: We propose a new end-to-end trainable model for lossy image compression which includes a number of novel components. This approach incorporates 1) a hierarchical auto-regressive model; 2)it also incorporates saliency in the images and focuses on reconstructing the salient regions better; 3) in addition, we empirically demonstrate that the popularly used evaluations metrics such as MS-SSIM and PSNR are inadequate for judging the performance of deep learned image compression techniques as they do not align well with human perceptual similarity. We, therefore propose an alternative metric, which is learned on perceptual similarity data specific to image compression. Our experiments show that this new metric aligns significantly better with human judgments when compared to other hand-crafted or learned metrics. The proposed compression model not only generates images that are visually better but also gives superior performance for subsequent computer vision tasks such as object detection and segmentation when compared to other engineered or learned codecs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了有损图像压缩，其包括许多新颖的部件的新的端至端的可训练模型。这种方法结合1）分层自回归模型; 2）它还采用在图像中的显着性，侧重于重构显着区域更好; 3）此外，我们凭经验证明普遍使用的评价指标，例如MS-SSIM和PSNR是不足判断的深了解到图像压缩技术的性能，因为它们不与人类感知相似井对齐。我们因此提出替代度量，这是在感知相似数据中的特定图像压缩获知。我们的实验表明，这种新的度量与对齐人为判断显著更好时，相对于其他手工制作或学习指标。所提出的压缩模式，不仅产生视觉上更好的图像，但相对于其他工程或学习编解码器时，也给出了后续的计算机视觉任务，如对象检测和分割卓越的性能。</font>
</div>


<hr>
<div id="paper10"> <b>10. Towards Precise Intra-camera Supervised Person Re-identification</b>  <a href="https://arxiv.org/pdf/2002.04932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Menglin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baisheng Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haokun Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianqiang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojin Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xian-Sheng Hua</a><br>
<font size="3">
Abstract: Intra-camera supervision (ICS) for person re-identification (Re-ID) assumes that identity labels are independently annotated within each camera view and no inter-camera identity association is labeled. It is a new setting proposed recently to reduce the burden of annotation while expect to maintain desirable Re-ID performance. However, the lack of inter-camera labels makes the ICS Re-ID problem much more challenging than the fully supervised counterpart. By investigating the characteristics of ICS, this paper proposes camera-specific non-parametric classifiers, together with a hybrid mining quintuplet loss, to perform intra-camera learning. Then, an inter-camera learning module consisting of a graph-based ID association step and a Re-ID model updating step is conducted. Extensive experiments on three large-scale Re-ID datasets show that our approach outperforms all existing ICS works by a great margin. Our approach performs even comparable to state-of-the-art fully supervised methods in two of the datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：帧内照相机监管（ICS），用于人重新鉴定（再ID）假设身份标签每个摄像机视图内独立地注释和没有摄影机间身份关联被标记。这是最近提出的减少注释的负担，同时希望保持理想的再ID性能的新设置。然而，由于缺乏摄像装置间的标签使得ICS重新编号的问题远远超过了充分监督对口挑战。通过调查ICS的特点，提出了具体的摄像头，非参数的分类，与混合动力采矿五元组一起损失，执行摄像机内学习。然后，将由基于图的ID关联步骤和再ID模型更新步骤的相机间学习模块中进行。三个大型再ID的数据集大量的实验表明，我们的方法有很大裕度优于所有现有的ICS作品。我们的方法执行甚至可以媲美国家的最先进的完全监督两个数据集的方法。</font>
</div>


<hr>
<div id="paper11"> <b>11. A Zero-Shot based Fingerprint Presentation Attack Detection System</b>  <a href="https://arxiv.org/pdf/2002.04908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haozhe Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wentian Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guojie Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Liu</a><br>
<font size="3">
Abstract: With the development of presentation attacks, Automated Fingerprint Recognition Systems(AFRSs) are vulnerable to presentation attack. Thus, numerous methods of presentation attack detection(PAD) have been proposed to ensure the normal utilization of AFRS. However, the demand of large-scale presentation attack images and the low-level generalization ability always astrict existing PAD methods' actual performances. Therefore, we propose a novel Zero-Shot Presentation Attack Detection Model to guarantee the generalization of the PAD model. The proposed ZSPAD-Model based on generative model does not utilize any negative samples in the process of establishment, which ensures the robustness for various types or materials based presentation attack. Different from other auto-encoder based model, the Fine-grained Map architecture is proposed to refine the reconstruction error of the auto-encoder networks and a task-specific gaussian model is utilized to improve the quality of clustering. Meanwhile, in order to improve the performance of the proposed model, 9 confidence scores are discussed in this article. Experimental results showed that the ZSPAD-Model is the state of the art for ZSPAD, and the MS-Score is the best confidence score. Compared with existing methods, the proposed ZSPAD-Model performs better than the feature-based method and under the multi-shot setting, the proposed method overperforms the learning based method with little training data. When large training data is available, their results are similar. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着演示攻击的发展，指纹自动识别系统（AFRSs）很容易受到攻击的演示。因此，已提出演示攻击检测（PAD）的多种方法，以确保AFRS的正常使用。然而，大规模的进攻呈现图像的需求和低级别的泛化能力总是astrict现有PAD方法的实际表现。因此，我们提出了一个新颖的零射击演示攻击检测模型，以保证PAD模型的泛化。基于生成模型的提出ZSPAD的模型没有利用任何负面样本中建立的过程中，这样可以确保基于演示攻击各种类型或材料的坚固性。从其他自动编码器来基于模型不同的是，细粒度的地图架构提出了改进自动编码器来网络和特定任务的高斯模型被用来提高聚类质量的重建误差。同时，为了提高该模型的性能，9个信心分数本文中讨论。实验结果表明，ZSPAD-模型是本领域中用于ZSPAD的状态，并且MS-分数是最好的置信度得分。与现有的方法相比，该ZSPAD-模型比基于特征的方法，并在多合一设定好，所提出的方法overperforms很少训练数据的学习为基础的方法。当大量的训练数据是可用的，其结果是相似的。</font>
</div>


<hr>
<div id="paper12"> <b>12. Bi-Directional Generation for Unsupervised Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2002.04869" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guanglei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haifeng Xia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingli Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengming Ding</a><br>
<font size="3">
Abstract: Unsupervised domain adaptation facilitates the unlabeled target domain relying on well-established source domain information. The conventional methods forcefully reducing the domain discrepancy in the latent space will result in the destruction of intrinsic data structure. To balance the mitigation of domain gap and the preservation of the inherent structure, we propose a Bi-Directional Generation domain adaptation model with consistent classifiers interpolating two intermediate domains to bridge source and target domains. Specifically, two cross-domain generators are employed to synthesize one domain conditioned on the other. The performance of our proposed method can be further enhanced by the consistent classifiers and the cross-domain alignment constraints. We also design two classifiers which are jointly optimized to maximize the consistency on target sample prediction. Extensive experiments verify that our proposed model outperforms the state-of-the-art on standard cross domain visual benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无监督领域适应性方便了未标记的目标域依托完善的源域信息。常规方法强行降低潜在空间域差异将导致固有的数据结构的破坏。为了平衡域间隙和内在结构的保存缓解，我们提出一致分类插两个中间域弥合源和目标域的双向代域适应模式。具体而言，两个交叉域发电机被用于合成一种域调节为另一方。我们提出的方法的性能可以由一致的分类器和跨域对齐约束来进一步增强。我们还设计了两个分类被联合优化，最大化的目标样本预测的一致性。大量的实验验证，我们提出的模型优于标准的跨域视觉基准的国家的最先进的。</font>
</div>


<hr>
<div id="paper13"> <b>13. Analysis Of Multi Field Of View Cnn And Attention Cnn On H&amp;E Stained  Whole-slide Images On Hepatocellular Carcinoma</b>  <a href="https://arxiv.org/pdf/2002.04836" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Say%C4%B1c%C4%B1%2C+M+B" target="_blank" rel="noopener" style="color:#0000EE;">Mehmet Burak Sayıcı</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yamashita%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rikiya Yamashita</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeanne Shen</a><br>
<font size="3">
Abstract: Hepatocellular carcinoma (HCC) is a leading cause of cancer-related death worldwide. Whole-slide imaging which is a method of scanning glass slides have been employed for diagnosis of HCC. Using high resolution Whole-slide images is infeasible for Convolutional Neural Network applications. Hence tiling the Whole-slide images is a common methodology for assigning Convolutional Neural Networks for classification and segmentation. Determination of the tile size affects the performance of the algorithms since small field of view can not capture the information on a larger scale and large field of view can not capture the information on a cellular scale. In this work, the effect of tile size on performance for classification problem is analysed. In addition, Multi Field of View CNN is assigned for taking advantage of the information provided by different tile sizes and Attention CNN is assigned for giving the capability of voting most contributing tile size. It is found that employing more than one tile size significantly increases the performance of the classification by 3.97% and both algorithms are found successful over the algorithm which uses only one tile size. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：肝细胞癌（HCC）是癌症相关死亡的全球的主要原因。全滑动成像这是已被用于HCC的诊断扫描的载玻片的方法。使用高分辨率全幻灯片图像是不可行的卷积神经网络的应用。因此平铺全幻灯片图像是用于分类和分割分配卷积神经网络共同的方法。平铺尺寸的测定影响到自视野小的算法的性能不能捕获在更大的规模和大视场的信息不能捕获在细胞水平的信息。在这项工作中，瓷砖的大小对分类问题性能的影响进行了分析。另外，查看CNN的多场被分配用于拍摄的CNN被分配给了投票贡献最大平铺尺寸的能力不同瓷砖的大小和注意事项中提供的信息优势。研究发现，使用一个以上的瓷砖尺寸由3.97％显著提高分类的性能和算法都被发现在成功只使用一个分块大小的算法。</font>
</div>


<hr>
<div id="paper14"> <b>14. End-to-End Face Parsing via Interlinked Convolutional Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.04831" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zi Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yiu%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Valentin Yiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Tang</a><br>
<font size="3">
Abstract: Face parsing is an important computer vision task that requires accurate pixel segmentation of facial parts (such as eyes, nose, mouth, etc.), providing a basis for further face analysis, modification, and other applications. In this paper, we introduce a simple, end-to-end face parsing framework: STN-aided iCNN (STN-iCNN), which extends interlinked Convolutional Neural Network (iCNN) by adding a Spatial Transformer Network (STN) between the two isolated stages. The STN-iCNN uses the STN to provide a trainable connection to the original two-stage iCNN pipe-line, making end-to-end joint training possible. Moreover, as a by-product, STN also provides more precise cropped parts than the original cropper. Due to the two advantages, our approach significantly improves the accuracy of the original model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面对解析是一项重要的计算机视觉任务，需要面部成分精确的像素分割（如眼睛，鼻子，嘴等），为进一步面上的分析，修改和其他应用程序的基础。在本文中，我们介绍一个简单的，端 - 端面上解析框架：STN辅助ICNN（STN-ICNN），其延伸通过两个分离之间添加空间变换器网（STN）相通卷积神经网络（ICNN）阶段。的STN-ICNN使用STN提供到原来的两阶段ICNN管线可训练连接，使得端至端联合培养成为可能。此外，作为副产物，STN还提供比原来的裁剪机更精确的裁切部分。由于两个优势，我们的做法显著提高了原有模型的准确性。</font>
</div>


<hr>
<div id="paper15"> <b>15. Uniform Interpolation Constrained Geodesic Learning on Data Manifold</b>  <a href="https://arxiv.org/pdf/2002.04829" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Geng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cong Geng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jia Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenbo Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chu Chu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyong Gao</a><br>
<font size="3">
Abstract: In this paper, we propose a method to learn a minimizing geodesic within a data manifold. Along the learned geodesic, our method can generate high-quality interpolations between two given data samples. Specifically, we use an autoencoder network to map data samples into latent space and perform interpolation via an interpolation net-work. We add prior geometric information to regularize our autoencoder for the convexity of representations so that for any given interpolation approach, the generated interpolations remain within the distribution of the data manifold. Before the learning of a geodesic, a proper Riemannianmetric should be defined. Therefore, we induce a Riemannian metric by the canonical metric in the Euclidean space which the data manifold is isometrically immersed in. Based on this defined Riemannian metric, we introduce a constant speed loss and a minimizing geodesic loss to regularize the interpolation network to generate uniform interpolation along the learned geodesic on the manifold. We provide a theoretical analysis of our model and use image translation as an example to demonstrate the effectiveness of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了学习数据歧管内的测地最小化的方法。除了学习大地，我们的方法可以产生两个给定的数据样本之间的高品质插值。具体地，我们使用自动编码器网络来的数据样本映射到潜在空间，并通过一个插网络执行内插。我们之前添加的几何信息来规范我们的交涉的凸自动编码，这样对于任何给定的插值方法，生成插值保持数据流形的分布范围内。测地的学习之前，适当Riemannianmetric应该被定义。因此，我们通过在欧几里德空间中的典型度量其中数据歧管等距浸入诱导黎曼度量。在此基础上定义的黎曼度量，我们引入一个恒定的速度损失和最小化测地损失到正规化的内插网络，以产生均匀的沿着歧管上的教训测地内插。我们提供我们的模型的理论分析和使用图像的平移作为一个例子来证明我们的方法的有效性。</font>
</div>


<hr>
<div id="paper16"> <b>16. Deep-HR: Fast Heart Rate Estimation from Face Video Under Realistic  Conditions</b>  <a href="https://arxiv.org/pdf/2002.04821" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabokrou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Sabokrou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pourreza%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Masoud Pourreza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaobai Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fathy%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahmood Fathy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoying Zhao</a><br>
<font size="3">
Abstract: This paper presents a novel method for remote heart rate (HR) estimation. Recent studies have proved that blood pumping by the heart is highly correlated to the intense color of face pixels, and surprisingly can be utilized for remote HR estimation. Researchers successfully proposed several methods for this task, but making it work in realistic situations is still a challenging problem in computer vision community. Furthermore, learning to solve such a complex task on a dataset with very limited annotated samples is not reasonable. Consequently, researchers do not prefer to use the deep learning approaches for this problem. In this paper, we propose a simple yet efficient approach to benefit the advantages of the Deep Neural Network (DNN) by simplifying HR estimation from a complex task to learning from very correlated representation to HR. Inspired by previous work, we learn a component called Front-End (FE) to provide a discriminative representation of face videos, afterward a light deep regression auto-encoder as Back-End (BE) is learned to map the FE representation to HR. Regression task on the informative representation is simple and could be learned efficiently on limited training samples. Beside of this, to be more accurate and work well on low-quality videos, two deep encoder-decoder networks are trained to refine the output of FE. We also introduce a challenging dataset (HR-D) to show that our method can efficiently work in realistic conditions. Experimental results on HR-D and MAHNOB datasets confirm that our method could run as a real-time method and estimate the average HR better than state-of-the-art ones. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了远程心脏速率（HR）估计的新方法。最近的研究已经证明，泵血由心脏高度相关面的像素的强烈的色彩，并出人意料地可用于远程HR估计。研究人员成功地提出了这个任务的几种方法，但使其工作在实际情况仍然是计算机视觉社区一个具有挑战性的问题。此外，学习来解决非常有限的注释样本数据集这样一个复杂的任务，是不合理的。因此，研究人员并不喜欢使用深层学习方法针对此问题。在本文中，我们提出了一个简单而有效的方法，由一个复杂的任务简化HR估计从非常相关的代表性学习人力资源，以造福于深层神经网络（DNN）的优点。通过前期工作的启发，我们了解到一个叫做前端（FE）组件来提供的面部视频的具有区分表示，后来光深回归自动编码器来作为后端（BE）被学习映射FE表示对HR。在信息表示回归的任务很简单，并且可以在有限的训练样本有效地学习。除了这一点，更准确，并且运作良好的低质量的视频，两道深深的编码器，解码器网络进行培训，以完善FE的输出。我们还引入了一个具有挑战性的数据集（HR-d）表明我们的方法可以有效地在现实条件下工作。在HR-d和MAHNOB数据集实验结果证实了我们的方法可以作为一个实时运行的方法，更好地估计平均HR比国家的最先进的。</font>
</div>


<hr>
<div id="paper17"> <b>17. A Visual-inertial Navigation Method for High-Speed Unmanned Aerial  Vehicles</b>  <a href="https://arxiv.org/pdf/2002.04791" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin-long Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jia-hui Lv</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Geng Sun</a><br>
<font size="3">
Abstract: This paper investigates the localization problem of high-speed high-altitude unmanned aerial vehicle (UAV) with a monocular camera and inertial navigation system. It proposes a navigation method utilizing the complementarity of vision and inertial devices to overcome the singularity which arises from the horizontal flight of UAV. Furthermore, it modifies the mathematical model of localization problem via separating linear parts from nonlinear parts and replaces a nonlinear least-squares problem with a linearly equality-constrained optimization problem. In order to avoid the ill-condition property near the optimal point of sequential unconstrained minimization techniques(penalty methods), it constructs a semi-implicit continuous method with a trust-region technique based on a differential-algebraic dynamical system to solve the linearly equality-constrained optimization problem. It also analyzes the global convergence property of the semi-implicit continuous method in an infinity integrated interval other than the traditional convergence analysis of numerical methods for ordinary differential equations in a finite integrated interval. Finally, the promising numerical results are also presented. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文使用单眼照相机和惯性导航系统调查高速高空无人驾驶飞行器（UAV）的定位问题。它提出了利用视觉和惯性器件的互补性，以克服其来自UAV的水平飞行的奇点的导航方法。此外，通过分离非线性份线性部分修改定位问题的数学模型，并替换一个非线性最小二乘问题线性等式约束的优化问题。为了避免顺序无约束极小化技术（惩罚的方法）的最佳点附近的病态属性，它构造与基于一个微分代数动力系统上的信赖域技术的半隐式连续方法，解决了线性平等T-受约束的优化问题。还分析在无限远的半隐式连续法的全局收敛性集成间隔以外的用于在有限常微分方程的数值方法的传统的收敛性分析集成间隔。最后，有前途的数值结果也。</font>
</div>


<hr>
<div id="paper18"> <b>18. MFFW: A new dataset for multi-focus image fusion</b>  <a href="https://arxiv.org/pdf/2002.04780" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoli Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunxia Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junmin Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiangshe Zhang</a><br>
<font size="3">
Abstract: Multi-focus image fusion (MFF) is a fundamental task in the field of computational photography. Current methods have achieved significant performance improvement. It is found that current methods are evaluated on simulated image sets or Lytro dataset. Recently, a growing number of researchers pay attention to defocus spread effect, a phenomenon of real-world multi-focus images. Nonetheless, defocus spread effect is not obvious in simulated or Lytro datasets, where popular methods perform very similar. To compare their performance on images with defocus spread effect, this paper constructs a new dataset called MFF in the wild (MFFW). It contains 19 pairs of multi-focus images collected on the Internet. We register all pairs of source images, and provide focus maps and reference images for part of pairs. Compared with Lytro dataset, images in MFFW significantly suffer from defocus spread effect. In addition, the scenes of MFFW are more complex. The experiments demonstrate that most state-of-the-art methods on MFFW dataset cannot robustly generate satisfactory fusion images. MFFW can be a new baseline dataset to test whether an MMF algorithm is able to deal with defocus spread effect. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多聚焦图像融合（MFF）是计算摄影领域的根本任务。目前的方法都取得了显著的性能提升。研究发现，目前的方法是在模拟图像集或数据集Lytro公司评估。近来，越来越多的研究者的注意散焦波及效应，真实世界的多聚焦图像的现象。尽管如此，散焦散布效果并不模拟或Lytro公司的数据集，其中常用的方法执行非常相似的明显。比较其与散焦散布效果的图像性能，本文构建了一个在野外（MFFW）称为MFF新的数据集。它包含19对收集互联网上的多聚焦图像。我们注册所有对源图像，以及对部分重点提供地图和参考图像。与Lytro公司的数据集相比，MFFW图像显著遭受散焦散布效果。此外，MFFW的场景都比较复杂。实验证明上MFFW数据集，大多数国家的最先进的方法不能生成鲁棒令人满意融合图像。 MFFW可以是一个新的基准数据集测试的MMF算法是否能够处理散焦散布效果。</font>
</div>


<hr>
<div id="paper19"> <b>19. Efficient Training of Deep Convolutional Neural Networks by Augmentation  in Embedding Space</b>  <a href="https://arxiv.org/pdf/2002.04776" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Abrishami%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Saeed Abrishami</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Eshratifar%2C+A+E" target="_blank" rel="noopener" style="color:#0000EE;">Amir Erfan Eshratifar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Eigen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Eigen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanzhi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nazarian%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shahin Nazarian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pedram%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Massoud Pedram</a><br>
<font size="3">
Abstract: Recent advances in the field of artificial intelligence have been made possible by deep neural networks. In applications where data are scarce, transfer learning and data augmentation techniques are commonly used to improve the generalization of deep learning models. However, fine-tuning a transfer model with data augmentation in the raw input space has a high computational cost to run the full network for every augmented input. This is particularly critical when large models are implemented on embedded devices with limited computational and energy resources. In this work, we propose a method that replaces the augmentation in the raw input space with an approximate one that acts purely in the embedding space. Our experimental results show that the proposed method drastically reduces the computation, while the accuracy of models is negligibly compromised. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在人工智能领域的最新进展已经通过深层神经网络成为可能。在数据稀少申请，转让学习和数据增强技术常用来改善深学习模式的推广。然而，微调的原始输入空间数据增强传输模型运行完整网络为每一个扩充输入计算成本高。当大型模型与有限的计算资源和能源的嵌入式设备中实现这一点特别重要。在这项工作中，我们提出了取代在大约一个在嵌入空间完全作用于原始输入空间增强的方法。我们的实验结果表明，该方法大大减少了计算量，而模型的准确性受到影响可以忽略不计。</font>
</div>


<hr>
<div id="paper20"> <b>20. Progressive Object Transfer Detection</b>  <a href="https://arxiv.org/pdf/2002.04741" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yali Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoyou Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Qiao</a><br>
<font size="3">
Abstract: Recent development of object detection mainly depends on deep learning with large-scale benchmarks. However, collecting such fully-annotated data is often difficult or expensive for real-world applications, which restricts the power of deep neural networks in practice. Alternatively, humans can detect new objects with little annotation burden, since humans often use the prior knowledge to identify new objects with few elaborately-annotated examples, and subsequently generalize this capacity by exploiting objects from wild images. Inspired by this procedure of learning to detect, we propose a novel Progressive Object Transfer Detection (POTD) framework. Specifically, we make three main contributions in this paper. First, POTD can leverage various object supervision of different domains effectively into a progressive detection procedure. Via such human-like learning, one can boost a target detection task with few annotations. Second, POTD consists of two delicate transfer stages, i.e., Low-Shot Transfer Detection (LSTD), and Weakly-Supervised Transfer Detection (WSTD). In LSTD, we distill the implicit object knowledge of source detector to enhance target detector with few annotations. It can effectively warm up WSTD later on. In WSTD, we design a recurrent object labelling mechanism for learning to annotate weakly-labeled images. More importantly, we exploit the reliable object supervision from LSTD, which can further enhance the robustness of target detector in the WSTD stage. Finally, we perform extensive experiments on a number of challenging detection benchmarks with different settings. The results demonstrate that, our POTD outperforms the recent state-of-the-art approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：目标检测的最新发展主要依赖于与大型基准深度学习。然而，收集这些完全标注的数据往往是困难的或昂贵的现实世界的应用，制约深层神经网络的力量，在实践中。另外，人类可以检测几乎没有注释负担新的对象，因为人们经常使用的先验知识来识别与几个精心标注的例子新的对象，然后通过利用野生图像中物体推广这方面的能力。通过学习来检测这个过程的启发，我们提出了一个新的进步对象传输检测（POTD）框架。具体来说，我们在这三个主要贡献。首先，POTD可以利用不同的域的各种对象监督有效成逐行检测过程。通过这种类似人类的学习，可以提高很少注释的目标探测任务。其次，POTD由两个精致的传输段的，亦即，低射击转移侦测（LSTD），和弱监督转移侦测（WSTD）。在LSTD，我们提炼源检测的隐式对象的知识，加强与一些注释靶标检测。它可以有效地热身WSTD以后。在WSTD，我们设计了一个经常性的对象标识机制，学习注释弱标记的图像。更重要的是，我们利用从LSTD可靠对象的监督，这可以进一步提高目标检测的鲁棒性的WSTD阶段。最后，我们在许多不同的设置具有挑战性的检测基准进行大量的实验。结果表明，我们的POTD优于近期国家的最先进的方法。</font>
</div>


<hr>
<div id="paper21"> <b>21. Improving Place Recognition Using Dynamic Object Detection</b>  <a href="https://arxiv.org/pdf/2002.04698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Munoz%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Juan Pablo Munoz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dexter%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Scott Dexter</a><br>
<font size="3">
Abstract: Traditional appearance-based place recognition algorithms based on handcrafted features have proven inadequate in environments with a significant presence of dynamic objects -- objects that may or may not be present in an agent's subsequent visits. Place representations from features extracted using Deep Learning approaches have gained popularity for their robustness and because the algorithms that used them yield better accuracy. Nevertheless, handcrafted features are still popular in devices that have limited resources. This article presents a novel approach that improves place recognition in environments populated by dynamic objects by incorporating the very knowledge of these objects to improve the overall quality of the representations of places used for matching. The proposed approach fuses object detection and place description, Deep Learning and handcrafted features, with the significance of reducing memory and storage requirements. This article demonstrates that the proposed approach yields improved place recognition accuracy, and was evaluated using both synthetic and real-world datasets. The adoption of the proposed approach will significantly improve place recognition results in environments populated by dynamic objects, and explored by devices with limited resources, with particular utility in both indoor and outdoor environments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于手工提供传统的外观，基于位置识别算法已经在环境中证明是不充分的动态对象的显著存在 - 对象可能会或可能不会出现在代理的后续访问。从使用功能的地方交涉提取深层学习方法已经得到普及为他们的鲁棒性和因为用他们的算法产生更好的精度。然而，手工制作的功能仍然在具有有限资源的设备上普及。本文给出了一个改善通过将这些对象的非常知识，提高用于匹配的地方交涉的整体质量动态对象居住环境的地方认同的新方法。所提出的方法保险丝目标检测与地方的描述，深入学习和手工制作的特点，以减少内存和存储需求的意义。本文演示了该方法的产量提高了地方的识别精度，并使用合成和真实世界的数据集进行了评价。该方法的通过将显著改善动态对象居住环境处的识别结果，并通过设备资源有限探索，在室内和室外环境中特别有用。</font>
</div>


<hr>
<div id="paper22"> <b>22. Learning spatio-temporal representations with temporal squeeze pooling</b>  <a href="https://arxiv.org/pdf/2002.04685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoxi Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bors%2C+A+G" target="_blank" rel="noopener" style="color:#0000EE;">Adrian G. Bors</a><br>
<font size="3">
Abstract: In this paper, we propose a new video representation learning method, named Temporal Squeeze (TS) pooling, which can extract the essential movement information from a long sequence of video frames and map it into a set of few images, named Squeezed Images. By embedding the Temporal Squeeze pooling as a layer into off-the-shelf Convolution Neural Networks (CNN), we design a new video classification model, named Temporal Squeeze Network (TeSNet). The resulting Squeezed Images contain the essential movement information from the video frames, corresponding to the optimization of the video classification task. We evaluate our architecture on two video classification benchmarks, and the results achieved are compared to the state-of-the-art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出了一个新的视频表示学习方法，命名为颞挤压（TS）池，它可以从视频帧的长序列中提取必要的运动信息，并将其映射到一组几张图片，命名为压缩图像的。通过嵌入的时空挤压池作为一个层进入关闭的，现成的卷积神经网络（CNN），我们设计了一个新的视频分类模型，命名为颞挤压网络（TeSNet）。得到的压缩映像包含视频帧的基本运动信息，对应的视频分类任务的最优化。我们评估我们对两个视频分类的基准架构，以及所取得的结果相比，国家的最先进的。</font>
</div>


<hr>
<div id="paper23"> <b>23. Object Detection as a Positive-Unlabeled Problem</b>  <a href="https://arxiv.org/pdf/2002.04672" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuewei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+K+J" target="_blank" rel="noopener" style="color:#0000EE;">Kevin J Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lawrence Carin</a><br>
<font size="3">
Abstract: As with other deep learning methods, label quality is important for learning modern convolutional object detectors. However, the potentially large number and wide diversity of object instances that can be found in complex image scenes makes constituting complete annotations a challenging task; objects missing annotations can be observed in a variety of popular object detection datasets. These missing annotations can be problematic, as the standard cross-entropy loss employed to train object detection models treats classification as a positive-negative (PN) problem: unlabeled regions are implicitly assumed to be background. As such, any object missing a bounding box results in a confusing learning signal, the effects of which we observe empirically. To remedy this, we propose treating object detection as a positive-unlabeled (PU) problem, which removes the assumption that unlabeled regions must be negative. We demonstrate that our proposed PU classification loss outperforms the standard PN loss on PASCAL VOC and MS COCO across a range of label missingness, as well as on Visual Genome and DeepLesion with full labels. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：与其他深的学习方法，标签质量是学习现代卷积对象探测器重要。然而，潜在的大量和对象实例的广泛多样性，可以在复杂的图像场景中找到使构成完整注释的具有挑战性的任务;对象缺少注释可以在各种流行的物体检测的数据集的被观察到。这些缺失的注释可以是有问题的，作为标准的交叉熵损失用于列车对象检测模型对待分类为正 - 负（PN）问题：未标记的区域被隐含地假定为背景。因此，任何物体缺少一个令人困惑的学习信号边框效果，其影响的我们经验观察。为了解决这个问题，我们提出治疗目标检测为阳性，未标记（PU）的问题，这消除假设未标记的区域必须是负的。我们证明了我们提出的PU分类损失优于上PASCAL VOC和MS COCO标准PN损失在一系列标签missingness的，以及对视觉基因组与DeepLesion全标签。</font>
</div>


<hr>
<div id="paper24"> <b>24. Validating uncertainty in medical image translation</b>  <a href="https://arxiv.org/pdf/2002.04639" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Reinhold%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">Jacob C. Reinhold</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yufan He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shizhong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunqiang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dashan Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junghoon Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prince%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jerry L. Prince</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carass%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aaron Carass</a><br>
<font size="3">
Abstract: Medical images are increasingly used as input to deep neural networks to produce quantitative values that aid researchers and clinicians. However, standard deep neural networks do not provide a reliable measure of uncertainty in those quantitative values. Recent work has shown that using dropout during training and testing can provide estimates of uncertainty. In this work, we investigate using dropout to estimate epistemic and aleatoric uncertainty in a CT-to-MR image translation task. We show that both types of uncertainty are captured, as defined, providing confidence in the output uncertainty estimates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：医学图像越来越多地用作输入深层神经网络，产生定量值援助研究人员和临床医生。但是，标准的深层神经网络的不确定性提供了可靠的测量这些定量值。最近的研究表明，训练期间使用辍学和测试可以提供不确定性的估计。在这项工作中，我们探讨用差来估计在CT对MR图像平移任务认知和肆意的不确定性。我们发现，这两种类型的不确定性被捕获，定义，提供的输出不确定性估计的信心。</font>
</div>


<hr>
<div id="paper25"> <b>25. Finding novelty with uncertainty</b>  <a href="https://arxiv.org/pdf/2002.04626" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Reinhold%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">Jacob C. Reinhold</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yufan He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shizhong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunqiang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dashan Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junghoon Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prince%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jerry L. Prince</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carass%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aaron Carass</a><br>
<font size="3">
Abstract: Medical images are often used to detect and characterize pathology and disease; however, automatically identifying and segmenting pathology in medical images is challenging because the appearance of pathology across diseases varies widely. To address this challenge, we propose a Bayesian deep learning method that learns to translate healthy computed tomography images to magnetic resonance images and simultaneously calculates voxel-wise uncertainty. Since high uncertainty occurs in pathological regions of the image, this uncertainty can be used for unsupervised anomaly segmentation. We show encouraging experimental results on an unsupervised anomaly segmentation task by combining two types of uncertainty into a novel quantity we call scibilic uncertainty. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：医学图像常常被用来检测和表征病理和疾病;然而，自动地识别和在医学图像中分割病理学是具有挑战性，因为病理的跨疾病的外观变化很大。为了应对这一挑战，我们提出了一个贝叶斯深度学习方法学会翻译健康的计算机断层成像图像磁共振图像，同时计算出体素明智的不确定性。由于高的不确定性在图像的病理区域发生时，这种不确定性可用于无监督异常分割。我们展示两种类型的不确定性组合为我们称之为scibilic不确定性的一种新型的数量，鼓励在无人监督的异常分割任务的实验结果。</font>
</div>


<hr>
<div id="paper26"> <b>26. Patternless Adversarial Attacks on Video Recognition Networks</b>  <a href="https://arxiv.org/pdf/2002.05123" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Naeh%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Itay Naeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pony%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roi Pony</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mannor%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shie Mannor</a><br>
<font size="3">
Abstract: Deep neural networks for classification of videos, just like image classification networks, may be subjected to adversarial manipulation. The main difference between image classifiers and video classifiers is that the latter usually use temporal information contained within the video in the form of optical flow or implicitly by various differences between adjacent frames. In this work we present a manipulation scheme for fooling video classifiers by introducing a spatial patternless temporal perturbation that is practically unnoticed by human observers and undetectable by leading image adversarial pattern detection algorithms. After demonstrating the manipulation of action classification of single videos, we generalize the procedure to make adversarial patterns with temporal invariance that generalizes across different classes for both targeted and untargeted attacks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频分类深层神经网络，就像图像分类网络，可能会受到敌对操作。图像分类器和分类器的视频之间的主要区别是，后者通常是通过在相邻帧之间的各种差异使用在光流的形式包含在所述视频内的时间信息或隐式。在这项工作中，我们通过引入用于呈现视频嘴硬分类器的操作方案的空间无图案颞扰动是通过实际上人类观察者忽视和领先的图像对抗性图案检测算法检测到。展示的单一视频行为分类的操作后，我们推广的过程，使对抗模式与时间不变性跨越不同类别归纳为有针对性和无针对性的攻击。</font>
</div>


<hr>
<div id="paper27"> <b>27. From IC Layout to Die Photo: A CNN-Based Data-Driven Approach</b>  <a href="https://arxiv.org/pdf/2002.04967" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Shao%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao-Chiang Shao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Peng%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chao-Yi Peng</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Wu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun-Rei Wu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chia-Wen Lin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shao-Yun Fang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tsai%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pin-Yen Tsai</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan-Hsiu Liu</a><br>
<font size="3">
Abstract: Since IC fabrication is costly and time-consuming, it is highly desirable to develop virtual metrology tools that can predict the properties of a wafer based on fabrication configurations without performing physical measurements on a fabricated IC. We propose a deep learning-based data-driven framework consisting of two convolutional neural networks: i) LithoNet that predicts the shape deformations on a circuit due to IC fabrication, and ii) OPCNet that suggests IC layout corrections to compensate for such shape deformations. By learning the shape correspondence between pairs of layout design patterns and their SEM images of the product wafer thereof, given an IC layout pattern, LithoNet can mimic the fabrication procedure to predict its fabricated circuit shape for virtual metrology. Furthermore, LithoNet can take the wafer fabrication parameters as a latent vector to model the parametric product variations that can be inspected on SEM images. In addition, traditional lithography simulation methods used to suggest a correction on a lithographic photomask is computationally expensive. Our proposed OPCNet mimics the optical proximity correction (OPC) procedure and efficiently generates a corrected photomask by collaborating with LithoNet to examine if the shape of a fabricated IC circuitry best matches its original layout design. As a result, the proposed LithoNet-OPCNet framework cannot only predict the shape of a fabricated IC from its layout pattern, but also suggests a layout correction according to the consistency between the predicted shape and the given layout. Experimental results with several benchmark layout patterns demonstrate the effectiveness of the proposed method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：由于IC制造是昂贵和费时的，这是非常需要开发虚拟计量工具，可以预测在晶片的基础上制造的配置属性，而无需在制造IC执行的物理测量。我们提出了一个深基于学习的数据驱动框架由两个卷积神经网络的：ⅰ）LithoNet，预测由于IC制造中的电路上的形状的变形，以及ii）OPCNet即表明IC布局校正以补偿这样的形状变形。通过学习的布局设计模式对以及它们的晶片，给定的IC布局图案的产品的它们的SEM图像之间的形状的对应关系，LithoNet可以模仿的制造程序，以预测其制造的电路形状为虚拟测量。此外，LithoNet可以采取在晶片制造参数作为潜矢量的是可在SEM图像被检参数变型产品进行建模。此外，用来建议光刻掩膜修正传统的光刻仿真方法在计算上是昂贵的。我们提出的OPCNet模仿光学邻近校正（OPC）过程，有效地生成由与LithoNet合作，以检查是否a的形状制造的IC电路最佳地匹配它的原始布局设计校正光掩模。其结果是，所提出的LithoNet-OPCNet框架不仅可以预测的形状从其布局图案制造IC，但也表明根据所预测的形状和给定​​的布局之间的一致性的布局校正。与几个基准布局模式的实验结果证明了该方法的有效性。</font>
</div>


<hr>
<div id="paper28"> <b>28. Synaptic Integration of Spatiotemporal Features with a Dynamic  Neuromorphic Processor</b>  <a href="https://arxiv.org/pdf/2002.04924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nilsson%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mattias Nilsson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liwicki%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Foteini Liwicki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sandin%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fredrik Sandin</a><br>
<font size="3">
Abstract: Spiking neurons can perform spatiotemporal feature detection by nonlinear synaptic and dendritic integration of presynaptic spike patterns. Multicompartment models of nonlinear dendrites and related neuromorphic circuit designs enable faithful imitation of such dynamic integration processes, but these approaches are also associated with a relatively high computing cost or circuit size. Here, we investigate synaptic integration of spatiotemporal spike patterns with multiple dynamic synapses on point-neurons in the DYNAP-SE neuromorphic processor, which can offer a complementary resource-efficient, albeit less flexible, approach to feature detection. We investigate how previously proposed excitatory--inhibitory pairs of dynamic synapses can be combined to integrate multiple inputs, and we generalize that concept to a case in which one inhibitory synapse is combined with multiple excitatory synapses. We characterize the resulting delayed excitatory postsynaptic potentials (EPSPs) by measuring and analyzing the membrane potentials of the neuromorphic neuronal circuits. We find that biologically relevant EPSP delays, with variability of order 10 milliseconds per neuron, can be realized in the proposed manner by selecting different synapse combinations, thanks to device mismatch. Based on these results, we demonstrate that a single point-neuron with dynamic synapses in the DYNAP-SE can respond selectively to presynaptic spikes with a particular spatiotemporal structure, which enables, for instance, visual feature tuning of single neurons. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：扣球神经元可以通过非线性突触和突触前尖峰图案树突集成执行时​​空特征检测。非线性树突和相关神经形态电路设计的多室模型使这样的动态集成过程的忠实模仿，但这些方法也具有相对高的计算成本或电路尺寸相关联。在这里，我们调查的时空秒杀模式的突触整合与点神经元多个动态突触在DYNAP-SE神经形态处理器，可提供互补资源利用率高，尽管不那么灵活，方法特征检测。我们研究如何先前提出的兴奋 - 动态突触抑制对可以合并整合多个输入，并推广了这一概念，其中一个抑制性突触与多个兴奋性突触结合的情况下。我们通过测量和分析的神经形态电路的神经元的膜电位表征所得延迟兴奋性突触后电位（EPSPS）。我们发现，生物学相关的EPSP延误，有秩序神经元每10毫秒，能够在建议的方式通过对设备不匹配选择不同的突触的组合，由于可以实现的可变性。基于这些结果，我们表明，单个点的神经元与所述DYNAP-SE动态突触可以选择性到突触前尖峰与特定时空结构，这使得能够，例如，单神经元的视觉特征的调谐响应。</font>
</div>


<hr>
<div id="paper29"> <b>29. Machine-Learning-Based Multiple Abnormality Prediction with Large-Scale  Chest Computed Tomography Volumes</b>  <a href="https://arxiv.org/pdf/2002.04752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Draelos%2C+R+L" target="_blank" rel="noopener" style="color:#0000EE;">Rachel Lea Draelos</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dov%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Dov</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mazurowski%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Maciej A. Mazurowski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lo%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Joseph Y. Lo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Henao%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ricardo Henao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Rubin%2C+G+D" target="_blank" rel="noopener" style="color:#0000EE;">Geoffrey D. Rubin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Carin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lawrence Carin</a><br>
<font size="3">
Abstract: Developing machine learning models for radiology requires large-scale imaging data sets with labels for abnormalities, but the process is challenging due to the size and complexity of the data as well as the cost of labeling. We curated and analyzed a chest computed tomography (CT) data set of 36,316 volumes from 20,201 unique patients. This is the largest multiply-annotated chest CT data set reported. To annotate this data set, we developed a rule-based method for automatically extracting abnormality labels from radiologist free-text reports with an average F-score of 0.976 (min 0.941, max 1.0). We also developed a model for multilabel abnormality classification of chest CT volumes that uses a deep convolutional neural network (CNN). This model reached a classification performance of AUROC greater than 0.90 for 18 abnormalities, with an average AUROC of 0.773 for all 83 abnormalities, demonstrating the feasibility of learning from unfiltered whole volume CT data. We show that training on more labels improves performance significantly: for a subset of 9 labels - nodule, opacity, atelectasis, pleural effusion, consolidation, mass, pericardial effusion, cardiomegaly, and pneumothorax - the model's average AUROC increased by 10 percent when the number of training labels was increased from 9 to all 83. All code for volume preprocessing, automated label extraction, and the volume abnormality prediction model will be made publicly available. The 36,316 CT volumes and labels will also be made publicly available pending institutional approval. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：放射开发机器学习模型需要大规模成像数据集的标签异常，但这一进程因以及标签的成本数据的规模和复杂性挑战。我们策划并分析了从20,201独特患者36,316卷的胸部CT扫描（CT）数据集。这是最大的多重注解胸部CT数据集的报道。为了诠释这组数据中，我们开发了从放射科医生自由文本报告，其中的0.976的平均F-得分（分0.941，最大1.0）自动提取异常标签基于规则的方法。我们还开发了一个使用深卷积神经网络（CNN）胸部CT卷的多标签分类异常的模型。这种模式达到更高的AUROC比0.90分类表现为18点的异常，与0.773为所有83种异常的平均AUROC，展示了从未经过滤的全容积CT数据中学习的可行性。我们展示更多的标签，培训提高性能显著：对于9个标签的一个子集 - 结节，不透明度，肺不张，胸腔积液，整合，质量，心包积液，心脏扩大，气胸 - 模型的平均AUROC增加时，10％的数量训练标签的从9增加到所有83.体积预处理的所有代码，自动标签提取，并且体积异常预测模型将被公开。在36,316 CT容积和标签也将公之于众未决机构的认可。</font>
</div>


<hr>
<div id="paper30"> <b>30. A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for  Healthcare</b>  <a href="https://arxiv.org/pdf/2002.04700" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziyang Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deligianni%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fani Deligianni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Voiculescu%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Irina Voiculescu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guang-Zhong Yang</a><br>
<font size="3">
Abstract: With the increasing awareness of high-quality life, there is a growing need for health monitoring devices running robust algorithms in home environment. Health monitoring technologies enable real-time analysis of users' health status, offering long-term healthcare support and reducing hospitalization time. The purpose of this work is twofold, the software focuses on the analysis of gait, which is widely adopted for joint correction and assessing any lower limb or spinal problem. On the hardware side, we design a novel marker-less gait analysis device using a low-cost RGB camera mounted on a mobile tele-robot. As gait analysis with a single camera is much more challenging compared to previous works utilizing multi-cameras, a RGB-D camera or wearable sensors, we propose using vision-based human pose estimation approaches. More specifically, based on the output of two state-of-the-art human pose estimation models (Openpose and VNect), we devise measurements for four bespoke gait parameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot progression angles. We thereby classify walking patterns into normal, supination, pronation and limp. We also illustrate how to run the purposed machine learning models in low-resource environments such as a single entry-level CPU. Experiments show that our single RGB camera method achieves competitive performance compared to state-of-the-art methods based on depth cameras or multi-camera motion capture system, at smaller hardware costs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着越来越多的高品质生活的意识，人们越来越需要健康监测运行在家庭环境中稳定的算法设备。健康监测技术使用户的健康状况进行实时分析，提供长期的医疗支持，减少住院时间。这项工作的目的是双重的，该软件侧重于步态分析，广泛联合校正和评估任何下肢或脊柱问题采纳。在硬件方面，我们使用搭载于移动远程机器人低成本RGB照相机设计的新型无标记步态分析装置。如同一台摄像机步态分析更具有挑战性相比，利用多摄像机以往的作品，一个RGB-d相机或穿戴式传感器，我们建议采用基于视觉的人体姿势估计方法。更具体地，基于对状态的最先进的两种人类姿势估计模型（Openpose和VNect）的输出中，我们设计出四个定制步态参数测量：反转/外翻，背屈/跖，踝关节和脚的进展角度。我们由此分类行走模式为正常，外旋，内旋和跛行。我们还说明了如何运行旨意机器学习在低资源环境等车型单一的入门级CPU。实验结果表明，相比于基于深度相机或多相机运动捕捉系统，在较小的硬件成本的国家的最先进的方法提供了单个RGB相机方法实现有竞争力的性能。</font>
</div>


<hr>
<div id="paper31"> <b>31. fastai: A Layered API for Deep Learning</b>  <a href="https://arxiv.org/pdf/2002.04688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Howard%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeremy Howard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gugger%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sylvain Gugger</a><br>
<font size="3">
Abstract: fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：fastai是深学习库，其提供从业人员与高级别组件，可以快速且容易地提供先进的最先进的结果在标准深度学习域，并且为研究人员提供低级别组件，可以是混合和匹配建立新的方法。它的目的是做没有实质性妥协的东西都在易用性，灵活性和性能。这可能要归功于精心分层的体系结构，它表达了解耦抽象的条款处理技术的许多深学习的共同的基本模式和数据。这些抽象能够借助底层Python语言的活力和PyTorch库的灵活性言简意赅地表达。 fastai包括：用于Python与语义类型层次结构张量沿着一个新型调度系统;一个GPU优化计算机视觉库可以在纯Python进行扩展;优化器，其refactors出现代优化的通用功能分成两个基本块，从而允许优化算法，以在4-5线的代码来实现;一种新颖的2路回调系统，可以访问该数据，模型，或优化器的任何部分，并在训练过程中的任何点进行更改;一个新的数据块的API;以及更多。我们使用这个库成功地创建一个完整的深度学习过程中，我们能够比使用以前的方法更快速地编写，并且代码更加清晰。该库已经在研究，工业和教学用途广。</font>
</div>


<hr>
<div id="paper32"> <b>32. A Non-Intrusive Correction Algorithm for Classification Problems with  Corrupted Data</b>  <a href="https://arxiv.org/pdf/2002.04658" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kailiang Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongbin Xiu</a><br>
<font size="3">
Abstract: A novel correction algorithm is proposed for multi-class classification problems with corrupted training data. The algorithm is non-intrusive, in the sense that it post-processes a trained classification model by adding a correction procedure to the model prediction. The correction procedure can be coupled with any approximators, such as logistic regression, neural networks of various architectures, etc. When training dataset is sufficiently large, we prove that the corrected models deliver correct classification results as if there is no corruption in the training data. For datasets of finite size, the corrected models produce significantly better recovery results, compared to the models without the correction algorithm. All of the theoretical findings in the paper are verified by our numerical examples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一种新的校正算法，提出了多类分类问题已损坏的训练数据。该算法是非侵入性的，通过将校正过程的模型预测在这个意义上它后处理一个训练的分类模型。修正过程可以配上任何逼近，如逻辑回归，各种结构的神经网络，等等。当训练数据集是足够大的，我们证明了修正模型提供正确的分类结果作为是否有在训练数据中没有损坏。对于有限大小的数据集，校正模型产生显著更好的恢复效果，相比于没有纠错算法模型。所有在纸上的理论成果都是由我们的算例验证。</font>
</div>


<hr>
<div id="paper33"> <b>33. Neuroevolution of Neural Network Architectures Using CoDeepNEAT and  Keras</b>  <a href="https://arxiv.org/pdf/2002.04634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=da+Silveira+Bohrer%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonas da Silveira Bohrer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Grisci%2C+B+I" target="_blank" rel="noopener" style="color:#0000EE;">Bruno Iochins Grisci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dorn%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marcio Dorn</a><br>
<font size="3">
Abstract: Machine learning is a huge field of study in computer science and statistics dedicated to the execution of computational tasks through algorithms that do not require explicit instructions but instead rely on learning patterns from data samples to automate inferences. A large portion of the work involved in a machine learning project is to define the best type of algorithm to solve a given problem. Neural networks - especially deep neural networks - are the predominant type of solution in the field. However, the networks themselves can produce very different results according to the architectural choices made for them. Finding the optimal network topology and configurations for a given problem is a challenge that requires domain knowledge and testing efforts due to a large number of parameters that need to be considered. The purpose of this work is to propose an adapted implementation of a well-established evolutionary technique from the neuroevolution field that manages to automate the tasks of topology and hyperparameter selection. It uses a popular and accessible machine learning framework - Keras - as the back-end, presenting results and proposed changes concerning the original algorithm. The implementation is available at GitHub (this https URL) with documentation and examples to reproduce the experiments performed for this work. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器学习是通过不需要明确的指示，而是依赖于从数据样本的学习模式自动推理算法专用于计算任务的执行在计算机科学和统计学研究的一个巨大的领域。参与机器学习项目工作中的很大一部分是定义算法来解决特定问题的最佳类型。神经网络 - 尤其是深层神经网络 - 是主要的类型在该领域的解决方案。然而，网络本身可以根据他们做出的架构选择产生非常不同的结果。寻找最佳的网络拓扑和配置，对于给定的问题是需要专业知识和测试工作，由于大量的需要考虑的参数是一个挑战。这项工作的目的是提出一种适合实现从neuroevolution现场管理自动拓扑和超参数选择的任务一套行之有效的进化技术。它采用了流行的和可访问的机器学习框架 -  Keras  - 作为后端，呈现的结果和有关原始算法修改建议。实施可在GitHub上（此HTTPS URL）与文档和示例重现对这项工作进行的实验。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-13</title>
    <url>/2020/02/14/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-13/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Learning to Compare for Better Training and Evaluation of Open Domain  Natural Language Generation Models <a href="https://arxiv.org/pdf/2002.05058" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Joint Embedding in Named Entity Linking on Sentence Level <a href="https://arxiv.org/pdf/2002.04936" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis  and Natural Language Inference <a href="https://arxiv.org/pdf/2002.04815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and  Diagnosing Dialogue Systems <a href="https://arxiv.org/pdf/2002.04793" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Two Huge Title and Keyword Generation Corpora of Research Articles <a href="https://arxiv.org/pdf/2002.04689" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Adjusting Image Attributes of Localized Regions with Low-level Dialogue <a href="https://arxiv.org/pdf/2002.04678" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Constructing a Highlight Classifier with an Attention Based LSTM Neural  Network <a href="https://arxiv.org/pdf/2002.04608" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Attentional Speech Recognition Models Misbehave on Out-of-domain  Utterances <a href="https://arxiv.org/pdf/2002.05150" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> DeepMutation: A Neural Mutation Tool <a href="https://arxiv.org/pdf/2002.04760" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> On Layer Normalization in the Transformer Architecture <a href="https://arxiv.org/pdf/2002.04745" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Superbloom: Bloom filter meets Transformer <a href="https://arxiv.org/pdf/2002.04723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> A Non-Intrusive Correction Algorithm for Classification Problems with  Corrupted Data <a href="https://arxiv.org/pdf/2002.04658" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Learning to Compare for Better Training and Evaluation of Open Domain  Natural Language Generation Models</b>  <a href="https://arxiv.org/pdf/2002.05058" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangchunshu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Xu</a><br>
<font size="3">
Abstract: Automated evaluation of open domain natural language generation (NLG) models remains a challenge and widely used metrics such as BLEU and Perplexity can be misleading in some cases. In our paper, we propose to evaluate natural language generation models by learning to compare a pair of generated sentences by fine-tuning BERT, which has been shown to have good natural language understanding ability. We also propose to evaluate the model-level quality of NLG models with sample-level comparison results with skill rating system. While able to be trained in a fully self-supervised fashion, our model can be further fine-tuned with a little amount of human preference annotation to better imitate human judgment. In addition to evaluating trained models, we propose to apply our model as a performance indicator during training for better hyperparameter tuning and early-stopping. We evaluate our approach on both story generation and chit-chat dialogue response generation. Experimental results show that our model correlates better with human preference compared with previous automated evaluation approaches. Training with the proposed metric yields better performance in human evaluation, which further demonstrates the effectiveness of the proposed model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：开域自然语言生成（NLG）模型自动评估仍然是一个挑战和广泛使用的指标，如BLEU和困惑可以在某些情况下会产生误导。在本文中，我们提出通过学习来比较微调BERT，这已被证明具有良好的自然语言理解能力对生成的句子来评估自然语言代车型。此外，我们建议评估NLG模型与样本级别的比较结果与技能等级系统模型的质量水平。虽然能在完全自我监督的方式进行训练，我们的模型可以进一步微调以人偏好补充说明的一点量，以便更好地模仿人的判断。除了评估训练的模型，我们建议采用我们的模型为更好地调整超参数和早期停止训练时的性能指标。我们评估我们的两个故事的产生和闲聊对话响应生成方法。实验结果表明，我们的模型相关因素与人类的喜好更好地与之前的自动评价方法进行了比较。在人的评价，这进一步表明了该模型的有效性提出的指标性能更好的训练。</font>
</div>


<hr>
<div id="paper2"> <b>2. Joint Embedding in Named Entity Linking on Sentence Level</b>  <a href="https://arxiv.org/pdf/2002.04936" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyuan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiwei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hong Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+J+X" target="_blank" rel="noopener" style="color:#0000EE;">Jeffrey Xu Yu</a><br>
<font size="3">
Abstract: Named entity linking is to map an ambiguous mention in documents to an entity in a knowledge base. The named entity linking is challenging, given the fact that there are multiple candidate entities for a mention in a document. It is difficult to link a mention when it appears multiple times in a document, since there are conflicts by the contexts around the appearances of the mention. In addition, it is difficult since the given training dataset is small due to the reason that it is done manually to link a mention to its mapping entity. In the literature, there are many reported studies among which the recent embedding methods learn vectors of entities from the training dataset at document level. To address these issues, we focus on how to link entity for mentions at a sentence level, which reduces the noises introduced by different appearances of the same mention in a document at the expense of insufficient information to be used. We propose a new unified embedding method by maximizing the relationships learned from knowledge graphs. We confirm the effectiveness of our method in our experimental studies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：命名实体链接是在文档的暧昧中提到的知识库映射到实体。命名实体链接是具有挑战性的，鉴于有对文档中提及多个候选实体。这是很难当多次出现在文档中链接一提，因为有通过围绕提的出场语境冲突。此外，它是困难的，因为给定的训练数据集是小，由于它是手工完成的提链接到它的映射实体的原因。在文献中，有许多研究报道其中最近嵌入方法在文件级学会从训练数据集实体的载体。为了解决这些问题，我们把重点放在如何链接实体提到在句子水平，从而降低了使用通过在信息不足的费用的文件在同一提的不同外观引入了噪音。我们通过最大限度地从知识图学的关系，提出了一种新的统一的嵌入方法。我们确认我们的方法在我们的实验研究的有效性。</font>
</div>


<hr>
<div id="paper3"> <b>3. Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis  and Natural Language Inference</b>  <a href="https://arxiv.org/pdf/2002.04815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youwei Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiahai Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiwei Liang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyue Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Jiang</a><br>
<font size="3">
Abstract: Aspect based sentiment analysis aims to identify the sentimental tendency towards a given aspect in text. Fine-tuning of pretrained BERT performs excellent on this task and achieves state-of-the-art performances. Existing BERT-based works only utilize the last output layer of BERT and ignore the semantic knowledge in the intermediate layers. This paper explores the potential of utilizing BERT intermediate layers to enhance the performance of fine-tuning of BERT. To the best of our knowledge, no existing work has been done on this research. To show the generality, we also apply this approach to a natural language inference task. Experimental results demonstrate the effectiveness and generality of the proposed approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于看点情感分析的目的是确定对在文本中给定方面的感伤倾向。预训练的BERT执行优秀这项任务和微调实现了国家的最先进的性能。现有的基于BERT-作品只能利用BERT的最后一个输出层，而忽略中间层的语义知识。本文探讨利用BERT中间层，以提高BERT的微调的性能的潜力。据我们所知，没有现有的工作已经在这项研究完成的。要显示的普遍性，我们也将这种方法用于自然语言推理任务。实验结果表明，该方法的有效性和普遍性。</font>
</div>


<hr>
<div id="paper4"> <b>4. ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and  Diagnosing Dialogue Systems</b>  <a href="https://arxiv.org/pdf/2002.04793" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Fang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Takanobu%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryuichi Takanobu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinchao Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Baolin Peng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyan Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minlie Huang</a><br>
<font size="3">
Abstract: We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab (Lee et al., 2019b), ConvLab-2 inherits ConvLab's framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们目前ConvLab-2，一个开源工具包，使研究人员能够与国家的最先进的机型构建面向任务的对话系统，执行结束到终端的评价，并诊断系统的弱点。作为ConvLab的继任者（Lee等，2019b），ConvLab-2继承ConvLab的框架，但集成功能更强大的对话模式，并支持多个数据集。此外，我们已经开发了一个分析工具和互动的工具，以帮助研究人员在诊断对话系统。该分析工具提供丰富的统计和总结了模拟对话，这有利于误差分析和系统改进常见的错误。交互式工具提供了一个用户接口，它允许开发者通过与系统进行交互和修改每个系统部件的输出来诊断组装对话系统。</font>
</div>


<hr>
<div id="paper5"> <b>5. Two Huge Title and Keyword Generation Corpora of Research Articles</b>  <a href="https://arxiv.org/pdf/2002.04689" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=%C3%87ano%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erion Çano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bojar%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Ondřej Bojar</a><br>
<font size="3">
Abstract: Recent developments in sequence-to-sequence learning with neural networks have considerably improved the quality of automatically generated text summaries and document keywords, stipulating the need for even bigger training corpora. Metadata of research articles are usually easy to find online and can be used to perform research on various tasks. In this paper, we introduce two huge datasets for text summarization (OAGSX) and keyword generation (OAGKX) research, containing 34 million and 23 million records, respectively. The data were retrieved from the Open Academic Graph which is a network of research profiles and publications. We carefully processed each record and also tried several extractive and abstractive methods of both tasks to create performance baselines for other researchers. We further illustrate the performance of those methods previewing their outputs. In the near future, we would like to apply topic modeling on the two sets to derive subsets of research articles from more specific disciplines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：按顺序对序列的最新发展与神经网络学习具有显着提高自动生成的文本摘要及文档的关键字的质量，规定了更大的训练库的需要。研究文章的元数据通常是很容易在网上找到，可用于对各种任务进行研究。在本文中，我们介绍了文摘（OAGSX）和关键字生成（OAGKX）研究两个巨大的数据集，包含3400万个23万条记录，分别。该数据来自于不限学历图是研究概况和出版物网络检索。我们认真处理每一条记录，也尝试过的两个任务数，并提取方法，抽象创建其他研究者的性能基准。我们进一步说明这些方法预览它们的输出性能。在不久的将来，我们想从更具体的学科上两套应用主题建模研究文章的派生子集。</font>
</div>


<hr>
<div id="paper6"> <b>6. Adjusting Image Attributes of Localized Regions with Low-level Dialogue</b>  <a href="https://arxiv.org/pdf/2002.04678" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tzu-Hsiang Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rudnicky%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Rudnicky</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bui%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trung Bui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+D+S" target="_blank" rel="noopener" style="color:#0000EE;">Doo Soon Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oh%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean Oh</a><br>
<font size="3">
Abstract: Natural Language Image Editing (NLIE) aims to use natural language instructions to edit images. Since novices are inexperienced with image editing techniques, their instructions are often ambiguous and contain high-level abstractions that tend to correspond to complex editing steps to accomplish. Motivated by this inexperience aspect, we aim to smooth the learning curve by teaching the novices to edit images using low-level commanding terminologies. Towards this end, we develop a task-oriented dialogue system to investigate low-level instructions for NLIE. Our system grounds language on the level of edit operations, and suggests options for a user to choose from. Though compelled to express in low-level terms, a user evaluation shows that 25% of users found our system easy-to-use, resonating with our motivation. An analysis shows that users generally adapt to utilizing the proposed low-level language interface. In this study, we identify that object segmentation as the key factor to the user satisfaction. Our work demonstrates the advantages of the low-level, direct language-action mapping approach that can be applied to other problem domains beyond image editing such as audio editing or industrial design. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自然语言图像编辑（NLIE）旨在利用自然语言指令来进行编辑图像。由于新手与图像编辑技术经验不足，他们的指示往往模糊不清，而含有高层次的抽象，往往对应于复杂的编辑步骤来完成。这一方面缺乏经验的启发，我们的目标是通过采用低层次的指挥用语教新手编辑图像平滑的学习曲线。为此，我们开发了一个面向任务的对话系统，以调查NLIE低级指令。我们对编辑操作的层次体系理由的语言，并提出了一个用户从选择的选项。虽然被迫在低层次的形式来表示，用户评价结果显示，25％的用户发现，我们的系统易于使用，我们的动机共鸣。分析表明，用户一般适应于利用提出的低级语言界面。在这项研究中，我们确定对象分割为用户满意度的关键因素。我们的工作表明，低层次的，直接的语言，动作映射，可以应用到其他问题领域超越图像编辑，如音频编辑或工业设计方法的优点。</font>
</div>


<hr>
<div id="paper7"> <b>7. Constructing a Highlight Classifier with an Attention Based LSTM Neural  Network</b>  <a href="https://arxiv.org/pdf/2002.04608" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuehne%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Kuehne</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Radu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marius Radu</a><br>
<font size="3">
Abstract: Data is being produced in larger quantities than ever before in human history. It's only natural to expect a rise in demand for technology that aids humans in sifting through and analyzing this inexhaustible supply of information. This need exists in the market research industry, where large amounts of consumer research data is collected through video recordings. At present, the standard method for analyzing video data is human labor. Market researchers manually review the vast majority of consumer research video in order to identify relevant portions - highlights. The industry state of the art turnaround ratio is 2.2 - for every hour of video content 2.2 hours of manpower are required. In this study we present a novel approach for NLP-based highlight identification and extraction based on a supervised learning model that aides market researchers in sifting through their data. Our approach hinges on a manually curated user-generated highlight clips constructed from long and short-form video data. The problem is best suited for an NLP approach due to the availability of video transcription. We evaluate multiple classes of models, from gradient boosting to recurrent neural networks, comparing their performance in extraction and identification of highlights. The best performing models are then evaluated using four sampling methods designed to analyze documents much larger than the maximum input length of the classifiers. We report very high performances for the standalone classifiers, ROC AUC scores in the range 0.93-0.94, but observe a significant drop in effectiveness when evaluated on large documents. Based on our results we suggest combinations of models/sampling algorithms for various use cases. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据数量较多，比以往任何时候被生产在人类历史之前。这是很自然的期望在技术，帮助人们通过筛选和分析的信息，这取之不尽的需求量明显上升。这需要存在于市场研究行业，大量的消费者调研数据通过录像收集。目前，用于分析视频数据的标准方法是人的劳动。市场研究人员人工审核广大消费者的研究视频，以确定有关的部分 - 亮点。本领域的周转率的行业状态是2.2  - 用于视频内容每一小时都需要2.2小时人力。在这项研究中，我们提出了基于监督学习模型基于NLP高亮识别和提取的新方法是幕僚市场研究人员通过他们的数据进行筛选。我们的方法的铰链上从长和短形式的视频数据构成的手动管理用户生成的精彩片段。这个问题是最适合的NLP方法由于视频转录的可用性。我们评估多个类别的车型，从梯度提高到回归神经网络，比较它们的提取和亮点的识别性能。然后表现最好的模型使用设计用来分析文档比分类器的最大输入长度大得多的四个采样方法进行评价。我们提出非常高的演出独立分类，范围为0.93-0.94 ROC AUC分数，但对大文件进行评估时，观察有效性显著下跌。根据我们的结果，我们建议的模型组合/取样各种用例的算法。</font>
</div>


<hr>
<div id="paper8"> <b>8. Attentional Speech Recognition Models Misbehave on Out-of-domain  Utterances</b>  <a href="https://arxiv.org/pdf/2002.05150" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Keung%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phillip Keung</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Niu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Niu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yichao Lu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Salazar%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julian Salazar</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bhardwaj%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vikas Bhardwaj</a><br>
<font size="3">
Abstract: We discuss the problem of echographic transcription in autoregressive sequence-to-sequence attentional architectures for automatic speech recognition, where a model produces very long sequences of repetitive outputs when presented with out-of-domain utterances. We decode audio from the British National Corpus with an attentional encoder-decoder model trained solely on the LibriSpeech corpus. We observe that there are many 5-second recordings that produce more than 500 characters of decoding output (i.e. more than 100 characters per second). A frame-synchronous hybrid (DNN-HMM) model trained on the same data does not produce these unusually long transcripts. These decoding issues are reproducible in a speech transformer model from ESPnet, and to a lesser extent in a self-attention CTC model, suggesting that these issues are intrinsic to the use of the attention mechanism. We create a separate length prediction model to predict the correct number of wordpieces in the output, which allows us to identify and truncate problematic decoding results without increasing word error rates on the LibriSpeech task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们讨论的自动语音识别，其中当与领域外的言论提出了一个模型产生重复输出很长的序列自回归序列对序列注意力架构回声转录的问题。我们解码来自英国国家语料库音频，单就LibriSpeech语料训练的一个所注意的编码器，解码器模型。我们观察到，有产生解码输出的超过500个字符（即，超过每秒100个字符）许多5秒记录。受过训练的对相同的数据的帧同步混合（DNN-HMM）模型不产生这些不寻常的长转录物。这些解码的问题是从ESPnet讲话变压器模型重复性好，在自我关注CTC模式在较小程度上，这表明这些问题是固有的使用注意机制。我们创建一个单独的长度预测模型来预测在输出中，这允许我们无需在LibriSpeech任务增加字错误率确定并截断问题的解码结果wordpieces的正确数目。</font>
</div>


<hr>
<div id="paper9"> <b>9. DeepMutation: A Neural Mutation Tool</b>  <a href="https://arxiv.org/pdf/2002.04760" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tufano%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michele Tufano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kimko%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Kimko</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiya Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Watson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cody Watson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bavota%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabriele Bavota</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Di+Penta%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Massimiliano Di Penta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poshyvanyk%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Denys Poshyvanyk</a><br>
<font size="3">
Abstract: Mutation testing can be used to assess the fault-detection capabilities of a given test suite. To this aim, two characteristics of mutation testing frameworks are of paramount importance: (i) they should generate mutants that are representative of real faults; and (ii) they should provide a complete tool chain able to automatically generate, inject, and test the mutants. To address the first point, we recently proposed an approach using a Recurrent Neural Network Encoder-Decoder architecture to learn mutants from ~787k faults mined from real programs. The empirical evaluation of this approach confirmed its ability to generate mutants representative of real faults. In this paper, we address the second point, presenting DeepMutation, a tool wrapping our deep learning model into a fully automated tool chain able to generate, inject, and test mutants learned from real faults. Video: this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：突变的测试可用于评估给定的测试套件的故障检测功能。为了达到这个目的，突变测试框架两个特点是极为重要的：（i）它们应产生代表实际故障的突变体;及（ii）它们应该提供一个完整的工具链能够自动生成，注入，并测试突变体。为了解决第一个问题，我们最近提出使用递归神经网络编码器，解码器架构，了解从现实的方案开采〜787k故障突变体的方法。这种方法的实证评价确认了其产生的突变体代表实际故障的能力。在本文中，我们要解决的第二个点，呈现DeepMutation，一个工具包，我们深切的学习模式到一个完全自动化的工具链能够产生，注入，并从实际故障了解到测试的突变体。视频：此HTTPS URL</font>
</div>


<hr>
<div id="paper10"> <b>10. On Layer Normalization in the Transformer Architecture</b>  <a href="https://arxiv.org/pdf/2002.04745" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruibin Xiong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunchang Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Di He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuxin Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xing%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chen Xing</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huishuai Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanyan Lan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liwei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tie-Yan Liu</a><br>
<font size="3">
Abstract: The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：变压器被广泛应用于自然语言处理任务。要培养一个变压器然而，人们通常需要一个精心设计的学习速度的热身阶段，这被证明是最后的表现至关重要，但会优化，带来更多超，参数调整放缓。在本文中，我们首先研究在理论上为什么学习率热身阶段是必要的，表明层正常化问题的位置。具体来说，我们用平均场理论证明，在初始化时，对原设计的后LN变压器，这使残余块之间的层正常化，输出层附近的参数的预期梯度很大。因此，使用这些渐变大的学习速率使得培训不稳定。在热身阶段，是为了避免这个问题实际上是有帮助的。在另一方面，我们的理论也表明，如果层标准化就是把剩余的块（最近提出的预LN变压器）内，梯度在初始化乖巧。这促使我们删除热身阶段为预LN变形金刚的培训。我们发现在我们的实验中，如果没有热身阶段预LN变压器可以达到基准比较的结果，同时要求显著减少培训时间和Hyper-参数整定了广泛的应用。</font>
</div>


<hr>
<div id="paper11"> <b>11. Superbloom: Bloom filter meets Transformer</b>  <a href="https://arxiv.org/pdf/2002.04723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Anderson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qingqing Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krichene%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Walid Krichene</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rendle%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steffen Rendle</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Zhang</a><br>
<font size="3">
Abstract: We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们延长的字块在自然语言模型对不透明IDS机器学习任务的想法。这通过将散列函数到每个ID多个散列令牌映射在一个更小的空间中，类似于一个布隆过滤器来实现的。我们发现，通过将多层变压器这些布隆过滤器摘要，我们能够高精度地获取模型。他们胜过一个同样大小的模型没有散列和，在很大程度上，一个更大尺寸的模型中使用抽样SOFTMAX与相同的计算预算培训。我们的主要发现是，它使用了多层互感器布隆过滤器消化在散列输入歧义除去是很重要的。我们认为，这提供了另一种解决大词汇量的问题。</font>
</div>


<hr>
<div id="paper12"> <b>12. A Non-Intrusive Correction Algorithm for Classification Problems with  Corrupted Data</b>  <a href="https://arxiv.org/pdf/2002.04658" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kailiang Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongbin Xiu</a><br>
<font size="3">
Abstract: A novel correction algorithm is proposed for multi-class classification problems with corrupted training data. The algorithm is non-intrusive, in the sense that it post-processes a trained classification model by adding a correction procedure to the model prediction. The correction procedure can be coupled with any approximators, such as logistic regression, neural networks of various architectures, etc. When training dataset is sufficiently large, we prove that the corrected models deliver correct classification results as if there is no corruption in the training data. For datasets of finite size, the corrected models produce significantly better recovery results, compared to the models without the correction algorithm. All of the theoretical findings in the paper are verified by our numerical examples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一种新的校正算法，提出了多类分类问题已损坏的训练数据。该算法是非侵入性的，通过将校正过程的模型预测在这个意义上它后处理一个训练的分类模型。修正过程可以配上任何逼近，如逻辑回归，各种结构的神经网络，等等。当训练数据集是足够大的，我们证明了修正模型提供正确的分类结果作为是否有在训练数据中没有损坏。对于有限大小的数据集，校正模型产生显著更好的恢复效果，相比于没有纠错算法模型。所有在纸上的理论成果都是由我们的算例验证。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【论文笔记】Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation</title>
    <url>/2020/02/13/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Unsupervised-Domain-Adaptation-for-Neural-Machine-Translation-with-Iterative-Back-Translation/</url>
    <content><![CDATA[<p><strong>Unsupervised Domain Adaptation for Neural Machine Translation with Iterative Back Translation</strong>. Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits. AAAI 2020. <a href="https://arxiv.org/pdf/2001.08140.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>构造领域平行数据成本很高，如何在没有领域平行数据的情况下训练领域翻译模型显得尤为重要。本文想要解决的就是非监督领域适应NMT问题，提出了一种新的构造领域平行数据的方法：迭代回翻。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="/images/UDA_IBT_1.png" alt></p>
<p>本文提出一种基于transformer的模型框架，修改了transformer的encoder和decoder的输入，加入了language embedding，该模型具有以下特点：<br><br>1.源语言和目标语言共享bpe词表 <br><br>2.源语言和目标语言共享隐空间 <br></p>
<p>本文使用该模型用来训练语言模型、S2T翻译模型、T2S翻译模型，并且它们共享参数。<br>训练过程分三个步骤：<br>1.使用领域单语数据训练语言模型<br><img src="/images/UDA_IBT_3.png" alt></p>
<p>2.使用S2T翻译模型构造伪平行数据训练T2S模型，使用T2S翻译模型构造伪平行数据训练S2T模型<br><img src="/images/UDA_IBT_4.png" alt><br><font style="color: red">*公式中应该是作者笔误，顺序写错了。</font></p>
<p>3.使用平行数据训练模型<br><img src="/images/UDA_IBT_5.png" alt></p>
<p>不断迭代三个步骤直到参数收敛。</p>
<p>算法表示如下</p>
<p><img src="/images/UDA_IBT_2.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><img src="/images/UDA_IBT_6.png" alt></p>
<ul>
<li><strong>COPY</strong>：混合(t_in, t_in)和(s_out, t_out)，一起训练nmt</li>
<li><strong>BACK</strong>：使用Model_out构造伪平行in-domain数据，混合out-domain数据</li>
<li><strong>DALI</strong>：使用in-domain词表翻译t_in sent，构造伪平行数据，finetune  Model_out</li>
<li><strong>DAFE</strong>：多任务，NMT_out和LM_in (insert domain and task embedding)</li>
<li><strong>IBT</strong>: 迭代回翻，但不使用out-domain数据（也就是没有步骤三，完全无监督翻译）</li>
<li><strong>IBT+OUTD</strong>: 使用out-domain数据训练步骤三</li>
<li><strong>IBT+BACK</strong>: 使用伪平行数据和out-domain数据一起训练步骤三</li>
</ul>
<p>消融实验<br><img src="/images/UDA_IBT_7.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>AAAI</tag>
        <tag>迭代回翻</tag>
        <tag>非监督</tag>
        <tag>领域适应</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-12</title>
    <url>/2020/02/12/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-12/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> The Rumour Mill: Making Misinformation Spread Visible and Tangible <a href="https://arxiv.org/pdf/2002.04494" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning <a href="https://arxiv.org/pdf/2002.04326" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Learning Coupled Policies for Simultaneous Machine Translation <a href="https://arxiv.org/pdf/2002.04306" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Non-Autoregressive Neural Dialogue Generation <a href="https://arxiv.org/pdf/2002.04250" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity  Recognition and Sentiment Analysis of Political Tweets <a href="https://arxiv.org/pdf/2002.04181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Training with Streaming Annotation <a href="https://arxiv.org/pdf/2002.04165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Automatic Discourse Segmentation: an evaluation in French <a href="https://arxiv.org/pdf/2002.04095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> An experiment exploring the theoretical and methodological challenges in  developing a semi-automated approach to analysis of small-N qualitative data <a href="https://arxiv.org/pdf/2002.04513" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> HGAT: Hierarchical Graph Attention Network for Fake News Detection <a href="https://arxiv.org/pdf/2002.04397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Convolutional Neural Networks and a Transfer Learning Strategy to  Classify Parkinson's Disease from Speech in Three Different Languages <a href="https://arxiv.org/pdf/2002.04374" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Adversarial Filters of Dataset Biases <a href="https://arxiv.org/pdf/2002.04108" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. The Rumour Mill: Making Misinformation Spread Visible and Tangible</b>  <a href="https://arxiv.org/pdf/2002.04494" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Inie%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nanna Inie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Olesen%2C+J+F" target="_blank" rel="noopener" style="color:#0000EE;">Jeanette Falk Olesen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Derczynski%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leon Derczynski</a><br>
<font size="3">
Abstract: The spread of misinformation presents a technological and social threat to society. With the advance of AI-based language models, automatically generated texts have become difficult to identify and easy to create at scale. We present the "Rumour Mill", a playful art piece, designed as a commentary on the spread of rumours and automatically-generated misinformation. The mill is a tabletop interactive machine, which invites a user to experience the process of creating believable text by interacting with different tangible controls on the mill. The user manipulates visible parameters to adjust the genre and type of an automatically generated text rumour. The Rumour Mill is a physical demonstration of the state of NLP technology and its ability to generate and manipulate natural language text, and of the act of starting and spreading rumours. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：误传的传播呈现给社会技术和社会的威胁。随着基于人工智能语言模型的推进，自动生成的文本已经变得难以识别，容易大规模制造。我们提出了“传闻”，一个好玩艺术片，设计为传言和自动生成的误传传播的评注。该工厂是一个桌面交互的机器，它邀请用户通过与轧机上不同的实际控制交互体验创造可信的文本的过程。用户操纵可见参数来调整体裁和键入一个自动生成的文本的传言。谣言是NLP技术状况及其产生和处理自然语言文字能力的物理演示，以及启动和传播谣言的行为。</font>
</div>


<hr>
<div id="paper2"> <b>2. ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning</b>  <a href="https://arxiv.org/pdf/2002.04326" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weihao Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanfei Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiashi Feng</a><br>
<font size="3">
Abstract: Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text. In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text. In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points and separate them into EASY set while the rest as HARD set. Empirical results show that state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set. However, they struggle on HARD set with poor performance near that of random guess, indicating more research is needed to essentially enhance the logical reasoning ability of current models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近期强大的预先训练的语言模型已经达到上最流行的数据集的表现可圈可点阅读理解。现在是时候推出更多具有挑战性的数据集，以推动这一领域的发展对文本的更全面的推理。在本文中，我们介绍一种新的阅读理解数据集需要从标准化的研究生入学考试中提取的逻辑推理（ReClor）。正如之前的研究表明，人类的注解数据集通常包含的偏见，这往往是由模型利用来达到较高的精度没有真正理解课文。为了全面评估对ReClor车型的逻辑推理能力，我们建议确定偏置数据点，将它们分开成易于设置，而其余硬集。实证结果表明，国家的最先进的机型有出色的能力，包含在与EASY集高精度数据集中采集偏见。然而，他们在HARD组与斗争是附近随机猜测的表现不佳，表示需要基本上提高现有模式的逻辑推理能力进行更多的研究。</font>
</div>


<hr>
<div id="paper3"> <b>3. Learning Coupled Policies for Simultaneous Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.04306" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Arthur%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philip Arthur</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohn%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Trevor Cohn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gholamreza Haffari</a><br>
<font size="3">
Abstract: In simultaneous machine translation, the system needs to incrementally generate the output translation before the input sentence ends. This is a coupled decision process consisting of a programmer and interpreter. The programmer's policy decides about when to WRITE the next output or READ the next input, and the interpreter's policy decides what word to write. We present an imitation learning (IL) approach to efficiently learn effective coupled programmer-interpreter policies. To enable IL, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. We attribute the effectiveness of the learned coupled policies to (i) scheduled sampling addressing the coupled exposure bias, and (ii) quality of oracle actions capturing enough information from the partial input before writing the output. Experiments show our method outperforms strong baselines in terms of translation quality and delay, when translating from German/Arabic/Czech/Bulgarian/Romanian to English. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在同时机器翻译系统需要逐步产生输入句子结束前的输出转换。这是由一个程序员和解释器的耦合决定处理。程序员的政策决定什么时候写下一个输出或读取下一个输入，而口译的政策决定写什么字。我们提出了一个模仿学习（IL）的方法来有效地学习有效耦合程序员解释政策。为了使IL，我们提出了一个算法甲骨文的Oracle产品的读/写操作训练用的词对齐的概念双语句子对。我们认为所学习的耦合政策的效力与（i）预定的采样寻址耦合曝光偏置，和（ii）的Oracle动作写入输出之前捕获来自所述部分输入足够的信息的质量。实验证明我们的方法优于在翻译质量和延迟方面强大的基线，从德国/阿拉伯文/捷克/保加利亚/罗马尼亚翻译成英文。</font>
</div>


<hr>
<div id="paper4"> <b>4. Non-Autoregressive Neural Dialogue Generation</b>  <a href="https://arxiv.org/pdf/2002.04250" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxian Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwei Li</a><br>
<font size="3">
Abstract: Maximum Mutual information (MMI), which models the bidirectional dependency between responses ($y$) and contexts ($x$), i.e., the forward probability $\log p(y|x)$ and the backward probability $\log p(x|y)$, has been widely used as the objective in the \sts model to address the dull-response issue in open-domain dialog generation. Unfortunately, under the framework of the \sts model, direct decoding from $\log p(y|x) + \log p(x|y)$ is infeasible since the second part (i.e., $p(x|y)$) requires the completion of target generation before it can be computed, and the search space for $y$ is enormous. Empirically, an N-best list is first generated given $p(y|x)$, and $p(x|y)$ is then used to rerank the N-best list, which inevitably results in non-globally-optimal solutions. In this paper, we propose to use non-autoregressive (non-AR) generation model to address this non-global optimality issue. Since target tokens are generated independently in non-AR generation, $p(x|y)$ for each target word can be computed as soon as it's generated, and does not have to wait for the completion of the whole sequence. This naturally resolves the non-global optimal issue in decoding. Experimental results demonstrate that the proposed non-AR strategy produces more diverse, coherent, and appropriate responses, yielding substantive gains in BLEU scores and in human evaluations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最大互信息（MMI），该款机型的响应（$ Y $）和环境（$ X $）之间，即双向依赖，前向概率$ \日志P（Y | X）$和反向概率$ \日志p（X | Y）$，已被广泛用作目标在\ STS模式，以解决开域对话生成的平淡反应的问题。不幸的是，\ STS模型的框架下，直接解码从$ \日志P（Y | X）+ \日志P（X | Y）$是因为第二部分（即，$ P（X不可行| Y）$ ）要求目标生成的完成可以计算它之前，和$ Y $的搜索空间是巨大的。根据经验，N-最佳列表首先生成给出$ P（Y | X）$和$ P（X | Y），则$用于重新排名的N最佳列表，这不可避免地导致了非全局最优解。在本文中，我们建议使用非自回归（非AR）代车型，以解决这种非全局最优的问题。由于目标令牌在非AR生成独立产生，$ P（X | Y）$每个目标词可以很快，因为它是生成的计算，而不必等待整个序列的完成。这自然解决了解码非全局最优的问题。实验结果表明，所提出的非AR战略产生更多样化的，连贯的，适当的反应，产生的BLEU分数和人类评估实质性收益。</font>
</div>


<hr>
<div id="paper5"> <b>5. Performance Comparison of Crowdworkers and NLP Tools onNamed-Entity  Recognition and Sentiment Analysis of Political Tweets</b>  <a href="https://arxiv.org/pdf/2002.04181" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jalal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mona Jalal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mays%2C+K+K" target="_blank" rel="noopener" style="color:#0000EE;">Kate K. Mays</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Betke%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Margrit Betke</a><br>
<font size="3">
Abstract: We report results of a comparison of the accuracy of crowdworkers and seven NaturalLanguage Processing (NLP) toolkits in solving two important NLP tasks, named-entity recognition (NER) and entity-level sentiment(ELS) analysis. We here focus on a challenging dataset, 1,000 political tweets that were collected during the U.S. presidential primary election in February 2016. Each tweet refers to at least one of four presidential candidates,i.e., four named entities. The groundtruth, established by experts in political communication, has entity-level sentiment information for each candidate mentioned in the tweet. We tested several commercial and open-source tools. Our experiments show that, for our dataset of political tweets, the most accurate NER system, Google Cloud NL, performed almost on par with crowdworkers, but the most accurate ELS analysis system, TensiStrength, did not match the accuracy of crowdworkers by a large margin of more than 30 percent points. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们报告crowdworkers在解决两个重要的NLP任务，命名实体识别（NER）和公司层面的情绪（ELS）分析的准确性和七个自然语言处理（NLP）工具包的比较结果。在这里，我们专注于一个具有挑战性的数据集，1000在2016年二月，美国总统初选每个鸣叫是指四个总统候选人至少一个，即，四个命名实体期间收集政治鸣叫。真实状况，由专家在政治传播成立以来，一直在鸣叫提到的每个候选实体层面的情绪信息。我们测试了几个商业和开源工具。我们的实验表明，对于我们的政治鸣叫，最准确的命名实体识别系统，谷歌云NL，几乎堪与crowdworkers执行的数据集，但最准确的ELS分析系统，TensiStrength，并没有大幅度匹配crowdworkers的准确性超过30个百分点。</font>
</div>


<hr>
<div id="paper6"> <b>6. Training with Streaming Annotation</b>  <a href="https://arxiv.org/pdf/2002.04165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tongtao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heng Ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shih-Fu Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Freedman%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marjorie Freedman</a><br>
<font size="3">
Abstract: In this paper, we address a practical scenario where training data is released in a sequence of small-scale batches and annotation in earlier phases has lower quality than the later counterparts. To tackle the situation, we utilize a pre-trained transformer network to preserve and integrate the most salient document information from the earlier batches while focusing on the annotation (presumably with higher quality) from the current batch. Using event extraction as a case study, we demonstrate in the experiments that our proposed framework can perform better than conventional approaches (the improvement ranges from 3.6 to 14.9% absolute F-score gain), especially when there is more noise in the early annotation; and our approach spares 19.1% time with regard to the best conventional method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们要解决这里的训练数据被释放的小规模批次序列和注释在早期阶段具有比同行晚低质量的实际情况。为了应对这种情况，我们利用预先训练变压器网络维护和最显着的文档信息从早期批次集成，并重点标注（大概有更高质量的）从目前的批次。使用事件提取作为个案研究，我们证明在实验中，我们提出的架构可以比传统方法更好地履行（改善的范围从3.6到14.9％的绝对F-分数增益），尤其是当有更多的噪音在早期的注释;而我们的方法免去19.1％的时间就以最好的常规方法。</font>
</div>


<hr>
<div id="paper7"> <b>7. Automatic Discourse Segmentation: an evaluation in French</b>  <a href="https://arxiv.org/pdf/2002.04095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saksik%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rémy Saksik</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Molina-Villegas%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alejandro Molina-Villegas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Linhares%2C+A+C" target="_blank" rel="noopener" style="color:#0000EE;">Andréa Carneiro Linhares</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torres-Moreno%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juan-Manuel Torres-Moreno</a><br>
<font size="3">
Abstract: In this article, we describe some discursive segmentation methods as well as a preliminary evaluation of the segmentation quality. Although our experiment were carried for documents in French, we have developed three discursive segmentation models solely based on resources simultaneously available in several languages: marker lists and a statistic POS labeling. We have also carried out automatic evaluations of these systems against the Annodis corpus, which is a manually annotated reference. The results obtained are very encouraging. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这篇文章中，我们介绍一些话语分割方法以及分割质量的初步评估。标记列表和统计POS标签：虽然我们的实验中，进行了在法国的文件中，我们基于几种语言的同时可利用的资源仅开发了三种话语分割模型。我们还进行了针对Annodis语料库，其是手动注释参考这些系统的自动评估。得到的结果是非常令人鼓舞的。</font>
</div>


<hr>
<div id="paper8"> <b>8. An experiment exploring the theoretical and methodological challenges in  developing a semi-automated approach to analysis of small-N qualitative data</b>  <a href="https://arxiv.org/pdf/2002.04513" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tsang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sandro Tsang</a><br>
<font size="3">
Abstract: This paper experiments with designing a semi-automated qualitative data analysis (QDA) algorithm to analyse 20 transcripts by using freeware. Text-mining (TM) and QDA were guided by frequency and association measures, because these statistics remain robust when the sample size is small. The refined TM algorithm split the text into various sizes based on a manually revised dictionary. This lemmatisation approach may reflect the context of the text better than uniformly tokenising the text into one single size. TM results were used for initial coding. Code repacking was guided by association measures and external data to implement a general inductive QDA approach. The information retrieved by TM and QDA was depicted in subgraphs for comparisons. The analyses were completed in 6-7 days. Both algorithms retrieved contextually consistent and relevant information. However, the QDA algorithm retrieved more specific information than TM alone. The QDA algorithm does not strictly comply with the convention of TM or of QDA, but becomes a more efficient, systematic and transparent text analysis approach than a conventional QDA approach. Scaling up QDA to reliably discover knowledge from text was exactly the research purpose. This paper also sheds light on understanding the relations between information technologies, theory and methodologies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：设计一个半自动化定性数据分析（QDA）算法实验通过使用免费软件来分析20组的转录本。文本挖掘（TM）和QDA通过频率和关联的措施引导，因为这些统计信息保持稳健当样本大小是小的。精制TM算法分割文成基于手动订正字典各种尺寸。这lemmatisation做法可能反映了文本的语境比文字均匀tokenising成一个单一的大小更好。 TM结果用于初始编码。代码重新包装是由协会的措施和外部数据导入到实施一般的感应QDA方法。由TM和QDA检索的信息在子图用于比较被描绘。分析是在6-7天内完成。这两种算法检索上下文一致的相关信息。然而，QDA算法获取更具体的信息比TM孤独。该QDA算法不严格遵守TM或QDA的惯例，但比传统的QDA方法更高效，系统，透明的文本分析方法。扩大QDA可靠地发现从文本知识正是研究目的。本文还揭示了理解信息技术，理论和方法之间的关系光。</font>
</div>


<hr>
<div id="paper9"> <b>9. HGAT: Hierarchical Graph Attention Network for Fake News Detection</b>  <a href="https://arxiv.org/pdf/2002.04397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxiang Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiawei Zhang</a><br>
<font size="3">
Abstract: The explosive growth of fake news has eroded the credibility of medias and governments. Fake news detection has become an urgent task. News articles along with other related components like news creators and news subjects can be modeled as a heterogeneous information network (HIN for short). In this paper, we focus on studying the HIN- based fake news detection problem. We propose a novel fake news detection framework, namely Hierarchical Graph Attention Network (HGAT) which employs a novel hierarchical attention mechanism to detect fake news by classifying news article nodes in the HIN. This method can effectively learn information from different types of related nodes through node-level and schema-level attention. Experiments with real-world fake news data show that our model can outperform text-based models and other network-based models. Besides, the experiments also demonstrate the expandability and potential of HGAT for heterogeneous graphs representation learning in the future. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：假新闻的爆炸性增长已经侵蚀媒体和政府的公信力。假新闻的检测已成为一项紧迫的任务。与像新闻创作者和新闻主体等相关组件一起的新闻文章可以模拟成一个异构信息网络（HIN的简称）。在本文中，我们重点研究基于HIN-假新闻的检测问题。我们提出了一个新的假新闻的检测框架，即层次图关注网络（HGAT），它采用了新的分级注意机制由HIN新闻文章分类节点检测到假新闻。这种方法可以有效地学习，通过节点级和模式的高度重视，从不同类型的相关节点的信息。与现实世界的假新闻数据实验表明，我们的模型可以超越基于文本的模型和其他基于网络的模型。此外，实验还证明HGAT的在未来的异构图形表示学习可扩展性和潜力。</font>
</div>


<hr>
<div id="paper10"> <b>10. Convolutional Neural Networks and a Transfer Learning Strategy to  Classify Parkinson's Disease from Speech in Three Different Languages</b>  <a href="https://arxiv.org/pdf/2002.04374" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=V%C3%A1squez-Correa%2C+J+C" target="_blank" rel="noopener" style="color:#0000EE;">J. C. Vásquez-Correa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Arias-Vergara%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">T. Arias-Vergara</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rios-Urrego%2C+C+D" target="_blank" rel="noopener" style="color:#0000EE;">C. D. Rios-Urrego</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">M. Schuster</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rusz%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">J. Rusz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Orozco-Arroyave%2C+J+R" target="_blank" rel="noopener" style="color:#0000EE;">J. R. Orozco-Arroyave</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=N%C3%B6th%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">E. Nöth</a><br>
<font size="3">
Abstract: Parkinson's disease patients develop different speech impairments that affect their communication capabilities. The automatic assessment of the speech of the patients allows the development of computer aided tools to support the diagnosis and the evaluation of the disease severity. This paper introduces a methodology to classify Parkinson's disease from speech in three different languages: Spanish, German, and Czech. The proposed approach considers convolutional neural networks trained with time frequency representations and a transfer learning strategy among the three languages. The transfer learning scheme aims to improve the accuracy of the models when the weights of the neural network are initialized with utterances from a different language than the used for the test set. The results suggest that the proposed strategy improves the accuracy of the models in up to 8\% when the base model used to initialize the weights of the classifier is robust enough. In addition, the results obtained after the transfer learning are in most cases more balanced in terms of specificity-sensitivity than those trained without the transfer learning strategy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：帕金森氏症患者制定影响其通信能力不同的语言障碍。患者的语音的自动评估允许的计算机辅助工具的开发，以支持诊断和疾病严重程度的评估。本文介绍一种方法，帕金森氏病从语音三种语言进行分类，西班牙语，德语和捷克。所提出的方法考虑了频率随时间的陈述和三种语言之间的迁移学习策略训练的卷积神经网络。转移学习方案，目的是提高模型的准确性时，神经网络的权与话语从不同的语言不是用于测试集初始化。结果表明，该策略提高了多达8 \％的模型的准确性时使用的基础模型来初始化权重的分类是足够强大的。此外，转移学习之后得到的结果都在比那些没有转移学习策略训练的特异性，灵敏度方面更平衡大多数情况下。</font>
</div>


<hr>
<div id="paper11"> <b>11. Adversarial Filters of Dataset Biases</b>  <a href="https://arxiv.org/pdf/2002.04108" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bras%2C+R+L" target="_blank" rel="noopener" style="color:#0000EE;">Ronan Le Bras</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Swayamdipta%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Swabha Swayamdipta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhagavatula%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chandra Bhagavatula</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zellers%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rowan Zellers</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peters%2C+M+E" target="_blank" rel="noopener" style="color:#0000EE;">Matthew E. Peters</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabharwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ashish Sabharwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Choi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yejin Choi</a><br>
<font size="3">
Abstract: Large neural models have demonstrated human-level performance on language and vision benchmarks such as ImageNet and Stanford Natural Language Inference (SNLI). Yet, their performance degrades considerably when tested on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting on spurious dataset biases. We investigate one recently proposed approach, AFLite, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLite, by situating it in the generalized framework for optimum bias reduction. Our experiments show that as a result of the substantial reduction of these biases, models trained on the filtered datasets yield better generalization to out-of-distribution tasks, especially when the benchmarks used for training are over-populated with biased samples. We show that AFLite is broadly applicable to a variety of both real and synthetic datasets for reduction of measurable dataset biases and provide extensive supporting analyses. Finally, filtering results in a large drop in model performance (e.g., from 92% to 63% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型神经模型已经证明在语言和视觉基准，如ImageNet和斯坦福大学自然语言推理（SNLI）人类水平的性能。然而，它们的性能会下降显着，当上对抗或分发外的样品进行测试。这就提出了这些模型是否已经学会对过度拟合数据集虚假偏见，解决了数据集而不是底层任务的问题。我们调查一个最近提出的方法，AFLite，这adversarially过滤器，例如数据集的偏见，作为一种手段来减轻整机性能普遍高估。我们为AFLite提供理论的理解，通过在最佳偏置降低广义框架的情境吧。我们的实验显示，这些偏见的大幅度减少的结果，训练有素的过滤数据集模型产生更好的推广到外的配送任务，特别是当用于训练的基准测试过填充偏置样品。我们表明，AFLite广泛适用于各种实际和综合数据集的减少衡量数据集的偏见，并提供广泛的支持分析。最后，过滤结果在模型的性能（例如SNLI，从92％至63％）大的下降，而人的性能仍然保持为高。因此，我们的工作表明，这种过滤的数据集可以作为升级基准构成了强大的推广新研究挑战。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-11</title>
    <url>/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-11/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Upper, Middle and Lower Region Learning for Facial Action Unit Detection <a href="https://arxiv.org/pdf/2002.04023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Deep Convolutional Neural Networks with Spatial Regularization, Volume  and Star-shape Priori for Image Segmentation <a href="https://arxiv.org/pdf/2002.03989" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Unconstrained Periocular Recognition: Using Generative Deep Learning  Frameworks for Attribute Normalization <a href="https://arxiv.org/pdf/2002.03985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> StickyPillars: Robust feature matching on point clouds using Graph  Neural Networks <a href="https://arxiv.org/pdf/2002.03983" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Joint Encoding of Appearance and Motion Features with Self-supervision  for First Person Action Recognition <a href="https://arxiv.org/pdf/2002.03982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> RePose: Learning Deep Kinematic Priors for Fast Human Pose Estimation <a href="https://arxiv.org/pdf/2002.03933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> 6DoF Object Pose Estimation via Differentiable Proxy Voting Loss <a href="https://arxiv.org/pdf/2002.03923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Hierarchical Multi-Process Fusion for Visual Place Recognition <a href="https://arxiv.org/pdf/2002.03895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> CIFAR-10 Image Classification Using Feature Ensembles <a href="https://arxiv.org/pdf/2002.03846" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Exploiting Temporal Coherence for Multi-modal Video Categorization <a href="https://arxiv.org/pdf/2002.03844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Attentive Group Equivariant Convolutional Networks <a href="https://arxiv.org/pdf/2002.03830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Level Three Synthetic Fingerprint Generation <a href="https://arxiv.org/pdf/2002.03809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Automatic image-based identification and biomass estimation of  invertebrates <a href="https://arxiv.org/pdf/2002.03807" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> CONVINCE: Collaborative Cross-Camera Video Analytics at the Edge <a href="https://arxiv.org/pdf/2002.03797" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Deep Learning for Classifying Food Waste <a href="https://arxiv.org/pdf/2002.03786" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Multi-stream Faster RCNN for Mitosis Counting in Breast Cancer Images <a href="https://arxiv.org/pdf/2002.03781" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Towards Deep Machine Reasoning: a Prototype-based Deep Neural Network  with Decision Tree Inference <a href="https://arxiv.org/pdf/2002.03776" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Deriving Emotions and Sentiments from Visual Content: A Disaster  Analysis Use Case <a href="https://arxiv.org/pdf/2002.03773" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Improving the Evaluation of Generative Models with Fuzzy Logic <a href="https://arxiv.org/pdf/2002.03772" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Learning Numerical Observers using Unsupervised Domain Adaptation <a href="https://arxiv.org/pdf/2002.03763" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Music2Dance: Music-driven Dance Generation using WaveNet <a href="https://arxiv.org/pdf/2002.03761" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> An Empirical Study of Person Re-Identification with Attributes <a href="https://arxiv.org/pdf/2002.03752" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Weighted Average Precision: Adversarial Example Detection in the Visual  Perception of Autonomous Vehicles <a href="https://arxiv.org/pdf/2002.03751" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> An Overview of Two Age Synthesis and Estimation Techniques <a href="https://arxiv.org/pdf/2002.03750" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> DFKI Cabin Simulator: A Test Platform for Visual In-Cabin Monitoring  Functions <a href="https://arxiv.org/pdf/2002.03749" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Black Box Explanation by Learning Image Exemplars in the Latent Feature  Space <a href="https://arxiv.org/pdf/2002.03746" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Dynamic Error-bounded Lossy Compression (EBLC) to Reduce the Bandwidth  Requirement for Real-time Vision-based Pedestrian Safety Applications <a href="https://arxiv.org/pdf/2002.03742" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Efficient Scene Text Detection with Textual Attention Tower <a href="https://arxiv.org/pdf/2002.03741" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Convolutional Hierarchical Attention Network for Query-Focused Video  Summarization <a href="https://arxiv.org/pdf/2002.03740" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Localizing Multi-scale Semantic Patches for Image Classification <a href="https://arxiv.org/pdf/2002.03737" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Universal Semantic Segmentation for Fisheye Urban Driving Images <a href="https://arxiv.org/pdf/2002.03736" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Real-Time Object Detection and Recognition on Low-Compute Humanoid  Robots using Deep Learning <a href="https://arxiv.org/pdf/2002.03735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Iterative energy-based projection on a normal data manifold for anomaly  localization <a href="https://arxiv.org/pdf/2002.03734" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Robust Multimodal Image Registration Using Deep Recurrent Reinforcement  Learning <a href="https://arxiv.org/pdf/2002.03733" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
<div id="title35">
<b>35.</b> Impact of Data Quality on Deep Neural Network Training <a href="https://arxiv.org/pdf/2002.03732" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper35" style="color:#0000EE;">摘要</a><br></div>
<div id="title36">
<b>36.</b> RSnet: An improvement for Darknet <a href="https://arxiv.org/pdf/2002.03729" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper36" style="color:#0000EE;">摘要</a><br></div>
<div id="title37">
<b>37.</b> Driver Drowsiness Detection Model Using Convolutional Neural Networks  Techniques for Android Application <a href="https://arxiv.org/pdf/2002.03728" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper37" style="color:#0000EE;">摘要</a><br></div>
<div id="title38">
<b>38.</b> Durocmien: A deep framework for duroc skeleton extraction in constraint  environment <a href="https://arxiv.org/pdf/2002.03727" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper38" style="color:#0000EE;">摘要</a><br></div>
<div id="title39">
<b>39.</b> Deep Frequent Spatial Temporal Learning for Face Anti-Spoofing <a href="https://arxiv.org/pdf/2002.03723" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper39" style="color:#0000EE;">摘要</a><br></div>
<div id="title40">
<b>40.</b> Unsupervised deep clustering for predictive texture pattern discovery in  medical images <a href="https://arxiv.org/pdf/2002.03721" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper40" style="color:#0000EE;">摘要</a><br></div>
<div id="title41">
<b>41.</b> Fabricated Pictures Detection with Graph Matching <a href="https://arxiv.org/pdf/2002.03720" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper41" style="color:#0000EE;">摘要</a><br></div>
<div id="title42">
<b>42.</b> Knowledge Distillation for Brain Tumor Segmentation <a href="https://arxiv.org/pdf/2002.03688" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper42" style="color:#0000EE;">摘要</a><br></div>
<div id="title43">
<b>43.</b> Deep Multi-task Multi-label CNN for Effective Facial Attribute  Classification <a href="https://arxiv.org/pdf/2002.03683" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper43" style="color:#0000EE;">摘要</a><br></div>
<div id="title44">
<b>44.</b> Uncertainty Estimation for End-To-End Learned Dense Stereo Matching via  Probabilistic Deep Learning <a href="https://arxiv.org/pdf/2002.03663" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper44" style="color:#0000EE;">摘要</a><br></div>
<div id="title45">
<b>45.</b> Distribution Distillation Loss: Generic Approach for Improving Face  Recognition from Hard Samples <a href="https://arxiv.org/pdf/2002.03662" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper45" style="color:#0000EE;">摘要</a><br></div>
<div id="title46">
<b>46.</b> CRVOS: Clue Refining Network for Video Object Segmentation <a href="https://arxiv.org/pdf/2002.03651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper46" style="color:#0000EE;">摘要</a><br></div>
<div id="title47">
<b>47.</b> Collaborative Training of Balanced Random Forests for Open Set Domain  Adaptation <a href="https://arxiv.org/pdf/2002.03642" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper47" style="color:#0000EE;">摘要</a><br></div>
<div id="title48">
<b>48.</b> End-to-End Facial Deep Learning Feature Compression with Teacher-Student  Enhancement <a href="https://arxiv.org/pdf/2002.03627" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper48" style="color:#0000EE;">摘要</a><br></div>
<div id="title49">
<b>49.</b> Post-Comparison Mitigation of Demographic Bias in Face Recognition Using  Fair Score Normalization <a href="https://arxiv.org/pdf/2002.03592" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper49" style="color:#0000EE;">摘要</a><br></div>
<div id="title50">
<b>50.</b> Prototype Refinement Network for Few-Shot Segmentation <a href="https://arxiv.org/pdf/2002.03579" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper50" style="color:#0000EE;">摘要</a><br></div>
<div id="title51">
<b>51.</b> Automatic detection and counting of retina cell nuclei using deep  learning <a href="https://arxiv.org/pdf/2002.03563" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper51" style="color:#0000EE;">摘要</a><br></div>
<div id="title52">
<b>52.</b> FAU, Facial Expressions, Valence and Arousal: A Multi-task Solution <a href="https://arxiv.org/pdf/2002.03557" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper52" style="color:#0000EE;">摘要</a><br></div>
<div id="title53">
<b>53.</b> Vehicle Driving Assistant <a href="https://arxiv.org/pdf/2002.03556" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper53" style="color:#0000EE;">摘要</a><br></div>
<div id="title54">
<b>54.</b> From Anchor Generation to Distribution Alignment: Learning a  Discriminative Embedding Space for Zero-Shot Recognition <a href="https://arxiv.org/pdf/2002.03554" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper54" style="color:#0000EE;">摘要</a><br></div>
<div id="title55">
<b>55.</b> UGRWO-Sampling: A modified random walk under-sampling approach based on  graphs to imbalanced data classification <a href="https://arxiv.org/pdf/2002.03521" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper55" style="color:#0000EE;">摘要</a><br></div>
<div id="title56">
<b>56.</b> A New Perspective for Flexible Feature Gathering in Scene Text  Recognition Via Character Anchor Pooling <a href="https://arxiv.org/pdf/2002.03509" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper56" style="color:#0000EE;">摘要</a><br></div>
<div id="title57">
<b>57.</b> Segmenting unseen industrial components in a heavy clutter using rgb-d  fusion and synthetic data <a href="https://arxiv.org/pdf/2002.03501" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper57" style="color:#0000EE;">摘要</a><br></div>
<div id="title58">
<b>58.</b> ABBA: Saliency-Regularized Motion-Based Adversarial Blur Attack <a href="https://arxiv.org/pdf/2002.03500" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper58" style="color:#0000EE;">摘要</a><br></div>
<div id="title59">
<b>59.</b> Medical Image Registration Using Deep Neural Networks: A Comprehensive  Review <a href="https://arxiv.org/pdf/2002.03401" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper59" style="color:#0000EE;">摘要</a><br></div>
<div id="title60">
<b>60.</b> Two-Stream Aural-Visual Affect Analysis in the Wild <a href="https://arxiv.org/pdf/2002.03399" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper60" style="color:#0000EE;">摘要</a><br></div>
<div id="title61">
<b>61.</b> MS-Net: Multi-Site Network for Improving Prostate Segmentation with  Heterogeneous MRI Data <a href="https://arxiv.org/pdf/2002.03366" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper61" style="color:#0000EE;">摘要</a><br></div>
<div id="title62">
<b>62.</b> Weakly Supervised Attention Pyramid Convolutional Neural Network for  Fine-Grained Visual Classification <a href="https://arxiv.org/pdf/2002.03353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper62" style="color:#0000EE;">摘要</a><br></div>
<div id="title63">
<b>63.</b> Dynamic Inference: A New Approach Toward Efficient Video Action  Recognition <a href="https://arxiv.org/pdf/2002.03342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper63" style="color:#0000EE;">摘要</a><br></div>
<div id="title64">
<b>64.</b> VIFB: A Visible and Infrared Image Fusion Benchmark <a href="https://arxiv.org/pdf/2002.03322" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper64" style="color:#0000EE;">摘要</a><br></div>
<div id="title65">
<b>65.</b> Unlabeled Data Deployment for Classification of Diabetic Retinopathy  Images Using Knowledge Transfer <a href="https://arxiv.org/pdf/2002.03321" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper65" style="color:#0000EE;">摘要</a><br></div>
<div id="title66">
<b>66.</b> FSD-10: A Dataset for Competitive Sports Content Analysis <a href="https://arxiv.org/pdf/2002.03312" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper66" style="color:#0000EE;">摘要</a><br></div>
<div id="title67">
<b>67.</b> Face Hallucination with Finishing Touches <a href="https://arxiv.org/pdf/2002.03308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper67" style="color:#0000EE;">摘要</a><br></div>
<div id="title68">
<b>68.</b> Splitting Convolutional Neural Network Structures for Efficient  Inference <a href="https://arxiv.org/pdf/2002.03302" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper68" style="color:#0000EE;">摘要</a><br></div>
<div id="title69">
<b>69.</b> Convolutional Neural Network Pruning Using Filter Attenuation <a href="https://arxiv.org/pdf/2002.03299" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper69" style="color:#0000EE;">摘要</a><br></div>
<div id="title70">
<b>70.</b> PointHop++: A Lightweight Learning Model on Point Sets for 3D  Classification <a href="https://arxiv.org/pdf/2002.03281" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper70" style="color:#0000EE;">摘要</a><br></div>
<div id="title71">
<b>71.</b> Asymmetric Rejection Loss for Fairer Face Recognition <a href="https://arxiv.org/pdf/2002.03276" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper71" style="color:#0000EE;">摘要</a><br></div>
<div id="title72">
<b>72.</b> Learning efficient structured dictionary for image classification <a href="https://arxiv.org/pdf/2002.03271" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper72" style="color:#0000EE;">摘要</a><br></div>
<div id="title73">
<b>73.</b> Weakly-Supervised Multi-Person Action Recognition in 360$^{\circ}$  Videos <a href="https://arxiv.org/pdf/2002.03266" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper73" style="color:#0000EE;">摘要</a><br></div>
<div id="title74">
<b>74.</b> GradMix: Multi-source Transfer across Domains and Tasks <a href="https://arxiv.org/pdf/2002.03264" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper74" style="color:#0000EE;">摘要</a><br></div>
<div id="title75">
<b>75.</b> Ensemble of Deep Convolutional Neural Networks for Automatic Pavement  Crack Detection and Measurement <a href="https://arxiv.org/pdf/2002.03241" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper75" style="color:#0000EE;">摘要</a><br></div>
<div id="title76">
<b>76.</b> Multi-Label Class Balancing Algorithm for Action Unit Detection <a href="https://arxiv.org/pdf/2002.03238" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper76" style="color:#0000EE;">摘要</a><br></div>
<div id="title77">
<b>77.</b> Intrinsic Dimension Estimation via Nearest Constrained Subspace  Classifier <a href="https://arxiv.org/pdf/2002.03228" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper77" style="color:#0000EE;">摘要</a><br></div>
<div id="title78">
<b>78.</b> Exocentric to Egocentric Image Generation via Parallel Generative  Adversarial Network <a href="https://arxiv.org/pdf/2002.03219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper78" style="color:#0000EE;">摘要</a><br></div>
<div id="title79">
<b>79.</b> Spatial-Temporal Multi-Cue Network for Continuous Sign Language  Recognition <a href="https://arxiv.org/pdf/2002.03187" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper79" style="color:#0000EE;">摘要</a><br></div>
<div id="title80">
<b>80.</b> Sparsity-Aware Deep Learning for Automatic 4D Facial Expression  Recognition <a href="https://arxiv.org/pdf/2002.03157" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper80" style="color:#0000EE;">摘要</a><br></div>
<div id="title81">
<b>81.</b> CTM: Collaborative Temporal Modeling for Action Recognition <a href="https://arxiv.org/pdf/2002.03152" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper81" style="color:#0000EE;">摘要</a><br></div>
<div id="title82">
<b>82.</b> Multi-Modality Cascaded Fusion Technology for Autonomous Driving <a href="https://arxiv.org/pdf/2002.03138" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper82" style="color:#0000EE;">摘要</a><br></div>
<div id="title83">
<b>83.</b> Symbiotic Attention with Privileged Information for Egocentric Action  Recognition <a href="https://arxiv.org/pdf/2002.03137" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper83" style="color:#0000EE;">摘要</a><br></div>
<div id="title84">
<b>84.</b> Variable-Viewpoint Representations for 3D Object Recognition <a href="https://arxiv.org/pdf/2002.03131" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper84" style="color:#0000EE;">摘要</a><br></div>
<div id="title85">
<b>85.</b> Attacking Optical Character Recognition (OCR) Systems with Adversarial  Watermarks <a href="https://arxiv.org/pdf/2002.03095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper85" style="color:#0000EE;">摘要</a><br></div>
<div id="title86">
<b>86.</b> Bone Suppression on Chest Radiographs With Adversarial Learning <a href="https://arxiv.org/pdf/2002.03073" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper86" style="color:#0000EE;">摘要</a><br></div>
<div id="title87">
<b>87.</b> Local Facial Attribute Transfer through Inpainting <a href="https://arxiv.org/pdf/2002.03040" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper87" style="color:#0000EE;">摘要</a><br></div>
<div id="title88">
<b>88.</b> Unsupervised Discovery of Interpretable Directions in the GAN Latent  Space <a href="https://arxiv.org/pdf/2002.03754" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper88" style="color:#0000EE;">摘要</a><br></div>
<div id="title89">
<b>89.</b> Learning End-to-End Lossy Image Compression: A Benchmark <a href="https://arxiv.org/pdf/2002.03711" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper89" style="color:#0000EE;">摘要</a><br></div>
<div id="title90">
<b>90.</b> Distributed Bayesian Matrix Decomposition for Big Data Mining and  Clustering <a href="https://arxiv.org/pdf/2002.03703" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper90" style="color:#0000EE;">摘要</a><br></div>
<div id="title91">
<b>91.</b> Adversarial TCAV -- Robust and Effective Interpretation of Intermediate  Layers in Neural Networks <a href="https://arxiv.org/pdf/2002.03549" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper91" style="color:#0000EE;">摘要</a><br></div>
<div id="title92">
<b>92.</b> Multi-object Monocular SLAM for Dynamic Environments <a href="https://arxiv.org/pdf/2002.03528" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper92" style="color:#0000EE;">摘要</a><br></div>
<div id="title93">
<b>93.</b> Ultra High Fidelity Image Compression with $\ell_\infty$-constrained  Encoding and Deep Decoding <a href="https://arxiv.org/pdf/2002.03482" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper93" style="color:#0000EE;">摘要</a><br></div>
<div id="title94">
<b>94.</b> Semi-Supervised Class Discovery <a href="https://arxiv.org/pdf/2002.03480" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper94" style="color:#0000EE;">摘要</a><br></div>
<div id="title95">
<b>95.</b> A Deep Learning Approach to Automate High-Resolution Blood Vessel  Reconstruction on Computerized Tomography Images With or Without the Use of  Contrast Agent <a href="https://arxiv.org/pdf/2002.03463" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper95" style="color:#0000EE;">摘要</a><br></div>
<div id="title96">
<b>96.</b> A Unified End-to-End Framework for Efficient Deep Image Compression <a href="https://arxiv.org/pdf/2002.03370" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper96" style="color:#0000EE;">摘要</a><br></div>
<div id="title97">
<b>97.</b> Multi-Task Learning by a Top-Down Control Network <a href="https://arxiv.org/pdf/2002.03335" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper97" style="color:#0000EE;">摘要</a><br></div>
<div id="title98">
<b>98.</b> Out-of-Distribution Detection with Distance Guarantee in Deep Generative  Models <a href="https://arxiv.org/pdf/2002.03328" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper98" style="color:#0000EE;">摘要</a><br></div>
<div id="title99">
<b>99.</b> Holographic Image Sensing <a href="https://arxiv.org/pdf/2002.03314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper99" style="color:#0000EE;">摘要</a><br></div>
<div id="title100">
<b>100.</b> Soft Threshold Weight Reparameterization for Learnable Sparsity <a href="https://arxiv.org/pdf/2002.03231" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper100" style="color:#0000EE;">摘要</a><br></div>
<div id="title101">
<b>101.</b> Correction of Chromatic Aberration from a Single Image Using Keypoints <a href="https://arxiv.org/pdf/2002.03196" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper101" style="color:#0000EE;">摘要</a><br></div>
<div id="title102">
<b>102.</b> Deep No-reference Tone Mapped Image Quality Assessment <a href="https://arxiv.org/pdf/2002.03165" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper102" style="color:#0000EE;">摘要</a><br></div>
<div id="title103">
<b>103.</b> Ramifications and Diminution of Image Noise in Iris Recognition System <a href="https://arxiv.org/pdf/2002.03125" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper103" style="color:#0000EE;">摘要</a><br></div>
<div id="title104">
<b>104.</b> An Empirical Evaluation of Perturbation-based Defenses <a href="https://arxiv.org/pdf/2002.03080" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper104" style="color:#0000EE;">摘要</a><br></div>
<div id="title105">
<b>105.</b> Predictive online optimisation with applications to optical flow <a href="https://arxiv.org/pdf/2002.03053" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper105" style="color:#0000EE;">摘要</a><br></div>
<div id="title106">
<b>106.</b> Cognitive Anthropomorphism of AI: How Humans and Computers Classify  Images <a href="https://arxiv.org/pdf/2002.03024" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper106" style="color:#0000EE;">摘要</a><br></div>
<div id="title107">
<b>107.</b> Improving the Adversarial Robustness of Transfer Learning via Noisy  Feature Distillation <a href="https://arxiv.org/pdf/2002.02998" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper107" style="color:#0000EE;">摘要</a><br></div>
<div id="title108">
<b>108.</b> DropCluster: A structured dropout for convolutional networks <a href="https://arxiv.org/pdf/2002.02997" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper108" style="color:#0000EE;">摘要</a><br></div>
<div id="title109">
<b>109.</b> SS-Auto: A Single-Shot, Automatic Structured Weight Pruning Framework of  DNNs with Ultra-High Efficiency <a href="https://arxiv.org/pdf/2001.08839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper109" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><!-- procjx-wenzhang2 --> <ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins> <script>      (adsbygoogle = window.adsbygoogle || []).push({}); </script>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Upper, Middle and Lower Region Learning for Facial Action Unit Detection</b>  <a href="https://arxiv.org/pdf/2002.04023" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xia%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yao Xia</a><br>
<font size="3">
Abstract: Facial action units (AUs) detection is fundamental to facial expression analysis. As AU occur only in a small area of face, region based learning has been widely recognized useful for AU detection. Most region based studies focus on a small region where the AU occurs. Focusing on a specific region is helpful in eliminating the influence of identity, but to be risk for losing information. It is difficult to find balance. In this study, I propose a simple strategy. I divide the face into three large regions, upper, middle and lower region, and group AUs based on where it occurs. I propose a new end-to-end deep learning framework named three regions based attention network (TRA-Net). After extracting the global feature, TRA-Net uses a hard attention module to extract three feature maps, each of which contains only a specific region. Each region-specific feature map is fed to an independent branch. For each branch, three continuous soft attention modules are used to extract higher-level features for final AU detection. In the DISFA dataset, this model achieves the highest F1 scores for the detection of AU1, AU2 and AU4, and produces the highest accuracy in comparison with the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：面部动作单元（AU）检测是面部表情分析的基础。由于AU只发生在脸上的小区域，基于区域的学习已得到广泛认可的AU检测有用。大多数基于区域的研究重点放在非盟发生小区域。专注于一个特定的区域是在消除身份的影响力有帮助，但对信息丢失的风险。这是很难找到平衡点。在这项研究中，我提出了一个简单的策略。我划分面为三个大区域，上部，中部和下部区域，并且组的AU基于其中它发生。我建议命名为三个区域以关注网络（TRA-网）一个新的终端到终端的深度学习的框架。提取全局特征后，TRA-Net使用硬关注模块中提取三个特征的地图，每一个都包含只针对特定区域。每个区域特异性特征地图被馈送到一个独立的分支。对于每个分支，三个连续软注意模块用于提取最终AU检测较高级别的功能。在DISFA数据集，该模型获得了最高的分数F1用于检测AU1，AU2和AU4的，并产生最高的精度在与国家的最先进的方法相比。</font>
</div>


<hr>
<div id="paper2"> <b>2. Deep Convolutional Neural Networks with Spatial Regularization, Volume  and Star-shape Priori for Image Segmentation</b>  <a href="https://arxiv.org/pdf/2002.03989" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyue Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tai%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xue-cheng Tai</a><br>
<font size="3">
Abstract: We use Deep Convolutional Neural Networks (DCNNs) for image segmentation problems. DCNNs can well extract the features from natural images. However, the classification functions in the existing network architecture of CNNs are simple and lack capabilities to handle important spatial information in a way that have been done for many well-known traditional variational models. Prior such as spatial regularity, volume prior and object shapes cannot be well handled by existing DCNNs. We propose a novel Soft Threshold Dynamics (STD) framework which can easily integrate many spatial priors of the classical variational models into the DCNNs for image segmentation. The novelty of our method is to interpret the softmax activation function as a dual variable in a variational problem, and thus many spatial priors can be imposed in the dual space. From this viewpoint, we can build a STD based framework which can enable the outputs of DCNNs to have many special priors such as spatial regularity, volume constraints and star-shape priori. The proposed method is a general mathematical framework and it can be applied to any semantic segmentation DCNNs. To show the efficiency and accuracy of our method, we applied it to the popular DeepLabV3+ image segmentation network, and the experiments results show that our method can work efficiently on data-driven image segmentation DCNNs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们使用深卷积神经网络（DCNNs）图像分割问题。 DCNNs能很好地提取自然图像的功能。然而，在细胞神经网络的现有网络架构的分类功能简单，缺乏能力来处理已为许多著名的传统模式变做一种方式重要的空间信息。现有如空间规律性，体积之前和对象的形状不能被很好地现有DCNNs处理。我们提出了一个新颖的软阈值的动力学（STD）的框架，可以很容易的经典车型变了许多空间先验融入DCNNs的图像分割。我们的方法的新颖性在于解释SOFTMAX激活函数如在变分问题双重可变，因此许多空间先验可以在对偶空间的罚款。从该观点出发，我们可以建立一个基于STD框架，可以使DCNNs的输出以有许多特殊的先验诸如空间规律性，体积限制和星形先验。所提出的方法是一般的数学框架，它可以被应用到任何语义分割DCNNs。为了显示我们的方法的效率和准确性，我们将其运用到流行DeepLabV3 +图像分割网络，实验结果表明，该方法可以在数据驱动的图像分割DCNNs提高工作效率。</font>
</div>


<hr>
<div id="paper3"> <b>3. Unconstrained Periocular Recognition: Using Generative Deep Learning  Frameworks for Attribute Normalization</b>  <a href="https://arxiv.org/pdf/2002.03985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zanlorensi%2C+L+A" target="_blank" rel="noopener" style="color:#0000EE;">Luiz A. Zanlorensi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Proen%C3%A7a%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hugo Proença</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Menotti%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Menotti</a><br>
<font size="3">
Abstract: Ocular biometric systems working in unconstrained environments usually face the problem of small within-class compactness caused by the multiple factors that jointly degrade the quality of the obtained data. In this work, we propose an attribute normalization strategy based on deep learning generative frameworks, that reduces the variability of the samples used in pairwise comparisons, without reducing their discriminability. The proposed method can be seen as a preprocessing step that contributes for data regularization and improves the recognition accuracy, being fully agnostic to the recognition strategy used. As proof of concept, we consider the "eyeglasses" and "gaze" factors, comparing the levels of performance of five different recognition methods with/without using the proposed normalization strategy. Also, we introduce a new dataset for unconstrained periocular recognition, composed of images acquired by mobile devices, particularly suited to perceive the impact of "wearing eyeglasses" in recognition effectiveness. Our experiments were performed in two different datasets, and support the usefulness of our attribute normalization scheme to improve the recognition performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：眼在不受约束的环境中工作的生物识别系统通常面临所造成的多种因素共同降解所获得的数据的质量小的类内紧凑的问题。在这项工作中，我们提出了一种基于深度学习生成框架属性正常化的策略，即减少了两两比较用的样品的可变性，而不会降低他们的辨别力。所提出的方法可以被看作是一个预处理步骤，对于数据的正则化有助于，提高了识别精度，被完全不可知的使用的识别策略。作为概念验证，我们认为“眼镜”和“凝视”的因素，在不使用所提出的标准化战略比较与/五种不同的识别方法的性能水平。此外，我们介绍的无约束眼周识别一个新的数据集，由移动设备，特别适合于感知“戴眼镜”的在识别有效性的影响获取的图像所组成。我们的实验是在两个不同的数据集进行，并支持我们的属性正常化方案，以提高识别性能的实用性。</font>
</div>


<hr>
<div id="paper4"> <b>4. StickyPillars: Robust feature matching on point clouds using Graph  Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.03983" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Simon%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Simon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fischer%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Fischer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milz%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Milz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Witt%2C+C+T" target="_blank" rel="noopener" style="color:#0000EE;">Christian Tobias Witt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gross%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Horst-Michael Gross</a><br>
<font size="3">
Abstract: StickyPillars introduces a sparse feature matching method on point clouds. It is the first approach applying Graph Neural Networks on point clouds to stick points of interest. The feature estimation and assignment relies on the optimal transport problem, where the cost is based on the neural network itself. We utilize a Graph Neural Network for context aggregation with the aid of multihead self and cross attention. In contrast to image based feature matching methods, the architecture learns feature extraction in an end-to-end manner. Hence, the approach does not rely on handcrafted features. Our method outperforms state-of-the art matching algorithms, while providing real-time capability. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：StickyPillars介绍了点云稀疏特征匹配方法。它是将点云图的神经网络坚持的兴趣点的第一种方法。该功能估计和分配依赖于最佳的交通问题，其中成本是基于神经网络本身。我们利用图的神经网络模型多头自我和交叉关注的援助范围内聚集。与基于图像特征匹配方法，该架构获悉设有在端至端的方式提取。因此，该方法不依赖于手工制作的特点。我们的方法优于国家的本领域匹配算法，同时提供实时能力。</font>
</div>


<hr>
<div id="paper5"> <b>5. Joint Encoding of Appearance and Motion Features with Self-supervision  for First Person Action Recognition</b>  <a href="https://arxiv.org/pdf/2002.03982" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Planamente%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mirco Planamente</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bottino%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Bottino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caputo%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Barbara Caputo</a><br>
<font size="3">
Abstract: Wearable cameras are becoming more and more popular in several applications, increasing the interest of the research community in developing approaches for recognizing actions from a first-person point of view. An open challenge is how to cope with the limited amount of motion information available about the action itself, as opposed to the more investigated third-person action recognition scenario. When focusing on manipulation tasks, videos tend to record only parts of the movement, making crucial the understanding of the objects being manipulated and of their context. Previous works addressed this issue with two-stream architectures, one dedicated to modeling the appearance of objects involved in the action, another dedicated to extracting motion features from optical flow. In this paper, we argue that features from these two information channels should be learned jointly to capture the spatio-temporal correlations between the two in a better way. To this end, we propose a single stream architecture able to do so, thanks to the addition of a self-supervised block that uses a pretext motion segmentation task to intertwine motion and appearance knowledge. Experiments on several publicly available databases show the power of our approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可穿戴式摄像机正变得越来越流行在几个应用程序，增加了研究界在发展从一个第一人称的角度认识行动方案的兴趣。一个开放的挑战是如何应对提供了有关行动本身数量有限的运动信息，而不是更多的研究第三人称动作识别场景。当着眼于操作任务，视频往往只记录运动的部件，使得关键的被操纵的对象的理解和他们的背景。以前的作品中解决了这个问题有两个流架构下，一个专门用于模拟参与行动对象的外观，另一个专门用于提取运动从光流的特征。在本文中，我们认为，这两个信息渠道功能应共同学会了捕捉两者之间的时空相关性以更好的方式。为此，我们提出了一个单一的数据流架构能够这样做，由于增加使用的借口运动分割任务纠结运动和外观知识自我监督的块。几个公共数据库实验证明我们的方法的力量。</font>
</div>


<hr>
<div id="paper6"> <b>6. RePose: Learning Deep Kinematic Priors for Fast Human Pose Estimation</b>  <a href="https://arxiv.org/pdf/2002.03933" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Isack%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossam Isack</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haene%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Haene</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keskin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cem Keskin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bouaziz%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sofien Bouaziz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Boykov%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Boykov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Izadi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shahram Izadi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khamis%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sameh Khamis</a><br>
<font size="3">
Abstract: We propose a novel efficient and lightweight model for human pose estimation from a single image. Our model is designed to achieve competitive results at a fraction of the number of parameters and computational cost of various state-of-the-art methods. To this end, we explicitly incorporate part-based structural and geometric priors in a hierarchical prediction framework. At the coarsest resolution, and in a manner similar to classical part-based approaches, we leverage the kinematic structure of the human body to propagate convolutional feature updates between the keypoints or body parts. Unlike classical approaches, we adopt end-to-end training to learn this geometric prior through feature updates from data. We then propagate the feature representation at the coarsest resolution up the hierarchy to refine the predicted pose in a coarse-to-fine fashion. The final network effectively models the geometric prior and intuition within a lightweight deep neural network, yielding state-of-the-art results for a model of this size on two standard datasets, Leeds Sports Pose and MPII Human Pose. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们从一个单一的形象提出了人体姿势估计一种新型高效和轻质的模型。我们的模型设计在参数和各种先进设备，最先进的方法计算成本的一小部分，以实现竞争的结果。为此，我们明确地纳入一个分层的预测基于框架部分结构和几何先验。在粗糙的分辨率，并以类似经典的基于部分的方法的方式，我们利用人体的运动结构传播的关键点或身体部位之间的卷积功能更新。不同于传统的方法，我们采用终端到终端的培训，学习这种几何之前通过功能从数据更新。然后，我们传播的特征表示，在最粗分辨率高达层次细化预测姿态在粗到精的方式。最终的网络有效地模型轻质深层神经网络内的几何之前和直觉，产生国家的最先进的结果对于该尺寸的两种标准数据集的模型，利兹体育姿和MPII人体姿势。</font>
</div>


<hr>
<div id="paper7"> <b>7. 6DoF Object Pose Estimation via Differentiable Proxy Voting Loss</b>  <a href="https://arxiv.org/pdf/2002.03923" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheyu Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Koniusz%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Koniusz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongdong Li</a><br>
<font size="3">
Abstract: Estimating a 6DOF object pose from a single image is very challenging due to occlusions or textureless appearances. Vector-field based keypoint voting has demonstrated its effectiveness and superiority on tackling those issues. However, direct regression of vector-fields neglects that the distances between pixels and keypoints also affect the deviations of hypotheses dramatically. In other words, small errors in direction vectors may generate severely deviated hypotheses when pixels are far away from a keypoint. In this paper, we aim to reduce such errors by incorporating the distances between pixels and keypoints into our objective. To this end, we develop a simple yet effective differentiable proxy voting loss (DPVL) which mimics the hypothesis selection in the voting procedure. By exploiting our voting loss, we are able to train our network in an end-to-end manner. Experiments on widely used datasets, i.e. LINEMOD and Occlusion LINEMOD, manifest that our DPVL improves pose estimation performance significantly and speeds up the training convergence. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：从估计单个图像6自由度对象姿势非常由于遮挡或无纹理出场挑战。矢量场根据关键点投票已经证明对解决这些问题，它的有效性和优越性。然而，矢量场忽略的直接回归的像素和关键点之间的距离也影响假设的偏差显着。换言之，在方向矢量小误差可能产生严重偏离假设当像素远离关键点。在本文中，我们的目标是通过将像素和关键点之间的距离为我们的目标，以减少此类错误。为此，我们开发了一个简单而有效的微代理投票损失（DPVL），它模仿了投票过程中的假设选择。通过利用我们的投票损失，我们能够训练我们的网络中的终端到终端的方式。广泛使用的数据集的实验，即LINEMOD和闭塞LINEMOD，体现我们的DPVL显著改善姿势估计性能并加快训练收敛。</font>
</div>


<hr>
<div id="paper8"> <b>8. Hierarchical Multi-Process Fusion for Visual Place Recognition</b>  <a href="https://arxiv.org/pdf/2002.03895" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hausler%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Hausler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milford%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Milford</a><br>
<font size="3">
Abstract: Combining multiple complementary techniques together has long been regarded as a way to improve performance. In visual localization, multi-sensor fusion, multi-process fusion of a single sensing modality, and even combinations of different localization techniques have been shown to result in improved performance. However, merely fusing together different localization techniques does not account for the varying performance characteristics of different localization techniques. In this paper we present a novel, hierarchical localization system that explicitly benefits from three varying characteristics of localization techniques: the distribution of their localization hypotheses, their appearance- and viewpoint-invariant properties, and the resulting differences in where in an environment each system works well and fails. We show how two techniques deployed hierarchically work better than in parallel fusion, how combining two different techniques works better than two levels of a single technique, even when the single technique has superior individual performance, and develop two and three-tier hierarchical structures that progressively improve localization performance. Finally, we develop a stacked hierarchical framework where localization hypotheses from techniques with complementary characteristics are concatenated at each layer, significantly improving retention of the correct hypothesis through to the final localization stage. Using two challenging datasets, we show the proposed system outperforming state-of-the-art techniques. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结合使用多种互补技术的配合一直被认为是提高性能的一种方式。在视觉定位，多传感器融合，单个感测模态的多进程融合，和不同的定位技术，即使组合已显示导致改善的性能。然而，仅仅融合在一起不同的定位技术不考虑不同的定位技术不同的性能特点。在本文中，我们提出了一种新的分层定位系统，从定位技术3个变特征明确的好处：其本地化的假说，他们appearance-和观点不变性质的分配，并在一个环境中，其中产生的差异各系统的工作原理以及与失败。我们发现分级部署两种技术如何更好地工作比并行融合，如何结合两种不同的技术更好地工作比单一技术两个层面，即使在单一技术具有优异的个人表现和发展二，三梯队层次结构是渐进提高定位性能。最后，我们开发了一个层叠的分级框架，其中从具有互补特性的技术定位的假设，在每个层级联，通过对最终定位阶段显著提高正确假设的保持。使用两个挑战数据集，我们证明了该系统超越国家的最先进的技术。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-11</title>
    <url>/2020/02/11/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-11/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> End-to-End Multi-speaker Speech Recognition with Transformer <a href="https://arxiv.org/pdf/2002.03921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> A Probabilistic Formulation of Unsupervised Text Style Transfer <a href="https://arxiv.org/pdf/2002.03912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> A Study of Human Summaries of Scientific Articles <a href="https://arxiv.org/pdf/2002.03604" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> What Changed Your Mind: The Roles of Dynamic Topics and Discourse in  Argumentation Process <a href="https://arxiv.org/pdf/2002.03536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Multilingual Alignment of Contextual Word Representations <a href="https://arxiv.org/pdf/2002.03518" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Limits of Detecting Text Generated by Large-Scale Language Models <a href="https://arxiv.org/pdf/2002.03438" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Abstractive Summarization for Low Resource Data using Domain Transfer  and Data Synthesis <a href="https://arxiv.org/pdf/2002.03407" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Attend to the beginning: A study on using bidirectional attention for  extractive summarization <a href="https://arxiv.org/pdf/2002.03405" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Short Text Classification via Knowledge powered Attention with  Similarity Matrix based CNN <a href="https://arxiv.org/pdf/2002.03350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Rough Set based Aggregate Rank Measure &amp; its Application to Supervised  Multi Document Summarization <a href="https://arxiv.org/pdf/2002.03259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Mining Commonsense Facts from the Physical World <a href="https://arxiv.org/pdf/2002.03149" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> HHH: An Online Medical Chatbot System based on Knowledge Graph and  Hierarchical Bi-Directional Attention <a href="https://arxiv.org/pdf/2002.03140" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> LAVA NAT: A Non-Autoregressive Translation Model with Look-Around  Decoding and Vocabulary Attention <a href="https://arxiv.org/pdf/2002.03084" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Blank Language Models <a href="https://arxiv.org/pdf/2002.03079" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Description Based Text Classification with Reinforcement Learning <a href="https://arxiv.org/pdf/2002.03067" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> autoNLP: NLP Feature Recommendations for Text Analytics Applications <a href="https://arxiv.org/pdf/2002.03056" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Snippext: Semi-supervised Opinion Mining with Augmented Data <a href="https://arxiv.org/pdf/2002.03049" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Pre-training Tasks for Embedding-based Large-scale Retrieval <a href="https://arxiv.org/pdf/2002.03932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> A Novel Kuhnian Ontology for Epistemic Classification of STM Scholarly  Articles <a href="https://arxiv.org/pdf/2002.03531" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> SPA: Verbal Interactions between Agents and Avatars in Shared Virtual  Environments using Propositional Planning <a href="https://arxiv.org/pdf/2002.03246" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Time-aware Large Kernel Convolutions <a href="https://arxiv.org/pdf/2002.03184" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. End-to-End Multi-speaker Speech Recognition with Transformer</b>  <a href="https://arxiv.org/pdf/2002.03921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuankai Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangyou Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qian%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanmin Qian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roux%2C+J+L" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Le Roux</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Watanabe%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shinji Watanabe</a><br>
<font size="3">
Abstract: Recently, fully recurrent neural network (RNN) based end-to-end models have been proven to be effective for multi-speaker speech recognition in both the single-channel and multi-channel scenarios. In this work, we explore the use of Transformer models for these tasks by focusing on two aspects. First, we replace the RNN-based encoder-decoder in the speech recognition model with a Transformer architecture. Second, in order to use the Transformer in the masking network of the neural beamformer in the multi-channel case, we modify the self-attention component to be restricted to a segment rather than the whole sequence in order to reduce computation. Besides the model architecture improvements, we also incorporate an external dereverberation preprocessing, the weighted prediction error (WPE), enabling our model to handle reverberated signals. Experiments on the spatialized wsj1-2mix corpus show that the Transformer-based models achieve 40.9% and 25.6% relative WER reduction, down to 12.1% and 6.4% WER, under the anechoic condition in single-channel and multi-channel tasks, respectively, while in the reverberant case, our methods achieve 41.5% and 13.8% relative WER reduction, down to 16.5% and 15.2% WER. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，完全回归神经网络（RNN）的端至高端机型已被证明是有效的在单通道和多通道两种情况下多说话者声音识别。在这项工作中，我们侧重于两个方面探讨使用Transformer模型为这些任务的。首先，我们更换了变压器架构的语音识别模型基于RNN编码器，解码器。其次，为了使用Transformer在多通道情况下，神经波束形成器的屏蔽网络中，我们修改了自注意成分被限制在一个段，而不是整个序列，以减少计算量。除了模型体系结构的改进，我们还包含一个外部去混响预处理，加权预测误差（WPE），使我们的模型来处理混响信号。在空间化wsj1-2mix语料库表明，基于变压器的模型达到40.9％和25.6％的相对减少WER，下降到12.1％和6.4％WER，在单通道和多通道任务的消声条件下，分别的实验，而在混响情况下，我们的方法达到41.5％和13.8％的相对减少WER，下降到16.5％和15.2％WER。</font>
</div>


<hr>
<div id="paper2"> <b>2. A Probabilistic Formulation of Unsupervised Text Style Transfer</b>  <a href="https://arxiv.org/pdf/2002.03912" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Junxian He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neubig%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Graham Neubig</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taylor Berg-Kirkpatrick</a><br>
<font size="3">
Abstract: We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了统一了先前提出的非生殖技术监督的文本样式转移了深刻的生成模型。从两个结构域为部分观察到平行语料库我们的概率方法模型非并行数据。通过假设到生成每个观察到的序列的并行潜序列，我们的模型学习从一个域变换序列到另一个在完全无监督方式。与传统的生成序列的模型（例如，HMM）相比之下，我们的模型使得它生成数据一些假设：它采用的是复发性语言模型作为先验和编码器 - 解码器作为转导分布。虽然边际数据可能性的计算是在这个模型类棘手，我们表明，摊销变推理承认一个现实的替代。此外，通过我们的目标变和其他最近的无监督式的转移和机器翻译技术之间绘制连接，我们将展示我们的概率观点如何能够统一一些已知的非生成目标，如回译和对抗性的损失。最后，我们证明我们的方法对大范围的无监督式的传输任务，包括情绪转移，转让手续，文字解读，作者模仿，以及相关的语言翻译的有效性。在所有风格的传输任务，我们的做法得到了国家的最先进的非生成基线大有斩获，其中包括国家的最先进的无监督的机器翻译技术，我们的方法推广。此外，我们在标准无监督的机器翻译任务进行实验，发现我们统一的方法当前国家的最先进的匹配。</font>
</div>


<hr>
<div id="paper3"> <b>3. A Study of Human Summaries of Scientific Articles</b>  <a href="https://arxiv.org/pdf/2002.03604" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Boni%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Odellia Boni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feigenblat%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guy Feigenblat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cohen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Doron Cohen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roitman%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haggai Roitman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Konopnicki%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Konopnicki</a><br>
<font size="3">
Abstract: Researchers and students face an explosion of newly published papers which may be relevant to their work. This led to a trend of sharing human summaries of scientific papers. We analyze the summaries shared in one of these platforms this http URL. The goal is to characterize human summaries of scientific papers, and use some of the insights obtained to improve and adapt existing automatic summarization systems to the domain of scientific papers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：研究人员和学生面临的新发表的论文可能是与其工作相关的爆炸。这导致了共享的科学论文人类总结的趋势。我们分析在这些平台上的这个HTTP URL一个共享的摘要。我们的目标是表征的科学论文人类汇总，并使用一些得到改善和现有的自动摘要系统适应的科学论文域的见解。</font>
</div>


<hr>
<div id="paper4"> <b>4. What Changed Your Mind: The Roles of Dynamic Topics and Discourse in  Argumentation Process</b>  <a href="https://arxiv.org/pdf/2002.03536" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jichuan Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yulan He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cuiyun Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+M+R" target="_blank" rel="noopener" style="color:#0000EE;">Michael R. Lyu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=King%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Irwin King</a><br>
<font size="3">
Abstract: In our world with full of uncertainty, debates and argumentation contribute to the progress of science and society. Despite of the increasing attention to characterize human arguments, most progress made so far focus on the debate outcome, largely ignoring the dynamic patterns in argumentation processes. This paper presents a study that automatically analyzes the key factors in argument persuasiveness, beyond simply predicting who will persuade whom. Specifically, we propose a novel neural model that is able to dynamically track the changes of latent topics and discourse in argumentative conversations, allowing the investigation of their roles in influencing the outcomes of persuasion. Extensive experiments have been conducted on argumentative conversations on both social media and supreme court. The results show that our model outperforms state-of-the-art models in identifying persuasive arguments via explicitly exploring dynamic factors of topic and discourse. We further analyze the effects of topics and discourse on persuasiveness, and find that they are both useful - topics provide concrete evidence while superior discourse styles may bias participants, especially in social media arguments. In addition, we draw some findings from our empirical results, which will help people better engage in future persuasive conversations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在我们与充满不确定性，辩论和论证的世界做出贡献的科学和社会的进步。尽管日益关注人类的特征参数，大部分取得的进展至今专注于辩论结果如何，在很大程度上忽视了在论证过程中的动态模式。本文提出了一种研究一种能够自动分析的关键因素，论证的说服力，超越了简单的预测谁将会说服谁。具体来说，我们提出了一种新的神经模型，该模型能够动态跟踪的潜在主题和议论交谈变化的话语，让自己的角色的影响说服效果的调查。大量的实验已经在这两个社交媒体和最高法院议论对话进行。结果表明，我们的模型优于国家的最先进的车型在通过主题和话语的明确探索动态因素识别有说服力的论据。我们进一步分析主题和话语的影响说服力，并且发现它们都是有用的 - 主题提供了确凿的证据，而优越的话语风格可偏向的参与者，尤其是在社交媒体上的参数。此外，我们从实证结果，这将帮助人们更好地参与未来有说服力的交谈得出一些结论。</font>
</div>


<hr>
<div id="paper5"> <b>5. Multilingual Alignment of Contextual Word Representations</b>  <a href="https://arxiv.org/pdf/2002.03518" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Steven Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kitaev%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikita Kitaev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Klein%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dan Klein</a><br>
<font size="3">
Abstract: We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs written in different scripts, that are corrected by the alignment procedure. These results support contextual alignment as a useful concept for understanding large multilingual pre-trained models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了评估和加强情境嵌入定位，并表明他们是在分析和改善多语种BERT有用的程序。特别是，我们提出的调整过程之后，BERT展品显著上XNLI相比基本模型提高零射门的表现，非常匹配伪充分监督翻译火车模型，保加利亚和希腊。此外，测量校准的程度，我们介绍检索词的上下文版本，并表明它与下游的零次转让很好的相关性。使用这个检索词的任务，我们也分析BERT，发现它具有系统性缺陷，例如对于用不同的脚本开放类零件的词性和词的对，由校准程序纠正糟糕对齐。这些结果支持上下文定位为了解大型多语种预训练模型一个有用的概念。</font>
</div>


<hr>
<div id="paper6"> <b>6. Limits of Detecting Text Generated by Large-Scale Language Models</b>  <a href="https://arxiv.org/pdf/2002.03438" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Varshney%2C+L+R" target="_blank" rel="noopener" style="color:#0000EE;">Lav R. Varshney</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Keskar%2C+N+S" target="_blank" rel="noopener" style="color:#0000EE;">Nitish Shirish Keskar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Socher%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Socher</a><br>
<font size="3">
Abstract: Some consider large-scale language models that can generate long and coherent pieces of text as dangerous, since they may be used in misinformation campaigns. Here we formulate large-scale language model output detection as a hypothesis testing problem to classify text as genuine or generated. We show that error exponents for particular language models are bounded in terms of their perplexity, a standard measure of language generation performance. Under the assumption that human language is stationary and ergodic, the formulation is extended from considering specific language models to considering maximum likelihood language models, among the class of k-order Markov approximations; error probabilities are characterized. Some discussion of incorporating semantic side information is also given. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：有些人认为大型语言模型，可以产生长期而连贯的作品文本的危险，因为它们可能在误导广告系列。在这里，我们制定的大型语言模型输出检测为假设检验问题进行分类文本作为真正的或产生的。我们表明，特定的语言模型误差的指数在他们困惑的语言生成的性能衡量标准方面是有界的。在假设人类语言是固定的，并且遍历，该制剂是从考虑特定的语言模型来考虑最大似然语言模型中，类k阶马尔可夫近似值之间延伸;错误概率表征。结合语义方面信息的一些讨论也给出。</font>
</div>


<hr>
<div id="paper7"> <b>7. Abstractive Summarization for Low Resource Data using Domain Transfer  and Data Synthesis</b>  <a href="https://arxiv.org/pdf/2002.03407" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Magooda%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmed Magooda</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Litman%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diane Litman</a><br>
<font size="3">
Abstract: Training abstractive summarization models typically requires large amounts of data, which can be a limitation for many domains. In this paper we explore using domain transfer and data synthesis to improve the performance of recent abstractive summarization methods when applied to small corpora of student reflections. First, we explored whether tuning state of the art model trained on newspaper data could boost performance on student reflection data. Evaluations demonstrated that summaries produced by the tuned model achieved higher ROUGE scores compared to model trained on just student reflection data or just newspaper data. The tuned model also achieved higher scores compared to extractive summarization baselines, and additionally was judged to produce more coherent and readable summaries in human evaluations. Second, we explored whether synthesizing summaries of student data could additionally boost performance. We proposed a template-based model to synthesize new data, which when incorporated into training further increased ROUGE scores. Finally, we showed that combining data synthesis with domain transfer achieved higher ROUGE scores compared to only using one of the two approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：培训抽象概括模型通常需要大量的数据，这可能是许多领域的限制。在本文中，我们探讨使用域传输和数据合成在应用于学生思考的小语料库提高近期抽象总结方法的性能。首先，我们探讨的培训在报纸上的数据的艺术模型的第二调谐状态是否能提高学生反映数据的表现。评估表明，通过调谐模型产生摘要相比，模型中训练的只是学生的反射数据，或只报数据来实现更高的分数ROUGE。调谐模型相比也萃取汇总基线实现较高的分数，并且还判定为产生更为一致的和人类可读的评价汇总。其次，我们探讨是否合成学生数据的汇总可以额外提高性能。我们提出了一个基于模板的模型来合成新的数据，这些数据在纳入培训进一步提高ROUGE得分。最后，我们显示，与域转移组合数据合成相比仅使用两种方法之一来实现更高ROUGE分数。</font>
</div>


<hr>
<div id="paper8"> <b>8. Attend to the beginning: A study on using bidirectional attention for  extractive summarization</b>  <a href="https://arxiv.org/pdf/2002.03405" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Magooda%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ahmed Magooda</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marcjan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cezary Marcjan</a><br>
<font size="3">
Abstract: Forum discussion data differ in both structure and properties from generic form of textual data such as news. Henceforth, summarization techniques should, in turn, make use of such differences, and craft models that can benefit from the structural nature of discussion data. In this work, we propose attending to the beginning of a document, to improve the performance of extractive summarization models when applied to forum discussion data. Evaluations demonstrated that with the help of bidirectional attention mechanism, attending to the beginning of a document (initial comment/post) in a discussion thread, can introduce a consistent boost in ROUGE scores, as well as introducing a new State Of The Art (SOTA) ROUGE scores on the forum discussions dataset. Additionally, we explored whether this hypothesis is extendable to other generic forms of textual data. We make use of the tendency of introducing important information early in the text, by attending to the first few sentences in generic textual data. Evaluations demonstrated that attending to introductory sentences using bidirectional attention, improves the performance of extractive summarization models when even applied to more generic form of textual data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：论坛讨论数据在结构和性能的文本数据的一般形式不同，如新闻。今后，概括技术应该反过来，利用这种差异，工艺模型，可以从讨论数据的结构性质中受益。在这项工作中，我们建议参加到文档的开头，当应用到论坛讨论数据，以提高采掘总结模型的性能。评估表明，随着双向注意机制的帮助下，参加到讨论线索文件（初始评论/后）的开始，也会引入ROUGE分数一致的提振，以及引入一个新的国家的艺术（SOTA在论坛上讨论的数据集）ROUGE得分。此外，我们探讨这个假设是否扩展到文本数据的其他一般形式。我们利用文本早期引进的重要信息，通过参加在通用文本数据的前几句的倾向。评估表明，使用双向注意参加到介绍性的句子，提高采掘总结机型的表现时，甚至应用于文本数据的更通用的形式。</font>
</div>


<hr>
<div id="paper9"> <b>9. Short Text Classification via Knowledge powered Attention with  Similarity Matrix based CNN</b>  <a href="https://arxiv.org/pdf/2002.03350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingchen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Clinton%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabtone.Clinton</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yijia Miao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Feng Gao</a><br>
<font size="3">
Abstract: Short text is becoming more and more popular on the web, such as Chat Message, SMS and Product Reviews. Accurately classifying short text is an important and challenging task. A number of studies have difficulties in addressing this problem because of the word ambiguity and data sparsity. To address this issue, we propose a knowledge powered attention with similarity matrix based convolutional neural network (KASM) model, which can compute comprehensive information by utilizing the knowledge and deep neural network. We use knowledge graph (KG) to enrich the semantic representation of short text, specially, the information of parent-entity is introduced in our model. Meanwhile, we consider the word interaction in the literal-level between short text and the representation of label, and utilize similarity matrix based convolutional neural network (CNN) to extract it. For the purpose of measuring the importance of knowledge, we introduce the attention mechanisms to choose the important information. Experimental results on five standard datasets show that our model significantly outperforms state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：短文本正在变得越来越流行网络，比如聊天信息，短信和产品评论的。准确分类短文本是一项重要而艰巨的任务。许多研究都在解决，因为这个词的模糊性和数据稀疏的这个问题的困难。为了解决这个问题，我们提出了基于相似矩阵卷积神经网络（KASM）模型，它可以利用的知识和深层神经网络计算的综合信息知识供电关注。我们用知识图（KG）充实简短的文字，特别是，母公司的实体的信息在我们的模型引入的语义表示。同时，我们认为短文本和标签的表示之间的文字级别的字互动，并利用相似矩阵基于卷积神经网络（CNN）将其解压。用来衡量知识的重要性的目的，我们引入注意机制选择的重要信息。五个标准数据集实验结果表明，我们的模型显著优于国家的最先进的方法。</font>
</div>


<hr>
<div id="paper10"> <b>10. Rough Set based Aggregate Rank Measure &amp; its Application to Supervised  Multi Document Summarization</b>  <a href="https://arxiv.org/pdf/2002.03259" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yadav%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nidhika Yadav</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chatterjee%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Niladri Chatterjee</a><br>
<font size="3">
Abstract: Most problems in Machine Learning cater to classification and the objects of universe are classified to a relevant class. Ranking of classified objects of universe per decision class is a challenging problem. We in this paper propose a novel Rough Set based membership called Rank Measure to solve to this problem. It shall be utilized for ranking the elements to a particular class. It differs from Pawlak Rough Set based membership function which gives an equivalent characterization of the Rough Set based approximations. It becomes paramount to look beyond the traditional approach of computing memberships while handling inconsistent, erroneous and missing data that is typically present in real world problems. This led us to propose the aggregate Rank Measure. The contribution of the paper is three fold. Firstly, it proposes a Rough Set based measure to be utilized for numerical characterization of within class ranking of objects. Secondly, it proposes and establish the properties of Rank Measure and aggregate Rank Measure based membership. Thirdly, we apply the concept of membership and aggregate ranking to the problem of supervised Multi Document Summarization wherein first the important class of sentences are determined using various supervised learning techniques and are post processed using the proposed ranking measure. The results proved to have significant improvement in accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在机器学习的大多数问题迎合分类和宇宙的对象分类的相关类别。每个决策类宇宙的分类对象的排名是一个具有挑战性的问题。我们在本文中提出了所谓的排名衡量一个新的基于粗糙集的成员来解决这个问题。它应被用于排序的元素到一个特定的类。它不同于帕夫拉克基于粗糙集的隶属度函数这给基于粗糙集近似的等价刻画。它成为极为重要的超越计算成员在处理不一致的，错误的，缺少通常存在于现实世界的问题数据的传统方式。这使我们提出的总排名措施。本文的贡献是三倍。首先，提出了将要用于的内类对象的排名数值表征粗集基于度量。其次，提出并建立等级测量和总浏览量措施的会员的属性。第三，我们申请会员资格的概念和总排名，其中第一使用各种监督学习技术确定句子的重要的一类，并利用所提出的衡量排名的后处理监督多文档文摘的问题。结果证明，在精度显著的改善。</font>
</div>


<hr>
<div id="paper11"> <b>11. Mining Commonsense Facts from the Physical World</b>  <a href="https://arxiv.org/pdf/2002.03149" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zou%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanyan Zou</a><br>
<font size="3">
Abstract: Textual descriptions of the physical world implicitly mention commonsense facts, while the commonsense knowledge bases explicitly represent such facts as triples. Compared to dramatically increased text data, the coverage of existing knowledge bases is far away from completion. Most of the prior studies on populating knowledge bases mainly focus on Freebase. To automatically complete commonsense knowledge bases to improve their coverage is under-explored. In this paper, we propose a new task of mining commonsense facts from the raw text that describes the physical world. We build an effective new model that fuses information from both sequence text and existing knowledge base resource. Then we create two large annotated datasets each with approximate 200k instances for commonsense knowledge base completion. Empirical results demonstrate that our model significantly outperforms baselines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对物理世界的文本描述隐含提到常识的事实，而常识性的知识基础明确表示这样的事实三倍。相比大幅增加文本数据，现有的知识基础的覆盖面是远离完成。对大多数填充知识库事先研究主要集中在游离碱自动完成常识性的知识基础，提高其覆盖面是充分开发。在本文中，我们提出了从描述物理世界的原始文本挖掘常识性事实的新任务。我们构建一个融合了来自两个序列的文本和已有的知识基础资源信息的有效新模式。然后，我们创建每两个大型注释的数据集与常识的知识基础完成近似200K实例。实证结果表明，我们的模型显著优于基准。</font>
</div>


<hr>
<div id="paper12"> <b>12. HHH: An Online Medical Chatbot System based on Knowledge Graph and  Hierarchical Bi-Directional Attention</b>  <a href="https://arxiv.org/pdf/2002.03140" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiming Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Ni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiamou Liu</a><br>
<font size="3">
Abstract: This paper proposes a chatbot framework that adopts a hybrid model which consists of a knowledge graph and a text similarity model. Based on this chatbot framework, we build HHH, an online question-and-answer (QA) Healthcare Helper system for answering complex medical questions. HHH maintains a knowledge graph constructed from medical data collected from the Internet. HHH also implements a novel text representation and similarity deep learning model, Hierarchical BiLSTM Attention Model (HBAM), to find the most similar question from a large QA dataset. We compare HBAM with other state-of-the-art language models such as bidirectional encoder representation from transformers (BERT) and Manhattan LSTM Model (MaLSTM). We train and test the models with a subset of the Quora duplicate questions dataset in the medical area. The experimental results show that our model is able to achieve a superior performance than these existing methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种聊天机器人框架，采用它由一个知识图形和文本相似模型的混合模式。在此基础上聊天机器人框架，我们建立HHH，在线提问和回答（QA）医疗辅助系统是回答复杂的医学问题。 HHH保持从网上收集的医疗数据构建一个知识图谱。 HHH还实现了一个新的文本表示和相似性深度学习模型，分层BiLSTM注意力模型（HBAM），发现从大的QA数据集的最类似的问题。我们比较HBAM与国家的最先进的其他语言模型如变压器双向编码表示（BERT）和曼哈顿LSTM模型（MaLSTM）。我们培养和使用的Quora的重复问题的一个子集测试模型在医疗领域的数据集。实验结果表明，我们的模型能够实现比现有的这些方法优越的性能。</font>
</div>


<hr>
<div id="paper13"> <b>13. LAVA NAT: A Non-Autoregressive Translation Model with Look-Around  Decoding and Vocabulary Attention</b>  <a href="https://arxiv.org/pdf/2002.03084" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoya Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuxian Meng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arianna Yuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwei Li</a><br>
<font size="3">
Abstract: Non-autoregressive translation (NAT) models generate multiple tokens in one forward pass and is highly efficient at inference stage compared with autoregressive translation (AT) methods. However, NAT models often suffer from the multimodality problem, i.e., generating duplicated tokens or missing tokens. In this paper, we propose two novel methods to address this issue, the Look-Around (LA) strategy and the Vocabulary Attention (VA) mechanism. The Look-Around strategy predicts the neighbor tokens in order to predict the current token, and the Vocabulary Attention models long-term token dependencies inside the decoder by attending the whole vocabulary for each position to acquire knowledge of which token is about to generate. %We also propose a dynamic bidirectional decoding approach to accelerate the inference process of the LAVA model while preserving the high-quality of the generated output. Our proposed model uses significantly less time during inference compared with autoregressive models and most other NAT models. Our experiments on four benchmarks (WMT14 En$\rightarrow$De, WMT14 De$\rightarrow$En, WMT16 Ro$\rightarrow$En and IWSLT14 De$\rightarrow$En) show that the proposed model achieves competitive performance compared with the state-of-the-art non-autoregressive and autoregressive models while significantly reducing the time cost in inference phase. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：非自回归转换（NAT）模型生成一个直传多个令牌，并在推论阶段高效自回归转换（AT）方法相比。然而，NAT模式经常遭受来自多模式问题，即，产生重复的令牌或丢失令牌。在本文中，我们提出了两种新的方法来解决这个问题，环视（LA）策略和词汇注意（VA）的机制。环视战略，每个位置上的所有词汇主治地获取知识，其中令牌即将产生预测，以预测当前令牌邻居令牌，解码器内部的词汇注意模型的长期令牌的依赖。 ％我们也提出了一个动态的双向解码方式，加快LAVA模型的推理过程，同时保留生成的输出的高品质。我们提出的模型采用与自回归模型和其他大多数NAT车型相比推理过程中显著的时间更少。我们的四个基准试验（WMT14恩$ \ RIGHTARROW $德，WMT14德$ \ RIGHTARROW $恩，WMT16滚装$ \ RIGHTARROW $恩和IWSLT14德$ \ RIGHTARROW $恩）显示，随着国家相比，该模型实现了有竞争力的性能-of最先进的非自回归和自回归模型，同时显著降低推断阶段的时间成本。</font>
</div>


<hr>
<div id="paper14"> <b>14. Blank Language Models</b>  <a href="https://arxiv.org/pdf/2002.03079" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianxiao Shen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Quach%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Victor Quach</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Barzilay%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Regina Barzilay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jaakkola%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tommi Jaakkola</a><br>
<font size="3">
Abstract: We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. Unlike previous masked language models or the Insertion Transformer, BLM uses blanks to control which part of the sequence to expand. This fine-grained control of generation is ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood, and achieves perplexity comparable to traditional left-to-right language models on the Penn Treebank and WikiText datasets. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出空白语言模型（BLM），通过动态地创建和填补空白生成序列模型。不同于以往的蒙面语言模型或插入变压器，BLM使用空格来控制流程的一部分，扩大它。这一代的细粒度控制是适用于各种文本编辑和重写任务。该模型可以从一个单一的空白开始或部分完成具有在指定位置空白文本。它反复确定哪些词来代替一个空白，是否插入新的空白，而当没有空格都留给填充停止发电。 BLM可以使用下界边际数据的可能性被有效的培训，并达到相当的困惑对宾州树库和数据集wikitext的传统左到右的语言模型。在填充缺失的文本片段的任务，BLM显著优于在准确性和流畅性方面的所有其他基线。款式转移和破坏古文字复原实验证明该框架为广泛的应用潜力。</font>
</div>


<hr>
<div id="paper15"> <b>15. Description Based Text Classification with Reinforcement Learning</b>  <a href="https://arxiv.org/pdf/2002.03067" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duo Chai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwei Li</a><br>
<font size="3">
Abstract: The task of text classification is usually divided into two stages: {\it text feature extraction} and {\it classification}. In this standard formalization categories are merely represented as indexes in the label vocabulary, and the model lacks for explicit instructions on what to classify. Inspired by the current trend of formalizing NLP problems as question answering tasks, we propose a new framework for text classification, in which each category label is associated with a category description. Descriptions are generated by hand-crafted templates or using abstractive/extractive models from reinforcement learning. The concatenation of the description and the text is fed to the classifier to decide whether or not the current label should be assigned to the text. The proposed strategy forces the model to attend to the most salient texts with respect to the label, which can be regarded as a hard version of attention, leading to better performances. We observe significant performance boosts over strong baselines on a wide range of text classification tasks including single-label classification, multi-label classification and multi-aspect sentiment analysis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文本分类的任务通常分为两个阶段：{\它的文本特征提取}和{\它分类}。在这个标准形式化类别只是表示为标签的词汇索引，该模型缺少什么就分类明确的指示。通过正式NLP问题答疑任务的当前趋势的启发，我们提出了文本分类的新框架，其中每个类别标签与类别描述相关联。说明由手工制作的模板或使用抽象/采掘车型从强化学习产生。描述和文字的级联被送到分类，以决定当前标签是否应该被分配到的文本。拟议的战略力量模型出席中最突出的文字相对于标签，这可以看作是人们关注的硬的版本，从而获得更好的性能。我们观察到了一个大范围的文本分类的任务，包括单标签分类，多标签分类和多方位的情感分析强基线显著的性能提升。</font>
</div>


<hr>
<div id="paper16"> <b>16. autoNLP: NLP Feature Recommendations for Text Analytics Applications</b>  <a href="https://arxiv.org/pdf/2002.03056" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Misra%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Janardan Misra</a><br>
<font size="3">
Abstract: While designing machine learning based text analytics applications, often, NLP data scientists manually determine which NLP features to use based upon their knowledge and experience with related problems. This results in increased efforts during feature engineering process and renders automated reuse of features across semantically related applications inherently difficult. In this paper, we argue for standardization in feature specification by outlining structure of a language for specifying NLP features and present an approach for their reuse across applications to increase likelihood of identifying optimal features. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在设计基于机器学习的文本分析应用中，常，NLP数据科学家手动确定NLP功能，才能使用根据其与相关问题的知识和经验。在功能设计过程，这导致加大工作力度和渲染自动化的跨越语义相关的应用程序本身就难以功能重用。在本文中，我们通过概述用于指定NLP特征的语言的结构主张在特征规格标准化和呈现的方法用于其再利用跨应用程序以增加识别最佳特征的可能性。</font>
</div>


<hr>
<div id="paper17"> <b>17. Snippext: Semi-supervised Opinion Mining with Augmented Data</b>  <a href="https://arxiv.org/pdf/2002.03049" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Miao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengjie Miao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuliang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wang-Chiew Tan</a><br>
<font size="3">
Abstract: Online services are interested in solutions to opinion mining, which is the problem of extracting aspects, opinions, and sentiments from text. One method to mine opinions is to leverage the recent success of pre-trained language models which can be fine-tuned to obtain high-quality extractions from reviews. However, fine-tuning language models still requires a non-trivial amount of training data. In this paper, we study the problem of how to significantly reduce the amount of labeled training data required in fine-tuning language models for opinion mining. We describe Snippext, an opinion mining system developed over a language model that is fine-tuned through semi-supervised learning with augmented data. A novelty of Snippext is its clever use of a two-prong approach to achieve state-of-the-art (SOTA) performance with little labeled training data through: (1) data augmentation to automatically generate more labeled training data from existing ones, and (2) a semi-supervised learning technique to leverage the massive amount of unlabeled data in addition to the (limited amount of) labeled data. We show with extensive experiments that Snippext performs comparably and can even exceed previous SOTA results on several opinion mining tasks with only half the training data required. Furthermore, it achieves new SOTA results when all training data are leveraged. By comparison to a baseline pipeline, we found that Snippext extracts significantly more fine-grained opinions which enable new opportunities of downstream applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在线服务，有兴趣的解决方案，意见挖掘，这是从文本中提取方面，意见和情绪的问题。一种方法矿井的意见是利用近期的预先训练语言模型，可以进行微调，以从审查获得高品质的提取成功。但是，微调语言模型仍然需要训练数据的不平凡的量。在本文中，我们研究如何显著减少微调语言模型所需的意见挖掘标记的训练数据量的问题。我们描述Snippext，发展了语言模型的意见挖掘系统进行微调，通过半监督学习与增强的数据。 Snippext的一个新的特点是其巧妙利用一个双叉方式通过与小标记的训练数据实现状态的最先进的（SOTA）性能：（1）数据的增强自动生成从现有的多个标记的训练数据，和（2）半监督学习技术来利用除了标记的数据（的限制量）的未标记数据的巨量。我们发现有大量的实验证明，Snippext执行同等而且甚至超过几个意见挖掘任务以前SOTA结果只需要训练数据的一半。此外，实现了新的SOTA结果时，所有的训练数据利用。通过比较基线管道，我们发现，Snippext提取显著更细粒度的意见这使下游应用新的机遇。</font>
</div>


<hr>
<div id="paper18"> <b>18. Pre-training Tasks for Embedding-based Large-scale Retrieval</b>  <a href="https://arxiv.org/pdf/2002.03932" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei-Cheng Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+F+X" target="_blank" rel="noopener" style="color:#0000EE;">Felix X. Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yin-Wen Chang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiming Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sanjiv Kumar</a><br>
<font size="3">
Abstract: We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well studied. Most previous works rely on classic Information Retrieval (IR) methods such as BM-25 (token matching + TF-IDF weights). These models only accept sparse handcrafted features and can not be optimized for different downstream tasks of interest. In this paper, we conduct a comprehensive study on the embedding-based retrieval models. We show that the key ingredient of learning a strong embedding-based Transformer model is the set of pre-training tasks. With adequately designed paragraph-level pre-training tasks, the Transformer models can remarkably improve over the widely-used BM-25 as well as embedding models without Transformers. The paragraph-level pre-training tasks we studied are Inverse Cloze Task (ICT), Body First Selection (BFS), Wiki Link Prediction (WLP), and the combination of all three. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们认为大规模的查询，文献检索问题：给定一个查询（例如，一个问题），返回一组相关文件（例如，含有答案的段落）从一个大的文档语料库。这个问题往往解决了两个步骤。检索阶段一来降低解空间，返回候选文档的子集。该评价阶段再重新排名文档。重要的是，检索算法不仅欲火高涨的召回，也要求必须高效，及时次线性回归考生文件的数量。不同于评价阶段最近由于跨关注车型BERT式前培训任务目睹显著的进步，检索阶段仍不很好的研究。大多数以前的作品依靠传统的信息检索（IR）的方法，如BM-25（令牌匹配+ TF-IDF权重）。这些模型只接受稀疏手工制作的特点和利益不同的下游任务不能被优化。在本文中，我们进行的基于嵌入的检索模型的综合研究。我们证明了浓厚的学习基于嵌入变压器模型的关键成分是一组前培训任务。随着设计恰当段落级前的训练任务时，Transformer模型可以显着提高与广泛使用的BM-25以及嵌入模型没有变压器。我们研究的段落级前的训练任务是反完形填空任务（ICT），身体的第一选择（BFS），维基连结预测（WLP），三个人的组合。</font>
</div>


<hr>
<div id="paper19"> <b>19. A Novel Kuhnian Ontology for Epistemic Classification of STM Scholarly  Articles</b>  <a href="https://arxiv.org/pdf/2002.03531" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Saqr%2C+K+M" target="_blank" rel="noopener" style="color:#0000EE;">Khalid M. Saqr</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Elsharawy%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abdelrahman Elsharawy</a><br>
<font size="3">
Abstract: Thomas Kuhn proposed his paradigmatic view of scientific discovery five decades ago. The concept of paradigm has not only explained the progress of science, but has also become the central epistemic concept among STM scientists. Here, we adopt the principles of Kuhnian philosophy to construct a novel ontology aims at classifying and evaluating the impact of STM scholarly articles. First, we explain how the Kuhnian cycle of science describes research at different epistemic stages. Second, we show how the Kuhnian cycle could be reconstructed into modular ontologies which classify scholarly articles according to their contribution to paradigm-centred knowledge. The proposed ontology and its scenarios are discussed. To the best of the authors knowledge, this is the first attempt for creating an ontology for describing scholarly articles based on the Kuhnian paradigmatic view of science. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：托马斯·库恩建议他的科学发现的范式观五十年前。范式的概念不仅解释科学的进步，而且已经成为STM科学家中央认知概念。在这里，我们采用库恩的哲学原理在分类和评估STM学术文章的影响，构建了一个新的本体的目的。首先，我们解释科学的库恩的周期是如何描述在不同认知阶段的研究。其次，我们展示了库恩的周期怎么可能被重建成分类根据自己的范式为中心的知识贡献学术文章模块化本体。所提出的本体及其情景进行了讨论。为了最好的作者的知识，这是一个用于创建用于描述基于科学的范式库恩视学术文章本体的第一次尝试。</font>
</div>


<hr>
<div id="paper20"> <b>20. SPA: Verbal Interactions between Agents and Avatars in Shared Virtual  Environments using Propositional Planning</b>  <a href="https://arxiv.org/pdf/2002.03246" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Best%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Best</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Narang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sahil Narang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manocha%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dinesh Manocha</a><br>
<font size="3">
Abstract: We present a novel approach for generating plausible verbal interactions between virtual human-like agents and user avatars in shared virtual environments. Sense-Plan-Ask, or SPA, extends prior work in propositional planning and natural language processing to enable agents to plan with uncertain information, and leverage question and answer dialogue with other agents and avatars to obtain the needed information and complete their goals. The agents are additionally able to respond to questions from the avatars and other agents using natural-language enabling real-time multi-agent multi-avatar communication environments. Our algorithm can simulate tens of virtual agents at interactive rates interacting, moving, communicating, planning, and replanning. We find that our algorithm creates a small runtime cost and enables agents to complete their goals more effectively than agents without the ability to leverage natural-language communication. We demonstrate quantitative results on a set of simulated benchmarks and detail the results of a preliminary user-study conducted to evaluate the plausibility of the virtual interactions generated by SPA. Overall, we find that participants prefer SPA to prior techniques in 84\% of responses including significant benefits in terms of the plausibility of natural-language interactions and the positive impact of those interactions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出一个新的方法，用于产生之间合理的口头交互虚拟人样在共享虚拟环境代理和用户化身。感-计划-ASK，或SPA，扩展了命题规划和自然语言处理以前的工作，使代理商计划，不确定信息，并利用问题，并与其他代理和化身答案对话，以获得所需的信息，并完成自己的目标。这些代理还能够使用自然语言实现实时多Agent多具象通信环境从化身的问题和其他代理人回应。我们的算法可以在互动率模拟几十虚拟代理交互，移动，通信，规划，重新规划和。我们发现，我们的算法创建一个小的运行成本，使代理人没有充分利用自然语言交流的能力比药物更有效地完成自己的目标。我们展示了一套模拟基准和细节进行评估通过SPA生成的虚拟互动的合理性进行初步的用户研究结果的定量结果。总体而言，我们发现，参与者更喜欢SPA现有技术中的反应，包括在自然语言交互的真实性和这些交互的积极影响方面显著收益84 \％。</font>
</div>


<hr>
<div id="paper21"> <b>21. Time-aware Large Kernel Convolutions</b>  <a href="https://arxiv.org/pdf/2002.03184" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lioutas%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasileios Lioutas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhong Guo</a><br>
<font size="3">
Abstract: To date, most state-of-the-art sequence modelling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using the fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation and language modelling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：到目前为止，国家的最先进最序列建模架构使用时注意建立生成模型基于语言的任务。其中一些模型使用所有可用的序列令牌来产生注意力分布，结果在时间的$ O（N ^ 2）$的复杂性。可替换地，它们利用在深度方向上与卷积归SOFTMAX大小$ $ķ的内核充当有限窗口自关注，导致时间的O- $（K {\ CDOT} N）$复杂性。在本文中，我们引入时间感知较大的内核（TALK）卷积，一个新的自适应卷积运算该学习如何预测求和内核，而不是使用固定尺寸的内核矩阵的大小。该方法得到的O- $（n）的一个$时间复杂度，有效地使编码处理线到的令牌的数量的序列。我们评估对大型标准机器翻译和语言模型的数据集，表明该方法会说话的卷积构成超越其它基于关注/卷积方法的有效改善。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-10</title>
    <url>/2020/02/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-10/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Revisiting Spatial Invariance with Low-Rank Local Connectivity <a href="https://arxiv.org/pdf/2002.02959" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> $M^3$T: Multi-Modal Continuous Valence-Arousal Estimation in the Wild <a href="https://arxiv.org/pdf/2002.02957" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> On the Robustness of Face Recognition Algorithms Against Attacks and  Bias <a href="https://arxiv.org/pdf/2002.02942" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> How Does Gender Balance In Training Data Affect Face Recognition  Accuracy? <a href="https://arxiv.org/pdf/2002.02934" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> SPN-CNN: Boosting Sensor-Based Source Camera Attribution With Deep  Learning <a href="https://arxiv.org/pdf/2002.02927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Subspace Capsule Network <a href="https://arxiv.org/pdf/2002.02924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Temporal Segmentation of Surgical Sub-tasks through Deep Learning with  Multiple Data Sources <a href="https://arxiv.org/pdf/2002.02921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> iqiyi Submission to ActivityNet Challenge 2019 Kinetics-700 challenge:  Hierarchical Group-wise Attention <a href="https://arxiv.org/pdf/2002.02918" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Data augmentation with Möbius transformations <a href="https://arxiv.org/pdf/2002.02917" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Domain Embedded Multi-model Generative Adversarial Networks for  Image-based Face Inpainting <a href="https://arxiv.org/pdf/2002.02909" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy  Images <a href="https://arxiv.org/pdf/2002.02857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Input Dropout for Spatially Aligned Modalities <a href="https://arxiv.org/pdf/2002.02852" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Switchable Precision Neural Networks <a href="https://arxiv.org/pdf/2002.02815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding  Network <a href="https://arxiv.org/pdf/2002.02814" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> FourierNet: Compact mask representation for instance segmentation using  differentiable shape decoders <a href="https://arxiv.org/pdf/2002.02709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Deep Robust Multilevel Semantic Cross-Modal Hashing <a href="https://arxiv.org/pdf/2002.02698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Learning Class Regularized Features for Action Recognition <a href="https://arxiv.org/pdf/2002.02651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Statistical Outlier Identification in Multi-robot Visual SLAM using  Expectation Maximization <a href="https://arxiv.org/pdf/2002.02638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> SideInfNet: A Deep Neural Network for Semi-Automatic Semantic  Segmentation with Side Information <a href="https://arxiv.org/pdf/2002.02634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Visual search over billions of aerial and satellite images <a href="https://arxiv.org/pdf/2002.02624" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Image Fine-grained Inpainting <a href="https://arxiv.org/pdf/2002.02609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Adaptive Deep Metric Embeddings for Person Re-Identification under  Occlusions <a href="https://arxiv.org/pdf/2002.02603" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Object-Adaptive LSTM Network for Real-time Visual Tracking with  Adversarial Data Augmentation <a href="https://arxiv.org/pdf/2002.02598" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Poisson Kernel Avoiding Self-Smoothing in Graph Convolutional Networks <a href="https://arxiv.org/pdf/2002.02589" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Learning Hyperspectral Feature Extraction and Classification with  ResNeXt Network <a href="https://arxiv.org/pdf/2002.02585" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> Impact of ImageNet Model Selection on Domain Adaptation <a href="https://arxiv.org/pdf/2002.02559" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Opposite Structure Learning for Semi-supervised Domain Adaptation <a href="https://arxiv.org/pdf/2002.02545" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Continuous Geodesic Convolutions for Learning on 3D Shapes <a href="https://arxiv.org/pdf/2002.02506" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> Activation Density driven Energy-Efficient Pruning in Training <a href="https://arxiv.org/pdf/2002.02949" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> AnimePose: Multi-person 3D pose estimation and animation <a href="https://arxiv.org/pdf/2002.02792" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> Trust Your Model: Iterative Label Improvement and Robust Training by  Confidence Based Filtering and Dataset Partitioning <a href="https://arxiv.org/pdf/2002.02705" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Optimization of Structural Similarity in Mathematical Imaging <a href="https://arxiv.org/pdf/2002.02657" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Quantifying the Value of Lateral Views in Deep Learning for Chest X-rays <a href="https://arxiv.org/pdf/2002.02582" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
<div id="title34">
<b>34.</b> Closing the Dequantization Gap: PixelCNN as a Single-Layer Flow <a href="https://arxiv.org/pdf/2002.02547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper34" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>



<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Revisiting Spatial Invariance with Low-Rank Local Connectivity</b>  <a href="https://arxiv.org/pdf/2002.02959" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Elsayed%2C+G+F" target="_blank" rel="noopener" style="color:#0000EE;">Gamaleldin F. Elsayed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramachandran%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prajit Ramachandran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shlens%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathon Shlens</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kornblith%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Kornblith</a><br>
<font size="3">
Abstract: Convolutional neural networks are among the most successful architectures in deep learning. This success is at least partially attributable to the efficacy of spatial invariance as an inductive bias. Locally connected layers, which differ from convolutional layers in their lack of spatial invariance, usually perform poorly in practice. However, these observations still leave open the possibility that some degree of relaxation of spatial invariance may yield a better inductive bias than either convolution or local connectivity. To test this hypothesis, we design a method to relax the spatial invariance of a network layer in a controlled manner. In particular, we create a \textit{low-rank} locally connected layer, where the filter bank applied at each position is constructed as a linear combination of basis set of filter banks. By varying the number of filter banks in the basis set, we can control the degree of departure from spatial invariance. In our experiments, we find that relaxing spatial invariance improves classification accuracy over both convolution and locally connected layers across MNIST, CIFAR-10, and CelebA datasets. These results suggest that spatial invariance in convolution layers may be overly restrictive. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络在深学习最成功的架构之中。这一成功是至少部分地归因于空间不变性的功效为感应偏压。本地连接的层，其从卷积层的区别在于它们缺少空间不变性的，通常在实践中表现不佳。然而，这些意见仍然保持打开的可能性，一定程度的空间不变性的放松可能会产生比任何回旋或本地连接更好的归纳偏置。为了检验这一假设，我们设计放松的网络层的空间不变性以受控的方式的方法。特别是，我们创建了一个\ textit {低秩}本地连接的层，其中，所述滤波器组施加在每个位置被构造为基组滤波器组的线性组合。通过改变基组滤波器组的数量，我们可以控制背离空间不变性的程度。在我们的实验中，我们发现，放松的空间不变性在两个卷积提高了分类的准确性和本地连接跨MNIST，CIFAR-10和CelebA数据集层。这些结果表明，在卷积层的空间不变性可能过于严格。</font>
</div>


<hr>
<div id="paper2"> <b>2. $M^3$T: Multi-Modal Continuous Valence-Arousal Estimation in the Wild</b>  <a href="https://arxiv.org/pdf/2002.02957" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan-Hang Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rulin Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiabei Zeng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiguang Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xilin Chen</a><br>
<font size="3">
Abstract: This report describes a multi-modal multi-task ($M^3$T) approach underlying our submission to the valence-arousal estimation track of the Affective Behavior Analysis in-the-wild (ABAW) Challenge, held in conjunction with the IEEE International Conference on Automatic Face and Gesture Recognition (FG) 2020. In the proposed $M^3$T framework, we fuse both visual features from videos and acoustic features from the audio tracks to estimate the valence and arousal. The spatio-temporal visual features are extracted with a 3D convolutional network and a bidirectional recurrent neural network. Considering the correlations between valence / arousal, emotions, and facial actions, we also explores mechanisms to benefit from other tasks. We evaluated the $M^3$T framework on the validation set provided by ABAW and it significantly outperforms the baseline method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：该报告描述了我们提交基本的情感行为分析的价觉醒估计轨道多模式多任务（$ M ^ 3 $ T）的方式在最狂野结合举行（ABAW）的挑战，在自动面部和手势识别（FG）2020年提出的$ M ^ 3 $ T框架的IEEE国际会议，我们融合从音轨视频和声音特征的视觉特征来估计效价和唤醒。时空视觉特征与3D卷积网络和双向回归神经网络萃取。考虑价/觉醒，情绪和面部动作之间的相关性，我们还探讨了其他的任务机制的好处。我们评估了由ABAW提供的验证集的$ M ^ 3 $ T框架，它显著优于基线法。</font>
</div>


<hr>
<div id="paper3"> <b>3. On the Robustness of Face Recognition Algorithms Against Attacks and  Bias</b>  <a href="https://arxiv.org/pdf/2002.02942" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richa Singh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Akshay Agarwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maneet Singh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nagpal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shruti Nagpal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Vatsa%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mayank Vatsa</a><br>
<font size="3">
Abstract: Face recognition algorithms have demonstrated very high recognition performance, suggesting suitability for real world applications. Despite the enhanced accuracies, robustness of these algorithms against attacks and bias has been challenged. This paper summarizes different ways in which the robustness of a face recognition algorithm is challenged, which can severely affect its intended working. Different types of attacks such as physical presentation attacks, disguise/makeup, digital adversarial attacks, and morphing/tampering using GANs have been discussed. We also present a discussion on the effect of bias on face recognition models and showcase that factors such as age and gender variations affect the performance of modern algorithms. The paper also presents the potential reasons for these challenges and some of the future research directions for increasing the robustness of face recognition models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人脸识别算法已经证明非常高的识别性能，这对于现实世界的应用程序的适用性。尽管提高精度，这些算法受到攻击与偏见稳健性受到了挑战。本文总结了不同的方式，其中人脸识别算法的鲁棒性受到质疑，这会严重影响其预期工作。不同类型的攻击，例如物理呈现攻击，伪装/化妆，数字对抗攻击，变形/篡改使用甘斯进行了讨论。我们还提出关于面部识别模型偏差的影响的讨论，也展示了因素，如年龄和性别变化而影响的现代算法的性能。本文还介绍了这些挑战和一些未来的研究方向为增加脸部识别模型的鲁棒性的潜在原因。</font>
</div>


<hr>
<div id="paper4"> <b>4. How Does Gender Balance In Training Data Affect Face Recognition  Accuracy?</b>  <a href="https://arxiv.org/pdf/2002.02934" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Albiero%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vítor Albiero</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kai Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bowyer%2C+K+W" target="_blank" rel="noopener" style="color:#0000EE;">Kevin W. Bowyer</a><br>
<font size="3">
Abstract: Even though deep learning methods have greatly increased the overall accuracy of face recognition, an old problem still persists: accuracy is higher for men than for women. Previous researchers have speculated that the difference could be due to cosmetics, head pose, or hair covering the face. It is also often speculated that the lower accuracy for women is caused by women being under-represented in the training data. This work aims to investigate if gender imbalance in the training data is actually the cause of lower accuracy for females. Using a state-of-the-art deep CNN, three different loss functions, and two training datasets, we train each on seven subsets with different male/female ratios, totaling forty two train-ings. The trained face matchers are then tested on three different testing datasets. Results show that gender-balancing the dataset has an overall positive effect, with higher accuracy for most of the combinations of loss functions and datasets when a balanced subset is used. However, for the best combination of loss function and dataset, the original training dataset shows better accuracy on 3 out of 4 times. We observe that test accuracy for males is higher when the training data is all male. However, test accuracy for females is not maximized when the training data is all female. Fora number of combinations of loss function and test dataset, accuracy for females is higher when only 75% of the train-ing data is female than when 100% of the training data is female. This suggests that lower accuracy for females is nota simple result of the fraction of female training data. By clustering face features, we show that in general, male faces are closer to other male faces than female faces, and female faces are closer to other female faces than male faces </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管深度学习方法已经人脸识别的整体精度大大提高，一个老问题仍然存在：精度是男性高于女性。先前的研究人员推测，差异可能是由于化妆品，头部姿势，或头发遮住了脸。它也经常被推测为女性较低的精度是由妇女是在训练数据代表性不足引起的。这项工作旨在调查，如果在训练数据性别失衡实际上是低精度为女性的原因。用一个国家的最先进的深CNN，三种不同的损失函数，和两个训练数据集，我们每次训练七子集与不同的男性/女性的比例，共计42列车英格斯。训练有素的脸的匹配，然后在三个不同的测试数据集进行测试。结果表明，两性平衡数据集具有总体积极作用，以较高的精度对大多数的损失函数和数据集的组合中的，当使用平衡子集。然而，对于损失函数和数据集，3开出4次原训练数据集显示了更好的精确度的最佳组合。我们观察到，测试精度男性要高，当训练数据是所有男性。然而，当训练数据是所有女性为女性测试精度没有最大化。损失函数和测试数据集的组合的数目论坛，精度为女性更高时只有75％的训练数据的情况相比，在训练数据的100％是女女。这表明，对于女性低精度诺塔女训练数据的分数的简单的结果。通过聚类面部特征，我们表明，在一般情况下，男性的面孔更接近其他男性的脸比女性的面孔，和女性的面孔更接近其他女性的面孔比男性面孔</font>
</div>


<hr>
<div id="paper5"> <b>5. SPN-CNN: Boosting Sensor-Based Source Camera Attribution With Deep  Learning</b>  <a href="https://arxiv.org/pdf/2002.02927" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kirchner%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Kirchner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Johnson%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cameron Johnson</a><br>
<font size="3">
Abstract: We explore means to advance source camera identification based on sensor noise in a data-driven framework. Our focus is on improving the sensor pattern noise (SPN) extraction from a single image at test time. Where existing works suppress nuisance content with denoising filters that are largely agnostic to the specific SPN signal of interest, we demonstrate that a~deep learning approach can yield a more suitable extractor that leads to improved source attribution. A series of extensive experiments on various public datasets confirms the feasibility of our approach and its applicability to image manipulation localization and video source attribution. A critical discussion of potential pitfalls completes the text. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于在数据驱动框架传感器噪声探索手段预先源摄像机识别。我们的重点是在测试时间改善从单个图像传感器图案噪声（SPN）萃取。如果现有的工作与抑制去噪是很大程度上不可知的感兴趣的特定SPN​​信号过滤器扰民的内容，我们证明了〜深深的学习方法可以产生更适合提取这会改善来源归属。一系列的各种公共数据集大量的实验证明我们的方法和适用于图像处理的定位和视频源归属的可行性。潜在缺陷的一个重要讨论完成的文字。</font>
</div>


<hr>
<div id="paper6"> <b>6. Subspace Capsule Network</b>  <a href="https://arxiv.org/pdf/2002.02924" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Edraki%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marzieh Edraki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahnavard%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nazanin Rahnavard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mubarak Shah</a><br>
<font size="3">
Abstract: Convolutional neural networks (CNNs) have become a key asset to most of fields in AI. Despite their successful performance, CNNs suffer from a major drawback. They fail to capture the hierarchy of spatial relation among different parts of an entity. As a remedy to this problem, the idea of capsules was proposed by Hinton. In this paper, we propose the SubSpace Capsule Network (SCN) that exploits the idea of capsule networks to model possible variations in the appearance or implicitly defined properties of an entity through a group of capsule subspaces instead of simply grouping neurons to create capsules. A capsule is created by projecting an input feature vector from a lower layer onto the capsule subspace using a learnable transformation. This transformation finds the degree of alignment of the input with the properties modeled by the capsule subspace. We show that SCN is a general capsule network that can successfully be applied to both discriminative and generative models without incurring computational overhead compared to CNN during test time. Effectiveness of SCN is evaluated through a comprehensive set of experiments on supervised image classification, semi-supervised image classification and high-resolution image generation tasks using the generative adversarial network (GAN) framework. SCN significantly improves the performance of the baseline models in all 3 tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：卷积神经网络（细胞神经网络）已经成为一个重要的资产，以最人工智能领域。尽管他们的成功表现，细胞神经网络从一大缺点。他们并没有捕捉到一个实体的不同部分之间的空间关系的层次结构。作为补救这一问题，胶囊的想法被提出韩丁。在本文中，我们提出了子空间胶囊网络（SCN），它利用胶囊网络的想法通过一组子空间胶囊而不是简单地分组的神经元来创建胶囊以一个实体的外观或隐式地定义的属性可能的变化进行建模。胶囊是通过使用可学习变换从较低层的输入特征向量投影到子空间胶囊创建。这种转变中找到与由胶囊子空间模型化的特性输入的对准程度。我们发现，SCN是可以成功地应用到辨别和生成模型，而不会在测试时间招致相比，CNN的计算开销一般胶囊网络。 SCN的有效性是通过一套综合的有关使用生成对抗网络（GAN）框架监督图像分类，半监督图像分类和高分辨率图像生成任务实验进行评价。 SCN显著提高了基准模型中的所有3个任务的性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. Temporal Segmentation of Surgical Sub-tasks through Deep Learning with  Multiple Data Sources</b>  <a href="https://arxiv.org/pdf/2002.02921" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yidan Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pedram%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Sahba Aghajani Pedram</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feyzabadi%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seyedshams Feyzabadi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Allan%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Max Allan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=McLeod%2C+A+J" target="_blank" rel="noopener" style="color:#0000EE;">A. Jonathan McLeod</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Burdick%2C+J+W" target="_blank" rel="noopener" style="color:#0000EE;">Joel W. Burdick</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Azizian%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahdi Azizian</a><br>
<font size="3">
Abstract: Many tasks in robot-assisted surgeries (RAS) can be represented by finite-state machines (FSMs), where each state represents either an action (such as picking up a needle) or an observation (such as bleeding). A crucial step towards the automation of such surgical tasks is the temporal perception of the current surgical scene, which requires a real-time estimation of the states in the FSMs. The objective of this work is to estimate the current state of the surgical task based on the actions performed or events occurred as the task progresses. We propose Fusion-KVE, a unified surgical state estimation model that incorporates multiple data sources including the Kinematics, Vision, and system Events. Additionally, we examine the strengths and weaknesses of different state estimation models in segmenting states with different representative features or levels of granularity. We evaluate our model on the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), as well as a more complex dataset involving robotic intra-operative ultrasound (RIOUS) imaging, created using the da Vinci Xi surgical system. Our model achieves a superior frame-wise state estimation accuracy up to 89.4%, which improves the state-of-the-art surgical state estimation models in both JIGSAWS suturing dataset and our RIOUS dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在机器人辅助的外科手术（RAS）的许多任务可以通过有限状态机（FSM），其中每个状态代表任一种动作（诸如拿起针）或观察（如出血）来表示。对这样的手术任务的自动化的一个关键步骤是目前的手术场景，这需要在有限状态机的状态的实时估计的时间感知。这项工作的目的是评估基础上进行的操作或事件发生的任务进展手术任务的当前状态。我们提出了Fusion-KVE，了采用多种数据源，包括运动学，视觉，和系统事件统一的手术状态估计模型。此外，我们研究不同的状态估计模型的优势和劣势在具有不同代表​​性的特征或粒度级别分割的状态。我们评估我们在约翰霍普金斯大学，ISI手势和技能评估工作组（拼图）模型，以及一个涉及机器人术中超声（RIOUS）成像更复杂的数据集，使用达芬奇手术兮系统创建。我们的模型实现了卓越的逐帧状态估计精度高达89.4％，提高了两个拼图的国家的最先进的手术状态估计模型缝合数据集，我们RIOUS数据集。</font>
</div>


<hr>
<div id="paper8"> <b>8. iqiyi Submission to ActivityNet Challenge 2019 Kinetics-700 challenge:  Hierarchical Group-wise Attention</b>  <a href="https://arxiv.org/pdf/2002.02918" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongyang Cai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Wang</a><br>
<font size="3">
Abstract: In this report, the method for the iqiyi submission to the task of ActivityNet 2019 Kinetics-700 challenge is described. Three models are involved in the model ensemble stage: TSN, HG-NL and StNet. We propose the hierarchical group-wise non-local (HG-NL) module for frame-level features aggregation for video classification. The standard non-local (NL) module is effective in aggregating frame-level features on the task of video classification but presents low parameters efficiency and high computational cost. The HG-NL method involves a hierarchical group-wise structure and generates multiple attention maps to enhance performance. Basing on this hierarchical group-wise structure, the proposed method has competitive accuracy, fewer parameters and smaller computational cost than the standard NL. For the task of ActivityNet 2019 Kinetics-700 challenge, after model ensemble, we finally obtain an averaged top-1 and top-5 error percentage 28.444% on the test set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这个报告中，被描述为爱奇艺提交ActivityNet 2019动力学-700挑战任务的方法。三种型号都参与模式集合阶段：TSN，HG-NL和StNet。我们提出了帧级的分级组明智的非本地（HG-NL）模块支持视频分类聚集。标准的非本地（NL）模块是有效的聚合帧级特征的视频分类，但呈现低参数效率和高的计算成本的任务。在HG-NL方法涉及分级组明智的结构和生成多个关注的地图，以提高性能。在此基础上分级组明智结构，所提出的方法具有竞争力的精确度，更少的参数和比标准NL较小的计算成本。对于ActivityNet 2019动力学-700挑战的任务，模式集合后，我们终于在测试组获得的平均最高-1和前五名误差百分比28.444％。</font>
</div>


<hr>
<div id="paper9"> <b>9. Data augmentation with Möbius transformations</b>  <a href="https://arxiv.org/pdf/2002.02917" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sharon Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiequan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lundh%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Torbjörn Lundh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ng%2C+A+Y" target="_blank" rel="noopener" style="color:#0000EE;">Andrew Y. Ng</a><br>
<font size="3">
Abstract: Data augmentation has led to substantial improvements in the performance and generalization of deep models, and remain a highly adaptable method to evolving model architectures and varying amounts of data---in particular, extremely scarce amounts of available training data. In this paper, we present a novel method of applying Möbius transformations to augment input images during training. Möbius transformations are bijective conformal maps that generalize image translation to operate over complex inversion in pixel space. As a result, Möbius transformations can operate on the sample level and preserve data labels. We show that the inclusion of Möbius transformations during training enables improved generalization over prior sample-level data augmentation techniques such as cutout and standard crop-and-flip transformations, most notably in low data regimes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据增强导致了深模型的性能和泛化实质性的改善，并保持高度适应性的方法来进化模型架构和不同的数据量---尤其是极为稀缺的可用金额的训练数据。在本文中，我们提出了训练期间施加莫比乌斯变换到扩充输入图像的新方法。莫比乌斯变换是双射共形映射是广义含图像平移了在像素空间复杂反转来操作。其结果是，莫比乌斯转换可以在样品上水平和操作保持数据的标签。我们发现，莫比乌斯变换的训练中列入允许超过前一个样级别的数据增强技术，如切口和标准的作物和翻动的转换，特别是在低数据制度改进的概括。</font>
</div>


<hr>
<div id="paper10"> <b>10. Domain Embedded Multi-model Generative Adversarial Networks for  Image-based Face Inpainting</b>  <a href="https://arxiv.org/pdf/2002.02909" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xian Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kong%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bin Kong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youbing Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lyu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siwei Lyu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiancheng Lv</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Canghong Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaojie Li</a><br>
<font size="3">
Abstract: Prior knowledge of face shape and location plays an important role in face inpainting. However, traditional facing inpainting methods mainly focus on the generated image resolution of the missing portion but without consideration of the special particularities of the human face explicitly and generally produce discordant facial parts. To solve this problem, we present a stable variational latent generative model for large inpainting of face images. We firstly represent only face regions with the latent variable space but simultaneously constraint the random vectors to offer control over the distribution of latent variables, and combine with the non-face parts textures to generate a face image with plausible contents. Two adversarial discriminators are finally used to judge whether the generated distribution is close to the real distribution or not. It can not only synthesize novel image structures but also explicitly utilize the latent space with Eigenfaces to make better predictions. Furthermore, our work better evaluates the side face impainting problem. Experiments on both CelebA and CelebA-HQ face datasets demonstrate that our proposed approach generates higher quality inpainting results than existing ones. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：脸的形状和位置的先验知识起着面修补了重要作用。然而，传统的面向图像修复方法主要集中在缺失部分的所产生的图像分辨率，但不考虑人脸的特殊特殊性明确和一般产生不和谐的面部部分。为了解决这个问题，我们提出了一个稳定的潜在变生成模型对于大修补面部图像。我们首先仅代表面孔区域与潜变量空间，但同时在潜变量的分布约束随机向量提供控制，并与非人脸部分的纹理相结合，生成具有合理内容的人脸图像。两个敌对的鉴别最终用于判断产生的分布是否接近真实分布与否。它不仅可以合成新的图像结构，而且还明确利用与特征脸的潜在空间，以做出更好的预测。此外，我们的工作更好地评估侧面impainting问题。两个CelebA和CelebA-HQ面数据集实验证明，我们提出的方法生成更高质量的图像修补效果比现有的。</font>
</div>


<hr>
<div id="paper11"> <b>11. An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy  Images</b>  <a href="https://arxiv.org/pdf/2002.02857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hirsch%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peter Hirsch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kainmueller%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dagmar Kainmueller</a><br>
<font size="3">
Abstract: Segmentation of cell nuclei in microscopy images is a prevalent necessity in cell biology. Especially for three-dimensional datasets, manual segmentation is prohibitively time-consuming, motivating the need for automated methods. Learning-based methods trained on pixel-wise ground-truth segmentations have been shown to yield state-of-the-art results on 2d benchmark image data of nuclei, yet a respective benchmark is missing for 3d image data. In this work, we perform a comparative evaluation of nuclei segmentation algorithms on a database of manually segmented 3d light microscopy volumes. We propose a novel learning strategy that boosts segmentation accuracy by means of a simple auxiliary task, thereby robustly outperforming each of our baselines. Furthermore, we show that one of our baselines, the popular three-label model, when trained with our proposed auxiliary task, outperforms the recent StarDist-3D. As an additional, practical contribution, we benchmark nuclei segmentation against nuclei detection, i.e. the task of merely pinpointing individual nuclei without generating respective pixel-accurate segmentations. For learning nuclei detection, large 3d training datasets of manually annotated nuclei center points are available. However, the impact on detection accuracy caused by training on such sparse ground truth as opposed to dense pixel-wise ground truth has not yet been quantified. To this end, we compare nuclei detection accuracy yielded by training on dense vs. sparse ground truth. Our results suggest that training on sparse ground truth yields competitive nuclei detection rates. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在显微镜图像细胞核的分割是细胞生物学中普遍存在的必要性。特别是对于三维数据集，手动分割是过于费时的，激励为自动化方法的需要。基于学习训练的上逐像素地面实况分割方法已经显示出产生关于细胞核的2D基准图像数据状态的最先进的结果，但各自的基准缺少3D图像数据。在这项工作中，我们手动分割的3D光学显微镜卷的数据库上执行的细胞核分割算法的比较评价。我们提出了一种新的学习策略，提升分割精度通过简单的辅助任务的手段，从而有力跑赢我们每一个基线。此外，我们表明，我们的基准之一，流行的三标签模型，当我们提出的辅助任务的训练，优于近期StarDist-3D。作为一个附加的，实用的贡献，对细胞核检测我们基准细胞核分割，即，仅仅确定个体细胞核，而不会产生相应的像素精确的分割的任务。对于学习核检测，人工标注的核中心点大型3D训练数据是可用的。然而，由这种稀疏的地面实况训练，而不是密集的逐像素的地面实况对检测精度的影响尚未量化。为此，我们通过比较致密与稀疏地面实况训练产生的核检测精度。我们的研究结果表明，在稀疏的地面实况训练产生有竞争力的核检测率。</font>
</div>


<hr>
<div id="paper12"> <b>12. Input Dropout for Spatially Aligned Modalities</b>  <a href="https://arxiv.org/pdf/2002.02852" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Blois%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sébastien de Blois</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garon%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mathieu Garon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gagn%C3%A9%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Gagné</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lalonde%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-François Lalonde</a><br>
<font size="3">
Abstract: Computer vision datasets containing multiple modalities such as color, depth, and thermal properties are now commonly accessible and useful for solving a wide array of challenging tasks. However, deploying multi-sensor heads is not possible in many scenarios. As such many practical solutions tend to be based on simpler sensors, mostly for cost, simplicity and robustness considerations. In this work, we propose a training methodology to take advantage of these additional modalities available in datasets, even if they are not available at test time. By assuming that the modalities have a strong spatial correlation, we propose Input Dropout, a simple technique that consists in stochastic hiding of one or many input modalities at training time, while using only the canonical (e.g. RGB) modalities at test time. We demonstrate that Input Dropout trivially combines with existing deep convolutional architectures, and improves their performance on a wide range of computer vision tasks such as dehazing, 6-DOF object tracking, pedestrian detection and object classification. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：含有多种方式，如颜色，深度和热性能的计算机视觉的数据集，现在是解决了各种各样的挑战性的任务阵列通常访问并从中受益。但是，在部署多传感器头是不可能在许多情况下。因此许多切实可行的解决方案往往是基于简单的传感器，主要是出于成本，简单性和稳健性的考虑。在这项工作中，我们提出了一种培训方法采取数据集提供这些附加模式的优势，即使他们不提供测试时间。通过假设方式具有很强的空间相关性，我们建议输入差，一个简单的技术，其在于在训练时间的一个或多个输入模态随机遮盖力，而只使用的规范（例如，RGB）模式在测试时间。我们表明，输入差平凡与现有的深卷积架构相结合，并提高他们对范围广泛的计算机视觉任务，如除雾，6-DOF目标跟踪，行人检测和对象分类性能。</font>
</div>


<hr>
<div id="paper13"> <b>13. Switchable Precision Neural Networks</b>  <a href="https://arxiv.org/pdf/2002.02815" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guerra%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luis Guerra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bohan Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reid%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ian Reid</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Drummond%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Drummond</a><br>
<font size="3">
Abstract: Instantaneous and on demand accuracy-efficiency trade-off has been recently explored in the context of neural networks slimming. In this paper, we propose a flexible quantization strategy, termed Switchable Precision neural Networks (SP-Nets), to train a shared network capable of operating at multiple quantization levels. At runtime, the network can adjust its precision on the fly according to instant memory, latency, power consumption and accuracy demands. For example, by constraining the network weights to 1-bit with switchable precision activations, our shared network spans from BinaryConnect to Binarized Neural Network, allowing to perform dot-products using only summations or bit operations. In addition, a self-distillation scheme is proposed to increase the performance of the quantized switches. We tested our approach with three different quantizers and demonstrate the performance of SP-Nets against independently trained quantized models in classification accuracy for Tiny ImageNet and ImageNet datasets using ResNet-18 and MobileNet architectures. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：瞬时和按需精度效率的权衡已经在最近减肥神经网络的环境中探索。在本文中，我们提出了一种灵活的量化策略，称为切换精密神经网络（SP-网），培养能够在多个量化等级操作的共享的网络。在运行时，网络可以根据即时记忆，延迟，功耗和精度要求在飞行中调整其精度。例如，通过限制网络的权重为1位具有可切换精度激活，我们的共享网络从跨度到BinaryConnect二值化神经网络，允许进行点副产物仅使用加法运算或位操作。此外，自蒸馏方案提出增加量化开关的性能。我们使用RESNET-18和MobileNet架构测试我们有三个不同的量化方法，并展示SP-篮网对独立训练的量化模型，分类准确率的表现为微小ImageNet和ImageNet数据集。</font>
</div>


<hr>
<div id="paper14"> <b>14. Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding  Network</b>  <a href="https://arxiv.org/pdf/2002.02814" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhe Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianfeng Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Long%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongzi Long</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=He%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuan He</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Xue</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ji%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shouling Ji</a><br>
<font size="3">
Abstract: This paper strives to learn fine-grained fashion similarity. In this similarity paradigm, one should pay more attention to the similarity in terms of a specific design/attribute among fashion items, which has potential values in many fashion related applications such as fashion copyright protection. To this end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn multiple attribute-specific embeddings in an end-to-end manner, thus measure the fine-grained similarity in the corresponding space. With two attention modules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel Attention, ASEN is able to locate the related regions and capture the essential patterns under the guidance of the specified attribute, thus make the learned attribute-specific embeddings better reflect the fine-grained similarity. Extensive experiments on four fashion-related datasets show the effectiveness of ASEN for fine-grained fashion similarity learning and its potential for fashion reranking. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文力求学习细粒度的方式相似。在这种相似的模式，应该在当中的时尚单品的特定设计/属性，它在很多时尚相关的应用，如时尚版权保护的潜在价值方面更注重的相似性。为此，提出了一种属性特定嵌入网络（ASEN）共同学习多个属性特定的嵌入在端至端的方式，从而测量在相应的空间中的细粒的相似性。有两个注意模块，即属性感知空间注意和属性感知通道注意，日月能够找到相关的区域和指定属性的指导下拍摄的基本模式，从而使学习特定属性的嵌入更好地反映细粒度的相似性。在四大时装相关的数据集大量的实验表明日月的细粒度方式相似的学习和有效性及其对时尚的重新排名的潜力。</font>
</div>


<hr>
<div id="paper15"> <b>15. FourierNet: Compact mask representation for instance segmentation using  differentiable shape decoders</b>  <a href="https://arxiv.org/pdf/2002.02709" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Benbarka%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nuri Benbarka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Riaz%2C+H+u+M" target="_blank" rel="noopener" style="color:#0000EE;">Hamd ul Moqeet Riaz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zell%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andreas Zell</a><br>
<font size="3">
Abstract: We present FourierNet a single shot, anchor-free, fully convolutional instance segmentation method, which predicts a shape vector that is converted into contour points using a numerical transformation. Compared to previous methods, we introduce a new training technique, where we utilize a differentiable shape decoder, which achieves automatic weight balancing of the shape vector's coefficients. Fourier series was utilized as a shape encoder because of its coefficient interpretability and fast implementation. By using its lower frequencies we were able to retrieve smooth and compact masks. FourierNet shows promising results compared to polygon representation methods, achieving 30.6 mAP on the MS COCO 2017 benchmark. At lower image resolutions, it runs at 26.6 FPS with 24.3 mAP. It achieves 23.3 mAP using just 8 parameters to represent the mask, which is double the amount of parameters to predict a bounding box. Code will be available at: this http URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出FourierNet单杆，锚自由，充分卷积实例分割方法，该方法预测，被转换成使用数字变换的轮廓点的形状向量。相比以前的方法中，我们引入一个新的训练技术，在这里我们利用微分的形状解码器，它实现了自动重平衡形状矢量的系数。傅立叶系列被用作形状编码器，因为它的系数解释性和快速实现的。通过使用它的频率较低，我们能够取得光滑紧致口罩。 FourierNet显示有希望的结果相比，多边形表示方法，实现对MS COCO 2017年基准30.6地图。在较低的图像分辨率，它运行在26.6 FPS 24.3地图。它实现只用8个参数来表示掩模，这是参数双倍量来预测的边界框23.3地图。代码将可在：这个HTTP URL。</font>
</div>


<hr>
<div id="paper16"> <b>16. Deep Robust Multilevel Semantic Cross-Modal Hashing</b>  <a href="https://arxiv.org/pdf/2002.02698" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Ge Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyang Tan</a><br>
<font size="3">
Abstract: Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel Robust Multilevel Semantic Hashing (RMSH) for more accurate cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics, while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this, we give an effective bound of this value based on the information coding-theoretic analysis, and the above goals are embodied into a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics, alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds, and our method achieves state-of-the-art performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于散列的跨模态获取最近取得显著的进展。但来自不同模态直接的数据嵌入到一个关节海明空间不可避免地会产生虚假的代码由于固有形态差异和噪声。我们提出了一个新颖的多级鲁棒语义散列（RMSH），用于更精确的跨通道检索。它旨在保护具有丰富的语义数据中细粒度的相似性，同时明确要求不同的点之间的距离比为较强的鲁棒性的特定值。对于这一点，我们给出一个有效的结合的该值的基础上，信息编码-理论分析，和上述目标被实现成一个余量自适应三重态损耗。此外，我们通过融合多个散列码来探索很少见过语义，减轻相似信息的稀疏问题引入伪代码。在三个基准实验表明派生边界的有效性，以及我们的方法实现国家的最先进的性能。</font>
</div>


<hr>
<div id="paper17"> <b>17. Learning Class Regularized Features for Action Recognition</b>  <a href="https://arxiv.org/pdf/2002.02651" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Stergiou%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandros Stergiou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poppe%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ronald Poppe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Veltkamp%2C+R+C" target="_blank" rel="noopener" style="color:#0000EE;">Remco C. Veltkamp</a><br>
<font size="3">
Abstract: Training Deep Convolutional Neural Networks (CNNs) is based on the notion of using multiple kernels and non-linearities in their subsequent activations to extract useful features. The kernels are used as general feature extractors without specific correspondence to the target class. As a result, the extracted features do not correspond to specific classes. Subtle differences between similar classes are modeled in the same way as large differences between dissimilar classes. To overcome the class-agnostic use of kernels in CNNs, we introduce a novel method named Class Regularization that performs class-based regularization of layer activations. We demonstrate that this not only improves feature search during training, but also allows an explicit assignment of features per class during each stage of the feature extraction process. We show that using Class Regularization blocks in state-of-the-art CNN architectures for action recognition leads to systematic improvement gains of 1.8%, 1.2% and 1.4% on the Kinetics, UCF-101 and HMDB-51 datasets, respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：培训深卷积神经网络（细胞神经网络）是基于使用多个内核和非线性在其随后的激活，提取有用的功能的概念。将所述核用作为一般特征提取器没有具体的对应于目标类。其结果是，所提取的特征不对应于特定的类。相似的类之间的细微差别以同样的方式被建模为不同阶层之间的巨大差异。为了克服类无关的使用在细胞神经网络内核，我们引入已命名的类的正则化，其执行基于类的层的激活的正则化的新方法。我们证明，这不仅提高了训练中的搜索功能，还允许在特征提取过程的每一个阶段，每级功能的明确任务。我们分别显示在国家的最先进的美国有线电视新闻网的架构，使用正则班块动作识别导致的1.8％，1.2％和1.4％，在动力学，UCF-101和HMDB-51数据集系统化改善收益。</font>
</div>


<hr>
<div id="paper18"> <b>18. Statistical Outlier Identification in Multi-robot Visual SLAM using  Expectation Maximization</b>  <a href="https://arxiv.org/pdf/2002.02638" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Karimian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arman Karimian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqi Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tron%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roberto Tron</a><br>
<font size="3">
Abstract: This paper introduces a novel and distributed method for detecting inter-map loop closure outliers in simultaneous localization and mapping (SLAM). The proposed algorithm does not rely on a good initialization and can handle more than two maps at a time. In multi-robot SLAM applications, maps made by different agents have nonidentical spatial frames of reference which makes initialization very difficult in the presence of outliers. This paper presents a probabilistic approach for detecting incorrect orientation measurements prior to pose graph optimization by checking the geometric consistency of rotation measurements. Expectation-Maximization is used to fine-tune the model parameters. As ancillary contributions, a new approximate discrete inference procedure is presented which uses evidence on loops in a graph and is based on optimization (Alternate Direction Method of Multipliers). This method yields superior results compared to Belief Propagation and has convergence guarantees. Simulation and experimental results are presented that evaluate the performance of the outlier detection method and the inference algorithm on synthetic and real-world data. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了一种新颖的和用于同时定位和地图创建（SLAM）检测地图间环路闭合离群值分布的方法。该算法不依赖于良好的初始化，可以同时处理两个以上的地图。在多机器人SLAM应用，地图由不同试剂制成具有不相同的参考帧的空间，这使得在异常值的存在初始化非常困难。本文提出了通过检查转动测量值的几何一致性检测姿态图形优化之前不正确的方向测量值的概率方法。期望最大化用于微调模型参数。作为辅助的贡献，提出了一种新的近似离散推理过程，它使用的环路证据的曲线图，并且基于优化（乘法器的交替方向法）。这种方法可以得到比置信传播效果出众，具有收敛的保证。仿真和实验结果都认为评估异常检测方法的性能和合成和真实数据的推理算法。</font>
</div>


<hr>
<div id="paper19"> <b>19. SideInfNet: A Deep Neural Network for Semi-Automatic Semantic  Segmentation with Side Information</b>  <a href="https://arxiv.org/pdf/2002.02634" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Koh%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Jing Yu Koh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+D+T" target="_blank" rel="noopener" style="color:#0000EE;">Duc Thanh Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Truong%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Quang-Trung Truong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yeung%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sai-Kit Yeung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Binder%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexander Binder</a><br>
<font size="3">
Abstract: Fully-automatic execution is the ultimate goal for many Computer Vision applications. However, this objective is not always realistic in tasks associated with high failure costs, such as medical applications. For these tasks, a compromise between fully-automatic execution and user interactions is often preferred due to desirable accuracy and performance. Semi-automatic methods require minimal effort from experts by allowing them to provide cues that guide computer algorithms. Inspired by the practicality and applicability of the semi-automatic approach, this paper proposes a novel deep neural network architecture, namely SideInfNet that effectively integrates features learnt from images with side information extracted from user annotations to produce high quality semantic segmentation results. To evaluate our method, we applied the proposed network to three semantic segmentation tasks and conducted extensive experiments on benchmark datasets. Experimental results and comparison with prior work have verified the superiority of our model, suggesting the generality and effectiveness of the model in semi-automatic semantic segmentation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：全自动执行是许多计算机视觉应用的终极目标。然而，这个目标并非总是与高失败成本，如医疗应用相关的任务逼真。对于这些任务的，完全自动执行与用户的交互之间的折中通常优选的，因为所希望的精度和性能。半自动方法，让他们提供线索引导计算机算法需要专家最小的努力。本文通过实用性和半自动方法的适用性的启发，提出了一种新颖深层神经网络体系结构，即SideInfNet有效地集成了各种功能从与来自用户的注释提取以生产高品质的语义分割结果侧信息图像获知。为了评估我们的方法，我们应用所提出的网络三个语义分割任务，并进行了基准数据集广泛的实验。实验结果与以前的工作相比，已经验证了我们的模型的优势，这在半自动语义分割模型的通用性和有效性。</font>
</div>


<hr>
<div id="paper20"> <b>20. Visual search over billions of aerial and satellite images</b>  <a href="https://arxiv.org/pdf/2002.02624" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Keisler%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ryan Keisler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Skillman%2C+S+W" target="_blank" rel="noopener" style="color:#0000EE;">Samuel W. Skillman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gonnabathula%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sunny Gonnabathula</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poehnelt%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Poehnelt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rudelis%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xander Rudelis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Warren%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Michael S. Warren</a><br>
<font size="3">
Abstract: We present a system for performing visual search over billions of aerial and satellite images. The purpose of visual search is to find images that are visually similar to a query image. We define visual similarity using 512 abstract visual features generated by a convolutional neural network that has been trained on aerial and satellite imagery. The features are converted to binary values to reduce data and compute requirements. We employ a hash-based search using Bigtable, a scalable database service from Google Cloud. Searching the continental United States at 1-meter pixel resolution, corresponding to approximately 2 billion images, takes approximately 0.1 seconds. This system enables real-time visual search over the surface of the earth, and an interactive demo is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种系统，用于超千亿航空和卫星图像，进行视觉搜索。视觉搜索的目的是找到在视觉上类似于查询图像的图像。我们定义使用经过训练的空中和卫星图像卷积神经网络产生512个抽象的视觉特征视觉相似。特征被转换为二进制值，以减少数据和计算要求。我们采用使用Bigtable的，从谷歌云可扩展的数据库服务基于散列的搜索。搜索美国大陆在1米像素的分辨率，对应于大约2十亿图像，需要大约为0.1秒。这个系统使地球表面上的实时可视化搜索和互动演示可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper21"> <b>21. Image Fine-grained Inpainting</b>  <a href="https://arxiv.org/pdf/2002.02609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hui%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zheng Hui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiumei Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinbo Gao</a><br>
<font size="3">
Abstract: Image inpainting techniques have shown promising improvement with the assistance of generative adversarial networks (GANs) recently. However, most of them often suffered from completed results with unreasonable structure or blurriness. To mitigate this problem, in this paper, we present a one-stage model that utilizes dense combinations of dilated convolutions to obtain larger and more effective receptive fields. Benefited from the property of this network, we can more easily recover large regions in an incomplete image. To better train this efficient generator, except for frequently-used VGG feature matching loss, we design a novel self-guided regression loss for concentrating on uncertain areas and enhancing the semantic details. Besides, we devise a geometrical alignment constraint item to compensate for the pixel-based distance between prediction features and ground-truth ones. We also employ a discriminator with local and global branches to ensure local-global contents consistency. To further improve the quality of generated images, discriminator feature matching on the local branch is introduced, which dynamically minimizes the similarity of intermediate features between synthetic and ground-truth patches. Extensive experiments on several public datasets demonstrate that our approach outperforms current state-of-the-art methods. Code is available at~\url{this https URL}. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图像修复技术已显示出大有希望与生成对抗网络（甘斯）最近的协助改善。然而，大多数人往往是因与结构不合理或模糊完成结果遭遇。为了缓解这个问题，在该论文中，我们提出了利用扩张卷积的致密组合，以获得更大的和更有效的感受域的一阶段的模型。从这个网络的性能中受益，我们可以更容易在不完整的图像恢复大区。为了更好地培养这种高效的发电机，除了常用VGG特征匹配的损失，我们设计了一个新的自导回归亏损集中在不确定的领域，提高语义细节。此外，我们设计的几何对齐约束的项目，以弥补预测的功能和地面实况的人之间基于像素的距离。我们还采用了与本地和全球的分支机构鉴别，以确保地方 - 全球内容的一致性。为了进一步提高生成的图像的质量，对本地分支鉴别特征匹配被引入，其动态地最小化的合成的和地面实况贴片之间的中间特征的相似性。在几个公开的数据集大量的实验证明我们的方法优于国家的最先进的通用方法。代码可以在〜\ {URL这HTTPS URL}。</font>
</div>


<hr>
<div id="paper22"> <b>22. Adaptive Deep Metric Embeddings for Person Re-Identification under  Occlusions</b>  <a href="https://arxiv.org/pdf/2002.02603" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wanxiang Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Si Chen</a><br>
<font size="3">
Abstract: Person re-identification (ReID) under occlusions is a challenging problem in video surveillance. Most of existing person ReID methods take advantage of local features to deal with occlusions. However, these methods usually independently extract features from the local regions of an image without considering the relationship among different local regions. In this paper, we propose a novel person ReID method, which learns the spatial dependencies between the local regions and extracts the discriminative feature representation of the pedestrian image based on Long Short-Term Memory (LSTM), dealing with the problem of occlusions. In particular, we propose a novel loss (termed the adaptive nearest neighbor loss) based on the classification uncertainty to effectively reduce intra-class variations while enlarging inter-class differences within the adaptive neighborhood of the sample. The proposed loss enables the deep neural network to adaptively learn discriminative metric embeddings, which significantly improve the generalization capability of recognizing unseen person identities. Extensive comparative evaluations on challenging person ReID datasets demonstrate the significantly improved performance of the proposed method compared with several state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人重新鉴定（里德）下闭塞是视频监控一个具有挑战性的问题。大多数现有的人里德方法利用的地方特色，以应对闭塞。然而，这些方法通常是独立地从提取的图像的局部区域的特征而没有考虑不同的局部区域之间的关系。在本文中，我们提出了一种新的人雷德法，其学习的局部区域之间的空间的依赖，并提取基于长短期记忆（LSTM）行人图像的判别特征表示，处理阻塞的问题。特别是，我们提出基于分类的不确定性，以有效地减少类内变化，同时增大样本的自适应邻域内类间差异的新型的损失（称为自适应最近邻损失）。所提出的损失使深层神经网络自适应学习判别指标的嵌入，这显著提高认识看不见人身份的泛化能力。上具有挑战性的人里德数据集广泛比较评价证明了该方法的显著改进的性能与国家的最先进的几种方法进行比较。</font>
</div>


<hr>
<div id="paper23"> <b>23. Object-Adaptive LSTM Network for Real-time Visual Tracking with  Adversarial Data Augmentation</b>  <a href="https://arxiv.org/pdf/2002.02598" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yihan Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yan Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Si Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hua%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Hua</a><br>
<font size="3">
Abstract: In recent years, deep learning based visual tracking methods have obtained great success owing to the powerful feature representation ability of Convolutional Neural Networks (CNNs). Among these methods, classification-based tracking methods exhibit excellent performance while their speeds are heavily limited by the expensive computation for massive proposal feature extraction. In contrast, matching-based tracking methods (such as Siamese networks) possess remarkable speed superiority. However, the absence of online updating renders these methods unadaptable to significant object appearance variations. In this paper, we propose a novel real-time visual tracking method, which adopts an object-adaptive LSTM network to effectively capture the video sequential dependencies and adaptively learn the object appearance variations. For high computational efficiency, we also present a fast proposal selection strategy, which utilizes the matching-based tracking method to pre-estimate dense proposals and selects high-quality ones to feed to the LSTM network for classification. This strategy efficiently filters out some irrelevant proposals and avoids the redundant computation for feature extraction, which enables our method to operate faster than conventional classification-based tracking methods. In addition, to handle the problems of sample inadequacy and class imbalance during online tracking, we adopt a data augmentation technique based on the Generative Adversarial Network (GAN) to facilitate the training of the LSTM network. Extensive experiments on four visual tracking benchmarks demonstrate the state-of-the-art performance of our method in terms of both tracking accuracy and speed, which exhibits great potentials of recurrent structures for visual tracking. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近年来，深度学习基于视觉跟踪方法已获得由于卷积神经网络（细胞神经网络）的强大功能表现能力，取得巨大成功。在这些方法中，基于分类的跟踪方法表现出优异的性能，而他们的速度很大程度上受到了大量的建议特征提取昂贵的计算限制。相反，基于匹配追踪方法（如连体网络）具有显着的速度优势。然而，不存在在线更新的呈现这些方法不能适应显著对象的外观的变化。在本文中，我们提出了一种新的实时视觉跟踪方法，即采用一个目的自适应LSTM网络有效地捕捉视频顺序依赖性和自适应学习对象的外观的变化。对于高计算效率，我们还提出了一种快速建议选择策略，其利用基于匹配追踪方法预先估计密提案和选择高品质的那些，以进料LSTM网络进行分类。这种策略有效地过滤掉一些不相关的建议，并避免了特征提取，这使得我们的方法比传统的基于分类的跟踪方法更快地操作冗余计算。此外，处理样品不足和不平衡类在线跟踪过程中的问题，我们采用了基于创成对抗性网络（GAN）的数据增强技术，以方便LSTM网络的培训。在四个视觉跟踪基准广泛的实验表明在这两种跟踪准确性和速度，其表现出对视觉跟踪复发性结构的巨大潜力方面我们的方法的状态的最先进的性能。</font>
</div>


<hr>
<div id="paper24"> <b>24. Poisson Kernel Avoiding Self-Smoothing in Graph Convolutional Networks</b>  <a href="https://arxiv.org/pdf/2002.02589" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqing Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shoudong Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Zhao</a><br>
<font size="3">
Abstract: Graph convolutional network (GCN) is now an effective tool to deal with non-Euclidean data, such as social networks in social behavior analysis, molecular structure analysis in the field of chemistry, and skeleton-based action recognition. Graph convolutional kernel is one of the most significant factors in GCN to extract nodes' feature, and some improvements of it have reached promising performance theoretically and experimentally. However, there is limited research about how exactly different data types and graph structures influence the performance of these kernels. Most existing methods used an adaptive convolutional kernel to deal with a given graph structure, which still not reveals the internal reasons. In this paper, we started from theoretical analysis of the spectral graph and studied the properties of existing graph convolutional kernels. While taking some designed datasets with specific parameters into consideration, we revealed the self-smoothing phenomenon of convolutional kernels. After that, we proposed the Poisson kernel that can avoid self-smoothing without training any adaptive kernel. Experimental results demonstrate that our Poisson kernel not only works well on the benchmark dataset where state-of-the-art methods work fine, but also is evidently superior to them in synthetic datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图形卷积网络（GCN）现在是处理非欧几里得数据，如社会行为分析社交网络，在化学领域的分子结构分析，以及基于骨架动作识别的有效工具。图卷积内核的GCN以提取节点的功能，最显著的因素之一，而它的一些改进，已经达到了理论和实验有前途的性能。然而，有关数据类型和图形结构究竟如何影响不同这些内核的性能有限的研究。大多数现有的方法中使用的自适应卷积内核来处理一个给定的图形结构，仍然没有揭示的内在原因。在本文中，我们从谱图的理论分析开始，研究了现有的图形内核卷积的性质。虽然采取了一些设计数据集以特定参数加以考虑，我们揭示了卷积核的自流平现象。在那之后，我们提出的泊松内核，可避免自平滑无任何训练适应核。实验结果表明，我们的泊松内核不仅行之有效的基准数据集，其中国家的最先进的方法，做工精细，而且是在合成数据集明显优于它们。</font>
</div>


<hr>
<div id="paper25"> <b>25. Learning Hyperspectral Feature Extraction and Classification with  ResNeXt Network</b>  <a href="https://arxiv.org/pdf/2002.02585" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nyasaka%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Divinah Nyasaka</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jing Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tinega%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haron Tinega</a><br>
<font size="3">
Abstract: The Hyperspectral image (HSI) classification is a standard remote sensing task, in which each image pixel is given a label indicating the physical land-cover on the earth's surface. The achievements of image semantic segmentation and deep learning approaches on ordinary images have accelerated the research on hyperspectral image classification. Moreover, the utilization of both the spectral and spatial cues in hyperspectral images has shown improved classification accuracy in hyperspectral image classification. The use of only 3D Convolutional Neural Networks (3D-CNN) to extract both spatial and spectral cues from Hyperspectral images results in an explosion of parameters hence high computational cost. We propose network architecture called the MixedSN that utilizes the 3D convolutions to modeling spectral-spatial information in the early layers of the architecture and the 2D convolutions at the top layers which majorly deal with semantic abstraction. We constrain our architecture to ResNeXt block because of their performance and simplicity. Our model drastically reduced the number of parameters and achieved comparable classification performance with state-of-the-art methods on Indian Pine (IP) scene dataset, Pavia University scene (PU) dataset, Salinas (SA) Scene dataset, and Botswana (BW) dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：高光谱图像（HSI）分类是一个标准的远程感测任务，其中，每个图像像素被赋予了标签指示在地球表面上的物理土地覆盖。图像语义分割和深学习方法对普通图像的成就，加速了对高光谱影像分类研究。此外，光谱和空间线索两者在高光谱图像的利用率已经显示出在高光谱图像分类改进的分类精度。仅使用三维卷积神经网络（3D-CNN）的提取从高光谱图像的结果的空间和频谱线索在的参数因此具有高的计算成本爆炸。我们建议网络架构，名为利用三维回旋在顶层这majorly处理语义抽象建模架构的早期层和二维卷积谱空间信息MixedSN。我们限制，因为它们的性能和简单了系统架构以ResNeXt块。我们的模型大大减少参数的数量，并实现与印度的松树（IP）的场景数据集的国家的最先进的方法相同的分类性能，帕维亚大学场景（PU）的数据集，萨利纳斯（SA）场景的数据集，以及博茨瓦纳（BW ）数据集。</font>
</div>


<hr>
<div id="paper26"> <b>26. Impact of ImageNet Model Selection on Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2002.02559" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youshan Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Davison%2C+B+D" target="_blank" rel="noopener" style="color:#0000EE;">Brian D. Davison</a><br>
<font size="3">
Abstract: Deep neural networks are widely used in image classification problems. However, little work addresses how features from different deep neural networks affect the domain adaptation problem. Existing methods often extract deep features from one ImageNet model, without exploring other neural networks. In this paper, we investigate how different ImageNet models affect transfer accuracy on domain adaptation problems. We extract features from sixteen distinct pre-trained ImageNet models and examine the performance of twelve benchmarking methods when using the features. Extensive experimental results show that a higher accuracy ImageNet model produces better features, and leads to higher accuracy on domain adaptation problems (with a correlation coefficient of up to 0.95). We also examine the architecture of each neural network to find the best layer for feature extraction. Together, performance from our features exceeds that of the state-of-the-art in three benchmark datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：深层神经网络被广泛应用于图像分类问题。然而，很少工作地址是如何从不同的深层神经网络的功能影响领域适应性问题。现有的方法常从一个ImageNet模型深的特点，没有探索其他神经网络。在本文中，我们研究了不同型号ImageNet如何影响域的适应问题传递的准确性。我们提取从16不同的预先训练ImageNet机型的功能和使用功能检查时，十二基准方法的性能。广泛的实验结果表明，较高的精度ImageNet模型产生更好的功能，并导致更高的准确度上域的适应的问题（与最多的相关系数0.95）。我们还检查每个神经网络的体系结构，以找到特征提取的最佳层。总之，从我们的特色性能超过了国家的最先进的三个地基准数据集。</font>
</div>


<hr>
<div id="paper27"> <b>27. Opposite Structure Learning for Semi-supervised Domain Adaptation</b>  <a href="https://arxiv.org/pdf/2002.02545" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Can Qin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lichen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianqian Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Yin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Huan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun Fu</a><br>
<font size="3">
Abstract: Current adversarial adaptation methods attempt to align the cross-domain features whereas two challenges remain unsolved: 1) conditional distribution mismatch between different domains and 2) the bias of decision boundary towards the source domain. To solve these challenges, we propose a novel framework for semi-supervised domain adaptation by unifying the learning of opposite structures (UODA). UODA consists of a generator and two classifiers (i.e., the source-based and the target-based classifiers respectively) which are trained with opposite forms of losses for a unified object. The target-based classifier attempts to cluster the target features to improve intra-class density and enlarge inter-class divergence. Meanwhile, the source-based classifier is designed to scatter the source features to enhance the smoothness of decision boundary. Through the alternation of source-feature expansion and target-feature clustering procedures, the target features are well-enclosed within the dilated boundary of the corresponding source features. This strategy effectively makes the cross-domain features precisely aligned. To overcome the model collapse through training, we progressively update the measurement of distance and the feature representation on both domains via an adversarial training paradigm. Extensive experiments on the benchmarks of DomainNet and Office-home datasets demonstrate the effectiveness of our approach over the state-of-the-art method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当前对抗性适应方法试图对准跨域特征而两个挑战仍然没有解决：1）不同的结构域和2之间条件分布不匹配）决策边界的偏置朝向源域。为了解决这些难题，我们通过统一相反的结构（UODA）的学习提出了半监督领域适应一个新的框架。 UODA由发电机和两个分类器（即，基于源和分别与基于目标的分类器），其与一个统一的对象损失相对形式的训练。基于目标的分类器试图群集目标功能，以提高的类内的密度和放大级间发散性。同时，基于源代码的分类被设计成散射源功能，以提高决策边界的平滑度。通过源极 - 功能扩展和目标特征聚类程序的交替，所述目标特征的对应的源特征的扩张型边界内孔封闭。这种策略有效地使交叉域特征精确地对准。为了克服通过培训模式崩溃，我们不断更新的距离的测量，并通过对抗性训练模式在两个域的特征表示。在DomainNet和Office家庭数据集的基准广泛的实验，证明了我们在国家的最先进的方法，该方法的有效性。</font>
</div>


<hr>
<div id="paper28"> <b>28. Continuous Geodesic Convolutions for Learning on 3D Shapes</b>  <a href="https://arxiv.org/pdf/2002.02506" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhangsihao Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Litany%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Or Litany</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Birdal%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tolga Birdal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sridhar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Srinath Sridhar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guibas%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leonidas Guibas</a><br>
<font size="3">
Abstract: The majority of descriptor-based methods for geometric processing of non-rigid shape rely on hand-crafted descriptors. Recently, learning-based techniques have been shown effective, achieving state-of-the-art results in a variety of tasks. Yet, even though these methods can in principle work directly on raw data, most methods still rely on hand-crafted descriptors at the input layer. In this work, we wish to challenge this practice and use a neural network to learn descriptors directly from the raw mesh. To this end, we introduce two modules into our neural architecture. The first is a local reference frame (LRF) used to explicitly make the features invariant to rigid transformations. The second is continuous convolution kernels that provide robustness to sampling. We show the efficacy of our proposed network in learning on raw meshes using two cornerstone tasks: shape matching, and human body parts segmentation. Our results show superior results over baseline methods that use hand-crafted descriptors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大多数的非刚性形状的几何处理基于描述符的方法依赖于手工制作的描述符。近年来，基于学习的技术已被证明有效，实现多种任务的国家的最先进的成果。然而，尽管这些方法可以直接在原始数据的原理工作的，大多数方法还是依靠在输入层手工制作的描述符。在这项工作中，我们要挑战这一做法，并用神经网络直接从原网学习描述。为此，我们引入两个模块到我们的神经结构。第一种是用于显式地使功能不变的刚性变换的本地参考帧（LRF）。第二个是连续卷积核，要采样提供鲁棒性。我们发现在学习上使用两个基石任务原料网我们提出的网络的功效：人体部位分割形状匹配和。我们的研究结果表明在基线的方法是用手工制作的描述效果出众。</font>
</div>


<hr>
<div id="paper29"> <b>29. Activation Density driven Energy-Efficient Pruning in Training</b>  <a href="https://arxiv.org/pdf/2002.02949" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Foldy-Porto%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Timothy Foldy-Porto</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Panda%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Priyadarshini Panda</a><br>
<font size="3">
Abstract: The process of neural network pruning with suitable fine-tuning and retraining can yield networks with considerably fewer parameters than the original with comparable degrees of accuracy. Typically, pruning methods require large, pre-trained networks as a starting point from which they perform a time-intensive iterative pruning and retraining algorithm. We propose a novel pruning in-training method that prunes a network real-time during training, reducing the overall training time to achieve an optimal compressed network. To do so, we introduce an activation density based analysis that identifies the optimal relative sizing or compression for each layer of the network. Our method removes the need for pre-training and is architecture agnostic, allowing it to be employed on a wide variety of systems. For VGG-19 and ResNet18 on CIFAR-10, CIFAR-100, and TinyImageNet, we obtain exceedingly sparse networks (up to 200x reduction in parameters and >60x reduction in inference compute operations in the best case) with comparable accuracies (up to 2%-3% loss with respect to the baseline network). By reducing the network size periodically during training, we achieve total training times that are shorter than those of previously proposed pruning methods. Furthermore, training compressed networks at different epochs with our proposed method yields considerable reduction in training compute complexity (1.6x -3.2x lower) at near iso-accuracy as compared to a baseline network trained entirely from scratch. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：与合适的微调神经网络修剪和再培训可以产生网络具有比具有可比较的精确度的原始参数相当少的方法。典型地，修剪方法需要大的，预训练的网络与其所执行时间密集的迭代修剪和再培训算法的起点。我们提出了一个新的修剪在训练方法训练李子期间网络的实时性，降低整体的训练时间，以达到最佳的压缩网络。要做到这一点，我们引入一个激活基于密度分析标识所述最佳相对尺寸或压缩为网络的每个层。我们的方法消除了对预训练的必要性和架构是不可知的，允许它被在各种各样的系统中采用。为VGG-19和ResNet18上CIFAR-10，CIFAR-100，和TinyImageNet，我们得到极其稀疏的网络（高达参数200X减少和> 60倍的减少在推理计算操作在最佳情况下）具有可比较的精度（最多2个％-3相对于基线网络％的损失）。通过培训期间定期降低了网络规模，我们实现了总的训练时间是比那些先前提出的修剪方法更短。此外，相比于完全从头培养了基线网络训练在与我们在接近异精度提出的方法的产率显着降低在训练计算复杂度（1.6倍-3.2x降低）不同时期压缩网络。</font>
</div>


<hr>
<div id="paper30"> <b>30. AnimePose: Multi-person 3D pose estimation and animation</b>  <a href="https://arxiv.org/pdf/2002.02792" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumarapu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laxman Kumarapu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mukherjee%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prerana Mukherjee</a><br>
<font size="3">
Abstract: 3D animation of humans in action is quite challenging as it involves using a huge setup with several motion trackers all over the person's body to track the movements of every limb. This is time-consuming and may cause the person discomfort in wearing exoskeleton body suits with motion sensors. In this work, we present a trivial yet effective solution to generate 3D animation of multiple persons from a 2D video using deep learning. Although significant improvement has been achieved recently in 3D human pose estimation, most of the prior works work well in case of single person pose estimation and multi-person pose estimation is still a challenging problem. In this work, we firstly propose a supervised multi-person 3D pose estimation and animation framework namely AnimePose for a given input RGB video sequence. The pipeline of the proposed system consists of various modules: i) Person detection and segmentation, ii) Depth Map estimation, iii) Lifting 2D to 3D information for person localization iv) Person trajectory prediction and human pose tracking. Our proposed system produces comparable results on previous state-of-the-art 3D multi-person pose estimation methods on publicly available datasets MuCo-3DHP and MuPoTS-3D datasets and it also outperforms previous state-of-the-art human pose tracking methods by a significant margin of 11.7% performance gain on MOTA score on Posetrack 2018 dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在行动人的3D动画是相当具有挑战性的，因为它涉及到使用一个巨大的设置与几个运动跟踪器遍布人的身体来跟踪每一个肢体的运动。这是耗时的，并且可能导致人不适穿着外骨骼紧身衣与运动传感器。在这项工作中，我们提出了一个平凡而有效的解决方案以使用深度学习从2D视频多人的3D动画。虽然显著的改善已经在3D人体姿势估计最近取得，大部分之前的作品在一个人的姿态估计和多方人士的姿势估计的情况下很好地工作仍然是一个具有挑战性的问题。在这项工作中，我们首先提出了一个给定的输入RGB视频序列的监督多人3D姿态估计和动画框架，即AnimePose。所提出的系统的流水线由各种模块组成：i）人检测和分割，ⅱ）深度图估计，ⅲ）起重2D到3D信息用于人本地化ⅳ）人轨迹预测和人类姿态跟踪。我们所提出的系统产生的可公开获得的数据集以前的状态的最先进的3D多人姿势估计方法粘膜3DHP和MuPoTS-3D数据集比较的结果，同时也优于国家的最先进的前面的人体姿势的跟踪方法通过对MOTA 11.7％的性能增益显著保证金得分Posetrack 2018集。</font>
</div>


<hr>
<div id="paper31"> <b>31. Trust Your Model: Iterative Label Improvement and Robust Training by  Confidence Based Filtering and Dataset Partitioning</b>  <a href="https://arxiv.org/pdf/2002.02705" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Haase-Sch%C3%BCtz%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christian Haase-Schütz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stal%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rainer Stal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hertlein%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heinz Hertlein</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sick%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernhard Sick</a><br>
<font size="3">
Abstract: State-of-the-art, high capacity deep neural networks not only require large amounts of labelled training data, they are also highly susceptible to label errors in this data, typically resulting in large efforts and costs and therefore limiting the applicability of deep learning. To alleviate this issue, we propose a novel meta training and labelling scheme that is able to use inexpensive unlabelled data by taking advantage of the generalization power of deep neural networks. We show experimentally that by solely relying on one network architecture and our proposed scheme of iterative training and prediction steps, both label quality and resulting model accuracy can be improved significantly. Our method achieves state-of-the-art results, while being architecture agnostic and therefore broadly applicable. Compared to other methods dealing with erroneous labels, our approach does neither require another network to be trained, nor does it necessarily need an additional, highly accurate reference label set. Instead of removing samples from a labelled set, our technique uses additional sensor data without the need for manual labelling. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：国家的最先进的，高容量的深层神经网络，不仅需要大量的标记的训练数据，他们也很容易受到标签错误，在此数据，通常会生成大量的努力和成本，并因此限制的适用性深度学习。为了缓解这一问题，我们提出了一个新颖元的培训和标签计划，能够通过利用深层神经网络的推广力量的优势，使用廉价的未标记的数据。我们实验表明，单纯依靠一个网络架构和我们所提出的迭代训练和预测的步骤方案，无论是标签质量和得到的模型精度可以提高显著。我们的方法实现状态的最先进的结果，而被架构无关，因此广泛适用的。相比于处理错误标签的其他方法，我们的做法既没有要求其他网络进行训练，也不一定需要一个额外的，高度准确的参考符号集。而不是从标记组取出样品，我们的技术使用附加的传感器数据，而无需手动标记。</font>
</div>


<hr>
<div id="paper32"> <b>32. Optimization of Structural Similarity in Mathematical Imaging</b>  <a href="https://arxiv.org/pdf/2002.02657" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/math?searchtype=author&query=Otero%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">D. Otero</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=La+Torre%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">D. La Torre</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Michailovich%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">O. Michailovich</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&query=Vrscay%2C+E+R" target="_blank" rel="noopener" style="color:#0000EE;">E.R. Vrscay</a><br>
<font size="3">
Abstract: It is now generally accepted that Euclidean-based metrics may not always adequately represent the subjective judgement of a human observer. As a result, many image processing methodologies have been recently extended to take advantage of alternative visual quality measures, the most prominent of which is the Structural Similarity Index Measure (SSIM). The superiority of the latter over Euclidean-based metrics have been demonstrated in several studies. However, being focused on specific applications, the findings of such studies often lack generality which, if otherwise acknowledged, could have provided a useful guidance for further development of SSIM-based image processing algorithms. Accordingly, instead of focusing on a particular image processing task, in this paper, we introduce a general framework that encompasses a wide range of imaging applications in which the SSIM can be employed as a fidelity measure. Subsequently, we show how the framework can be used to cast some standard as well as original imaging tasks into optimization problems, followed by a discussion of a number of novel numerical strategies for their solution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：现在人们普遍认为，基于欧几里得度量可以不总是充分代表人类观察者的主观判断。其结果是，许多图像处理方法已经扩展最近采取的另类视觉质量的措施，其中最突出的是结构相似度指数度量（SSIM）的优势。后者通过基于欧几里得度量的优越性已被证明在一些研究。然而，被集中在特定的应用程序，这些研究的结果往往缺乏，如果其它方法确认，可能会对基于SSIM图像处理算法的进一步发展提供了有益的指导普遍性。因此，而不是集中在一个特定的图像处理任务，在本文中，我们引入包括宽范围的，其中，SSIM可以用作一个保真度测度成像应用的一般框架。随后，我们展示了框架如何可以用来施放一些标准以及原始成像任务为优化问题，其次是一些对他们的解决方案的新的数字战略的讨论。</font>
</div>


<hr>
<div id="paper33"> <b>33. Quantifying the Value of Lateral Views in Deep Learning for Chest X-rays</b>  <a href="https://arxiv.org/pdf/2002.02582" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Hashir%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Hashir</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Bertrand%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hadrien Bertrand</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Cohen%2C+J+P" target="_blank" rel="noopener" style="color:#0000EE;">Joseph Paul Cohen</a><br>
<font size="3">
Abstract: Most deep learning models in chest X-ray prediction utilize the posteroanterior (PA) view due to the lack of other views available. PadChest is a large-scale chest X-ray dataset that has almost 200 labels and multiple views available. In this work, we use PadChest to explore multiple approaches to merging the PA and lateral views for predicting the radiological labels associated with the X-ray image. We find that different methods of merging the model utilize the lateral view differently. We also find that including the lateral view increases performance for 32 labels in the dataset, while being neutral for the others. The increase in overall performance is comparable to the one obtained by using only the PA view with twice the amount of patients in the training set. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：胸片预测最深刻的学习模型，利用后前（PA）视图由于缺乏可用的其他意见。 PadChest的是，有近200个标签和多视图提供一个大型的胸部X射线数据集。在这项工作中，我们使用PadChest探索多种方法来合并PA和横向视图预测与X射线图像有关的放射性标签。我们发现合并模型利用横向视图不同的，不同的方法。我们还发现，包括横向视图提高性能，在数据集32级的标签，同时保持中立的人。整体性能的增加是与通过仅使用PA视图与在训练集中两次患者的量而得到的一个。</font>
</div>


<hr>
<div id="paper34"> <b>34. Closing the Dequantization Gap: PixelCNN as a Single-Layer Flow</b>  <a href="https://arxiv.org/pdf/2002.02547" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title34" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nielsen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Didrik Nielsen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Winther%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Ole Winther</a><br>
<font size="3">
Abstract: Flow models have recently made great progress at modeling quantized sensor data such as images and audio. Due to the continuous nature of flow models, dequantization is typically applied when using them for such quantized data. In this paper, we propose subset flows, a class of flows which can tractably transform subsets of the input space in one pass. As a result, they can be applied directly to quantized data without the need for dequantization. Based on this class of flows, we present a novel interpretation of several existing autoregressive models, including WaveNet and PixelCNN, as single-layer flow models defined through an invertible transformation between uniform noise and data samples. This interpretation suggests that these existing models, 1) admit a latent representation of data and 2) can be stacked in multiple flow layers. We demonstrate this by exploring the latent space of a PixelCNN and by stacking PixelCNNs in multiple flow layers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：流模型建模时量化传感器数据，如图像和音频最近取得了很大进展。由于流模型的连续性质，使用它们用于这种量化的数据去量化时典型地施加。在本文中，我们提议子集流动，一类可tractably将输入空间的子集在一个通流。其结果是，它们可以被直接应用到量化数据，而不需要反量化。基于此类流中，我们提出的几种现有的自回归模型，包括WaveNet和PixelCNN，作为单层流模型通过均匀噪声和数据样本之间的可逆变换中定义的新的解释。这种解释表明，这些现有的模型，1）承认数据和2的潜表示）可以在多个流动层堆叠。我们通过探讨PixelCNN的潜在空间，并通过在多个流程层层积PixelCNNs证明这一点。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-10</title>
    <url>/2020/02/10/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-10/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> A Multilingual View of Unsupervised Machine Translation <a href="https://arxiv.org/pdf/2002.02955" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> BERT-of-Theseus: Compressing BERT by Progressive Module Replacing <a href="https://arxiv.org/pdf/2002.02925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Neural Machine Translation System of Indic Languages -- An Attention  based Approach <a href="https://arxiv.org/pdf/2002.02758" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> On-Device Information Extraction from SMS using Hybrid Hierarchical  Classification <a href="https://arxiv.org/pdf/2002.02755" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Incorporating Visual Semantics into Sentence Representations within a  Grounded Space <a href="https://arxiv.org/pdf/2002.02734" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Multimodal Matching Transformer for Live Commenting <a href="https://arxiv.org/pdf/2002.02649" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Translating Web Search Queries into Natural Language Questions <a href="https://arxiv.org/pdf/2002.02631" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Introducing Aspects of Creativity in Automatic Poetry Generation <a href="https://arxiv.org/pdf/2002.02511" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Goal-Oriented Multi-Task BERT-Based Dialogue State Tracker <a href="https://arxiv.org/pdf/2002.02450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> I love your chain mail! Making knights smile in a fantasy game world:  Open-domain goal-orientated dialogue agents <a href="https://arxiv.org/pdf/2002.02878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Unsupervised pretraining transfers well across languages <a href="https://arxiv.org/pdf/2002.02848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Depressed individuals express more distorted thinking on social media <a href="https://arxiv.org/pdf/2002.02800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> LEAP System for SRE19 Challenge -- Improvements and Error Analysis <a href="https://arxiv.org/pdf/2002.02735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Transformer Transducer: A Streamable Speech Recognition Model with  Transformer Encoders and RNN-T Loss <a href="https://arxiv.org/pdf/2002.02562" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Robust Multi-channel Speech Recognition using Frequency Aligned Network <a href="https://arxiv.org/pdf/2002.02520" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Consistency of a Recurrent Language Model With Respect to Incomplete  Decoding <a href="https://arxiv.org/pdf/2002.02492" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. A Multilingual View of Unsupervised Machine Translation</b>  <a href="https://arxiv.org/pdf/2002.02955" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Garcia%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xavier Garcia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Foret%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre Foret</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sellam%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thibault Sellam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Parikh%2C+A+P" target="_blank" rel="noopener" style="color:#0000EE;">Ankur P. Parikh</a><br>
<font size="3">
Abstract: We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one language in the (source, target) pair is not associated with any parallel data, but there may exist auxiliary parallel data that contains the other. This auxiliary data can naturally be utilized in our probabilistic framework via a novel cross-translation loss term. Empirically, we show that our approach results in higher BLEU scores over state-of-the-art unsupervised models on the WMT'14 English-French, WMT'16 English-German, and WMT'16 English-Romanian datasets in most directions. In particular, we obtain a +1.65 BLEU advantage over the best-performing unsupervised model in the Romanian-English direction. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出了多语种神经机器翻译概率框架，包括监管和监督的设置，注重监督的翻译。除了仅存在单语数据可用的研究香草情况下，我们提出了其中在（源，目标）一种语言对不与任何并行数据相关联的新的设置，但也有可能存在包含其它辅助的并行数据。该辅助数据可以自然地在我们的概率框架通过一种新颖的横翻译损耗项利用。根据经验，我们表明，我们的方法得到更高的分数BLEU在国家的最先进的无人监督的车型上WMT'14英法，WMT'16英语 - 德语和英语WMT'16  - 罗马尼亚数据集在大部分方向。特别是，我们获得了在罗马尼亚英语方向表现最好的无监督模型1.65 BLEU优势。</font>
</div>


<hr>
<div id="paper2"> <b>2. BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</b>  <a href="https://arxiv.org/pdf/2002.02925" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Canwen Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangchunshu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models, and smooths the training process. Compared to the previous knowledge distillation approaches for BERT compression, our approach leverages only one loss function and one hyper-parameter, liberating human effort from hyper-parameter tuning. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们建议逐步模块更换一个新的模型的压缩方式，有效压缩BERT。我们的方法首先将原始BERT分成几个模块，并建立其紧凑的替代品。然后，我们随机与他们的替代品代替原来的模块的紧凑型模块训练到原来模块的模仿行为。我们不断通过培训提高替代的可能性。这样一来，我们的方法所带来的原始和紧凑车型之间的相互作用更深层次的，和平滑的训练过程。相较于以前的知识蒸馏方法用于BERT压缩，我们的方法利用只有一个损失函数和一个超参数，释放从高参数整定人的努力。我们的方法比现有的知识蒸馏方法胶水标杆，展示模型压缩的一个新的视角。</font>
</div>


<hr>
<div id="paper3"> <b>3. Neural Machine Translation System of Indic Languages -- An Attention  based Approach</b>  <a href="https://arxiv.org/pdf/2002.02758" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Parth Shah</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bakrola%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vishvajit Bakrola</a><br>
<font size="3">
Abstract: Neural machine translation (NMT) is a recent and effective technique which led to remarkable improvements in comparison of conventional machine translation techniques. Proposed neural machine translation model developed for the Gujarati language contains encoder-decoder with attention mechanism. In India, almost all the languages are originated from their ancestral language Sanskrit. They are having inevitable similarities including lexical and named entity similarity. Translating into Indic languages is always be a challenging task. In this paper, we have presented the neural machine translation system (NMT) that can efficiently translate Indic languages like Hindi and Gujarati that together covers more than 58.49 percentage of total speakers in the country. We have compared the performance of our NMT model with automatic evaluation matrices such as BLEU, perplexity and TER matrix. The comparison of our network with Google translate is also presented where it outperformed with a margin of 6 BLEU score on English-Gujarati translation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经机器翻译（NMT）是最近的和有效的技术，其导致显着改善在常规机器翻译技术相比。在古吉拉特语语言开发的建议神经机器翻译模型包含编码器，解码器，注意机制。在印度，几乎所有的语言都源于他们祖先的语言梵语。他们有着必然的相似，包括词汇和命名实体的相似性。翻译成印度语始终是一项艰巨的任务。在本文中，我们提出了神经机器翻译系统（NMT），可以有效地翻译印度语像印地文和古吉拉特一起覆盖全国总扬声器超过58.49百分比。我们比较我们与自动评估NMT模型的性能矩阵如BLEU，困惑和TER矩阵。还提出了我们与谷歌翻译网络的比较在那里与6 BLEU得分上英语翻译古吉拉特语保证金跑赢。</font>
</div>


<hr>
<div id="paper4"> <b>4. On-Device Information Extraction from SMS using Hybrid Hierarchical  Classification</b>  <a href="https://arxiv.org/pdf/2002.02755" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Vatsal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shubham Vatsal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Purre%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Naresh Purre</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moharana%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sukumar Moharana</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramena%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gopi Ramena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohanty%2C+D+P" target="_blank" rel="noopener" style="color:#0000EE;">Debi Prasanna Mohanty</a><br>
<font size="3">
Abstract: Cluttering of SMS inbox is one of the serious problems that users today face in the digital world where every online login, transaction, along with promotions generate multiple SMS. This problem not only prevents users from searching and navigating messages efficiently but often results in users missing out the relevant information associated with the corresponding SMS like offer codes, payment reminders etc. In this paper, we propose a unique architecture to organize and extract the appropriate information from SMS and further display it in an intuitive template. In the proposed architecture, we use a Hybrid Hierarchical Long Short Term Memory (LSTM)-Convolutional Neural Network (CNN) to categorize SMS into multiple classes followed by a set of entity parsers used to extract the relevant information from the classified message. The architecture using its preprocessing techniques not only takes into account the enormous variations observed in SMS data but also makes it efficient for its on-device (mobile phone) functionalities in terms of inference timing and size. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：短信收件箱的杂波环境下是严重的问题之一是用户面对今天的数字世界里，所有的在线登录，交易，以得到提拔生成多个短信。这个问题不仅防止用户搜索和浏览效率消息，但通常会导致用户错过了与像优惠代码相应的SMS相关联的相关信息，催款等。在本文中，我们提出了一个独特的体系结构来组织和提取相应的以直观的模板从SMS，并进一步显示它的信息。在所提出的架构中，我们使用了基于分层长短期记忆（LSTM）-Convolutional神经网络（CNN）归类短信到多个类，然后一组用于提取分类信息相关的信息实体解析器。使用它的预处理技术的架构不仅考虑到了SMS数据中观察到的巨大的变化，但也使得有效用于推理定时和尺寸方面及其对设备（移动电话）的功能。</font>
</div>


<hr>
<div id="paper5"> <b>5. Incorporating Visual Semantics into Sentence Representations within a  Grounded Space</b>  <a href="https://arxiv.org/pdf/2002.02734" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bordes%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Bordes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zablocki%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eloi Zablocki</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soulier%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laure Soulier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Piwowarski%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benjamin Piwowarski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gallinari%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Patrick Gallinari</a><br>
<font size="3">
Abstract: Language grounding is an active field aiming at enriching textual representations with visual information. Generally, textual and visual elements are embedded in the same representation space, which implicitly assumes a one-to-one correspondence between modalities. This hypothesis does not hold when representing words, and becomes problematic when used to learn sentence representations --- the focus of this paper --- as a visual scene can be described by a wide variety of sentences. To overcome this limitation, we propose to transfer visual information to textual representations by learning an intermediate representation space: the grounded space. We further propose two new complementary objectives ensuring that (1) sentences associated with the same visual content are close in the grounded space and (2) similarities between related elements are preserved across modalities. We show that this model outperforms the previous state-of-the-art on classification and semantic relatedness tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言接地是一个活跃的领域，旨在丰富文本表示视觉信息。一般地，文本和视觉元素嵌入在相同的表示空间，这隐含地假设模态之间的一对一的对应关系。代表句话的时候这个假设不成立，并且在使用时要学会一句表述---本文的重点---作为一个视觉场景可以通过各种各样的句子来描述成为问题。为了克服这种局限性，我们提出通过学习中间表示空间的视觉信息传递到文本表示：接地的空间。我们进一步提出了两种新补充的目标，确保用相同的视觉内容相关：（1）句子接近接地的空间和（2）的相关要素之间的相似跨形式保留。我们表明，这种模型优于以前的分类和语义相关任务的国家的最先进的。</font>
</div>


<hr>
<div id="paper6"> <b>6. Multimodal Matching Transformer for Live Commenting</b>  <a href="https://arxiv.org/pdf/2002.02649" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chaoqun Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cui%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lei Cui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shuming Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Conghui Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tiejun Zhao</a><br>
<font size="3">
Abstract: Automatic live commenting aims to provide real-time comments on videos for viewers. It encourages users engagement on online video sites, and is also a good benchmark for video-to-text generation. Recent work on this task adopts encoder-decoder models to generate comments. However, these methods do not model the interaction between videos and comments explicitly, so they tend to generate popular comments that are often irrelevant to the videos. In this work, we aim to improve the relevance between live comments and videos by modeling the cross-modal interactions among different modalities. To this end, we propose a multimodal matching transformer to capture the relationships among comments, vision, and audio. The proposed model is based on the transformer framework and can iteratively learn the attention-aware representations for each modality. We evaluate the model on a publicly available live commenting dataset. Experiments show that the multimodal matching transformer model outperforms the state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自动活评论旨在对影片为观众提供实时评论。它鼓励对在线视频网站的用户参与，并且也是视频到文本生成一个很好的标杆。此任务最近的工作，采用编码器，解码器模型来生成评论。然而，这些方法没有视频和评论之间的相互作用明确建模，因此他们往往会产生流行的评论说，往往无关的视频。在这项工作中，我们的目标是通过模拟不同方式之间的跨模态的相互作用，以提高现场评论和视频之间的相关性。为此，我们提出了一种多模式匹配变压器捕捉到的意见，视觉和音频之间的关系。该模型是基于变压器的框架，并可以反复学习注意力感知表示每个模式。我们评估在公开的现场评论数据集模型。实验表明，该多模态匹配变压器模型优于国家的最先进的方法。</font>
</div>


<hr>
<div id="paper7"> <b>7. Translating Web Search Queries into Natural Language Questions</b>  <a href="https://arxiv.org/pdf/2002.02631" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adarsh Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dandapat%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sandipan Dandapat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chordia%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sushil Chordia</a><br>
<font size="3">
Abstract: Users often query a search engine with a specific question in mind and often these queries are keywords or sub-sentential fragments. For example, if the users want to know the answer for "What's the capital of USA", they will most probably query "capital of USA" or "USA capital" or some keyword-based variation of this. For example, for the user entered query "capital of USA", the most probable question intent is "What's the capital of USA?". In this paper, we are proposing a method to generate well-formed natural language question from a given keyword-based query, which has the same question intent as the query. Conversion of keyword-based web query into a well-formed question has lots of applications, with some of them being in search engines, Community Question Answering (CQA) website and bots communication. We found a synergy between query-to-question problem with standard machine translation(MT) task. We have used both Statistical MT (SMT) and Neural MT (NMT) models to generate the questions from the query. We have observed that MT models perform well in terms of both automatic and human evaluation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用户经常查询与具体问题的搜索引擎在心中，往往这些查询的关键字或子句子片段。例如，如果用户想知道的答案“什么是美国的首都”，他们将最有可能的查询“美国资本”或“美国资本”或一些这方面的基于关键字的变化。例如，用户输入查询“美国资本”，最有可能的问题，目的是“什么是美国的首都呢？”。在本文中，我们提议从给定的基于关键字的查询，其中有意向的询问同样的问题，良好的自然语言问题的方法。基于关键字的网页查询转换成一个结构良好的问题有很多的应用，在搜索引擎中的一些人是社区问答（CQA）的网站和漫游通信。我们发现查询到问题的问题，标准的机器翻译（MT）的任务之间的协同作用。我们都用了统计MT（SMT）和神经MT（NMT）模型来生成从查询的问题。我们观察到，MT车型在自动和人工评估方面表现良好。</font>
</div>


<hr>
<div id="paper8"> <b>8. Introducing Aspects of Creativity in Automatic Poetry Generation</b>  <a href="https://arxiv.org/pdf/2002.02511" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bena%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Brendan Bena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kalita%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jugal Kalita</a><br>
<font size="3">
Abstract: Poetry Generation involves teaching systems to automatically generate text that resembles poetic work. A deep learning system can learn to generate poetry on its own by training on a corpus of poems and modeling the particular style of language. In this paper, we propose taking an approach that fine-tunes GPT-2, a pre-trained language model, to our downstream task of poetry generation. We extend prior work on poetry generation by introducing creative elements. Specifically, we generate poems that express emotion and elicit the same in readers, and poems that use the language of dreams---called dream poetry. We are able to produce poems that correctly elicit the emotions of sadness and joy 87.5 and 85 percent, respectively, of the time. We produce dreamlike poetry by training on a corpus of texts that describe dreams. Poems from this model are shown to capture elements of dream poetry with scores of no less than 3.2 on the Likert scale. We perform crowdsourced human-evaluation for all our poems. We also make use of the Coh-Metrix tool, outlining metrics we use to gauge the quality of text generated. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：诗歌生成涉及教学系统自动生成的文本类似于诗的工作。深学习系统可以学习在诗的语料库培训和建模语言的特殊风格产生对自己的诗歌。在本文中，我们建议采取的做法，微调GPT-2，预先训练的语言模型，我们的诗歌产生的下游任务。我们通过引入创意元素延长诗代前期工作。具体而言，我们产生表达情感和引发相同的读者，用梦想的语言---所谓的梦想诗诗和诗歌。我们能够产生诗歌分别是正确引起的时间悲伤和喜悦87.5％和85％，的情绪。我们通过描述梦想文本语料库培训产生梦幻般的诗意。从这个模型诗被示出为与在李克特量表的不小于3.2的分数梦想诗歌捕获元件。我们进行众包的人评价为我们所有的诗。我们还利用COH-Metrix的工具，概述我们用衡量生成的文本的质量指标。</font>
</div>


<hr>
<div id="paper9"> <b>9. Goal-Oriented Multi-Task BERT-Based Dialogue State Tracker</b>  <a href="https://arxiv.org/pdf/2002.02450" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gulyaev%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pavel Gulyaev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Elistratova%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eugenia Elistratova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Konovalov%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasily Konovalov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kuratov%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuri Kuratov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pugachev%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Leonid Pugachev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Burtsev%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mikhail Burtsev</a><br>
<font size="3">
Abstract: Dialogue State Tracking (DST) is a core component of virtual assistants such as Alexa or Siri. To accomplish various tasks, these assistants need to support an increasing number of services and APIs. The Schema-Guided State Tracking track of the 8th Dialogue System Technology Challenge highlighted the DST problem for unseen services. The organizers introduced the Schema-Guided Dialogue (SGD) dataset with multi-domain conversations and released a zero-shot dialogue state tracking model. In this work, we propose a GOaL-Oriented Multi-task BERT-based dialogue state tracker (GOLOMB) inspired by architectures for reading comprehension question answering systems. The model "queries" dialogue history with descriptions of slots and services as well as possible values of slots. This allows to transfer slot values in multi-domain dialogues and have a capability to scale to unseen slot types. Our model achieves a joint goal accuracy of 53.97% on the SGD dataset, outperforming the baseline model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对话状态跟踪（DST）是虚拟助理如Alexa或锡里的核心部件。要完成各种任务，这些助手需要支持服务和API的越来越多。第八对话系统技术挑战赛的模式制导状态跟踪轨迹突出了DST问题的看不见的服务。主办方引入了多领域的对话架构制导对话（SGD）数据集，并发布了零射门的对话状态跟踪模型。在这项工作中，我们建议架构的启发基于BERT面向目标的多任务对话状态追踪器（哥伦布）阅读理解问答系统。该模型“查询”对话的历史与插槽的说明和服务，以及插槽的可能值。这允许在多域的对话能力转移槽值并具有刻度以看不见的插槽类型。我们的模型实现了对SGD数据集的53.97％的合资目标的准确性，跑赢基准模型。</font>
</div>


<hr>
<div id="paper10"> <b>10. I love your chain mail! Making knights smile in a fantasy game world:  Open-domain goal-orientated dialogue agents</b>  <a href="https://arxiv.org/pdf/2002.02878" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Prabhumoye%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shrimai Prabhumoye</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Margaret Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Urbanek%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jack Urbanek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dinan%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emily Dinan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kiela%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Douwe Kiela</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weston%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Weston</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Szlam%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Arthur Szlam</a><br>
<font size="3">
Abstract: Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a straightforward learning signal. Humans effortlessly combine the two, for example engaging in chit-chat with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning against an imitation-learned ``chit-chat'' model with two approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-K utterances from the chit-chat model. We show that both models outperform an inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对话研究倾向于闲聊和面向目标的任务区分。前者无疑是更自然，并具有广泛应用的语言，后者有更清晰的指标和一个简单的学习用信号。人类毫不费力地将二者结合起来，例如在闲聊从事与交换信息或引发特异性反应的目标。在这里，我们弥补了丰富的基于文本的多玩家幻想环境的设置这两个领域，其中代理和人类从事这两个动作和对话之间的鸿沟。具体来说，我们训练与强化学习面向目标的模型对模仿学习的``闲聊'模型方法有两种：政策要么学会选择一个主题或学会挑给从顶部-K话语的话语在闲聊模型。我们发现，这两种模式超越逆模型基线和为了达到目标，可以与他们的对话伙伴自然交谈。</font>
</div>


<hr>
<div id="paper11"> <b>11. Unsupervised pretraining transfers well across languages</b>  <a href="https://arxiv.org/pdf/2002.02848" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Rivi%C3%A8re%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Morgane Rivière</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Joulin%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Armand Joulin</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mazar%C3%A9%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre-Emmanuel Mazaré</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Dupoux%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emmanuel Dupoux</a><br>
<font size="3">
Abstract: Cross-lingual and multi-lingual training of Automatic Speech Recognition (ASR) has been extensively investigated in the supervised setting. This assumes the existence of a parallel corpus of speech and orthographic transcriptions. Recently, contrastive predictive coding (CPC) algorithms have been proposed to pretrain ASR systems with unlabelled data. In this work, we investigate whether unsupervised pretraining transfers well across languages. We show that a slight modification of the CPC pretraining extracts features that transfer well to other languages, being on par or even outperforming supervised pretraining. This shows the potential of unsupervised methods for languages with few linguistic resources. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：跨语言和自动语音识别（ASR）的多语种培训的监督设置了广泛的研究。这是假设的语音和正字改编的平行语料库的存在。近日，对比预测编码（CPC）算法被提出来与未标记的数据pretrain ASR系统。在这项工作中，我们调查是否无监督的训练前转移以及跨语言。我们表明，训练前中共提取物的稍微修改的特点是传输以及其他语言，是媲美甚至超越监督训练前。这显示了与一些语言资源语言的无监督方法的潜力。</font>
</div>


<hr>
<div id="paper12"> <b>12. Depressed individuals express more distorted thinking on social media</b>  <a href="https://arxiv.org/pdf/2002.02800" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bathina%2C+K+C" target="_blank" rel="noopener" style="color:#0000EE;">Krishna C. Bathina</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thij%2C+M+t" target="_blank" rel="noopener" style="color:#0000EE;">Marijn ten Thij</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lorenzo-Luaces%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lorenzo Lorenzo-Luaces</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rutter%2C+L+A" target="_blank" rel="noopener" style="color:#0000EE;">Lauren A. Rutter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bollen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Johan Bollen</a><br>
<font size="3">
Abstract: Depression is a leading cause of disability worldwide, but is often under-diagnosed and under-treated. One of the tenets of cognitive-behavioral therapy (CBT) is that individuals who are depressed exhibit distorted modes of thinking, so-called cognitive distortions, which can negatively affect their emotions and motivation. Here, we show that individuals with a self-reported diagnosis of depression on social media express higher levels of distorted thinking than a random sample. Some types of distorted thinking were found to be more than twice as prevalent in our depressed cohort, in particular Personalizing and Emotional Reasoning. This effect is specific to the distorted content of the expression and can not be explained by the presence of specific topics, sentiment, or first-person pronouns. Our results point towards the detection, and possibly mitigation, of patterns of online language that are generally deemed depressogenic. They may also provide insight into recent observations that social media usage can have a negative impact on mental health. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：抑郁症是全世界残疾的主要原因，但往往没有得到诊断和治疗不足。一个认知行为疗法（CBT）的原则之一是，谁是抑郁个体表现出扭曲的思维方式，所谓的认知扭曲，可自己的情绪和动机产生负面影响。在这里，我们表明，抑郁对社交媒体的自我报告诊断的个体表达较高水平的扭曲的思维不是随机抽样的。发现某些类型的扭曲的思维方式是在我们的沮丧人群普遍两倍以上，尤其是个性化和情感推理。这种效果是特定于表达的失真内容，并且不能由特定的主题，情绪，或第一人称代词的存在来解释。我们的研究结果指向了检测，并可能减缓，那一般都认为depressogenic在线语言模式。他们还可以提供洞察到最近的观察，社交媒体的使用会对心理健康产生负面影响。</font>
</div>


<hr>
<div id="paper13"> <b>13. LEAP System for SRE19 Challenge -- Improvements and Error Analysis</b>  <a href="https://arxiv.org/pdf/2002.02735" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Ramoji%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shreyas Ramoji</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Krishnan%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prashant Krishnan</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mysore%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bhargavram Mysore</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Singh%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Prachi Singh</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Ganapathy%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sriram Ganapathy</a><br>
<font size="3">
Abstract: The NIST Speaker Recognition Evaluation - Conversational Telephone Speech (CTS) challenge 2019 was an open evaluation for the task of speaker verification in challenging conditions. In this paper, we provide a detailed account of the LEAP SRE system submitted to the CTS challenge focusing on the novel components in the back-end system modeling. All the systems used the time-delay neural network (TDNN) based x-vector embeddings. The x-vector system in our SRE19 submission used a large pool of training speakers (about 14k speakers). Following the x-vector extraction, we explored a neural network approach to backend score computation that was optimized for a speaker verification cost. The system combination of generative and neural PLDA models resulted in significant improvements for the SRE evaluation dataset. We also found additional gains for the SRE systems based on score normalization and calibration. Subsequent to the evaluations, we have performed a detailed analysis of the submitted systems. The analysis revealed the incremental gains obtained for different training dataset combinations as well as the modeling methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：NIST说话人识别评估 - 会话电话语音（CTS）挑战2019是为在艰难条件下的说话人确认的任务一个开放的评价。在本文中，我们提供了一个详细的帐户提交CTS挑战着眼于后端系统建模的新组件的LEAP SRE系统。所有的系统中使用的时间延迟神经网络（TDNN）基于X的矢量的嵌入。在我们SRE19提交的X-载体系统使用的培训扬声器（约14K扬声器）的大型游泳池。继X向量提取，我们探讨了神经网络的方法来后端分数计算这是该扬声器核查成本优化。生成和神经PLDA模型的系统组合导致的SRE评估数据集显著的改善。我们还发现基于分数标准化和校准SRE系统的额外收益。继评估，我们已经完成了提交系统的详细分析。分析揭示了不同的训练数据集组合以及建模方法获得的增量收益。</font>
</div>


<hr>
<div id="paper14"> <b>14. Transformer Transducer: A Streamable Speech Recognition Model with  Transformer Encoders and RNN-T Loss</b>  <a href="https://arxiv.org/pdf/2002.02562" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qian Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Lu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Sak%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hasim Sak</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Tripathi%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anshuman Tripathi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=McDermott%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erik McDermott</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Koo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephen Koo</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shankar Kumar</a><br>
<font size="3">
Abstract: In this paper we present an end-to-end speech recognition model with Transformer encoders that can be used in a streaming speech recognition system. Transformer computation blocks based on self-attention are used to encode both audio and label sequences independently. The activations from both audio and label encoders are combined with a feed-forward layer to compute a probability distribution over the label space for every combination of acoustic frame position and label history. This is similar to the Recurrent Neural Network Transducer (RNN-T) model, which uses RNNs for information encoding instead of Transformer encoders. The model is trained with a monotonic RNN-T loss well-suited to frame-synchronous, streaming decoding. We present results on the LibriSpeech dataset showing that limiting the left context for self-attention in the Transformer layers makes decoding computationally tractable for streaming, with only a slight degradation in accuracy. We also show that the full attention version of our model achieves competitive performance compared to existing LibriSpeech benchmarks for attention-based models trained with cross-entropy loss. Our results also show that we can bridge the gap between full attention and limited attention versions of our model by attending to a limited number of future frames. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了具有可在流式语音识别系统中使用的变压器编码器的终端到终端的语音识别模型。基于自我关注变压器计算块用于独立编码音频和标签序列。从音频和标签编码器的激活相结合，与前馈层，以计算在所述标签空间上的概率分布的声学帧位置和标签历史的每个组合。这是类似于回归神经网络传感器（RNN-T）模型，它使用RNNs用于编码代替变压器的编码器的信息。该模型被训练以单调RNN-T损耗非常适用于帧同步，流解码。我们上显示，限制自我关注的左上下文变压器层使得解码流媒体，只有在准确度稍有下降，易于计算的LibriSpeech数据集目前的结果。我们还表明，相对于现有的LibriSpeech基准注意力基础的模式与交叉熵损失训练的我们的模型的充分重视版本实现了有竞争力的表现。我们的研究结果还表明我们可以通过参加未来的帧数量有限弥合充分重视和关注有限的版本我们的模型之间的差距。</font>
</div>


<hr>
<div id="paper15"> <b>15. Robust Multi-channel Speech Recognition using Frequency Aligned Network</b>  <a href="https://arxiv.org/pdf/2002.02520" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taejin Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumatani%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kenichi Kumatani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minhua Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sundaram%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shiva Sundaram</a><br>
<font size="3">
Abstract: Conventional speech enhancement technique such as beamforming has known benefits for far-field speech recognition. Our own work in frequency-domain multi-channel acoustic modeling has shown additional improvements by training a spatial filtering layer jointly within an acoustic model. In this paper, we further develop this idea and use frequency aligned network for robust multi-channel automatic speech recognition (ASR). Unlike an affine layer in the frequency domain, the proposed frequency aligned component prevents one frequency bin influencing other frequency bins. We show that this modification not only reduces the number of parameters in the model but also significantly and improves the ASR performance. We investigate effects of frequency aligned network through ASR experiments on the real-world far-field data where users are interacting with an ASR system in uncontrolled acoustic environments. We show that our multi-channel acoustic model with a frequency aligned network shows up to 18% relative reduction in word error rate. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：传统的语音增强技术，如波束赋形已经知道好处远场语音识别。我们自己的在频域多通道声学建模工作已经由声学模型内共同培养了空间滤波层示出的额外的改进。在本文中，我们进一步发展为强大的多通道自动语音识别（ASR）这个想法，并使用频率对准网络。不像在频域中的仿射层，所提出的频率对准部件防止一个频率窗口影响其它频率仓。我们表明，这种修改不仅显著减少了参数的数量模型，而且，提高了ASR性能。我们调查通过ASR实验上，用户与失控的声学环境ASR系统交互的真实世界的远场数据的频率对准网络的影响。我们表明，在字差错率我们与频率对准网络显示多通道声学模型高达18％的相对减少。</font>
</div>


<hr>
<div id="paper16"> <b>16. Consistency of a Recurrent Language Model With Respect to Incomplete  Decoding</b>  <a href="https://arxiv.org/pdf/2002.02492" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Welleck%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sean Welleck</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kulikov%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ilia Kulikov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaedeok Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pang%2C+R+Y" target="_blank" rel="noopener" style="color:#0000EE;">Richard Yuanzhe Pang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cho%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kyunghyun Cho</a><br>
<font size="3">
Abstract: Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管在各种任务的强大的性能，具有最大似然训练的神经序列模型已显示表现出的问题，如长度偏差和退化重复。我们研究使用常见的解码算法时，从经常性的语言模型接收无限长序列的相关问题。为了分析这个问题，我们首先定义的解码算法的不一致，这意味着该算法可以产生一个具有模型下零概率无限长度的序列。我们证明了常用的不完整的解码算法 - 贪婪搜索，波束搜索，前k个采样，和细胞核采样 - 不一致，尽管事实上，经常性的语言模型被训练来有限长度的生产序列。根据这些分析，我们提出了两种补救措施，地址不一致：前k和核取样，并自终止复发语言模型的一致变种。实证结果表明，发生矛盾的做法，而且所提出的方法防止不一致。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>综述类论文</title>
    <url>/2020/02/08/%E7%BB%BC%E8%BF%B0%E7%B1%BB%E8%AE%BA%E6%96%87/</url>
    <content><![CDATA[<h1 id="综述类论文"><a href="#综述类论文" class="headerlink" title="综述类论文"></a>综述类论文</h1><h2 id="领域自适应翻译"><a href="#领域自适应翻译" class="headerlink" title="领域自适应翻译"></a>领域自适应翻译</h2><ul>
<li>A Survey of Domain Adaptation for Neural Machine Translation. Chenhui Chu, Rui Wang. COLING 2018. <a href="https://arxiv.org/pdf/1806.00258" target="_blank" rel="noopener">[PDF]</a></li>
</ul><h2 id="篇章翻译"><a href="#篇章翻译" class="headerlink" title="篇章翻译"></a>篇章翻译</h2><ul>
<li>A Survey on Document-level Machine Translation: Methods and Evaluation. Sameen Maruf, Fahimeh Saleh, Gholamreza Haffari. arXiv 1912. <a href="https://arxiv.org/pdf/1912.08494" target="_blank" rel="noopener">[PDF]</a></li>
</ul><a id="more"></a>

<h2 id="多语言翻译"><a href="#多语言翻译" class="headerlink" title="多语言翻译"></a>多语言翻译</h2><ul>
<li>A Comprehensive Survey of Multilingual Neural Machine Translation. Raj Dabre, Chenhui Chu, Anoop Kunchukuttan. arXiv 2001. <a href="https://arxiv.org/pdf/2001.01115" target="_blank" rel="noopener">[PDF]</a></li>
</ul>
]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-07</title>
    <url>/2020/02/07/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-07/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Irony Detection in a Multilingual Context <a href="https://arxiv.org/pdf/2002.02427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Conversational Structure Aware and Context Sensitive Topic Model for  Online Discussions <a href="https://arxiv.org/pdf/2002.02353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Citation Data of Czech Apex Courts <a href="https://arxiv.org/pdf/2002.02224" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Related Tasks can Share! A Multi-task Framework for Affective language <a href="https://arxiv.org/pdf/2002.02154" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> A Neural Topical Expansion Framework for Unstructured Persona-oriented  Dialogue Generation <a href="https://arxiv.org/pdf/2002.02153" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Multilingual acoustic word embedding models for processing zero-resource  languages <a href="https://arxiv.org/pdf/2002.02109" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Attractive or Faithful? Popularity-Reinforced Learning for Inspired  Headline Generation <a href="https://arxiv.org/pdf/2002.02095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Aligning the Pretraining and Finetuning Objectives of Language Models <a href="https://arxiv.org/pdf/2002.02000" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B,  Phase-B <a href="https://arxiv.org/pdf/2002.01984" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Zero-Shot Activity Recognition with Videos <a href="https://arxiv.org/pdf/2002.02265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Understanding Car-Speak: Replacing Humans in Dealerships <a href="https://arxiv.org/pdf/2002.02070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Stimulating Creativity with FunLines: A Case Study of Humor Generation  in Headlines <a href="https://arxiv.org/pdf/2002.02031" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Irony Detection in a Multilingual Context</b>  <a href="https://arxiv.org/pdf/2002.02427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghanem%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bilal Ghanem</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Karoui%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jihen Karoui</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Benamara%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Farah Benamara</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rosso%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paolo Rosso</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moriceau%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Véronique Moriceau</a><br>
<font size="3">
Abstract: This paper proposes the first multilingual (French, English and Arabic) and multicultural (Indo-European languages vs. less culturally close languages) irony detection system. We employ both feature-based models and neural architectures using monolingual word representation. We compare the performance of these systems with state-of-the-art systems to identify their capabilities. We show that these monolingual models trained separately on different languages using multilingual word representation or text-based features can open the door to irony detection in languages that lack of annotated data for irony. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一个多语种（法语，英语和阿拉伯语）和多元文化（印欧语言与文化少接近语言）具有讽刺意味的检测系统。我们采用使用单语单词表示既基于特征的模型和神经结构。我们比较这些系统与国家的最先进的系统的性能，以确定自己的能力。我们发现，这些单语车型上使用多语言的单词表示或基于文本的功能，可以打开在缺乏讽刺注释数据的语言门讽刺检测不同的语言单独训练。</font>
</div>


<hr>
<div id="paper2"> <b>2. Conversational Structure Aware and Context Sensitive Topic Model for  Online Discussions</b>  <a href="https://arxiv.org/pdf/2002.02353" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yingcheng Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Loparo%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kenneth Loparo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kolacinski%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Richard Kolacinski</a><br>
<font size="3">
Abstract: Millions of online discussions are generated everyday on social media platforms. Topic modelling is an efficient way of better understanding large text datasets at scale. Conventional topic models have had limited success in online discussions, and to overcome their limitations, we use the discussion thread tree structure and propose a "popularity" metric to quantify the number of replies to a comment to extend the frequency of word occurrences, and the "transitivity" concept to characterize topic dependency among nodes in a nested discussion thread. We build a Conversational Structure Aware Topic Model (CSATM) based on popularity and transitivity to infer topics and their assignments to comments. Experiments on real forum datasets are used to demonstrate improved performance for topic extraction with six different measurements of coherence and impressive accuracy for topic assignments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数以百万计的在线讨论的是在社会化媒体平台上产生的每一天。主题造型是在规模更好地理解大数据集文字的有效方式。传统主题模型曾在网上讨论有限的成功，并克服其局限性，我们用话题树形结构，并提出了“人气”指标，以回复的数量量化为一个注释，延长词出现的频率，和“及物”的概念来描述话题依赖嵌套话题节点之间。我们建立一个基于普及和传递来推断主题和他们的任务，以评论的会话结构感知主题模型（CSATM）。真实数据集论坛的实验来证明与连贯性和令人印象深刻的精度为主题分配六种不同的测量话题提取改进性能。</font>
</div>


<hr>
<div id="paper3"> <b>3. Citation Data of Czech Apex Courts</b>  <a href="https://arxiv.org/pdf/2002.02224" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hara%C5%A1ta%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jakub Harašta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Novotn%C3%A1%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tereza Novotná</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=%C5%A0avelka%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jaromír Šavelka</a><br>
<font size="3">
Abstract: In this paper, we introduce the citation data of the Czech apex courts (Supreme Court, Supreme Administrative Court and Constitutional Court). This dataset was automatically extracted from the corpus of texts of Czech court decisions - CzCDC 1.0. We obtained the citation data by building the natural language processing pipeline for extraction of the court decision identifiers. The pipeline included the (i) document segmentation model and the (ii) reference recognition model. Furthermore, the dataset was manually processed to achieve high-quality citation data as a base for subsequent qualitative and quantitative analyses. The dataset will be made available to the general public. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们介绍了捷克顶点法院的引用数据（最高法院，最高行政法院和宪法法院）。 CzCDC 1.0  - 这个数据集自动从捷克法院判决文本的语料库中提取。我们通过建立自然语言处理管道的法院判决标识符萃取得到的引文数据。该管道包括第（i）文件分割模型和（ⅱ）参考识别模型。此外，该数据集被人工处理，以实现高品质的引用数据作为后续定性和定量分析的位置。该数据集将提供给广大市民。</font>
</div>


<hr>
<div id="paper4"> <b>4. Related Tasks can Share! A Multi-task Framework for Affective language</b>  <a href="https://arxiv.org/pdf/2002.02154" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Deep%2C+K+S" target="_blank" rel="noopener" style="color:#0000EE;">Kumar Shikhar Deep</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Akhtar%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Md Shad Akhtar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ekbal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Asif Ekbal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bhattacharyya%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pushpak Bhattacharyya</a><br>
<font size="3">
Abstract: Expressing the polarity of sentiment as 'positive' and 'negative' usually have limited scope compared with the intensity/degree of polarity. These two tasks (i.e. sentiment classification and sentiment intensity prediction) are closely related and may offer assistance to each other during the learning process. In this paper, we propose to leverage the relatedness of multiple tasks in a multi-task learning framework. Our multi-task model is based on convolutional-Gated Recurrent Unit (GRU) framework, which is further assisted by a diverse hand-crafted feature set. Evaluation and analysis suggest that joint-learning of the related tasks in a multi-task framework can outperform each of the individual tasks in the single-task frameworks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：表达情绪的极性为“正”和“负”通常与强度/程度的极性相比具有有限的范围。这两个任务（即情感分类和情感强度预测）紧密相关，并在学习过程中可以互相提供协助。在本文中，我们提出了利用多任务的关联性在多任务学习框架。我们的多任务模式是基于卷积门控重复单元（GRU）的框架，这是一个多元化的手工制作的功能集进一步协助。评估和分析表明，联合学习在多任务框架的相关任务可以在单任务框架超越每个单独的任务。</font>
</div>


<hr>
<div id="paper5"> <b>5. A Neural Topical Expansion Framework for Unstructured Persona-oriented  Dialogue Generation</b>  <a href="https://arxiv.org/pdf/2002.02153" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghong Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piji Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoran Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengjie Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ren%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhaochun Ren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhumin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Ma</a><br>
<font size="3">
Abstract: Unstructured Persona-oriented Dialogue Systems (UPDS) has been demonstrated effective in generating persona consistent responses by utilizing predefined natural language user persona descriptions (e.g., "I am a vegan"). However, the predefined user persona descriptions are usually short and limited to only a few descriptive words, which makes it hard to correlate them with the dialogues. As a result, existing methods either fail to use the persona description or use them improperly when generating persona consistent responses. To address this, we propose a neural topical expansion framework, namely Persona Exploration and Exploitation (PEE), which is able to extend the predefined user persona description with semantically correlated content before utilizing them to generate dialogue responses. PEE consists of two main modules: persona exploration and persona exploitation. The former learns to extend the predefined user persona description by mining and correlating with existing dialogue corpus using a variational auto-encoder (VAE) based topic model. The latter learns to generate persona consistent responses by utilizing the predefined and extended user persona description. In order to make persona exploitation learn to utilize user persona description more properly, we also introduce two persona-oriented loss functions: Persona-oriented Matching (P-Match) loss and Persona-oriented Bag-of-Words (P-BoWs) loss which respectively supervise persona selection in encoder and decoder. Experimental results show that our approach outperforms state-of-the-art baselines, in terms of both automatic and human evaluations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：非结构化假面为本对话系统（UPDS）已经通过利用预定义的自然语言用户个性描述（例如，“我是素食主义者”）证明有效生成人物一致响应。然而，预定义用户的人物角色描述通常是短且仅限于一些描述性词语，这使得它很难将它们与对话相关联。其结果是，现有方法要么不能使用的人物角色描述或生成人物一致响应时不正确地使用它们。为了解决这个问题，我们提出了一个神经局部扩展的框架，即假面勘探和开采（PEE），这是能够利用它们来生成对话响应之前延长与语义相关的内容的预定义的用户角色的描述。 PEE包括两个主要模块：人物的勘探和开采的人物。前者学习到挖掘扩展预定义的用户角色的描述和使用基于主题模型，变分自动编码器（VAE）与现有的对话语料库相关。后者学会生成通过利用预定义的和扩展的用户个性描述人物一致响应。为了使人物开采学会更合理的利用用户的人物角色描述，我们还推出两款面向角色损功能：假面为本匹配（P-匹配）的损失和人物角色的导向一袋字（P-弓）损失分别在监督编码器和解码器的人物的选择。实验结果表明，我们的方法优于国家的最先进的基线，在自动和人的评估方面。</font>
</div>


<hr>
<div id="paper6"> <b>6. Multilingual acoustic word embedding models for processing zero-resource  languages</b>  <a href="https://arxiv.org/pdf/2002.02109" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kamper%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Herman Kamper</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Matusevych%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yevgen Matusevych</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldwater%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sharon Goldwater</a><br>
<font size="3">
Abstract: Acoustic word embeddings are fixed-dimensional representations of variable-length speech segments. In settings where unlabelled speech is the only available resource, such embeddings can be used in "zero-resource" speech search, indexing and discovery systems. Here we propose to train a single supervised embedding model on labelled data from multiple well-resourced languages and then apply it to unseen zero-resource languages. For this transfer learning approach, we consider two multilingual recurrent neural network models: a discriminative classifier trained on the joint vocabularies of all training languages, and a correspondence autoencoder trained to reconstruct word pairs. We test these using a word discrimination task on six target zero-resource languages. When trained on seven well-resourced languages, both models perform similarly and outperform unsupervised models trained on the zero-resource languages. With just a single training language, the second model works better, but performance depends more on the particular training--testing language pair. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：声字的嵌入被固定维可变长度的语音段的表示。在设置里未标记的讲话是唯一可用的资源，这样的嵌入可在“零资源”的声音检索，索引和发现系统中使用。在这里，我们提出培养从多个资源充足的语言标记数据的单一监督嵌入模型，然后把它应用到看不见的零资源的语言。对于这种转移的学习方法，我们考虑两个多语种回归神经网络模型：辨别分类培训了所有训练语言的词汇联合，培养重建的单词对对应的自动编码。我们这些使用上的六个标靶零资源语言文字辨别任务测试。当七，资源丰富语言的训练，这两款车型同样执行和超越训练有素的零资源语言的无监督模型。只是一个单一的语言训练，第二个模型更好地工作，但性能更依赖于特定的训练 - 测试语言对。</font>
</div>


<hr>
<div id="paper7"> <b>7. Attractive or Faithful? Popularity-Reinforced Learning for Inspired  Headline Generation</b>  <a href="https://arxiv.org/pdf/2002.02095" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun-Zhu Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shuai%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hong-Han Shuai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yeh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sung-Lin Yeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi-Lun Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ku%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lun-Wei Ku</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wen-Chih Peng</a><br>
<font size="3">
Abstract: With the rapid proliferation of online media sources and published news, headlines have become increasingly important for attracting readers to news articles, since users may be overwhelmed with the massive information. In this paper, we generate inspired headlines that preserve the nature of news articles and catch the eye of the reader simultaneously. The task of inspired headline generation can be viewed as a specific form of Headline Generation (HG) task, with the emphasis on creating an attractive headline from a given news article. To generate inspired headlines, we propose a novel framework called POpularity-Reinforced Learning for inspired Headline Generation (PORL-HG). PORL-HG exploits the extractive-abstractive architecture with 1) Popular Topic Attention (PTA) for guiding the extractor to select the attractive sentence from the article and 2) a popularity predictor for guiding the abstractor to rewrite the attractive sentence. Moreover, since the sentence selection of the extractor is not differentiable, techniques of reinforcement learning (RL) are utilized to bridge the gap with rewards obtained from a popularity score predictor. Through quantitative and qualitative experiments, we show that the proposed PORL-HG significantly outperforms the state-of-the-art headline generation models in terms of attractiveness evaluated by both human (71.03%) and the predictor (at least 27.60%), while the faithfulness of PORL-HG is also comparable to the state-of-the-art generation model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着网络媒体来源和公布的消息迅速扩散，标题已成为吸引读者的新闻文章越来越重要，因为用户可能会用大量的信息所淹没。在本文中，我们产生灵感的头条新闻保持新闻报道的本质，同时吸引读者的眼球。启发标题一代人的任务，可以被看作是头条代（HG）任务的具体形式，并把重点放在建立从给定的新闻文章的标题吸引人。为了产生灵感的头条新闻，我们提出了一个所谓的流行，增强学习的启发标题代（PORL-HG）的新框架。 PORL-HG利用与1）热门话题注意（PTA）萃取-抽象体系结构用于引导所述提取器从物品和2）的流行度预测器用于引导提取器重写吸引力句子中选择有吸引力的句子。此外，由于提取的例句选择是不可微的，（RL）强化学习的技术用于桥接与从普及的分数的预测获得奖励的间隙。通过定量和定性实验，我们表明，该PORL-HG显著优于国家的最先进的标题代车型由两个人（71.03％）和预测（至少27.60％）评估吸引力方面，而PORL-HG的信实也比得上状态的最先进的生成模型。</font>
</div>


<hr>
<div id="paper8"> <b>8. Aligning the Pretraining and Finetuning Objectives of Language Models</b>  <a href="https://arxiv.org/pdf/2002.02000" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pierse%2C+N+W" target="_blank" rel="noopener" style="color:#0000EE;">Nuo Wang Pierse</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingwen Lu</a><br>
<font size="3">
Abstract: We demonstrate that explicitly aligning the pretraining objectives to the finetuning objectives in language model training significantly improves the finetuning task performance and reduces the minimum amount of finetuning examples required. The performance margin gained from objective alignment allows us to build language models with smaller sizes for tasks with less available training data. We provide empirical evidence of these claims by applying objective alignment to concept-of-interest tagging and acronym detection tasks. We found that, with objective alignment, our 768 by 3 and 512 by 3 transformer language models can reach accuracy of 83.9%/82.5% for concept-of-interest tagging and 73.8%/70.2% for acronym detection using only 200 finetuning examples per task, outperforming the 768 by 3 model pretrained without objective alignment by +4.8%/+3.4% and +9.9%/+6.3%. We name finetuning small language models in the presence of hundreds of training examples or less "Few Example learning". In practice, Few Example Learning enabled by objective alignment not only saves human labeling costs, but also makes it possible to leverage language models in more real-time applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们证明，明确对准训练前的目标的目标细化和微调在语言模型训练显著提高了任务细化和微调性能和降低微调所需的例子的最低金额。从客观比对所获得的性能裕量使我们能够建立语言模型尺寸较小与较少的可用训练数据的任务。我们通过将目标对准概念的兴趣标记和缩写检测任务提供这些说法的经验证据。我们发现，与目标定位，我们768 3和512 3变压器的语言模型可以达到83.9％/ 82.5％，准确度概念的兴趣标签和73.8％/ 70.2％的首字母缩写，检测只用200元细化和微调的例子任务，表现优于768由3模型由4.8％/ + 3.4％和9.9％/ + 6.3％没有客观对准预训练。我们的名字在数百个训练范例以下“几个示例学习”的存在微调小语言模型。在实践中，能够通过客观对准几个示例学习不仅节约了人工标识的成本，而且还能够利用语言模型在多个实时应用。</font>
</div>


<hr>
<div id="paper9"> <b>9. UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B,  Phase-B</b>  <a href="https://arxiv.org/pdf/2002.01984" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Telukuntla%2C+S+K" target="_blank" rel="noopener" style="color:#0000EE;">Sai Krishna Telukuntla</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kapri%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aditya Kapri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zadrozny%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wlodek Zadrozny</a><br>
<font size="3">
Abstract: In this paper, we detail our submission to the 2019, 7th year, BioASQ competition. We present our approach for Task-7b, Phase B, Exact Answering Task. These Question Answering (QA) tasks include Factoid, Yes/No, List Type Question answering. Our system is based on a contextual word embedding model. We have used a Bidirectional Encoder Representations from Transformers(BERT) based system, fined tuned for biomedical question answering task using BioBERT. In the third test batch set, our system achieved the highest MRR score for Factoid Question Answering task. Also, for List type question answering task our system achieved the highest recall score in the fourth test batch set. Along with our detailed approach, we present the results for our submissions, and also highlight identified downsides for our current approach and ways to improve them in our future experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们详细介绍了提交给2019年，第7年，BioASQ竞争。我们提出我们的任务-7B的方法，B相，精确应答任务。这些问题回答（QA）任务包括FACTOID，是/否，列表类型答疑。我们的系统是基于上下文的单词嵌入模型。我们使用来自变形金刚双向编码表示（BERT）的系统，罚款调整为使用BioBERT生物医学问题回答任务。在第三个试验批次设置，我们的系统取得了最高MRR得分事实型询问应答任务。另外，对于列表类型问答任务我们的系统实现了在第四一批测试集最高得分召回。随着我们的详细的方法，我们目前的结果为我们的意见，并且还强调确定了我们目前的方法和途径，以提高他们在我们未来的实验缺点。</font>
</div>


<hr>
<div id="paper10"> <b>10. Zero-Shot Activity Recognition with Videos</b>  <a href="https://arxiv.org/pdf/2002.02265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ornek%2C+E+P" target="_blank" rel="noopener" style="color:#0000EE;">Evin Pinar Ornek</a><br>
<font size="3">
Abstract: In this paper, we examined the zero-shot activity recognition task with the usage of videos. We introduce an auto-encoder based model to construct a multimodal joint embedding space between the visual and textual manifolds. On the visual side, we used activity videos and a state-of-the-art 3D convolutional action recognition network to extract the features. On the textual side, we worked with GloVe word embeddings. The zero-shot recognition results are evaluated by top-n accuracy. Then, the manifold learning ability is measured by mean Nearest Neighbor Overlap. In the end, we provide an extensive discussion over the results and the future directions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们研究了零次活动识别任务与视频的使用。我们引入了自动编码器基于模型的构建视觉和文本歧管之间的多模式联合嵌入空间。在可视侧，我们使用活动视频和一个国家的最先进的三维卷积动作识别网络来提取特征。在文字方面，我们曾与手套字的嵌入。零射门的识别结果被顶n准确评估。然后，歧管学习能力通过平均最近邻重叠测量。最后，我们提供对结果和未来的发展方向进行了广泛讨论。</font>
</div>


<hr>
<div id="paper11"> <b>11. Understanding Car-Speak: Replacing Humans in Dealerships</b>  <a href="https://arxiv.org/pdf/2002.02070" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hooshmand%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Habeeb Hooshmand</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caverlee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Caverlee</a><br>
<font size="3">
Abstract: A large portion of the car-buying experience in the United States involves interactions at a car dealership. At the dealership, the car-buyer relays their needs to a sales representative. However, most car-buyers are only have an abstract description of the vehicle they need. Therefore, they are only able to describe their ideal car in "car-speak". Car-speak is abstract language that pertains to a car's physical attributes. In this paper, we define car-speak. We also aim to curate a reasonable data set of car-speak language. Finally, we train several classifiers in order to classify car-speak. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在美国的购车体验的很大一部分涉及在汽车经销店的互动。在经销店，汽车买方中继其销售代表的需求。然而，大多数汽车购买者只拥有他们所需要的车辆的抽象描述。因此，他们只能够描述自己理想中的车“汽车说话。”租车发言是抽象的语言，涉及到汽车的物理属性。在本文中，我们定义汽车发言。我们还致力于策划的车讲的语言合理的数据集。最后，我们培养几个分类，以分类车说话。</font>
</div>


<hr>
<div id="paper12"> <b>12. Stimulating Creativity with FunLines: A Case Study of Humor Generation  in Headlines</b>  <a href="https://arxiv.org/pdf/2002.02031" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hossain%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nabil Hossain</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krumm%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">John Krumm</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sajed%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tanvir Sajed</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kautz%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Henry Kautz</a><br>
<font size="3">
Abstract: Building datasets of creative text, such as humor, is quite challenging. We introduce FunLines, a competitive game where players edit news headlines to make them funny, and where they rate the funniness of headlines edited by others. FunLines makes the humor generation process fun, interactive, collaborative, rewarding and educational, keeping players engaged and providing humor data at a very low cost compared to traditional crowdsourcing approaches. FunLines offers useful performance feedback, assisting players in getting better over time at generating and assessing humor, as our analysis shows. This helps to further increase the quality of the generated dataset. We show the effectiveness of this data by training humor classification models that outperform a previous benchmark, and we release this dataset to the public. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：创作文本的建筑数据集，如幽默，极具挑战性。我们介绍FunLines，有竞争力的游戏，玩家编辑新闻标题，使他们逗的，在那里他们率他人编辑头条funniness。 FunLines使得幽默生成过程的乐趣，互动，合作，奖励，教育，保持玩家参与，并与传统的众包接近以非常低的成本提供幽默的数据。 FunLines提供了有用的绩效反馈，帮助玩家在发电渐入佳境随着时间的推移和评估幽默，因为我们的分析显示。这有助于进一步提高所产生的数据集的质量。我们表明，该数据由超越以前的基准培训幽默分类模型的有效性，以及我们发布这个数据集给公众。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-07</title>
    <url>/2020/02/07/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-07/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Reliability Validation of Learning Enabled Vehicle Tracking <a href="https://arxiv.org/pdf/2002.02424" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Lane Boundary Geometry Extraction from Satellite Imagery <a href="https://arxiv.org/pdf/2002.02362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Random VLAD based Deep Hashing for Efficient Image Retrieval <a href="https://arxiv.org/pdf/2002.02333" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Fine-Grained Urban Flow Inference <a href="https://arxiv.org/pdf/2002.02318" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Person Re-identification by Contour Sketch under Moderate Clothing  Change <a href="https://arxiv.org/pdf/2002.02295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Zero-Shot Activity Recognition with Videos <a href="https://arxiv.org/pdf/2002.02265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Looking GLAMORous: Vehicle Re-Id in Heterogeneous Cameras Networks with  Global and Local Attention <a href="https://arxiv.org/pdf/2002.02256" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Unsupervised Bidirectional Cross-Modality Adaptation via Deeply  Synergistic Image and Feature Alignment for Medical Image Segmentation <a href="https://arxiv.org/pdf/2002.02255" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> RGB-based Semantic Segmentation Using Self-Supervised Depth Pre-Training <a href="https://arxiv.org/pdf/2002.02200" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Joint Deep Learning of Facial Expression Synthesis and Recognition <a href="https://arxiv.org/pdf/2002.02194" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Pose-Aware Instance Segmentation Framework from Cone Beam CT Images for  Tooth Segmentation <a href="https://arxiv.org/pdf/2002.02143" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> An Information-rich Sampling Technique over Spatio-Temporal CNN for  Classification of Human Actions in Videos <a href="https://arxiv.org/pdf/2002.02100" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Forensic Scanner Identification Using Machine Learning <a href="https://arxiv.org/pdf/2002.02079" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Driver Gaze Estimation in the Real World: Overcoming the Eyeglass  Challenge <a href="https://arxiv.org/pdf/2002.02077" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Residual-Recursion Autoencoder for Shape Illustration Images <a href="https://arxiv.org/pdf/2002.02063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Rotation-invariant Mixed Graphical Model Network for 2D Hand Pose  Estimation <a href="https://arxiv.org/pdf/2002.02033" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> 3DPIFCM Segmentation Algorithm for brain MRI <a href="https://arxiv.org/pdf/2002.01985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Parallel 3DPIFCM Algorithm for Noisy Brain MRI Images <a href="https://arxiv.org/pdf/2002.01981" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> StegColNet: Steganalysis based on an ensemble colorspace approach <a href="https://arxiv.org/pdf/2002.02413" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Covering the News with (AI) Style <a href="https://arxiv.org/pdf/2002.02369" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> VGAI: A Vision-Based Decentralized Controller Learning Framework for  Robot Swarms <a href="https://arxiv.org/pdf/2002.02308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> From Data to Actions in Intelligent Transportation Systems: a  Prescription of Functional Requirements for Model Actionability <a href="https://arxiv.org/pdf/2002.02210" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Unbalanced GANs: Pre-training the Generator of Generative Adversarial  Network using Variational Autoencoder <a href="https://arxiv.org/pdf/2002.02112" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Brain Tumor Segmentation by Cascaded Deep Neural Networks Using Multiple  Image Scales <a href="https://arxiv.org/pdf/2002.01975" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Crowdsourcing the Perception of Machine Teaching <a href="https://arxiv.org/pdf/2002.01618" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<font>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
</font></font>]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-06</title>
    <url>/2020/02/06/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-06/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Rapid Adaptation of BERT for Information Extraction on Domain-Specific  Business Documents <a href="https://arxiv.org/pdf/2002.01861" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Automatic Location Type Classification From Social-Media Posts <a href="https://arxiv.org/pdf/2002.01846" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Discontinuous Constituent Parsing with Pointer Networks <a href="https://arxiv.org/pdf/2002.01824" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters <a href="https://arxiv.org/pdf/2002.01808" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and  Manual Correction <a href="https://arxiv.org/pdf/2002.01761" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Parsing as Pretraining <a href="https://arxiv.org/pdf/2002.01685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Identification of Indian Languages using Ghost-VLAD pooling <a href="https://arxiv.org/pdf/2002.01664" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Lightweight Convolutional Representations for On-Device Natural Language  Processing <a href="https://arxiv.org/pdf/2002.01535" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Generalizing meanings from partners to populations: Hierarchical  inference supports convention formation on networks <a href="https://arxiv.org/pdf/2002.01510" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> If I Hear You Correctly: Building and Evaluating Interview Chatbots with  Active Listening Skills <a href="https://arxiv.org/pdf/2002.01862" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Rapid Adaptation of BERT for Information Extraction on Domain-Specific  Business Documents</b>  <a href="https://arxiv.org/pdf/2002.01861" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruixue Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luyun Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengkai Tu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuqing Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zihang Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhao Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luchen Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kun Xiong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jimmy Lin</a><br>
<font size="3">
Abstract: Techniques for automatically extracting important content elements from business documents such as contracts, statements, and filings have the potential to make business operations more efficient. This problem can be formulated as a sequence labeling task, and we demonstrate the adaption of BERT to two types of business documents: regulatory filings and property lease agreements. There are aspects of this problem that make it easier than "standard" information extraction tasks and other aspects that make it more difficult, but on balance we find that modest amounts of annotated data (less than 100 documents) are sufficient to achieve reasonable accuracy. We integrate our models into an end-to-end cloud platform that provides both an easy-to-use annotation interface as well as an inference interface that allows users to upload documents and inspect model outputs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：技术的自动提取业务文档的重要内容元素，如合同，报表和申报必须让企业运营更为有效的潜力。这个问题可以配制成序列标注任务，我们证明BERT的两种类型商务文档的适应：监管机构备案和物业租赁协议。有迹象表明，使它比“标准”信息提取任务等各个方面，使之更加困难，更容易，但总的来说，我们发现，适量的注释数据（小于100个文件）的足以实现合理的准确性这个问题的各个方面。我们我们的模型集成到一个终端到终端的云平台，同时提供了一个易于使用的界面注释以及推理接口，允许用户上传文档并检查模型输出。</font>
</div>


<hr>
<div id="paper2"> <b>2. Automatic Location Type Classification From Social-Media Posts</b>  <a href="https://arxiv.org/pdf/2002.01846" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kravi%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elad Kravi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kimelfeld%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benny Kimelfeld</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kanza%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaron Kanza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Reichart%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roi Reichart</a><br>
<font size="3">
Abstract: We introduce the problem of Automatic Location Type Classification from social media posts. Our goal is to correctly associate a set of messages posted in a small radius around a given location with their corresponding location type, e.g., school, church, restaurant or museum. We provide a dataset of locations associated with tweets posted in close geographical proximity. We explore two approaches to the problem: (a) a pipeline approach where each message is first classified, and then the location associated with the message set is inferred from the individual message labels; and (b) a joint approach where the individual messages are simultaneously processed to yield the desired location type. Our results demonstrate the superiority of the joint approach. Moreover, we show that due to the unique structure of the problem, where weakly-related messages are jointly processed to yield a single final label, simpler linear classifiers outperform deep neural network alternatives that have shown superior in previous text classification tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们从社交媒体文章介绍了自动定位类型划分的问题。我们的目标是正确的一组贴在小半径围绕给定的位置，其对应的位置类型，例如，学校，教堂，餐馆或博物馆的消息联系起来。我们提供的与贴在缘相近微博相关联的位置的数据集。我们探索两种方法的问题：（1）管线方法，其中每个消息首先分类，然后与消息集相关联的位置被从单独的消息标签推断;和（b）在各个消息被同时处理的联合方法，得到所需的位置类型。我们的研究结果表明共同方法的优越性。此外，我们表明，由于问题，在弱相关的消息被联合处理，以产生一个最终标签的独特结构，简单的线性分类跑赢大盘已显示出在以前的文本分类任务优良的深神经网络的替代品。</font>
</div>


<hr>
<div id="paper3"> <b>3. Discontinuous Constituent Parsing with Pointer Networks</b>  <a href="https://arxiv.org/pdf/2002.01824" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fern%C3%A1ndez-Gonz%C3%A1lez%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Fernández-González</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Gómez-Rodríguez</a><br>
<font size="3">
Abstract: One of the most complex syntactic representations used in computational linguistics and NLP are discontinuous constituent trees, crucial for representing all grammatical phenomena of languages such as German. Recent advances in dependency parsing have shown that Pointer Networks excel in efficiently parsing syntactic relations between words in a sentence. This kind of sequence-to-sequence models achieve outstanding accuracies in building non-projective dependency trees, but its potential has not been proved yet on a more difficult task. We propose a novel neural network architecture that, by means of Pointer Networks, is able to generate the most accurate discontinuous constituent representations to date, even without the need of Part-of-Speech tagging information. To do so, we internally model discontinuous constituent structures as augmented non-projective dependency structures. The proposed approach achieves state-of-the-art results on the two widely-used NEGRA and TIGER benchmarks, outperforming previous work by a wide margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一个在计算语言学和自然语言处理中使用的最复杂的句法表征的是不连续的组成部分树木，为代表的语言的所有语法现象，如德国的关键。在依存分析的最新进展表明，指针网络高强高效地分析词与词之间句法关系的句子。这种顺序对序列模型的实现建立非投影依赖树出色的精度，但它的潜力还没有得到一个更艰巨的任务尚未证实。我们提出了一种新的神经网络结构，通过指针网络的手段，能够产生最准确的不连续的组成表示到目前为止，即使没有需要的部分，词性标注信息。要做到这一点，我们在内部不连续的成分结构建模为增强非投影依赖结构。所提出的方法实现对两种广泛使用的NEGRA和TIGER基准国家的先进成果，大幅跑赢以前的工作。</font>
</div>


<hr>
<div id="paper4"> <b>4. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</b>  <a href="https://arxiv.org/pdf/2002.01808" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruize Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Duyu Tang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhongyu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuanjing Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=ji%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianshu ji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cuihong Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daxin Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they may suffer from the problem of catastrophic forgetting. To address this, we propose K-Adapter, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus different adapters are efficiently trained in a distributed way. We inject two kinds of knowledge, including factual knowledge obtained from automatically aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge obtained from dependency parsing. Results on three knowledge-driven tasks (total six datasets) including relation classification, entity typing and question answering demonstrate that each adapter improves the performance, and the combination of both adapters brings further improvements. Probing experiments further show that K-Adapter captures richer factual and commonsense knowledge than RoBERTa. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们研究的知识注入到大预先训练的车型，如BERT和罗伯塔的问题。现有的方法通常注射知识时更新的预先训练模型的原始参数。然而，当多个种类的知识注入，它们可以从灾难性遗忘的问题的困扰。为了解决这个问题，我们提出了K-适配器，这仍然是预先训练模型固定和支持持续的知识灌输的原始参数。以罗伯塔作为预先训练模型，K-适配器有各种灌输的知识的神经适配器，就像连接到一个罗伯塔插件。有不同的适配器，从而不同的适配器以分布式方式有效地训练之间没有信息流。我们注入两种知识，包括自动对齐文本三联维基百科和维基数据获得的实际知识，并从依赖分析获得的语言知识。三个知识驱动型任务（共六集），包括有关分类，实体打字和答疑结果表明，每个适配器提高性能，并且这两个适配器的组合带来了进一步的改进。探测实验进一步表明，K-适配器捕捉比罗伯塔更丰富的事实和常识性知识。</font>
</div>


<hr>
<div id="paper5"> <b>5. Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and  Manual Correction</b>  <a href="https://arxiv.org/pdf/2002.01761" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingchen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zili Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanna Wang</a><br>
<font size="3">
Abstract: Princeton WordNet (PWN) is a lexicon-semantic network based on cognitive linguistics, which promotes the development of natural language processing. Based on PWN, five Chinese wordnets have been developed to solve the problems of syntax and semantics. They include: Northeastern University Chinese WordNet (NEW), Sinica Bilingual Ontological WordNet (BOW), Southeast University Chinese WordNet (SEW), Taiwan University Chinese WordNet (CWN), Chinese Open WordNet (COW). By using them, we found that these word networks have low accuracy and coverage, and cannot completely portray the semantic network of PWN. So we decided to make a new Chinese wordnet called Multi-Fusion Chinese Wordnet (MCW) to make up those shortcomings. The key idea is to extend the SEW with the help of Oxford bilingual dictionary and Xinhua bilingual dictionary, and then correct it. More specifically, we used machine learning and manual adjustment in our corrections. Two standards were formulated to help our work. We conducted experiments on three tasks including relatedness calculation, word similarity and word sense disambiguation for the comparison of lemma's accuracy, at the same time, coverage also was compared. The results indicate that MCW can benefit from coverage and accuracy via our method. However, it still has room for improvement, especially with lemmas. In the future, we will continue to enhance the accuracy of MCW and expand the concepts in it. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：普林斯顿的WordNet（PWN）是一种基于认知语言学的词汇，语义网络，促进自然语言处理的发展。基于PWN，五个中国词汇网络已发展到解决语法和语义的问题。它们包括：东北大学中国共发现（NEW），报双语本体共发现（BOW），东南大学中国共发现（SEW），台湾大学中国共发现（CWN），中国公开赛共发现（COW）。通过使用它们，我们发现，这些字网络具有低精度和覆盖范围，并不能完全刻画PWN的语义网络。所以我们决定称为Multi-融合中国WORDNET（MCW）新中国共发现来弥补这些缺陷。其核心思想是将与牛津双解词典和新华双语词典的帮助延长SEW，然后纠正它。更具体地讲，我们用机器学习和手动调节我们的更正。两个标准配制，以帮助我们的工作。我们三个任务，包括相关性计算，词语相似度和引理的精度比较多义进行了实验，在同一时间，范围也进行了比较。结果表明，MCW可以从覆盖范围和精度通过我们的方法中受益。然而，它仍然有改进的余地，尤其是与引理。今后，我们将继续加强MCW的准确性和扩大它的概念。</font>
</div>


<hr>
<div id="paper6"> <b>6. Parsing as Pretraining</b>  <a href="https://arxiv.org/pdf/2002.01685" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Vilares%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Vilares</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Strzyz%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michalina Strzyz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=S%C3%B8gaard%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anders Søgaard</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Carlos Gómez-Rodríguez</a><br>
<font size="3">
Abstract: Recent analyses suggest that encoders pretrained for language modeling capture certain morpho-syntactic structure. However, probing frameworks for word vectors still do not report results on standard setups such as constituent and dependency parsing. This paper addresses this problem and does full parsing (on English) relying only on pretraining architectures -- and no decoding. We first cast constituent and dependency parsing as sequence tagging. We then use a single feed-forward layer to directly map word vectors to labels that encode a linearized tree. This is used to: (i) see how far we can reach on syntax modelling with just pretrained encoders, and (ii) shed some light about the syntax-sensitivity of different word vectors (by freezing the weights of the pretraining network during training). For evaluation, we use bracketing F1-score and LAS, and analyze in-depth differences across representations for span lengths and dependency displacements. The overall results surpass existing sequence tagging parsers on the PTB (93.5%) and end-to-end EN-EWT UD (78.8%). </font>
<br>
<font size="2" style="line-height:30px;">
摘要：最近的分析表明预训练的语言模型捕捉特定的形态句法结构的编码器。然而，对于词矢量探测框架仍然不报告的标准设置，如成分和依存分析结果。本文将解决这个问题，不完全解析（英语）只在训练前的架构依赖 - 没有解码。首先，我们投的组成和依赖解析为序列标记。然后，我们使用一个单一的前馈层直接字矢量映射到编码的线性化树标签。这被用来：（ⅰ）见多远我们可以语法建模与刚刚预训练的编码器达到，和（ii）棚约不同字向量的语法灵敏度一些光（由训练期间冻结预训练网络的权重） 。对于评价，我们采用包围F1-得分和LAS，并分析跨表示深度的差异跨度长度和依赖位移。总的结果超过上PTB（93.5％）和端至端EN-EWT UD（78.8％）现有的序列标记的解析器。</font>
</div>


<hr>
<div id="paper7"> <b>7. Identification of Indian Languages using Ghost-VLAD pooling</b>  <a href="https://arxiv.org/pdf/2002.01664" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=N%2C+K+D" target="_blank" rel="noopener" style="color:#0000EE;">Krishna D N</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Patil%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankita Patil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Raj%2C+M+S+P" target="_blank" rel="noopener" style="color:#0000EE;">M.S.P Raj</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=S%2C+S+P+H" target="_blank" rel="noopener" style="color:#0000EE;">Sai Prasad H S</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Garapati%2C+P+A" target="_blank" rel="noopener" style="color:#0000EE;">Prabhu Aashish Garapati</a><br>
<font size="3">
Abstract: In this work, we propose a new pooling strategy for language identification by considering Indian languages. The idea is to obtain utterance level features for any variable length audio for robust language recognition. We use the GhostVLAD approach to generate an utterance level feature vector for any variable length input audio by aggregating the local frame level features across time. The generated feature vector is shown to have very good language discriminative features and helps in getting state of the art results for language identification task. We conduct our experiments on 635Hrs of audio data for 7 Indian languages. Our method outperforms the previous state of the art x-vector [11] method by an absolute improvement of 1.88% in F1-score and achieves 98.43% F1-score on the held-out test data. We compare our system with various pooling approaches and show that GhostVLAD is the best pooling approach for this task. We also provide visualization of the utterance level embeddings generated using Ghost-VLAD pooling and show that this method creates embeddings which has very good language discriminative features. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，我们考虑印度语提出了语言识别新的合并策略。这样做是为了获得话语级功能为强大的语言识别的任何可变长度的音频。我们使用GhostVLAD方法，通过在时间上聚集所述本地帧级特征，以生成用于任何可变长度的输入音频发声水平特征向量。所生成的特征向量显示出具有很好的语言辨别功能，并获得艺术效果的语言识别任务的状态有所帮助。我们进行了对7种印度语言的音频数据的635Hrs我们的实验。我们的方法由1.88％的F1-得分的绝对改进优于现有技术的x矢量[11]的方法的先前状态，并实现所保持的输出测试数据98.43％F1-得分。我们比较了各种池系统接近，并表明GhostVLAD是这个任务的最佳方式汇集。我们还提供了使用Ghost-VLAD汇集和展示所产生的话语层面的嵌入的可视化，这种方法可以创建具有很好的语言判别特征的嵌入。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-02-06</title>
    <url>/2020/02/06/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-02-06/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> TPPO: A Novel Trajectory Predictor with Pseudo Oracle <a href="https://arxiv.org/pdf/2002.01852" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Analyzing the Dependency of ConvNets on Spatial Information <a href="https://arxiv.org/pdf/2002.01827" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Geocoding of trees from street addresses and street-level images <a href="https://arxiv.org/pdf/2002.01708" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> CHAIN: Concept-harmonized Hierarchical Inference Interpretation of Deep  Convolutional Neural Networks <a href="https://arxiv.org/pdf/2002.01660" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Solving Raven's Progressive Matrices with Neural Networks <a href="https://arxiv.org/pdf/2002.01646" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Enhancing Feature Invariance with Learned Image Transformations for  Image Retrieval <a href="https://arxiv.org/pdf/2002.01642" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Illumination adaptive person reid based on teacher-student model and  adversarial training <a href="https://arxiv.org/pdf/2002.01625" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Monocular 3D Object Detection with Decoupled Structured Polygon  Estimation and Height-Guided Depth Estimation <a href="https://arxiv.org/pdf/2002.01619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Generating Interpretable Poverty Maps using Object Detection in  Satellite Images <a href="https://arxiv.org/pdf/2002.01612" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Accelerating Object Detection by Erasing Background Activations <a href="https://arxiv.org/pdf/2002.01609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Anomaly Detection by Latent Regularized Dual Adversarial Networks <a href="https://arxiv.org/pdf/2002.01607" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Unsupervised Community Detection with a Potts Model Hamiltonian, an  Efficient Algorithmic Solution, and Applications in Digital Pathology <a href="https://arxiv.org/pdf/2002.01599" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Ego-Lane Estimation by Modelling Lanes and Sensor Failures <a href="https://arxiv.org/pdf/2002.01913" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> A neural network model that learns differences in diagnosis strategies  among radiologists has an improved area under the curve for aneurysm status  classification in magnetic resonance angiography image series <a href="https://arxiv.org/pdf/2002.01891" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Proximity Preserving Binary Code using Signed Graph-Cut <a href="https://arxiv.org/pdf/2002.01793" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Human Posture Recognition and Gesture Imitation with a Humanoid Robot <a href="https://arxiv.org/pdf/2002.01779" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Feature-map-level Online Adversarial Knowledge Distillation <a href="https://arxiv.org/pdf/2002.01775" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Entropy Minimization vs. Diversity Maximization for Domain Adaptation <a href="https://arxiv.org/pdf/2002.01690" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Concept Whitening for Interpretable Image Recognition <a href="https://arxiv.org/pdf/2002.01650" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>python 设置超时退出</title>
    <url>/2020/02/04/python-%E8%AE%BE%E7%BD%AE%E8%B6%85%E6%97%B6%E9%80%80%E5%87%BA/</url>
    <content><![CDATA[<p>使用<strong>eventlet</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> eventlet</span><br><span class="line">eventlet.monkey_patch()</span><br><span class="line"><span class="keyword">with</span> eventlet.Timeout(<span class="number">10</span>,<span class="literal">False</span>):<span class="comment">#设置超时时间为10秒</span></span><br><span class="line">	time.sleep(<span class="number">20</span>) </span><br><span class="line">	print(<span class="string">'1'</span>)</span><br><span class="line">print(<span class="string">'2'</span>)</span><br></pre></td></tr></table></figure><p>上面程序只输出</p><a id="more"></a>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-02-03</title>
    <url>/2020/02/03/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-02-03/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Pretrained Transformers for Simple Question Answering over Knowledge  Graphs <a href="https://arxiv.org/pdf/2001.11985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> An efficient automated data analytics approach to large scale  computational comparative linguistics <a href="https://arxiv.org/pdf/2001.11899" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Hybrid Tiled Convolutional Neural Networks for Text Sentiment  Classification <a href="https://arxiv.org/pdf/2001.11857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Break It Down: A Question Understanding Benchmark <a href="https://arxiv.org/pdf/2001.11770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Teaching Machines to Converse <a href="https://arxiv.org/pdf/2001.11701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Pseudo-Bidirectional Decoding for Local Sequence Transduction <a href="https://arxiv.org/pdf/2001.11694" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Self-Adversarial Learning with Comparative Discrimination for Text  Generation <a href="https://arxiv.org/pdf/2001.11691" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Augmenting Visual Question Answering with Semantic Frame Information in  a Multitask Learning Approach <a href="https://arxiv.org/pdf/2001.11673" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Enhancement of Short Text Clustering by Iterative Classification <a href="https://arxiv.org/pdf/2001.11631" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Unwanted Advances in Higher Education: Uncovering Sexual Harassment  Experiences in Academia with Text Mining <a href="https://arxiv.org/pdf/2001.11552" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Pretrained Transformers for Simple Question Answering over Knowledge  Graphs</b>  <a href="https://arxiv.org/pdf/2001.11985" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lukovnikov%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">D. Lukovnikov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fischer%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">A. Fischer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lehmann%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">J. Lehmann</a><br>
<font size="3">
Abstract: Answering simple questions over knowledge graphs is a well-studied problem in question answering. Previous approaches for this task built on recurrent and convolutional neural network based architectures that use pretrained word embeddings. It was recently shown that finetuning pretrained transformer networks (e.g. BERT) can outperform previous approaches on various natural language processing tasks. In this work, we investigate how well BERT performs on SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based models in datasparse scenarios. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在知识图回答简单的问题，在问答充分研究的问题。以前的方法完成这个任务建立在使用预训练字的嵌入复发和卷积神经网络基础架构。这是最近表明，微调预训练的变压器网络（例如BERT）可以超越各种自然语言处理任务，以前的方法。在这项工作中，我们探讨SimpleQuestions如何BERT执行和datasparse场景同时提供BERT和基于BiLSTM的模型的评估。</font>
</div>


<hr>
<div id="paper2"> <b>2. An efficient automated data analytics approach to large scale  computational comparative linguistics</b>  <a href="https://arxiv.org/pdf/2001.11899" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mikulyte%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gabija Mikulyte</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gilbert%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Gilbert</a><br>
<font size="3">
Abstract: This research project aimed to overcome the challenge of analysing human language relationships, facilitate the grouping of languages and formation of genealogical relationship between them by developing automated comparison techniques. Techniques were based on the phonetic representation of certain key words and concept. Example word sets included numbers 1-10 (curated), large database of numbers 1-10 and sheep counting numbers 1-10 (other sources), colours (curated), basic words (curated). To enable comparison within the sets the measure of Edit distance was calculated based on Levenshtein distance metric. This metric between two strings is the minimum number of single-character edits, operations including: insertions, deletions or substitutions. To explore which words exhibit more or less variation, which words are more preserved and examine how languages could be grouped based on linguistic distances within sets, several data analytics techniques were involved. Those included density evaluation, hierarchical clustering, silhouette, mean, standard deviation and Bhattacharya coefficient calculations. These techniques lead to the development of a workflow which was later implemented by combining Unix shell scripts, a developed R package and SWI Prolog. This proved to be computationally efficient and permitted the fast exploration of large language sets and their analysis. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本研究项目旨在克服分析人类语言的关系的挑战，通过开发自动比较技术促进语言和形成它们之间的关系家谱分组。技术是基于某些关键词和概念的语音表示。例如字组包括编号1-10（策划），大数据库编号1-10和羊计数编号1-10（其它来源），颜色（策划）的，基本字（策划）。为了使基于Levenshtein距离度量计算编辑距离的测量集合中的比较。两个字符串之间的度量是单字符编辑，操作，包括的最小数目：插入，缺失或取代。探讨其中的话表现出或多或少的变化，这词更保存和研究如何可以语言基于集内的语言距离进行分组，几个数据分析技术的参与。这些问题包括浓度评价，层次聚类，侧影，平均值，标准偏差和查亚系数的计算。这些技术导致后来被合并的Unix shell脚本，一个开发[R包，SWI Prolog的实现工作流的发展。事实证明，这是计算效率和许可的大型语言组和他们的分析快速探索。</font>
</div>


<hr>
<div id="paper3"> <b>3. Hybrid Tiled Convolutional Neural Networks for Text Sentiment  Classification</b>  <a href="https://arxiv.org/pdf/2001.11857" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Trusca%2C+M+M" target="_blank" rel="noopener" style="color:#0000EE;">Maria Mihaela Trusca</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Spanakis%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerasimos Spanakis</a><br>
<font size="3">
Abstract: The tiled convolutional neural network (tiled CNN) has been applied only to computer vision for learning invariances. We adjust its architecture to NLP to improve the extraction of the most salient features for sentiment analysis. Knowing that the major drawback of the tiled CNN in the NLP field is its inflexible filter structure, we propose a novel architecture called hybrid tiled CNN that applies a filter only on the words that appear in the similar contexts and on their neighbor words (a necessary step for preventing the loss of some n-grams). The experiments on the datasets of IMDB movie reviews and SemEval 2017 demonstrate the efficiency of the hybrid tiled CNN that performs better than both CNN and tiled CNN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：平铺卷积神经网络（CNN平铺）已经只适用于计算机视觉学习不变性。我们调整公司架构，以NLP提高最显着的特征为情感分析提取。明知平铺CNN在NLP领域的主要缺点是其不灵活的过滤器结构，我们提出了一种新的架构称为混合平铺CNN说，仅在出现在相似的背景和他们的邻居的话的话应用过滤器（必要步骤，用于防止一些的n-gram的损失）。对IMDB电影评论和SemEval 2017年的数据集上的实验证明了混合动力的效率平铺CNN说，比CNN都和瓷砖CNN性能更好。</font>
</div>


<hr>
<div id="paper4"> <b>4. Break It Down: A Question Understanding Benchmark</b>  <a href="https://arxiv.org/pdf/2001.11770" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wolfson%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tomer Wolfson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Geva%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mor Geva</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankit Gupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gardner%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matt Gardner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goldberg%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoav Goldberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deutch%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Deutch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Berant%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Berant</a><br>
<font size="3">
Abstract: Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：理解自然语言问题需要一个问题分解成用于计算其答案的必要步骤的能力。在这项工作中，我们介绍的问题一个问题分解含义表示（QDMR）。 QDMR构成的步骤，通过自然语言来表达，所必需的回答问题的有序列表。我们开发了一个众包管道，显示出质量QDMRs可以大规模进行标注，并释放中断的数据集，包含超过83K对遇到的问题进行QDMRs。我们通过展示（一），它可以被用来改善对HotpotQA数据集开放域问答，（B），可以确定性地转换成伪SQL形式语言，它可以在语义缓解标注证明QDMR的效用解析应用。最后，我们使用中断训练序列到序列模型复制，它分析问题到QDMR结构，并表明它大幅优于几种天然基线。</font>
</div>


<hr>
<div id="paper5"> <b>5. Teaching Machines to Converse</b>  <a href="https://arxiv.org/pdf/2001.11701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiwei Li</a><br>
<font size="3">
Abstract: The ability of a machine to communicate with humans has long been associated with the general success of AI. This dates back to Alan Turing's epoch-making work in the early 1950s, which proposes that a machine's intelligence can be tested by how well it, the machine, can fool a human into believing that the machine is a human through dialogue conversations. Many systems learn generation rules from a minimal set of authored rules or labels on top of hand-coded rules or templates, and thus are both expensive and difficult to extend to open-domain scenarios. Recently, the emergence of neural network models the potential to solve many of the problems in dialogue learning that earlier systems cannot tackle: the end-to-end neural frameworks offer the promise of scalability and language-independence, together with the ability to track the dialogue state and then mapping between states and dialogue actions in a way not possible with conventional systems. On the other hand, neural systems bring about new challenges: they tend to output dull and generic responses; they lack a consistent or a coherent persona; they are usually optimized through single-turn conversations and are incapable of handling the long-term success of a conversation; and they are not able to take the advantage of the interactions with humans. This dissertation attempts to tackle these challenges: Contributions are two-fold: (1) we address new challenges presented by neural network models in open-domain dialogue generation systems; (2) we develop interactive question-answering dialogue systems by (a) giving the agent the ability to ask questions and (b) training a conversation agent through interactions with humans in an online fashion, where a bot improves through communicating with humans and learning from the mistakes that it makes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一台机器与人沟通的能力一直与AI的普遍成功有关。这可以追溯到50年代初阿兰·图灵的划时代的工作，这提出了一个机器的智能可以通过如何，将本机，可以欺骗一个人相信该机器是通过对话谈话人进行测试。许多系统学习生成规则从一组上的手工编码的规则或模板顶部撰写规则或标签最小，从而既昂贵又难以扩展到开放域场景。最近，神经网络模型的出现的可能性，解决了许多在对话学习的问题，早期的系统无法应对：终端到终端的神经框架提供的可扩展性和语言独立性的承诺，与跟踪的能力一起对话状态，并与传统的系统不可能的方式国和对话的行动之间的映射，然后。在另一方面，神经系统带来了新的挑战：他们往往输出沉闷和通用的应对措施;他们缺乏一致或连贯的角色;他们通常是通过单圈的谈话进行了优化，不能处理的对话的长期成功;他们不能够采取互动的优势与人类。本文试图解决这些挑战：捐款有两方面：（1）我们解决在开放领域对话发电系统的神经网络模型提出了新的挑战; （2）我们开发给代理提问以在线的方式，其中一个机器人通过与人类和学习交流提高训练的对话代理通过互动与人类的能力，和（b）通过互动答疑对话系统（一）从它使错误。</font>
</div>


<hr>
<div id="paper6"> <b>6. Pseudo-Bidirectional Decoding for Local Sequence Transduction</b>  <a href="https://arxiv.org/pdf/2001.11694" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangchunshu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Xu</a><br>
<font size="3">
Abstract: Local sequence transduction (LST) tasks are sequence transduction tasks where there exists massive overlapping between the source and target sequences, such as Grammatical Error Correction (GEC) and spell or OCR correction. Previous work generally tackles LST tasks with standard sequence-to-sequence (seq2seq) models that generate output tokens from left to right and suffer from the issue of unbalanced outputs. Motivated by the characteristic of LST tasks, in this paper, we propose a simple but versatile approach named Pseudo-Bidirectional Decoding (PBD) for LST tasks. PBD copies the corresponding representation of source tokens to the decoder as pseudo future context to enable the decoder to attends to its bi-directional context. In addition, the bidirectional decoding scheme and the characteristic of LST tasks motivate us to share the encoder and the decoder of seq2seq models. The proposed PBD approach provides right side context information for the decoder and models the inductive bias of LST tasks, reducing the number of parameters by half and providing good regularization effects. Experimental results on several benchmark datasets show that our approach consistently improves the performance of standard seq2seq models on LST tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本地序列转导（LST）任务是在存在源和目标序列，例如语法纠错（GEC）和拼写或OCR校正之间大量重叠序列转导的任务。以前的工作通常铲球与生成输出令牌由左到右，并从非平衡输出的问题遭受标准序列对序列（seq2seq）模型LST任务。通过LST任务特性的启发，在本文中，我们提出了一个简单而通用的命名伪双向解码（PBD）为LST任务的方法。 PBD拷贝源的相应表示令牌给解码器作为伪未来上下文，以使解码器能够照顾到其双向上下文。此外，双向解码方案和LST任务的特点促使我们分享编码器和seq2seq车型的解码器。所提出的PBD方法提供了解码器和模型的LST任务归纳偏置，减少一半的参数的数量和提供良好的正规化效果右侧的上下文信息。在几个基准数据集的实验结果表明，该方法可以始终如一提高标准seq2seq车型上LST任务的性能。</font>
</div>


<hr>
<div id="paper7"> <b>7. Self-Adversarial Learning with Comparative Discrimination for Text  Generation</b>  <a href="https://arxiv.org/pdf/2001.11691" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wangchunshu Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ge%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Ge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Furu Wei</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality between a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This self-improvement reward mechanism allows the model to receive credits more easily and avoid collapsing towards the limited number of real samples, which not only helps alleviate the reward sparsity issue but also reduces the risk of mode collapse. Experiments on text generation benchmark datasets show that our proposed approach substantially improves both the quality and the diversity, and yields more stable performance compared to the previous GANs for text generation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：传统的生成性对抗性网络（甘斯）的文本生成往往具有影响的质量和产生的样本的多样性奖励稀疏和模式崩溃的问题。为了解决这个问题，我们提出了一个新的自我对抗学习（SAL）为提高文本生成甘斯的表现模式。与此相反使用二元分类器作为它的鉴别器以预测样品是否是真实的还是生成的标准甘斯，SAL采用比较鉴别器，其是用于在一对样品之间比较所述文本质量成对分类器。在训练期间，如果其目前产生的句子被认为比其以前生成的样本更好SAL奖励发电机。这种自强不息的奖励机制，使模型更容易获得信贷，并避免对数量有限的实际样品，这不仅有助于缓解奖励稀疏问题倒塌，但也降低了模式崩溃的风险。在文本生成基准数据集的实验表明，该方法显着提高的质量和多样性，并产生更稳定的性能相比之前的甘斯的文本生成。</font>
</div>


<hr>
<div id="paper8"> <b>8. Augmenting Visual Question Answering with Semantic Frame Information in  a Multitask Learning Approach</b>  <a href="https://arxiv.org/pdf/2001.11673" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Alizadeh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehrdad Alizadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Di+Eugenio%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Barbara Di Eugenio</a><br>
<font size="3">
Abstract: Visual Question Answering (VQA) concerns providing answers to Natural Language questions about images. Several deep neural network approaches have been proposed to model the task in an end-to-end fashion. Whereas the task is grounded in visual processing, if the question focuses on events described by verbs, the language understanding component becomes crucial. Our hypothesis is that models should be aware of verb semantics, as expressed via semantic role labels, argument types, and/or frame elements. Unfortunately, no VQA dataset exists that includes verb semantic information. Our first contribution is a new VQA dataset (imSituVQA) that we built by taking advantage of the imSitu annotations. The imSitu dataset consists of images manually labeled with semantic frame elements, mostly taken from FrameNet. Second, we propose a multitask CNN-LSTM VQA model that learns to classify the answers as well as the semantic frame elements. Our experiments show that semantic frame element classification helps the VQA system avoid inconsistent responses and improves performance. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉答疑（VQA）的担忧提供了回答有关图像自然语言问题。一些深层神经网络方法被提出来的任务结束到终端的时装模特。尽管任务是在视觉处理接地，如果问题集中在事件描述由动词，理解组件的语言变得至关重要。我们的假设是模型应该知道动词语义的，如通过语义角色标签，参数类型，和/或框架元素表示。不幸的是，没有VQA数据集存在，包括动词语义信息。我们的第一个贡献是一个新的VQA的数据集（imSituVQA），我们通过采取imSitu注释的优势构建。数据集由具有语义框架元件手动标记的图像的imSitu，大多是从框架网络服用。其次，我们提出了一个多任务CNN-LSTM VQA模型学会的答案，以及语义框架内容进行分类。我们的实验表明，语义框架元素的分类有助于VQA系统避免不一致的响应和提高性能。</font>
</div>


<hr>
<div id="paper9"> <b>9. Enhancement of Short Text Clustering by Iterative Classification</b>  <a href="https://arxiv.org/pdf/2001.11631" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rakib%2C+M+R+H" target="_blank" rel="noopener" style="color:#0000EE;">Md Rashadul Hasan Rakib</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeh%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Norbert Zeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jankowska%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Magdalena Jankowska</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Milios%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evangelos Milios</a><br>
<font size="3">
Abstract: Short text clustering is a challenging task due to the lack of signal contained in such short texts. In this work, we propose iterative classification as a method to b o ost the clustering quality (e.g., accuracy) of short texts. Given a clustering of short texts obtained using an arbitrary clustering algorithm, iterative classification applies outlier removal to obtain outlier-free clusters. Then it trains a classification algorithm using the non-outliers based on their cluster distributions. Using the trained classification model, iterative classification reclassifies the outliers to obtain a new set of clusters. By repeating this several times, we obtain a much improved clustering of texts. Our experimental results show that the proposed clustering enhancement method not only improves the clustering quality of different clustering methods (e.g., k-means, k-means--, and hierarchical clustering) but also outperforms the state-of-the-art short text clustering methods on several short text datasets by a statistically significant margin. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：短文本聚类是一个具有挑战性的任务，由于包含在如此短的文字缺乏的信号。在这项工作中，我们提出的迭代分类为B○OST短文本的聚类质量（例如，准确度）的方法。鉴于使用任意的聚类算法获得的短文本的聚类，分类迭代适用异常值去除以获得离群-空闲簇。然后训练使用基于其集群分布的非离群的分类算法。利用训练的分类模型，迭代分类重新分类离群获得一组新的集群。通过重复几次，我们得到的文本大大改善群集。我们的实验结果表明，所提出的聚类增强方法不仅提高了的不同的聚类方法（例如，k均值，K-指：，和层次聚类）聚类质量也优于状态的最先进的短文本有统计显著保证金聚类在几个简短的文本数据集的方法。</font>
</div>


<hr>
<div id="paper10"> <b>10. Unwanted Advances in Higher Education: Uncovering Sexual Harassment  Experiences in Academia with Text Mining</b>  <a href="https://arxiv.org/pdf/2001.11552" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/physics?searchtype=author&query=Karami%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Amir Karami</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=White%2C+C+N" target="_blank" rel="noopener" style="color:#0000EE;">Cynthia Nicole White</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Ford%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kayla Ford</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Swan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Suzanne Swan</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&query=Spinel%2C+M+Y" target="_blank" rel="noopener" style="color:#0000EE;">Melek Yildiz Spinel</a><br>
<font size="3">
Abstract: Sexual harassment in academia is often a hidden problem because victims are usually reluctant to report their experiences. Recently, a web survey was developed to provide an opportunity to share thousands of sexual harassment experiences in academia. Using an efficient approach, this study collected and investigated more than 2,000 sexual harassment experiences to better understand these unwanted advances in higher education. This paper utilized text mining to disclose hidden topics and explore their weight across three variables: harasser gender, institution type, and victim's field of study. We mapped the topics on five themes drawn from the sexual harassment literature and found that more than 50% of the topics were assigned to the unwanted sexual attention theme. Fourteen percent of the topics were in the gender harassment theme, in which insulting, sexist, or degrading comments or behavior was directed towards women. Five percent of the topics involved sexual coercion (a benefit is offered in exchange for sexual favors), 5% involved sex discrimination, and 7% of the topics discussed retaliation against the victim for reporting the harassment, or for simply not complying with the harasser. Findings highlight the power differential between faculty and students, and the toll on students when professors abuse their power. While some topics did differ based on type of institution, there were no differences between the topics based on gender of harasser or field of study. This research can be beneficial to researchers in further investigation of this paper's dataset, and to policymakers in improving existing policies to create a safe and supportive environment in academia. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在学术界性骚扰往往是一个隐藏的问题，因为受害者往往不愿意报告自己的经历。最近，网络调查的开发提供共享成千上万的性骚扰经历学术界的机会。使用一种有效的方法，这项研究收集和调查2000余和性骚扰的经验，以更好地了解高等教育这些不必要的进步。本文利用文本挖掘透露隐藏的主题和跨越三个变量探索自己的体重：骚扰者性别，机构类型和研究的受害人的领域。我们映射从性骚扰文献中提取，发现的主题超过50％被分配到不必要的性关注主题五个主题的主题。的主题十四％的人在性别骚扰主题，在这种侮辱，性别歧视，或侮辱性的评论或行为针对妇女。的主题百分之五参与性胁迫（一个好处是提供以换取性方面的好处），5％涉及性别歧视，以及主题7％讨论对受害者报复举报骚扰，或者干脆不与骚扰符合。发现突出的教师和学生，以及学生的收费之间的功率差时，教授滥用职权。虽然有些题目确实有所不同根据类型的机构，有基于研究的骚扰或领域的性别主题之间没有差异。这项研究可以在本文的数据集的进一步调查研究有利，对政策制定者改善现有的政策，创造学术界安全和支持的环境。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>pip 安装提示空间不足</title>
    <url>/2020/01/21/pip-%E5%AE%89%E8%A3%85%E6%8F%90%E7%A4%BA%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3/</url>
    <content><![CDATA[<p>pip 安装提示空间不足</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Could not install packages due to an EnvironmentError: [Errno 28] No space left on device</span><br></pre></td></tr></table></figure><p>这是服务器上的/tmp空间不足，可以在自己的根目录下简历~/tmp代替 /tmp</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir ~/tmp</span><br><span class="line">export TMPDIR=$HOME/tmp</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computer Vision and Pattern Recognition 2020-01-20</title>
    <url>/2020/01/20/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computer%20Vision%20and%20Pattern%20Recognition%202020-01-20/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Unsupervised Learning of Camera Pose with Compositional Re-estimation <a href="https://arxiv.org/pdf/2001.06479" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Combining PRNU and noiseprint for robust and efficient device source  identification <a href="https://arxiv.org/pdf/2001.06440" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> TailorGAN: Making User-Defined Fashion Designs <a href="https://arxiv.org/pdf/2001.06427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Subjective Annotation for a Frame Interpolation Benchmark using Artifact  Amplification <a href="https://arxiv.org/pdf/2001.06409" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> GraphBGS: Background Subtraction via Recovery of Graph Signals <a href="https://arxiv.org/pdf/2001.06404" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Latency-Aware Differentiable Neural Architecture Search <a href="https://arxiv.org/pdf/2001.06392" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> BigEarthNet Deep Learning Models with A New Class-Nomenclature for  Remote Sensing Image Understanding <a href="https://arxiv.org/pdf/2001.06372" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Efficient Facial Feature Learning with Wide Ensemble-based Convolutional  Neural Networks <a href="https://arxiv.org/pdf/2001.06338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Vision Meets Drones: Past, Present and Future <a href="https://arxiv.org/pdf/2001.06303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Predicting the Physical Dynamics of Unseen 3D Objects <a href="https://arxiv.org/pdf/2001.06291" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Review: deep learning on 3D point clouds <a href="https://arxiv.org/pdf/2001.06280" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Compounding the Performance Improvements of Assembled Techniques in a  Convolutional Neural Network <a href="https://arxiv.org/pdf/2001.06268" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On <a href="https://arxiv.org/pdf/2001.06265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Two-Phase Object-Based Deep Learning for Multi-temporal SAR Image Change  Detection <a href="https://arxiv.org/pdf/2001.06252" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Registration made easy -- standalone orthopedic navigation with HoloLens <a href="https://arxiv.org/pdf/2001.06209" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> FPCR-Net: Feature Pyramidal Correlation and Residual Reconstruction for  Semi-supervised Optical Flow Estimation <a href="https://arxiv.org/pdf/2001.06171" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Interpreting Galaxy Deblender GAN from the Discriminator's Perspective <a href="https://arxiv.org/pdf/2001.06151" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Learning to Augment Expressions for Few-shot Fine-grained Facial  Expression Recognition <a href="https://arxiv.org/pdf/2001.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Spatio-Temporal Ranked-Attention Networks for Video Captioning <a href="https://arxiv.org/pdf/2001.06127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Automatic Discovery of Political Meme Genres with Diverse Appearances <a href="https://arxiv.org/pdf/2001.06122" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> On- Device Information Extraction from Screenshots in form of tags <a href="https://arxiv.org/pdf/2001.06094" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
<div id="title22">
<b>22.</b> Tracking of Micro Unmanned Aerial Vehicles: A Comparative Study <a href="https://arxiv.org/pdf/2001.06066" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper22" style="color:#0000EE;">摘要</a><br></div>
<div id="title23">
<b>23.</b> Increasing the robustness of DNNs against image corruptions by playing  the Game of Noise <a href="https://arxiv.org/pdf/2001.06057" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper23" style="color:#0000EE;">摘要</a><br></div>
<div id="title24">
<b>24.</b> Modality-Balanced Models for Visual Dialogue <a href="https://arxiv.org/pdf/2001.06354" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper24" style="color:#0000EE;">摘要</a><br></div>
<div id="title25">
<b>25.</b> Tethered Aerial Visual Assistance <a href="https://arxiv.org/pdf/2001.06347" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper25" style="color:#0000EE;">摘要</a><br></div>
<div id="title26">
<b>26.</b> DeepSUM++: Non-local Deep Neural Network for Super-Resolution of  Unregistered Multitemporal Images <a href="https://arxiv.org/pdf/2001.06342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper26" style="color:#0000EE;">摘要</a><br></div>
<div id="title27">
<b>27.</b> Detection Method Based on Automatic Visual Shape Clustering for  Pin-Missing Defect in Transmission Lines <a href="https://arxiv.org/pdf/2001.06236" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper27" style="color:#0000EE;">摘要</a><br></div>
<div id="title28">
<b>28.</b> Sideways: Depth-Parallel Training of Video Models <a href="https://arxiv.org/pdf/2001.06232" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper28" style="color:#0000EE;">摘要</a><br></div>
<div id="title29">
<b>29.</b> FedVision: An Online Visual Object Detection Platform Powered by  Federated Learning <a href="https://arxiv.org/pdf/2001.06202" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper29" style="color:#0000EE;">摘要</a><br></div>
<div id="title30">
<b>30.</b> Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless  Approach <a href="https://arxiv.org/pdf/2001.06175" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper30" style="color:#0000EE;">摘要</a><br></div>
<div id="title31">
<b>31.</b> An adversarial learning framework for preserving users' anonymity in  face-based emotion recognition <a href="https://arxiv.org/pdf/2001.06103" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper31" style="color:#0000EE;">摘要</a><br></div>
<div id="title32">
<b>32.</b> Code-Bridged Classifier (CBC): A Low or Negative Overhead Defense for  Making a CNN Classifier Robust Against Adversarial Attacks <a href="https://arxiv.org/pdf/2001.06099" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper32" style="color:#0000EE;">摘要</a><br></div>
<div id="title33">
<b>33.</b> Curriculum Labeling: Self-paced Pseudo-Labeling for Semi-Supervised  Learning <a href="https://arxiv.org/pdf/2001.06001" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper33" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Unsupervised Learning of Camera Pose with Compositional Re-estimation</b>  <a href="https://arxiv.org/pdf/2001.06479" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Nabavi%2C+S+S" target="_blank" rel="noopener" style="color:#0000EE;">Seyed Shahabeddin Nabavi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hosseinzadeh%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mehrdad Hosseinzadeh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fahimi%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ramin Fahimi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Wang</a><br>
<font size="3">
Abstract: We consider the problem of unsupervised camera pose estimation. Given an input video sequence, our goal is to estimate the camera pose (i.e. the camera motion) between consecutive frames. Traditionally, this problem is tackled by placing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We propose an alternative approach that utilizes a compositional re-estimation process for camera pose estimation. Given an input, we first estimate a depth map. Our method then iteratively estimates the camera motion based on the estimated depth map. Our approach significantly improves the predicted camera motion both quantitatively and visually. Furthermore, the re-estimation resolves the problem of out-of-boundaries pixels in a novel and simple way. Another advantage of our approach is that it is adaptable to other camera pose estimation approaches. Experimental analysis on KITTI benchmark dataset demonstrates that our method outperforms existing state-of-the-art approaches in unsupervised camera ego-motion estimation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们认为监督的相机姿态估计的问题。给定的输入视频序列，我们的目标是估计连续帧之间的摄像机姿态（即，照相机运动）。传统上，这个问题是通过将严格的约束的转化载体或通过一个复杂的管道结合光流解决。我们建议，利用相机姿势估计的成分重新估计过程的替代方法。给定一个输入，我们首先估计深度图。然后，我们的迭代算法估计基于估计的深度地图上的摄像机运动。我们的方法在数量上和视觉上显著提高了预测的摄像机运动。此外，重新估计解决了一种新颖和简单的方式外的边界像素的问题。我们的方法的另一个优点是，它是适用于其他相机姿态估计方法。上KITTI基准数据集试验分析表明，我们现有的最先进的国家的方法优于在无监督照相机自运动估计方法。</font>
</div>


<hr>
<div id="paper2"> <b>2. Combining PRNU and noiseprint for robust and efficient device source  identification</b>  <a href="https://arxiv.org/pdf/2001.06440" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cozzolino%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Cozzolino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marra%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Francesco Marra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gragnaniello%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diego Gragnaniello</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poggi%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giovanni Poggi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Verdoliva%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luisa Verdoliva</a><br>
<font size="3">
Abstract: PRNU-based image processing is a key asset in digital multimedia forensics. It allows for reliable device identification and effective detection and localization of image forgeries, in very general conditions. However, performance impairs significantly in challenging conditions involving low quality and quantity of data. These include working on compressed and cropped images, or estimating the camera PRNU pattern based on only a few images. To boost the performance of PRNU-based analyses in such conditions we propose to leverage the image noiseprint, a recently proposed camera-model fingerprint that has proved effective for several forensic tasks. Numerical experiments on datasets widely used for source identification prove that the proposed method ensures a significant performance improvement in a wide range of challenging situations. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于PRNU图像处理是数字多媒体取证的重要资产。它允许可靠的装置识别和有效的检测和图像伪造的定位，在很一般的条件。然而，性能也妨碍显著在挑战包括低质量和数据量的条件。这些包括工作压缩和裁切图像，或估计基于只有少数图像中的相机PRNU图案。为了提高在这样的条件下基于PRNU-分析的性能，我们提出了利用图像noiseprint，已被证明有效的几个法医任务的最近提出的相机型号的指纹。上的数据集广泛用于源识别数值实验证明，该方法确保在广泛的挑战的情况一显著性能改进。</font>
</div>


<hr>
<div id="paper3"> <b>3. TailorGAN: Making User-Defined Fashion Designs</b>  <a href="https://arxiv.org/pdf/2001.06427" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lele Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Justin Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guo Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheng-Haw Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=King%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Erh-Kan King</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kuan-Ting Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hsieh%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shao-Hang Hsieh</a><br>
<font size="3">
Abstract: Attribute editing has become an important and emerging topic of computer vision. In this paper, we consider a task: given a reference garment image A and another image B with target attribute (collar/sleeve), generate a photo-realistic image which combines the texture from reference A and the new attribute from reference B. The highly convoluted attributes and the lack of paired data are the main challenges to the task. To overcome those limitations, we propose a novel self-supervised model to synthesize garment images with disentangled attributes (e.g., collar and sleeves) without paired data. Our method consists of a reconstruction learning step and an adversarial learning step. The model learns texture and location information through reconstruction learning. And, the model's capability is generalized to achieve single-attribute manipulation by adversarial learning. Meanwhile, we compose a new dataset, named GarmentSet, with annotation of landmarks of collars and sleeves on clean garment images. Extensive experiments on this dataset and real-world samples demonstrate that our method can synthesize much better results than the state-of-the-art methods in both quantitative and qualitative comparisons. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：属性编辑已经成为计算机视觉的一个重要和新兴的话题。在本文中，我们考虑一个任务：给定一个参考服装图像A和与目标属性（领/套筒）另一图像B，生成结合了从参考点A的质地和从参考B的新的属性的照片般逼真的图像高度令人费解的属性和缺乏配对数据是任务的主要挑战。为了克服这些限制，我们提出了一种新型的自监督模型以合成服装图像与解缠结的属性（例如，领子和袖子），而不配对数据。我们的方法包括一个重建学习步骤和敌对学习步骤的。该模型通过学习学习重建质地和位置信息。而且，该模型的能力是广义的对抗学习，实现单属性操作。同时，我们组成一个新的数据集，名为GarmentSet，用干净的服装图像领子和袖子的地标标注。在此数据集和真实世界的样本大量的实验表明，我们的方法可以合成比国家的最先进的方法，定量和定性的比较更好的结果。</font>
</div>


<hr>
<div id="paper4"> <b>4. Subjective Annotation for a Frame Interpolation Benchmark using Artifact  Amplification</b>  <a href="https://arxiv.org/pdf/2001.06409" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Men%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hui Men</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hosu%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vlad Hosu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hanhe Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bruhn%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrés Bruhn</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Saupe%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dietmar Saupe</a><br>
<font size="3">
Abstract: Current benchmarks for optical flow algorithms evaluate the estimation either directly by comparing the predicted flow fields with the ground truth or indirectly by using the predicted flow fields for frame interpolation and then comparing the interpolated frames with the actual frames. In the latter case, objective quality measures such as the mean squared error are typically employed. However, it is well known that for image quality assessment, the actual quality experienced by the user cannot be fully deduced from such simple measures. Hence, we conducted a subjective quality assessment crowdscouring study for the interpolated frames provided by one of the optical flow benchmarks, the Middlebury benchmark. It contains interpolated frames from 155 methods applied to each of 8 contents. We collected forced choice paired comparisons between interpolated images and corresponding ground truth. To increase the sensitivity of observers when judging minute difference in paired comparisons we introduced a new method to the field of full-reference quality assessment, called artifact amplification. From the crowdsourcing data we reconstructed absolute quality scale values according to Thurstone's model. As a result, we obtained a re-ranking of the 155 participating algorithms w.r.t. the visual quality of the interpolated frames. This re-ranking not only shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks, the results also provide the ground truth for designing novel image quality assessment (IQA) methods dedicated to perceptual quality of interpolated images. As a first step, we proposed such a new full-reference method, called WAE-IQA. By weighing the local differences between an interpolated image and its ground truth WAE-IQA performed slightly better than the currently best FR-IQA approach from the literature. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：用于光学流算法电流基准通过与地面实况地或间接地通过使用所预测的流场为帧内插，然后比较实际帧中的内插帧进行比较的预测的流场评价了估计直接。在后者的情况下，客观质量的措施，如均方误差，通常采用。但是，众所周知，对于图像质量评价，用户体验到的实际质量不能完全从这些简单的措施推出。因此，我们进行了由所述光流基准之一，所述明德基准提供的内插帧主观质量评估crowdscouring研究。它包含从施加到每个8项内容155点的方法的内插帧。我们收集了强制插入图片和相应的地面实况之间选择配对比较。为了增加观察员的灵敏度，当在配对比较判断分差，我们引入了一个新的方法来全参考质量评估领域，被称为神器放大。从众包数据，我们根据瑟斯顿模型重建质量绝对刻度值。其结果是，我们获得了155种参与算法的重新排名w.r.t.内插帧的视觉质量。这个重新排序不仅示出了视觉质量评估作为另一个评价度量光流和帧插值基准的必要性，该结果也提供了设计新的图像质量评价地面实况（IQA）的方法专用于内插图像的感知质量。作为第一步，我们提出了这样一个新的全参考方法，称为WAE-IQA。通过称重插入图像和地面实况WAE-IQA之间的局部差异不是从文献中目前最好的FR-IQA方法表现稍好。</font>
</div>


<hr>
<div id="paper5"> <b>5. GraphBGS: Background Subtraction via Recovery of Graph Signals</b>  <a href="https://arxiv.org/pdf/2001.06404" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Giraldo%2C+J+H" target="_blank" rel="noopener" style="color:#0000EE;">Jhony H. Giraldo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bouwmans%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thierry Bouwmans</a><br>
<font size="3">
Abstract: Graph-based algorithms have been successful approaching the problems of unsupervised and semi-supervised learning. Recently, the theory of graph signal processing and semi-supervised learning have been combined leading to new developments and insights in the field of machine learning. In this paper, concepts of recovery of graph signals and semi-supervised learning are introduced in the problem of background subtraction. We propose a new algorithm named GraphBGS, this method uses a Mask R-CNN for instances segmentation; temporal median filter for background initialization; motion, texture, color, and structural features for representing the nodes of a graph; k-nearest neighbors for the construction of the graph; and finally a semi-supervised method inspired from the theory of recovery of graph signals to solve the problem of background subtraction. The method is evaluated on the publicly available change detection, and scene background initialization databases. Experimental results show that GraphBGS outperforms unsupervised background subtraction algorithms in some challenges of the change detection dataset. And most significantly, this method outperforms generative adversarial networks in unseen videos in some sequences of the scene background initialization database. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于图的算法已经成功逼近无监督和半监督学习的问题。近日，图形信号处理和半监督学习的理论已被合并导致新的进展和见解，在机器学习领域。在本文中，图形信号及半监督学习的恢复的概念背景减除的问题进行了介绍。我们提出了一种新的算法命名GraphBGS，这种方法使用面膜R-CNN的情况下，分割;时间中值滤波器，用于背景初始化;运动，纹理，颜色和用于表示图中的节点的结构特征; k最近的图的构造的邻居;最后一个半监督方法从图信号的恢复的理论启发解决背景减除的问题。该方法在可公开获得的变化检测评价，并现场后台初始化数据库。实验结果表明，在变化检测数据集的一些挑战GraphBGS性能优于无人监督的背景减除算法。而最显著，这种方法优于在场景后台初始化数据库的一些序列看不见的视频生成对抗性的网络。</font>
</div>


<hr>
<div id="paper6"> <b>6. Latency-Aware Differentiable Neural Architecture Search</b>  <a href="https://arxiv.org/pdf/2001.06392" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuhui Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingxi Xie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaopeng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bowen Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongkai Xiong</a><br>
<font size="3">
Abstract: Differentiable neural architecture search methods became popular in automated machine learning, mainly due to their low search costs and flexibility in designing the search space. However, these methods suffer the difficulty in optimizing network, so that the searched network is often unfriendly to hardware. This paper deals with this problem by adding a differentiable latency loss term into optimization, so that the search process can tradeoff between accuracy and latency with a balancing coefficient. The core of latency prediction is to encode each network architecture and feed it into a multi-layer regressor, with the training data being collected from randomly sampling a number of architectures and evaluating them on the hardware. We evaluate our approach on NVIDIA Tesla-P100 GPUs. With 100K sampled architectures (requiring a few hours), the latency prediction module arrives at a relative error of lower than 10\%. Equipped with this module, the search method can reduce the latency by 20% meanwhile preserving the accuracy. Our approach also enjoys the ability of being transplanted to a wide range of hardware platforms with very few efforts, or being used to optimizing other non-differentiable factors such as power consumption. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：微的神经结构的搜索方法成为自动化机器学习流行，主要是由于其较低的搜寻成本和设计的搜索空间的灵活性。然而，这些方法在遭受网络优化的难度，使网络搜索往往是不友好的硬件。这与这个问题论文涉及通过添加微延迟损失项为优化，使之与平衡系数精度和延迟之间的搜索过程可以权衡。延迟预测的核心是编码每个网络结构和它送入多层回归，与从随机抽样的数架构和硬件评估他们被收集训练数据。我们评估我们对NVIDIA的Tesla-P100 GPU的方法。用100K采样架构（需要几个小时），等待时间预测模块到达的低于10 \％的相对误差。配备该模块，搜索方法可以通过20％的同时保持准确度降低延迟。我们的方法也享有被移植到了广泛的硬件平台用很少的努力，或者被用来优化其它非微因素，例如功耗的能力。</font>
</div>


<hr>
<div id="paper7"> <b>7. BigEarthNet Deep Learning Models with A New Class-Nomenclature for  Remote Sensing Image Understanding</b>  <a href="https://arxiv.org/pdf/2001.06372" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Sumbul%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gencer Sumbul</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Kang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kreuziger%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tristan Kreuziger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marcelino%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Filipe Marcelino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Costa%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hugo Costa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Benevides%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pedro Benevides</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Caetano%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mario Caetano</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Demir%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Begüm Demir</a><br>
<font size="3">
Abstract: Success of deep neural networks in the framework of remote sensing (RS) image analysis depends on the availability of a high number of annotated images. BigEarthNet is a new large-scale Sentinel-2 benchmark archive that has been recently introduced in RS to advance deep learning (DL) studies. Each image patch in BigEarthNet is annotated with multi-labels provided by the CORINE Land Cover (CLC) map of 2018 based on its most thematic detailed Level-3 class nomenclature. BigEarthNet has enabled data-hungry DL algorithms to reach high performance in the context of multi-label RS image retrieval and classification. However, initial research demonstrates that some CLC classes are challenging to be accurately described by considering only (single-date) Sentinel-2 images. To further increase the effectiveness of BigEarthNet, in this paper we introduce an alternative class-nomenclature to allow DL models for better learning and describing the complex spatial and spectral information content of the Sentinel-2 images. This is achieved by interpreting and arranging the CLC Level-3 nomenclature based on the properties of Sentinel-2 images in a new nomenclature of 19 classes. Then, the new class-nomenclature of BigEarthNet is used within state-of-the-art DL models (namely VGG model at the depth of 16 and 19 layers [VGG16 and VGG19] and ResNet model at the depth of 50, 101 and 152 layers [ResNet50, ResNet101, ResNet152] as well as K-Branch CNN model) in the context of multi-label classification. Experimental results show that the models trained from scratch on BigEarthNet outperform those pre-trained on ImageNet, especially in relation to some complex classes including agriculture and other vegetated and natural environments. All DL models are made publicly available, offering an important resource to guide future progress on content based image retrieval and scene classification problems in RS. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：遥感（RS）图像分析的框架深神经网络的成功取决于大量的注释的图像的可用性。 BigEarthNet是已在RS最近推出深处前进学习（DL）研究提供了新的大型哨兵-2基准存档。在BigEarthNet每个图像补丁标注有基于其最详细的专题级别3级命名的2018年CORINE土地覆盖（CLC）地图提供多标签。 BigEarthNet已使大量数据的DL算法，以达到多标签遥感影像检索和分类的情况下的高性能。然而，最初的研究表明，一些CLC类是具有挑战性的通过仅考虑（单日期）被精确地描述的Sentinel-2的图像。为了进一步提高BigEarthNet的有效性，本文介绍一种替代类的术语来允许DL模型更好的学习和描述哨兵2图像的复杂的空间和光谱信息的内容。这是通过解释和布置基于哨兵-2图像的在19类的新命名法的属性CLC 3级命名法来实现的。然后，BigEarthNet的新的类命名法是国家的最先进的DL模型（即VGG模型内以16层19的层[VGG16和VGG19]和RESNET模型的深度使用在50，101和152的深度层[ResNet50，ResNet101，ResNet152]以及K-科CNN模型）在多标签分类的上下文。实验结果表明，从头开始培训了BigEarthNet跑赢车型的预先训练上ImageNet，特别是涉及到一些复杂的类，包括农业和其他植被和自然环境。所有DL型号都公之于众，提供指导在RS基于内容的图像检索及场景分类问题未来发展的重要资源。</font>
</div>


<hr>
<div id="paper8"> <b>8. Efficient Facial Feature Learning with Wide Ensemble-based Convolutional  Neural Networks</b>  <a href="https://arxiv.org/pdf/2001.06338" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Siqueira%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Henrique Siqueira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Magg%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sven Magg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wermter%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Wermter</a><br>
<font size="3">
Abstract: Ensemble methods, traditionally built with independently trained de-correlated models, have proven to be efficient methods for reducing the remaining residual generalization error, which results in robust and accurate methods for real-world applications. In the context of deep learning, however, training an ensemble of deep networks is costly and generates high redundancy which is inefficient. In this paper, we present experiments on Ensembles with Shared Representations (ESRs) based on convolutional networks to demonstrate, quantitatively and qualitatively, their data processing efficiency and scalability to large-scale datasets of facial expressions. We show that redundancy and computational load can be dramatically reduced by varying the branching level of the ESR without loss of diversity and generalization power, which are both important for ensemble performance. Experiments on large-scale datasets suggest that ESRs reduce the remaining residual generalization error on the AffectNet and FER+ datasets, reach human-level performance, and outperform state-of-the-art methods on facial expression recognition in the wild using emotion and affect concepts. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：集成方法，传统上与​​独立的培训去相关模型构建，已被证明是减少残留的剩余泛化误差，有效的方法，这导致对现实世界的应用健全和准确的方法。在深学习的情况下，然而，培养深网络的集合是昂贵的，并且产生高冗余这是低效的。在本文中，我们对合奏基于卷积网络证明，定量和定性地共享交涉（的ESR）本实验中，它们的数据处理效率和可扩展性的面部表情的大规模数据集。我们发现可以通过改变ESR的无分集和概括断电分支水平，这既是对合奏表演重要的急剧减少了冗余和计算负载。在大型数据集的实验表明，的ESR减少对AffectNet和FER +数据集，达到人类水平的性能，以及使用情感上的野生面部表情识别跑赢大市的国家的最先进的方法，直接影响概念的剩余的残留泛化的错误。</font>
</div>


<hr>
<div id="paper9"> <b>9. Vision Meets Drones: Past, Present and Future</b>  <a href="https://arxiv.org/pdf/2001.06303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wen%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Longyin Wen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dawei Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bian%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Bian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinghua Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haibin Ling</a><br>
<font size="3">
Abstract: Drones, or general UAVs, equipped with cameras have been fast deployed with a wide range of applications, including agriculture, aerial photography, fast delivery, and surveillance. Consequently, automatic understanding of visual data collected from drones becomes highly demanding, bringing computer vision and drones more and more closely. To promote and track the developments of object detection and tracking algorithms, we have organized two challenge workshops in conjunction with European Conference on Computer Vision (ECCV) 2018, and IEEE International Conference on Computer Vision (ICCV) 2019, attracting more than 100 teams around the world. We provide a large-scale drone captured dataset, VisDrone, which includes four tracks, i.e., (1) image object detection, (2) video object detection, (3) single object tracking, and (4) multi-object tracking. This paper first presents a thorough review of object detection and tracking datasets and benchmarks, and discuss the challenges of collecting large-scale drone-based object detection and tracking datasets with fully manual annotations. After that, we describe our VisDrone dataset, which is captured over various urban/suburban areas of $14$ different cities across China from North to South. Being the largest such dataset ever published, VisDrone enables extensive evaluation and investigation of visual analysis algorithms on the drone platform. We provide a detailed analysis of the current state of the field of large-scale object detection and tracking on drones, and conclude the challenge as well as propose future directions and improvements. We expect the benchmark largely boost the research and development in video analysis on drone platforms. All the datasets and experimental results can be downloaded from the website: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：无人机或者一般的无人机，配备摄像头已经迅速部署具有广泛的应用，包括农业，航空摄影，交货快，和监视。因此，从无人机采集的视频数据的自动理解变得极高，将计算机视觉和无人驾驶飞机越来越紧密。为了促进和跟踪目标检测与跟踪算法的发展，我们已经组织了一起2次挑战研讨会，欧洲会议计算机视觉（ECCV）2018，以及计算机视觉（ICCV）2019 IEEE国际会议，吸引了100多个团队世界。我们提供了一个大型雄蜂捕获数据集，VisDrone，它包括四个磁道，即，（1）图像对象检测，（2）视频对象检测，（3）单目标跟踪，和（4）的多目标跟踪。本文首先介绍目标检测与跟踪数据集和基准进行彻底审查，并讨论收集大型无人机基于体检测，并与全手动注释跟踪数据集的挑战。在那之后，我们描述了我们VisDrone数据集，这是超过$ $ 14在中国不同的城市，从南到北各个城市/郊区抓获。作为最大的此类数据集出版过的，VisDrone使广泛的评估和无人机平台上的视觉分析算法调查。我们提供大型物体检测和跟踪在无人机领域的现状进行了详细分析，并得出结论以及提出未来的发展方向和改进的挑战。我们预计恒生很大程度上提高对无人机平台在视频分析的研究和开发。此HTTPS URL：所有数据集和实验结果可以从网站上下载。</font>
</div>


<hr>
<div id="paper10"> <b>10. Predicting the Physical Dynamics of Unseen 3D Objects</b>  <a href="https://arxiv.org/pdf/2001.06291" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rempe%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davis Rempe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sridhar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Srinath Sridhar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">He Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guibas%2C+L+J" target="_blank" rel="noopener" style="color:#0000EE;">Leonidas J. Guibas</a><br>
<font size="3">
Abstract: Machines that can predict the effect of physical interactions on the dynamics of previously unseen object instances are important for creating better robots and interactive virtual worlds. In this work, we focus on predicting the dynamics of 3D objects on a plane that have just been subjected to an impulsive force. In particular, we predict the changes in state - 3D position, rotation, velocities, and stability. Different from previous work, our approach can generalize dynamics predictions to object shapes and initial conditions that were unseen during training. Our method takes the 3D object's shape as a point cloud and its initial linear and angular velocities as input. We extract shape features and use a recurrent neural network to predict the full change in state at each time step. Our model can support training with data from both a physics engine or the real world. Experiments show that we can accurately predict the changes in state for unseen object geometries and initial conditions. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器，可以预测在以前看不见的对象实例的动态物理相互作用的作用是创造更好的机器人和互动的虚拟世界重要。在这项工作中，我们侧重于预测对刚刚经受冲击力的平面3D对象的动态。特别是，我们预测状态的变化 - 三维位置，旋转，速度和稳定性。从以前的工作不同的是，我们的方法可以概括的动态预测到物体的形状和初始条件的培训过程中看不见。我们的方法利用该3D对象的形状为点云和它的初始线速度和角速度作为输入。我们提取形状特征和使用回归神经网络在每个时间步来预测状态充满变化。我们的模型可以支持从两个物理引擎或现实世界的数据训练。实验结果表明，我们可以精确地预测为看不见的对象的几何形状和初始条件状态中的变化。</font>
</div>


<hr>
<div id="paper11"> <b>11. Review: deep learning on 3D point clouds</b>  <a href="https://arxiv.org/pdf/2001.06280" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Bello%2C+S+A" target="_blank" rel="noopener" style="color:#0000EE;">Saifullahi Aminu Bello</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shangshu Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cheng Wang</a><br>
<font size="3">
Abstract: Point cloud is point sets defined in 3D metric space. Point cloud has become one of the most significant data format for 3D representation. Its gaining increased popularity as a result of increased availability of acquisition devices, such as LiDAR, as well as increased application in areas such as robotics, autonomous driving, augmented and virtual reality. Deep learning is now the most powerful tool for data processing in computer vision, becoming the most preferred technique for tasks such as classification, segmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, point cloud, on the other hand, is unstructured. The unstructuredness of point clouds makes use of deep learning for its processing directly very challenging. Earlier approaches overcome this challenge by preprocessing the point cloud into a structured grid format at the cost of increased computational cost or lost of depth information. Recently, however, many state-of-the-arts deep learning techniques that directly operate on point cloud are being developed. This paper contains a survey of the recent state-of-the-art deep learning techniques that mainly focused on point cloud data. We first briefly discussed the major challenges faced when using deep learning directly on point cloud, we also briefly discussed earlier approaches which overcome the challenges by preprocessing the point cloud into a structured grid. We then give the review of the various state-of-the-art deep learning approaches that directly process point cloud in its unstructured form. We introduced the popular 3D point cloud benchmark datasets. And we also further discussed the application of deep learning in popular 3D vision tasks including classification, segmentation and detection. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：点云是3D度量空间定义的点集。点云已成为3D表示最显著数据格式之一。它获得越来越多的受欢迎程度增加采集设备，如激光雷达的可用性，以及在诸如机器人，自动驾驶等领域加强应用，增强和虚拟现实的结果。现在深学习是数据在计算机视觉处理的最有力的工具，成为任务，如分类，细分和检测的最优选的技术。虽然深学习技术主要应用于数据与结构化网格，点云，在另一方面，是非结构化的。该unstructuredness点云的利用深度学习的其直接处理非常具有挑战性。早期的方法通过预处理点云成结构化的网格格式以增加计算成本的成本或丢失的深度信息克服这一挑战。然而，最近深学习直接对点云操作的技术的许多艺术国家的正在开发中。本文件包含了一个调查国家的最先进的深得知主要集中在点云数据的技术，最近的。我们首先简要讨论了使用深直接在点云学习时所面临的重大挑战，我们还简要讨论克服通过预处理点云成结构化网格的挑战，早期的方法。然后，我们给国家的最先进的各种深学习的复习方法直接处理点云中的非结构化的形式。我们引进了当前流行的三维点云标准数据集。我们还进一步讨论在流行的3D视觉任务，包括分类，分割和检测深度学习的应用。</font>
</div>


<hr>
<div id="paper12"> <b>12. Compounding the Performance Improvements of Assembled Techniques in a  Convolutional Neural Network</b>  <a href="https://arxiv.org/pdf/2001.06268" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jungkyu Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Won%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Taeryun Won</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hong%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kiho Hong</a><br>
<font size="3">
Abstract: Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to a basic CNN model in combination can improve the accuracy and robustness of the model while minimizing the loss of throughput. For example, our proposed ResNet-50 shows an improvement in top-1 accuracy from 76.3% to 82.78%, and an mCE improvement from 76.0% to 48.9%, on the ImageNet ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. The resulting model significantly outperforms state-of-the-art models with similar accuracy in terms of mCE and inference throughput. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several open datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019, and the source code and trained models are available at this https URL </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在图像分类最近的研究表明多种用于改善卷积神经网络（细胞神经网络）的表现技法。不过，对现有技术结合起来，创造一个实际的模型仍屡见不鲜。在这项研究中，我们进行了广泛的实验，以验证仔细组装这些技术并将它们应用到结合的基本模式CNN能提高模型的精确度和耐用性，同时最大限度地减少产量损失。例如，我们所提出的RESNET-50示出了在顶部-1精度的提高，从76.3％到82.78％，和从76.0％的MCE改善48.9％，对ImageNet ILSVRC2012验证集。有了这些改进，推理可以通过仅降低从536到312得到的模型显著优于状态的最先进的模型具有类似的精度在MCE和推理吞吐量方面。为了验证迁移学习，细粒分类和图像检索任务的性能改进上几个开放的数据集进行了测试，结果表明，提高骨干网络的性能提升传输学习表现显著。我们的方法在iFood比赛细粒度的视觉识别在CVPR 2019获得第一名，源代码和训练的模型可在此HTTPS URL</font>
</div>


<hr>
<div id="paper13"> <b>13. SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On</b>  <a href="https://arxiv.org/pdf/2001.06265" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jandial%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Surgan Jandial</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chopra%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ayush Chopra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ayush%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kumar Ayush</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hemani%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mayur Hemani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Abhijeet Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Krishnamurthy%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Balaji Krishnamurthy</a><br>
<font size="3">
Abstract: Image-based virtual try-on for fashion has gained considerable attention recently. The task requires trying on a clothing item on a target model image. An efficient framework for this is composed of two stages: (1) warping (transforming) the try-on cloth to align with the pose and shape of the target model, and (2) a texture transfer module to seamlessly integrate the warped try-on cloth onto the target model image. Existing methods suffer from artifacts and distortions in their try-on output. In this work, we present SieveNet, a framework for robust image-based virtual try-on. Firstly, we introduce a multi-stage coarse-to-fine warping network to better model fine-grained intricacies (while transforming the try-on cloth) and train it with a novel perceptual geometric matching loss. Next, we introduce a try-on cloth conditioned segmentation mask prior to improve the texture transfer network. Finally, we also introduce a dueling triplet loss strategy for training the texture translation network which further improves the quality of the generated try-on results. We present extensive qualitative and quantitative evaluations of each component of the proposed pipeline and show significant performance improvements against the current state-of-the-art method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于映像的虚拟试穿时装最近获得了相当大的关注。任务需要试穿的目标模型图像上的衣物。这种高效的框架由两个阶段组成：（1）翘曲（变换）的试穿布与目标模型的姿态和形状对齐，和（2）的纹理传送模块无缝集成翘曲试戴上布到目标模型图像。现有的方法从它们的试穿输出文物和扭曲痛苦。在这项工作中，我们提出SieveNet，对于稳健的基于图像的虚拟试穿的框架。首先，我们引入一个多级粗到细的翘曲网络，以更好地模型细粒度错综复杂（同时改造试穿布），并用新的知觉几何匹配损耗训练它。接下来，我们引入一个试穿改善质地传递网络之前布空调分割掩码。最后，我们还引进了决斗三重损失的策略训练纹理翻译网络，进一步提高了产生试穿结果的质量。我们提出了广泛的定性和建议的管道中各组分的定量评估，显示对当前国家的最先进的方法显著的性能改进。</font>
</div>


<hr>
<div id="paper14"> <b>14. Two-Phase Object-Based Deep Learning for Multi-temporal SAR Image Change  Detection</b>  <a href="https://arxiv.org/pdf/2001.06252" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinzheng Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guo Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ce Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Atkinson%2C+P+M" target="_blank" rel="noopener" style="color:#0000EE;">Peter M Atkinson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoheng Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jian%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Jian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xichuan Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongming Li</a><br>
<font size="3">
Abstract: Change detection is one of the fundamental applications of synthetic aperture radar (SAR) images. However, speckle noise presented in SAR images has a much negative effect on change detection. In this research, a novel two-phase object-based deep learning approach is proposed for multi-temporal SAR image change detection. Compared with traditional methods, the proposed approach brings two main innovations. One is to classify all pixels into three categories rather than two categories: unchanged pixels, changed pixels caused by strong speckle (false changes), and changed pixels formed by real terrain variation (real changes). The other is to group neighboring pixels into segmented into superpixel objects (from pixels) such as to exploit local spatial context. Two phases are designed in the methodology: 1) Generate objects based on the simple linear iterative clustering algorithm, and discriminate these objects into changed and unchanged classes using fuzzy c-means (FCM) clustering and a deep PCANet. The prediction of this Phase is the set of changed and unchanged superpixels. 2) Deep learning on the pixel sets over the changed superpixels only, obtained in the first phase, to discriminate real changes from false changes. SLIC is employed again to achieve new superpixels in the second phase. Low rank and sparse decomposition are applied to these new superpixels to suppress speckle noise significantly. A further clustering step is applied to these new superpixels via FCM. A new PCANet is then trained to classify two kinds of changed superpixels to achieve the final change maps. Numerical experiments demonstrate that, compared with benchmark methods, the proposed approach can distinguish real changes from false changes effectively with significantly reduced false alarm rates, and achieve up to 99.71% change detection accuracy using multi-temporal SAR imagery. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：变化检测是合成孔径雷达（SAR）图像的基本应用中的一个。然而，斑点的SAR图像噪声提出了变化检测更负面的影响。在这项研究中，一种新型的两相基于对象的深度学习方法提出了多时相SAR图像变化检测。与传统方法相比，该方法带来了两个主要的创新。一是所有像素分为三类，而不是两类：不变的像素，改变像素造成强烈的斑点（假的变化），并改变了像素的实际地形的变化（真正的变化）而形成。另一种是相邻像素到分割成超像素的对象（从像素），如以利用局部空间上下文组。两个相被设计成在该方法：1）基于该简单的线性迭代聚类算法的目的，并区分这些对象到使用模糊c均值（FCM）聚类和深PCANet变与不变类。这个阶段的预测是一组变与不变的超像素。 2）上的像素集在所述改变仅超像素，在第一阶段中获得的，深学习辨别从虚假变化的实际变化。 SLIC被再次用来实现第二阶段的新的超像素。低等级和稀疏分解的噪音显著应用到这些新的超像素来抑制斑点。进一步的聚类步骤被施加到通过FCM这些新的超像素。然后，新的PCANet被训练2种改变超级像素的分类，以实现最终的变化图。数值结果表明，与基准方法相比，该方法可以区分有效地降低了显著的误报率假的变化真正的变化，实现了利用多时相SAR影像99.71％的变化检测精度。</font>
</div>


<hr>
<div id="paper15"> <b>15. Registration made easy -- standalone orthopedic navigation with HoloLens</b>  <a href="https://arxiv.org/pdf/2001.06209" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liebmann%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florentin Liebmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Roner%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Roner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=von+Atzigen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marco von Atzigen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wanivenhaus%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Florian Wanivenhaus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Neuhaus%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Caroline Neuhaus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Spirig%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">José Spirig</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scaramuzza%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Davide Scaramuzza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sutter%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Reto Sutter</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Snedeker%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jess Snedeker</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Farshad%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mazda Farshad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=F%C3%BCrnstahl%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Philipp Fürnstahl</a><br>
<font size="3">
Abstract: In surgical navigation, finding correspondence between preoperative plan and intraoperative anatomy, the so-called registration task, is imperative. One promising approach is to intraoperatively digitize anatomy and register it with the preoperative plan. State-of-the-art commercial navigation systems implement such approaches for pedicle screw placement in spinal fusion surgery. Although these systems improve surgical accuracy, they are not gold standard in clinical practice. Besides economical reasons, this may be due to their difficult integration into clinical workflows and unintuitive navigation feedback. Augmented Reality has the potential to overcome these limitations. Consequently, we propose a surgical navigation approach comprising intraoperative surface digitization for registration and intuitive holographic navigation for pedicle screw placement that runs entirely on the Microsoft HoloLens. Preliminary results from phantom experiments suggest that the method may meet clinical accuracy requirements. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在手术导航，术前计划及术中解剖，所谓的注册任务之间找到对应，势在必行。一个可行的方法是手术中数字化解剖，并与术前计划注册。国家的最先进的商用导航系统实现了对脊柱融合术椎弓根螺钉放置这些方法。虽然这些系统提高手术准确性，他们不是在临床实践中的金标准。除了经济上的原因，这可能是由于他们难以融入临床工作流程和直观的导航反馈。增强现实必须克服这些局限性的潜力。因此，我们提出了一种外科手术导航的方法，包括用于登记和直观的全息术中的导航表面的数字化椎弓根螺钉放置的是完全在Microsoft HoloLens运行。从幻像实验的初步结果表明，该方法可满足临床的精度要求。</font>
</div>


<hr>
<div id="paper16"> <b>16. FPCR-Net: Feature Pyramidal Correlation and Residual Reconstruction for  Semi-supervised Optical Flow Estimation</b>  <a href="https://arxiv.org/pdf/2001.06171" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingyu Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cuiling Lan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenjun Zeng</a><br>
<font size="3">
Abstract: Optical flow estimation is an important yet challenging problem in the field of video analytics. The features of different semantics levels/layers of a convolutional neural network can provide information of different granularity. To exploit such flexible and comprehensive information, we propose a semi-supervised Feature Pyramidal Correlation and Residual Reconstruction Network (FPCR-Net) for optical flow estimation from frame pairs. It consists of two main modules: pyramid correlation mapping and residual reconstruction. The pyramid correlation mapping module takes advantage of the multi-scale correlations of global/local patches by aggregating features of different scales to form a multi-level cost volume. The residual reconstruction module aims to reconstruct the sub-band high-frequency residuals of finer optical flow in each stage. Based on the pyramid correlation mapping, we further propose a correlation-warping-normalization (CWN) module to efficiently exploit the correlation dependency. Experiment results show that the proposed scheme achieves the state-of-the-art performance, with improvement by 0.80, 1.15 and 0.10 in terms of average end-point error (AEE) against competing baseline methods - FlowNet2, LiteFlowNet and PWC-Net on the Final pass of Sintel dataset, respectively. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：光流估计是视频分析领域的一个重要而具有挑战性的问题。不同的语义等级的特征/卷积神经网络的层可提供不同粒度的信息。为了利用这种柔性和全面的信息，我们提出了从帧双光流估计一个半监督功能锥体相关和残差重建网络（FPCR-净）。它包括两个主要模块：金字塔相关映射和残差重建。金字塔相关映射模块通过聚合不同尺度的特征，以形成多级成本体积利用全局/局部贴片的多尺度相关的。将残余的重建模块目标以重建在每个阶段中更精细的光流的子带的高频残差。基于金字塔的相关性映射，我们进一步提出的相关扭曲规范化（CWN）模块，以有效地利用的相关性依赖。实验结果表明，该方案由0.80，1.15和0.10，平均终点误差（AEE）来实现国家的最先进的性能，提高同台竞技基线方法 -  FlowNet2，LiteFlowNet和PWC-Net的上辛特尔数据集的最终道次，分别。</font>
</div>


<hr>
<div id="paper17"> <b>17. Interpreting Galaxy Deblender GAN from the Discriminator's Perspective</b>  <a href="https://arxiv.org/pdf/2001.06151" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heyi Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuewei Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mueller%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Klaus Mueller</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Xu</a><br>
<font size="3">
Abstract: Generative adversarial networks (GANs) are well known for their unsupervised learning capabilities. A recent success in the field of astronomy is deblending two overlapping galaxy images via a branched GAN model. However, it remains a significant challenge to comprehend how the network works, which is particularly difficult for non-expert users. This research focuses on behaviors of one of the network's major components, the Discriminator, which plays a vital role but is often overlooked, Specifically, we enhance the Layer-wise Relevance Propagation (LRP) scheme to generate a heatmap-based visualization. We call this technique Polarized-LRP and it consists of two parts i.e. positive contribution heatmaps for ground truth images and negative contribution heatmaps for generated images. Using the Galaxy Zoo dataset we demonstrate that our method clearly reveals attention areas of the Discriminator when differentiating generated galaxy images from ground truth images. To connect the Discriminator's impact on the Generator, we visualize the gradual changes of the Generator across the training process. An interesting result we have achieved there is the detection of a problematic data augmentation procedure that would else have remained hidden. We find that our proposed method serves as a useful visual analytical tool for a deeper understanding of GAN models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：创成对抗网络（甘斯）是众所周知的无监督的学习能力。在天文学领域最近的成功经由支GAN模型去混合两个重叠的星系图像。然而，它仍然是一个挑战显著理解如何在网络的作品，这对非专业用户特别困难。这项研究的重点是网络的主要组成部分之一的行为，鉴别，它起着至关重要的作用，但往往被忽视，特别是，我们提高了逐层关联传播（LRP）方案来生成一个基于热图可视化。我们称这种技术偏光LRP，它由两个部分组成，即积极的贡献热图的地面真理图像和生成的图像负贡献热图。利用星系动物园的数据集，我们证明了我们的方法区分从地面实况图像生成星系图像时，清楚地表明鉴别的关注的领域。要连接鉴别对发电机的影响，我们可以形象地发电机的整个训练过程中逐渐变化。我们已经实现了有一个有趣的结果是，将其他仍然隐藏着一个问题的数据增高过程的检测。我们发现，我们提出的方法作为一个有用的可视化分析工具，GAN模式有更深的了解。</font>
</div>


<hr>
<div id="paper18"> <b>18. Learning to Augment Expressions for Few-shot Fine-grained Facial  Expression Recognition</b>  <a href="https://arxiv.org/pdf/2001.06144" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenxuan Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanwei Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tao Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cao%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenjie Cao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Ziqi Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Guoqiang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu-Gang Jiang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Xue%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiangyang Xue</a><br>
<font size="3">
Abstract: Affective computing and cognitive theory are widely used in modern human-computer interaction scenarios. Human faces, as the most prominent and easily accessible features, have attracted great attention from researchers. Since humans have rich emotions and developed musculature, there exist a lot of fine-grained expressions in real-world applications. However, it is extremely time-consuming to collect and annotate a large number of facial images, of which may even require psychologists to correctly categorize them. To the best of our knowledge, the existing expression datasets are only limited to several basic facial expressions, which are not sufficient to support our ambitions in developing successful human-computer interaction systems. To this end, a novel Fine-grained Facial Expression Database - F2ED is contributed in this paper, and it includes more than 200k images with 54 facial expressions from 119 persons. Considering the phenomenon of uneven data distribution and lack of samples is common in real-world scenarios, we further evaluate several tasks of few-shot expression learning by virtue of our F2ED, which are to recognize the facial expressions given only few training instances. These tasks mimic human performance to learn robust and general representation from few examples. To address such few-shot tasks, we propose a unified task-driven framework Compositional Generative Adversarial Network (Comp-GAN) learning to synthesize facial images and thus augmenting the instances of few-shot expression classes. Extensive experiments are conducted on F2ED and existing facial expression datasets, i.e., JAFFE and FER2013, to validate the efficacy of our F2ED in pre-training facial expression recognition network and the effectiveness of our proposed approach Comp-GAN to improve the performance of few-shot recognition tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：情感计算和认知理论被广泛应用于现代的人机交互场景。人脸，作为最突出和方便的特点，从研究者的高度关注。由于人类具有丰富的情感和发达的肌肉，还存在很多现实世界的应用细粒度的表情。然而，这是非常耗时的收集和注释了大量面部图像，这甚至可能需要心理学家正确分类。据我们所知，现有的表达数据仅限于几个基本的面部表情，这是不足以支持我们的野心开发成功的人机交互系统。为此，一种新的细粒度面部表情数据库 -  F2ED在本文提供的，它包括超过200K的图像与来自119分的人54个的面部表情。考虑不均匀分布数据的现象，缺乏样品是现实世界的情景一样，我们还凭借我们F2ED，这是认识到只给出几个训练实例面部表情的评价几拍表达式学习的几个任务。这些任务模拟人类的表现从几个例子学习强大和一般的表示。为了解决这样的一些次任务，我们提出了一个统一的任务驱动的框架组成剖成对抗性网络（压缩 -  GAN）学习合成面部图像，从而增强几炮表达类的实例。大量的实验是在F2ED和现有的面部表情的数据集，即JAFFE和FER2013进行，以验证我们F2ED的功效在训练前的面部表情识别网络和我们提出的方法比较-GaN的有效性，提高few-性能镜头识别任务。</font>
</div>


<hr>
<div id="paper19"> <b>19. Spatio-Temporal Ranked-Attention Networks for Video Captioning</b>  <a href="https://arxiv.org/pdf/2001.06127" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cherian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anoop Cherian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jue Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hori%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chiori Hori</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marks%2C+T+K" target="_blank" rel="noopener" style="color:#0000EE;">Tim K. Marks</a><br>
<font size="3">
Abstract: Generating video descriptions automatically is a challenging task that involves a complex interplay between spatio-temporal visual features and language models. Given that videos consist of spatial (frame-level) features and their temporal evolutions, an effective captioning model should be able to attend to these different cues selectively. To this end, we propose a Spatio-Temporal and Temporo-Spatial (STaTS) attention model which, conditioned on the language state, hierarchically combines spatial and temporal attention to videos in two different orders: (i) a spatio-temporal (ST) sub-model, which first attends to regions that have temporal evolution, then temporally pools the features from these regions; and (ii) a temporo-spatial (TS) sub-model, which first decides a single frame to attend to, then applies spatial attention within that frame. We propose a novel LSTM-based temporal ranking function, which we call ranked attention, for the ST model to capture action dynamics. Our entire framework is trained end-to-end. We provide experiments on two benchmark datasets: MSVD and MSR-VTT. Our results demonstrate the synergy between the ST and TS modules, outperforming recent state-of-the-art methods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生成视频描述自动是一个具有挑战性的任务，涉及到时空视觉特征和语言模型之间的复杂的相互作用。鉴于影片由空间（帧级）的功能及其时间的演化，有效的字幕模型应该能够参加到这些不同的线索选择性。为此，我们提出了时空和时间空间（STATS）注意模型，该模型，条件上的语言状态，分层结合的空间和时间关注到视频中两个不同的顺序：（I）的时空（ST）子模型，该第一照顾到具有时间演变，区域然后在时间上从池这些区域的特征;和（ii）一个时间空间（TS）的子模型，该模型首先决定单个帧出席，然后应用于的帧内的空间的关注。我们提出了一个新的基于LSTM-时间排序功能，我们称之为排名的重视，对于ST模型捕捉行动力度。我们的整个框架的培训结束到终端。我们提供了两个标准数据集实验：MSVD和MSR-VTT。我们的结果证明了ST和TS模块之间的协同作用，优于国家的最先进的最近的方法。</font>
</div>


<hr>
<div id="paper20"> <b>20. Automatic Discovery of Political Meme Genres with Diverse Appearances</b>  <a href="https://arxiv.org/pdf/2001.06122" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Theisen%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Theisen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brogan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joel Brogan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Thomas%2C+P+B" target="_blank" rel="noopener" style="color:#0000EE;">Pamela Bilo Thomas</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moreira%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daniel Moreira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Phoa%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pascal Phoa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weninger%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tim Weninger</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Scheirer%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Walter Scheirer</a><br>
<font size="3">
Abstract: Forms of human communication are not static --- we expect some evolution in the way information is conveyed over time because of advances in technology. One example of this phenomenon is the image-based meme, which has emerged as a dominant form of political messaging in the past decade. While originally used to spread jokes on social media, memes are now having an outsized impact on public perception of world events. A significant challenge in automatic meme analysis has been the development of a strategy to match memes from within a single genre when the appearances of the images vary. Such variation is especially common in memes exhibiting mimicry. For example, when voters perform a common hand gesture to signal their support for a candidate. In this paper we introduce a scalable automated visual recognition pipeline for discovering political meme genres of diverse appearance. This pipeline can ingest meme images from a social network, apply computer vision-based techniques to extract local features and index new images into a database, and then organize the memes into related genres. To validate this approach, we perform a large case study on the 2019 Indonesian Presidential Election using a new dataset of over two million images collected from Twitter and Instagram. Results show that this approach can discover new meme genres with visually diverse images that share common stylistic elements, paving the way forward for further work in semantic analysis and content attribution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类的沟通方式不是一成不变的---我们期待的方式获取信息的一些变化传送随着时间的推移，因为技术的进步。这种现象的一个例子是基于图像的米姆，这已经成为过去十年政治信息的主要形式。虽然原本是用来传播的笑话在社会化媒体，模因现在不得不对世界事件的公众认知的丰厚影响。在自动梅梅分析的显著挑战是一项战略，从单一的体裁内匹配模因时图像的外观变化的发展。这种变化是中模仿记因尤其常见。例如，当执行选民一个共同的手势的信号其用于候选的支持。在本文中，我们介绍了用于发现不同外观的政治米姆流派一个可扩展的自动化视觉识别管道。这条管道可以从社交网络梅梅摄取图像，应用计算机基于视觉的技术来提取局部特征和指数新的图像到一个数据库，然后整理成模因相关流派。为了验证这种方法，我们使用从Twitter和Instagram的收集超过两百万图像的新的数据集上的2019印尼总统选举的一个大案例。结果表明，该方法可以发现新的米姆风格与有着共同的风格元素在视觉上不同的图像，铺平了道路前进为语义分析和内容属性的进一步工作。</font>
</div>


<hr>
<div id="paper21"> <b>21. On- Device Information Extraction from Screenshots in form of tags</b>  <a href="https://arxiv.org/pdf/2001.06094" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sumit Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramena%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gopi Ramena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manoj Goyal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohanty%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Debi Mohanty</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankur Agarwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Changmai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benu Changmai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moharana%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sukumar Moharana</a><br>
<font size="3">
Abstract: We propose a method to make mobile screenshots easily searchable. In this paper, we present the workflow in which we: 1) preprocessed a collection of screenshots, 2) identified script presentin image, 3) extracted unstructured text from images, 4) identifiedlanguage of the extracted text, 5) extracted keywords from the text, 6) identified tags based on image features, 7) expanded tag set by identifying related keywords, 8) inserted image tags with relevant images after ranking and indexed them to make it searchable on device. We made the pipeline which supports multiple languages and executed it on-device, which addressed privacy concerns. We developed novel architectures for components in the pipeline, optimized performance and memory for on-device computation. We observed from experimentation that the solution developed can reduce overall user effort and improve end user experience while searching, whose results are published. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议让移动截图易于搜索的方法。在本文中，我们提出我们在其中工作流：1）预处理截图的集合，2）识别的脚本presentin图像，3）提取从图像非结构化文本，4）提取的文本的identifiedlanguage，5）提取的关键词从文本，6）的基础上的图像特征识别的标签，7）膨胀通过识别相关的关键字标签集，8）与相关图像插入的图像标签的排名后和索引他们，使其可检索在设备上。我们做了哪些支持多种语言流水线开始执行它的设备，其中涉及隐私问题。我们开发新的架构在管线，优化的性能和内存设备上的计算组件。我们从实验观察到，解决方案开发可降低整体用户的努力和改善最终用户体验，同时搜索，其结果公布。</font>
</div>


<hr>
<div id="paper22"> <b>22. Tracking of Micro Unmanned Aerial Vehicles: A Comparative Study</b>  <a href="https://arxiv.org/pdf/2001.06066" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title22" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=G%C3%B6k%C3%A7e%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fatih Gökçe</a><br>
<font size="3">
Abstract: Micro unmanned aerial vehicles (mUAV) became very common in recent years. As a result of their widespread usage, when they are flown by hobbyists illegally, crucial risks are imposed and such mUAVs need to be sensed by security systems. Furthermore, the sensing of mUAVs are essential for also swarm robotics research where the individuals in a flock of robots require systems to sense and localize each other for coordinated operation. In order to obtain such systems, there are studies to detect mUAVs utilizing different sensing mediums, such as vision, infrared and sound signals, and small-scale radars. However, there are still challenges that awaits to be handled in this field such as integrating tracking approaches to the vision-based detection systems to enhance accuracy and computational complexity. For this reason, in this study, we combine various tracking approaches to a vision-based mUAV detection system available in the literature, in order to evaluate different tracking approaches in terms of accuracy and as well as investigate the effect of such integration to the computational cost. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：微型无人机（mUAV）在最近几年变得很普遍。由于其广泛使用的结果，当他们被非法爱好者飞行，关键的风险强加的，这样mUAVs需要通过安全系统进行检测。此外，还群机器人研究其中机器人的羊群个人要求系统意识和本地化相互协调运行mUAVs的检测是必不可少的。为了得到这样的系统，也有研究，以检测使用不同的感测介质，如视觉，红外线和声音信号，以及小规模雷达mUAVs。然而，仍然有挑战等待着在这一领域，如集成的跟踪方法，以基于视觉的检测系统，以提高精度和计算复杂性进行处理。为此，在本研究中，我们结合各种跟踪方法，以文献中的基于视觉的mUAV检测系统，以评估不同的跟踪方法在准确性方面和以及调查这种整合的计算效果成本。</font>
</div>


<hr>
<div id="paper23"> <b>23. Increasing the robustness of DNNs against image corruptions by playing  the Game of Noise</b>  <a href="https://arxiv.org/pdf/2001.06057" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title23" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Rusak%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Evgenia Rusak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schott%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lukas Schott</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zimmermann%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roland Zimmermann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bitterwolf%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julian Bitterwolf</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bringmann%2C+O" target="_blank" rel="noopener" style="color:#0000EE;">Oliver Bringmann</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bethge%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matthias Bethge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Brendel%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wieland Brendel</a><br>
<font size="3">
Abstract: The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the previous state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against uncorrelated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人类视觉系统对宽范围的天然存在的变型和损坏等雨或雪非常健壮。相比之下，现代的图像识别模型的性能上前所未见的损坏进行评估时，强烈地下降。在这里，我们证明了一个简单的，但适当调整训练加性高斯和斑点噪声推广出奇地好于看不见的腐败，很容易达到艺术的腐败基准ImageNet-C（含ResNet50）对以前的状态和MNIST-C。我们依靠这些强大的基准结果的顶部，并表明对不相关的最坏情况下的噪声分布引线识别模型的对抗性训练，在性能上的额外增加。这正可以进一步改进先前提出的防御方法相结合。</font>
</div>


<hr>
<div id="paper24"> <b>24. Modality-Balanced Models for Visual Dialogue</b>  <a href="https://arxiv.org/pdf/2001.06354" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title24" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyounghun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Bansal</a><br>
<font size="3">
Abstract: The Visual Dialog task requires a model to exploit both image and conversational context information to generate the next response to the dialogue. However, via manual analysis, we find that a large number of conversational questions can be answered by only looking at the image without any access to the context history, while others still need the conversation context to predict the correct answers. We demonstrate that due to this reason, previous joint-modality (history and image) models over-rely on and are more prone to memorizing the dialogue history (e.g., by extracting certain keywords or patterns in the context information), whereas image-only models are more generalizable (because they cannot memorize or extract keywords from history) and perform substantially better at the primary normalized discounted cumulative gain (NDCG) task metric which allows multiple correct answers. Hence, this observation encourages us to explicitly maintain two models, i.e., an image-only model and an image-history joint model, and combine their complementary abilities for a more balanced multimodal model. We present multiple methods for this integration of the two models, via ensemble and consensus dropout fusion with shared parameters. Empirically, our models achieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG and high balance across metrics), and substantially outperform the winner of the Visual Dialog challenge 2018 on most metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可视对话任务需要一个模型，同时利用图像和会话的上下文信息来生成到对话的下一个响应。然而，通过人工分析，我们发现了大量的对话问题只能由看图像，而不到上下文历史上的任何访问来回答，而其他人还需要对话上下文来预测正确的答案。我们表明，由于这个原因，以往合资模式（史和图像）模式过分依赖，而且更容易记住的对话记录（例如，通过上下文信息提取的关键字或模式），而只有图象模型更加普及（因为他们无法记住或者从历史中提取的关键字），并在主要贴现归累计收益（NDCG）任务指标，它允许多个正确答案大幅更好地履行。因此，这种观察鼓励我们要明确地保持两种模式，即只有一个影像的模型和图像的历史关节模型，并结合它们的互补能力，为一个更加平衡的多模式模型。我们提出了这种整合两个模型的多种方法，通过与共享参数合奏和共识辍学融合。根据经验，我们的模型实现对视觉对话挑战2019（关于NDCG和整个指标高平衡等级3）强劲的业绩，并基本跑赢视觉对话框挑战2018的大多数指标的赢家。</font>
</div>


<hr>
<div id="paper25"> <b>25. Tethered Aerial Visual Assistance</b>  <a href="https://arxiv.org/pdf/2001.06347" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title25" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xiao%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuesu Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dufek%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Dufek</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Murphy%2C+R+R" target="_blank" rel="noopener" style="color:#0000EE;">Robin R. Murphy</a><br>
<font size="3">
Abstract: In this paper, an autonomous tethered Unmanned Aerial Vehicle (UAV) is developed into a visual assistant in a marsupial co-robots team, collaborating with a tele-operated Unmanned Ground Vehicle (UGV) for robot operations in unstructured or confined environments. These environments pose extreme challenges to the remote tele-operator due to the lack of sufficient situational awareness, mostly caused by the unstructuredness and confinement, stationary and limited field-of-view and lack of depth perception from the robot's onboard cameras. To overcome these problems, a secondary tele-operated robot is used in current practices, who acts as a visual assistant and provides external viewpoints to overcome the perceptual limitations of the primary robot's onboard sensors. However, a second tele-operated robot requires extra manpower and teamwork demand between primary and secondary operators. The manually chosen viewpoints tend to be subjective and sub-optimal. Considering these intricacies, we develop an autonomous tethered aerial visual assistant in place of the secondary tele-operated robot and operator, to reduce human robot ratio from 2:2 to 1:2. Using a fundamental viewpoint quality theory, a formal risk reasoning framework, and a newly developed tethered motion suite, our visual assistant is able to autonomously navigate to good-quality viewpoints in a risk-aware manner through unstructured or confined spaces with a tether. The developed marsupial co-robots team could improve tele-operation efficiency in nuclear operations, bomb squad, disaster robots, and other domains with novel tasks or highly occluded environments, by reducing manpower and teamwork demand, and achieving better visual assistance quality with trustworthy risk-aware motion. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种自主拴无人机（UAV）的发展成为有袋动物共同的机器人团队视觉助理，具有远程操作的无人地面车辆（UGV），用于非结构化或狭窄的环境中机器人进行作业协作。这些环境造成由于缺乏足够的态势感知能力，主要由unstructuredness和约束，固定和有限领域的视图造成极端挑战远程远程操作，缺乏从机器人的车载摄像机的景深感知。为了克服这些问题，二次远程操作机器人在当前的实践，谁充当视觉辅助，并提供外部视点克服初级机器人的机载传感器的感知限制使用。然而，第二个远程操作机器人需要初级和次级运营商之间的额外的人力和团队需求。手动选择视点趋于主观和次优的。考虑到这些复杂性，我们开发代替二次远程操作机器人和操作员的一个自治系留空中视觉助理，从2减少人类机器人比为1:2至1:2。使用基本视点质量理论，正式的风险推理框架，和新开发的系绳运动套件，我们的视觉助手是能够通过与系绳非结构化或密闭空间自主导航至在风险意识的方式高质量的观点。所开发的有袋动物共同的机器人团队可以提高核作战远程操作效率，拆弹小组，灾难机器人，并与新的任务或非常闭塞的环境中，通过减少人力和团队协作需求，并实现更好的视觉援助质量值得信赖的风险其他领域知晓运动。</font>
</div>


<hr>
<div id="paper26"> <b>26. DeepSUM++: Non-local Deep Neural Network for Super-Resolution of  Unregistered Multitemporal Images</b>  <a href="https://arxiv.org/pdf/2001.06342" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title26" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Molini%2C+A+B" target="_blank" rel="noopener" style="color:#0000EE;">Andrea Bordone Molini</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Valsesia%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Diego Valsesia</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Fracastoro%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giulia Fracastoro</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Magli%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Enrico Magli</a><br>
<font size="3">
Abstract: Deep learning methods for super-resolution of a remote sensing scene from multiple unregistered low-resolution images have recently gained attention thanks to a challenge proposed by the European Space Agency. This paper presents an evolution of the winner of the challenge, showing how incorporating non-local information in a convolutional neural network allows to exploit self-similar patterns that provide enhanced regularization of the super-resolution problem. Experiments on the dataset of the challenge show improved performance over the state-of-the-art, which does not exploit non-local information. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：超分辨率从多个未注册的低分辨率图像的遥感场景的深度学习方法最近获得了感谢关注欧洲航天局提出了挑战。本文介绍了挑战冠军，展示了如何在卷积神经网络将非本地信息的发展允许利用自相似的模式，提供了增强的超分辨率问题的正规化。对挑战的数据集实验表明在国家的最先进的，它并没有利用非本地信息更好的性能。</font>
</div>


<hr>
<div id="paper27"> <b>27. Detection Method Based on Automatic Visual Shape Clustering for  Pin-Missing Defect in Transmission Lines</b>  <a href="https://arxiv.org/pdf/2001.06236" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title27" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenbing Zhao</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Qi%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongyu Qi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Qi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yincheng Qi</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhang%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ke Zhang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhai%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yongjie Zhai</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Zhao%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wenqing Zhao</a><br>
<font size="3">
Abstract: Bolts are the most numerous fasteners in transmission lines and are prone to losing their split pins. How to realize the automatic pin-missing defect detection for bolts in transmission lines so as to achieve timely and efficient trouble shooting is a difficult problem and the long-term research target of power systems. In this paper, an automatic detection model called Automatic Visual Shape Clustering Network (AVSCNet) for pin-missing defect is constructed. Firstly, an unsupervised clustering method for the visual shapes of bolts is proposed and applied to construct a defect detection model which can learn the difference of visual shape. Next, three deep convolutional neural network optimization methods are used in the model: the feature enhancement, feature fusion and region feature extraction. The defect detection results are obtained by applying the regression calculation and classification to the regional features. In this paper, the object detection model of different networks is used to test the dataset of pin-missing defect constructed by the aerial images of transmission lines from multiple locations, and it is evaluated by various indicators and is fully verified. The results show that our method can achieve considerably satisfactory detection effect. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：螺栓是输电线路最众多的紧固件，而且容易失去自己的开口销。如何实现对输电线路的螺栓自动销缺失的缺陷检测，从而及时实现高效的故障排除是一个困难的问题，电力系统的长期研究目标。在本文中，一种自动检测模型称为自动视觉形状聚类网络（AVSCNet）为销缺失缺陷构造。首先，对于螺栓的视觉形状的无监督聚类方法，并应用于构建其可以学习视觉形状的差异的缺陷检测模型。接下来，在模型中使用了三个深卷积神经网络优化方法：增强功能，特征融合和区域特征提取。缺陷检测结果通过将回归计算和分类区域特征获得。在本文中，不同网络的物体检测模型用于测试的通过的从多个位置传输线架空图像构建销缺失缺陷的数据集，并且它是由各种指示器评估并且被充分验证。结果表明，我们的方法可以达到相当满意的检测效果。</font>
</div>


<hr>
<div id="paper28"> <b>28. Sideways: Depth-Parallel Training of Video Models</b>  <a href="https://arxiv.org/pdf/2001.06232" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title28" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Malinowski%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mateusz Malinowski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Swirszcz%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Grzegorz Swirszcz</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Carreira%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Joao Carreira</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Patraucean%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Viorica Patraucean</a><br>
<font size="3">
Abstract: We propose Sideways, an approximate backpropagation scheme for training video models. In standard backpropagation, the gradients and activations at every computation step through the model are temporally synchronized. The forward activations need to be stored until the backward pass is executed, preventing inter-layer (depth) parallelization. However, can we leverage smooth, redundant input streams such as videos to develop a more efficient training scheme? Here, we explore an alternative to backpropagation; we overwrite network activations whenever new ones, i.e., from new frames, become available. Such a more gradual accumulation of information from both passes breaks the precise correspondence between gradients and activations, leading to theoretically more noisy weight updates. Counter-intuitively, we show that Sideways training of deep convolutional video networks not only still converges, but can also potentially exhibit better generalization compared to standard synchronized backpropagation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出侧身，培训视频机型的大致反向传播方案。在标准反向传播，在通过所述模型中的每个计算步骤中的梯度和激活在时间上同步。正向激活需要被存储，直到执行向后通，从而防止层间（深度）并行化。然而，我们可以利用平滑，冗余输入流，如视频，开发更有效的培训计划？在这里，我们探索反向传播的替代;我们覆盖的网络激活，每当新的，即由新的框架，变得可用。这样的来自两个信息更渐进累积通断梯度和激活之间的确切的对应，从而导致理论上更嘈杂重量的更新。与直觉相反，我们表明，与标准同步反向传播侧身培训深卷积视频网络不仅仍然收敛的，但也有可能表现出较好的泛化。</font>
</div>


<hr>
<div id="paper29"> <b>29. FedVision: An Online Visual Object Detection Platform Powered by  Federated Learning</b>  <a href="https://arxiv.org/pdf/2001.06202" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title29" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anbu Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">He Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youzhi Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuanyuan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Feng%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lican Feng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianjian Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Yang</a><br>
<font size="3">
Abstract: Visual object detection is a computer vision-based artificial intelligence (AI) technique which has many practical applications (e.g., fire hazard monitoring). However, due to privacy concerns and the high cost of transmitting video data, it is highly challenging to build object detection models on centrally stored large training datasets following the current approach. Federated learning (FL) is a promising approach to resolve this challenge. Nevertheless, there currently lacks an easy to use tool to enable computer vision application developers who are not experts in federated learning to conveniently leverage this technology and apply it in their systems. In this paper, we report FedVision - a machine learning engineering platform to support the development of federated learning powered computer vision applications. The platform has been deployed through a collaboration between WeBank and Extreme Vision to help customers develop computer vision-based safety monitoring solutions in smart city applications. Over four months of usage, it has achieved significant efficiency improvement and cost reduction while removing the need to transmit sensitive data for three major corporate customers. To the best of our knowledge, this is the first real application of FL in computer vision-based tasks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视觉对象检测是具有许多实际应用（例如，火灾监视）一个基于计算机视觉的人工智能（AI）技术。然而，由于隐私问题和传输视频数据的成本高，这是非常具有挑战性的集中存储大量训练数据构建物体检测模式下的电流的方法。联合学习（FL）是一种很有前途的方法来解决这一难题。尽管如此，目前缺乏一个易于使用的工具，使计算机视觉应用开发商谁是不是在联合学习专家能够方便地利用这一技术，并在他们的系统应用它。在本文中，我们报告FedVision  - 机器学习技术平台支持的联合学习动力的计算机视觉应用的开发。该平台已通过帮助客户WeBank和极端视觉之间的合作开发部署在智能城市应用基于计算机视觉的安全监控解决方案。四个多月的使用，它已经取得了显著提高效率和降低成本，同时消除需要发送的敏感数据有三个主要的企业客户。据我们所知，这是计算机基于视觉的任务FL的第一个真正的应用。</font>
</div>


<hr>
<div id="paper30"> <b>30. Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless  Approach</b>  <a href="https://arxiv.org/pdf/2001.06175" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title30" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Park%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chanoh Park</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moghadam%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Peyman Moghadam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Soohwan Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sridharan%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sridha Sridharan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fookes%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Clinton Fookes</a><br>
<font size="3">
Abstract: The demand for multimodal sensing systems for robotics is growing due to the increase in robustness, reliability and accuracy offered by these systems. These systems also need to be spatially and temporally co-registered to be effective. In this paper, we propose a targetless and structureless spatiotemporal camera-LiDAR calibration method. Our method combines a closed-form solution with a modified structureless bundle adjustment where the coarse-to-fine approach does not {require} an initial guess on the spatiotemporal parameters. Also, as 3D features (structure) are calculated from triangulation only, there is no need to have a calibration target or to match 2D features with the 3D point cloud which provides flexibility in the calibration process and sensor configuration. We demonstrate the accuracy and robustness of the proposed method through both simulation and real data experiments using multiple sensor payload configurations mounted to hand-held, aerial and legged robot systems. Also, qualitative results are given in the form of a colorized point cloud visualization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：多传感系统对机器人的需求正在不断增长，由于这些系统提供的耐用性，可靠性和精确度的提高。这些系统还需要在空间和时间上处于同一注册是有效的。在本文中，我们提出了一个无标的和无结构的时空相机，激光雷达校准方法。我们的方法结合了改性无结构束调整，其中粗到细的方法不要求{}上的时空参数的初始猜测的闭合形式解。另外，作为三维特征（结构）从三角测量计算只，没有必要有一个校准目标或匹配2D与3D点云，其提供在校准过程和传感器配置的灵活性的特点。我们证明了该方法的准确度和鲁棒性通过使用多个传感器的有效载荷的配置模拟和实际数据实验安装到手持式，空中和腿式机器人系统。此外，定性的结果以彩色点云可视化的形式给出。</font>
</div>


<hr>
<div id="paper31"> <b>31. An adversarial learning framework for preserving users' anonymity in  face-based emotion recognition</b>  <a href="https://arxiv.org/pdf/2001.06103" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title31" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Narula%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vansh Narula</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhangyang" target="_blank" rel="noopener" style="color:#0000EE;">Zhangyang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang" target="_blank" rel="noopener" style="color:#0000EE;">Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chaspari%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Theodora Chaspari</a><br>
<font size="3">
Abstract: Image and video-capturing technologies have permeated our every-day life. Such technologies can continuously monitor individuals' expressions in real-life settings, affording us new insights into their emotional states and transitions, thus paving the way to novel well-being and healthcare applications. Yet, due to the strong privacy concerns, the use of such technologies is met with strong skepticism, since current face-based emotion recognition systems relying on deep learning techniques tend to preserve substantial information related to the identity of the user, apart from the emotion-specific information. This paper proposes an adversarial learning framework which relies on a convolutional neural network (CNN) architecture trained through an iterative procedure for minimizing identity-specific information and maximizing emotion-dependent information. The proposed approach is evaluated through emotion classification and face identification metrics, and is compared against two CNNs, one trained solely for emotion recognition and the other trained solely for face identification. Experiments are performed using the Yale Face Dataset and Japanese Female Facial Expression Database. Results indicate that the proposed approach can learn a convolutional transformation for preserving emotion recognition accuracy and degrading face identity recognition, providing a foundation toward privacy-aware emotion recognition technologies. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：图像和视频捕捉技术已经渗透到我们每一天的生活。这种技术可连续监测在现实生活中设置个人的表现，获得了我们新的见解他们的情感状态和转换，从而铺平了道路新的福祉和医疗应用。然而，由于强烈的隐私问题，使用这种技术时遭到强烈的怀疑态度，因为当前面为基础的情感识别系统依托深学习技术倾向于从情感保存有关用户的身份基本信息，除了-具体信息。本文提出了一种对抗性的学习框架，它依赖于通过最小化身份的具体信息，并最大限度地提高情绪相关的信息的迭代过程，培养了卷积神经网络（CNN）架构。所提出的方法是通过情感分类和面部识别指标评估，并针对两种细胞神经网络，另一个只卖情感识别训练和其他专为面部识别训练的比较。实验使用的是Yale人脸数据集和日本女性表情数据库进行。结果表明，该方法可以学习卷积转变为维护情感识别的准确性和有辱人格的脸身份识别情况，提供秘密感知情感识别技术奠定了基础。</font>
</div>


<hr>
<div id="paper32"> <b>32. Code-Bridged Classifier (CBC): A Low or Negative Overhead Defense for  Making a CNN Classifier Robust Against Adversarial Attacks</b>  <a href="https://arxiv.org/pdf/2001.06099" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title32" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Behnia%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Farnaz Behnia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mirzaeian%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ali Mirzaeian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sabokrou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohammad Sabokrou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Manoj%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sai Manoj</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohsenin%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tinoosh Mohsenin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khasawneh%2C+K+N" target="_blank" rel="noopener" style="color:#0000EE;">Khaled N. Khasawneh</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Homayoun%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Houman Homayoun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sasan%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Avesta Sasan</a><br>
<font size="3">
Abstract: In this paper, we propose Code-Bridged Classifier (CBC), a framework for making a Convolutional Neural Network (CNNs) robust against adversarial attacks without increasing or even by decreasing the overall models' computational complexity. More specifically, we propose a stacked encoder-convolutional model, in which the input image is first encoded by the encoder module of a denoising auto-encoder, and then the resulting latent representation (without being decoded) is fed to a reduced complexity CNN for image classification. We illustrate that this network not only is more robust to adversarial examples but also has a significantly lower computational complexity when compared to the prior art defenses. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出代码桥接分类（CBC），用于进行卷积神经网络（细胞神经网络）相对抗强大的攻击不增加，甚至通过降低整体模型的计算复杂性的框架。更具体地，我们提出了一种层叠的编码器卷积模型，其中，所述输入图像首先被去噪的自动编码器的编码器模块编码，然后将得到的潜表示（没有被解码）被馈送到降低复杂度的CNN为图像分类。我们表明，该网络不仅更加坚固，以对抗的例子，但也有显著较低的计算复杂性相比，现有技术抗辩。</font>
</div>


<hr>
<div id="paper33"> <b>33. Curriculum Labeling: Self-paced Pseudo-Labeling for Semi-Supervised  Learning</b>  <a href="https://arxiv.org/pdf/2001.06001" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title33" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cascante-Bonilla%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paola Cascante-Bonilla</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fuwen Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanjun Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ordonez%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vicente Ordonez</a><br>
<font size="3">
Abstract: Semi-supervised learning aims to take advantage of a large amount of unlabeled data to improve the accuracy of a model that only has access to a small number of labeled examples. We propose curriculum labeling, an approach that exploits pseudo-labeling for propagating labels to unlabeled samples in an iterative and self-paced fashion. This approach is surprisingly simple and effective and surpasses or is comparable with the best methods proposed in the recent literature across all the standard benchmarks for image classification. Notably, we obtain 94.91% accuracy on CIFAR-10 using only 4,000 labeled samples, and 88.56% top-5 accuracy on Imagenet-ILSVRC using 128,000 labeled samples. In contrast to prior works, our approach shows improvements even in a more realistic scenario that leverages out-of-distribution unlabeled data samples. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：半监督学习的目标采取了大量的未标记数据的优势，提高了一个模型，只获得了少量的标识样本的准确性。我们建议的课程标签，它利用伪标签用于在迭代和自学的方式传播标签的未标记样本的方法。这种方法是非常简单和有效，超过或者是在最近的文献在所有标准的基准图像分类提出的最佳方法相媲美。值得注意的是，我们使用128000个标记的样品获得关于Imagenet-ILSVRC上CIFAR-10 94.91％的准确度仅使用4000标记的样品，以及88.56％顶5的精度。相较于之前的作品，我们的做法显示了改善，即使在更现实的情况下，充分利用外的分布未标记的数据样本。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-20</title>
    <url>/2020/01/20/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-20/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> A Common Semantic Space for Monolingual and Cross-Lingual  Meta-Embeddings <a href="https://arxiv.org/pdf/2001.06381" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Modality-Balanced Models for Visual Dialogue <a href="https://arxiv.org/pdf/2001.06354" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> A Hybrid Solution to Learn Turn-Taking in Multi-Party Service-based Chat  Groups <a href="https://arxiv.org/pdf/2001.06350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> RobBERT: a Dutch RoBERTa-based Language Model <a href="https://arxiv.org/pdf/2001.06286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue  System <a href="https://arxiv.org/pdf/2001.06206" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Plato Dialogue System: A Flexible Conversational AI Research Platform <a href="https://arxiv.org/pdf/2001.06463" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Supervised Speaker Embedding De-Mixing in Two-Speaker Environment <a href="https://arxiv.org/pdf/2001.06397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> On- Device Information Extraction from Screenshots in form of tags <a href="https://arxiv.org/pdf/2001.06094" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> User-in-the-loop Adaptive Intent Detection for Instructable Digital  Assistant <a href="https://arxiv.org/pdf/2001.06007" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. A Common Semantic Space for Monolingual and Cross-Lingual  Meta-Embeddings</b>  <a href="https://arxiv.org/pdf/2001.06381" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Garc%C3%ADa%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Iker García</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agerri%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rodrigo Agerri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rigau%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">German Rigau</a><br>
<font size="3">
Abstract: This paper presents a new technique for creating monolingual and cross-lingual meta-embeddings. Our method integrates multiple word embeddings created from complementary techniques, textual sources, knowledge bases and languages. Existing word vectors are projected to a common semantic space using linear transformations and averaging. With our method the resulting meta-embeddings maintain the dimensionality of the original embeddings without losing information while dealing with the out-of-vocabulary problem. An extensive empirical evaluation demonstrates the effectiveness of our technique with respect to previous work on various intrinsic and extrinsic multilingual evaluations, obtaining competitive results for Semantic Textual Similarity and state-of-the-art performance for word similarity and POS tagging (English and Spanish). The resulting cross-lingual meta-embeddings also exhibit excellent cross-lingual transfer learning capabilities. In other words, we can leverage pre-trained source embeddings from a resource-rich language in order to improve the word representations for under-resourced languages. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了创建单语和跨语言间的嵌入的新技术。我们的方法整合了互补技术，文本来源，知识库和语言创建多个字的嵌入。现有字矢量投影到使用线性变换和平均共同语义空间。随着我们的方法所产生的荟萃的嵌入保持原有的嵌入的维度，而不会丢失信息，在处理外的词汇的问题。广泛的实证评价表明了我们的技术相对于各种内在和外在的多语种评估先前的工作成效，获得了语义文本相似性和国家的最先进的性能竞争的结果词语相似度和词性标注（英语和西班牙语） 。产生的跨语种元的嵌入也表现出优异的跨语言迁移学习能力。换句话说，我们可以利用从资源丰富的语言预先训练源的嵌入，以提高资源不足的语言文字表述。</font>
</div>


<hr>
<div id="paper2"> <b>2. Modality-Balanced Models for Visual Dialogue</b>  <a href="https://arxiv.org/pdf/2001.06354" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hyounghun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hao Tan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Bansal</a><br>
<font size="3">
Abstract: The Visual Dialog task requires a model to exploit both image and conversational context information to generate the next response to the dialogue. However, via manual analysis, we find that a large number of conversational questions can be answered by only looking at the image without any access to the context history, while others still need the conversation context to predict the correct answers. We demonstrate that due to this reason, previous joint-modality (history and image) models over-rely on and are more prone to memorizing the dialogue history (e.g., by extracting certain keywords or patterns in the context information), whereas image-only models are more generalizable (because they cannot memorize or extract keywords from history) and perform substantially better at the primary normalized discounted cumulative gain (NDCG) task metric which allows multiple correct answers. Hence, this observation encourages us to explicitly maintain two models, i.e., an image-only model and an image-history joint model, and combine their complementary abilities for a more balanced multimodal model. We present multiple methods for this integration of the two models, via ensemble and consensus dropout fusion with shared parameters. Empirically, our models achieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG and high balance across metrics), and substantially outperform the winner of the Visual Dialog challenge 2018 on most metrics. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：可视对话任务需要一个模型，同时利用图像和会话的上下文信息来生成到对话的下一个响应。然而，通过人工分析，我们发现了大量的对话问题只能由看图像，而不到上下文历史上的任何访问来回答，而其他人还需要对话上下文来预测正确的答案。我们表明，由于这个原因，以往合资模式（史和图像）模式过分依赖，而且更容易记住的对话记录（例如，通过上下文信息提取的关键字或模式），而只有图象模型更加普及（因为他们无法记住或者从历史中提取的关键字），并在主要贴现归累计收益（NDCG）任务指标，它允许多个正确答案大幅更好地履行。因此，这种观察鼓励我们要明确地保持两种模式，即只有一个影像的模型和图像的历史关节模型，并结合它们的互补能力，为一个更加平衡的多模式模型。我们提出了这种整合两个模型的多种方法，通过与共享参数合奏和共识辍学融合。根据经验，我们的模型实现对视觉对话挑战2019（关于NDCG和整个指标高平衡等级3）强劲的业绩，并基本跑赢视觉对话框挑战2018的大多数指标的赢家。</font>
</div>


<hr>
<div id="paper3"> <b>3. A Hybrid Solution to Learn Turn-Taking in Multi-Party Service-based Chat  Groups</b>  <a href="https://arxiv.org/pdf/2001.06350" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Bayser%2C+M+G" target="_blank" rel="noopener" style="color:#0000EE;">Maira Gatti de Bayser</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guerra%2C+M+A" target="_blank" rel="noopener" style="color:#0000EE;">Melina Alberio Guerra</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cavalin%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Paulo Cavalin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pinhanez%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Claudio Pinhanez</a><br>
<font size="3">
Abstract: To predict the next most likely participant to interact in a multi-party conversation is a difficult problem. In a text-based chat group, the only information available is the sender, the content of the text and the dialogue history. In this paper we present our study on how these information can be used on the prediction task through a corpus and architecture that integrates turn-taking classifiers based on Maximum Likelihood Expectation (MLE), Convolutional Neural Networks (CNN) and Finite State Automata (FSA). The corpus is a synthetic adaptation of the Multi-Domain Wizard-of-Oz dataset (MultiWOZ) to a multiple travel service-based bots scenario with dialogue errors and was created to simulate user's interaction and evaluate the architecture. We present experimental results which show that the CNN approach achieves better performance than the baseline with an accuracy of 92.34%, but the integrated solution with MLE, CNN and FSA achieves performance even better, with 95.65%. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为了预测下一个最有可能的参与者进行互动的多方通话是一个棘手的问题。在基于文本的聊天群，唯一可用的信息是发送者，文本和对话历史的内容。在本文中，我们介绍如何将这些信息可以在预测任务中使用通过语料库和架构，集成了转向回吐基于最大似然期望（MLE），卷积神经网络（CNN）和有限状态自动分类（我们的研究FSA ）。该语料库是多域向导的盎司数据集（MultiWOZ）与对话错误多个旅游服务为主的机器人场景的合成适应和创建来模拟用户的交互和评估体系结构。我们这表明，CNN方法实现比92.34％的准确度基准更好的性能，但与MLE，CNN和FSA集成的解决方案实现性能更为出色，有95.65％目前的实验结果。</font>
</div>


<hr>
<div id="paper4"> <b>4. RobBERT: a Dutch RoBERTa-based Language Model</b>  <a href="https://arxiv.org/pdf/2001.06286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Delobelle%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pieter Delobelle</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Winters%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Winters</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Berendt%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bettina Berendt</a><br>
<font size="3">
Abstract: Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT (Bi-directional Encoders for Transformers), which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies showed that BERT models trained on a single language significantly outperform the multilingual results. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train their Dutch BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We show that RobBERT improves state of the art results in Dutch-specific language tasks, and also outperforms other existing Dutch BERT-based models in sentiment analysis. These results indicate that RobBERT is a powerful pre-trained model for fine-tuning for a large variety of Dutch language tasks. We publicly release this pre-trained model in hope of supporting further downstream Dutch NLP applications. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：预先训练语言模型已经主宰自然语言处理领域在最近几年，并导致显著的性能提升各种复杂的自然语言的任务。其中最突出的预先训练语言模型是BERT（变形金刚双向编码器），它被发布了作为一个英语和一个多语种的版本。虽然多语种BERT执行以及对许多任务，最近的研究显示，培训了一个单一的语言，BERT模型显著跑赢多种语言的结果。培训荷兰BERT模型因而具有广泛的荷兰NLP任务很大的潜力。虽然以前的方法已使用BERT的早期实现培养他们的荷兰BERT，我们使用了罗伯塔，一个稳健优化BERT的方法，培养所谓的RobBERT荷兰语言模型。我们发现，RobBERT改善状态荷兰人特有的语言任务的艺术效果，而且在情感分析优于其他现有的基于BERT荷模型。这些结果表明，RobBERT是微调功能强大的预先训练的模型种类繁多的荷兰语任务。我们在公开支持进一步的下游荷兰NLP应用希望释放此预先训练模式。</font>
</div>


<hr>
<div id="paper5"> <b>5. Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue  System</b>  <a href="https://arxiv.org/pdf/2001.06206" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yun-Wei Chu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kuan-Yen Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hsu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chao-Chun Hsu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ku%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lun-Wei Ku</a><br>
<font size="3">
Abstract: Understanding dynamic scenes and dialogue contexts in order to converse with users has been challenging for multimodal dialogue systems. The 8-th Dialog System Technology Challenge (DSTC8) proposed an Audio Visual Scene-Aware Dialog (AVSD) task, which contains multiple modalities including audio, vision, and language, to evaluate how dialogue systems understand different modalities and response to users. In this paper, we proposed a multi-step joint-modality attention network (JMAN) based on recurrent neural network (RNN) to reason on videos. Our model performs a multi-step attention mechanism and jointly considers both visual and textual representations in each reasoning process to better integrate information from the two different modalities. Compared to the baseline released by AVSD organizers, our model achieves a relative 12.1% and 22.4% improvement over the baseline on ROUGE-L score and CIDEr score. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：了解动态场景和对话的上下文，以便与用户交谈已具有挑战性的多模态对话系统。 8个对话系统技术挑战（DSTC8）提出了一个视听场景感知对话框（AVSD）任务，其中包含多种方式，包括音频，视觉和语言，以评估对话系统是如何理解不同的方式和响应用户。在本文中，我们提出了一种基于递归神经网络（RNN）一个多步骤的联合方式关注网络（JMAN）理性上的视频。我们的模型进行多步注意机制，共同考虑在每个推理过程视觉和文本表示，以更好的信息从两种不同的方式进行整合。相比于通过AVSD主办方公布的基线，我们的模型实现了对ROUGE-L分和苹果酒得分基线相对12.1％和22.4％的改善。</font>
</div>


<hr>
<div id="paper6"> <b>6. Plato Dialogue System: A Flexible Conversational AI Research Platform</b>  <a href="https://arxiv.org/pdf/2001.06463" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Papangelis%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandros Papangelis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Namazifar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mahdi Namazifar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khatri%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chandra Khatri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yi-Chia Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Molino%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piero Molino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tur%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gokhan Tur</a><br>
<font size="3">
Abstract: As the field of Spoken Dialogue Systems and Conversational AI grows, so does the need for tools and environments that abstract away implementation details in order to expedite the development process, lower the barrier of entry to the field, and offer a common test-bed for new ideas. In this paper, we present Plato, a flexible Conversational AI platform written in Python that supports any kind of conversational agent architecture, from standard architectures to architectures with jointly-trained components, single- or multi-party interactions, and offline or online training of any conversational agent component. Plato has been designed to be easy to understand and debug and is agnostic to the underlying learning frameworks that train each component. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：口语对话系统和会话人工智能领域的增长，确实需要工具和环境，为了加快开发进程，降低进入该领域的障碍，并提供一个共同的测试 - 抽象掉的实施细则床上躺了新的思路。在本文中，我们目前柏拉图，灵活的对话AI平台用Python编写的，它支持任何类型的会话代理架构，从标准架构与联合训练的成分，单或多方互动，以及离线或在线培训体系任何会话代理组件。柏拉图已经被设计成易于理解和调试，并是不可知的是培养每个组件的基础学习框架。</font>
</div>


<hr>
<div id="paper7"> <b>7. Supervised Speaker Embedding De-Mixing in Two-Speaker Environment</b>  <a href="https://arxiv.org/pdf/2001.06397" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanpei Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hain%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Hain</a><br>
<font size="3">
Abstract: In this work, a speaker embedding de-mixing approach is proposed. Instead of separating two-speaker signal in signal space like speech source separation, the proposed approach separates different speaker properties from two-speaker signal in embedding space. The proposed approach contains two steps. In step one, the clean speaker embeddings are learned and collected by a residual TDNN based network. In step two, the two-speaker signal and the embedding of one of the speakers are input to a speaker embedding de-mixing network. The de-mixing network is trained to generate the embedding of the other speaker of the by reconstruction loss. Speaker identification accuracy on the de-mixed speaker embeddings is used to evaluate the quality of the obtained embeddings. Experiments are done in two kind of data: artificial augmented two-speaker data (TIMIT) and real world recording of two-speaker data (MC-WSJ). Six diffident speaker embedding de-mixing architectures are investigated. Comparing with the speaker identification accuracy on the clean speaker embeddings (98.5%), the obtained results show that one of the speaker embedding de-mixing architectures obtain close performance, reaching 96.9% test accuracy on TIMIT when the SNR between the target speaker and interfering speaker is 5 dB. More surprisingly, we found choosing a simple subtraction as the embedding de-mixing function could obtain the second best performance, reaching 95.2% test accuracy. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在这项工作中，扬声器嵌入脱混合方法提出。代替在如语音源分离的信号分离空间两个扬声器信号的，所提出的方法分离两个扬声器信号在嵌入空间中的不同扬声器的特性。所提出的方法包括两个步骤。在第一步中，清洁扬声器的嵌入被学习和由残余基于TDNN网络收集。在步骤2中，两个扬声器信号和扬声器中的一个的嵌入被输入到扬声器中嵌入解混合网络。所述去混合网络进行训练，以产生的另一个扬声器的由重建丢失的嵌入。对解混合扬声器的嵌入扬声器识别精度被用于评估所获得的嵌入的质量。实验以两种类型的数据来完成：人工增强双扬声器数据（TIMIT）和双扬声器数据的真实世界记录（MC-WSJ）。六个心虚音箱嵌入脱混合体系结构进行了研究。与在干净的扬声器的嵌入扬声器识别精度（98.5％）相比较，所获得的结果表明，该扬声器中的一个嵌入脱混合架构获得紧密的性能，上TIMIT达到96.9％测试精度当目标讲话者和干扰之间的SNR扬声器为5dB。更令人惊讶的，我们发现选择一个简单的减法作为嵌入脱混合功能可以得到第二最佳性能，达到95.2％的测试精度。</font>
</div>


<hr>
<div id="paper8"> <b>8. On- Device Information Extraction from Screenshots in form of tags</b>  <a href="https://arxiv.org/pdf/2001.06094" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sumit Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramena%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gopi Ramena</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manoj Goyal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mohanty%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Debi Mohanty</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankur Agarwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Changmai%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Benu Changmai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Moharana%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sukumar Moharana</a><br>
<font size="3">
Abstract: We propose a method to make mobile screenshots easily searchable. In this paper, we present the workflow in which we: 1) preprocessed a collection of screenshots, 2) identified script presentin image, 3) extracted unstructured text from images, 4) identifiedlanguage of the extracted text, 5) extracted keywords from the text, 6) identified tags based on image features, 7) expanded tag set by identifying related keywords, 8) inserted image tags with relevant images after ranking and indexed them to make it searchable on device. We made the pipeline which supports multiple languages and executed it on-device, which addressed privacy concerns. We developed novel architectures for components in the pipeline, optimized performance and memory for on-device computation. We observed from experimentation that the solution developed can reduce overall user effort and improve end user experience while searching, whose results are published. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们建议让移动截图易于搜索的方法。在本文中，我们提出我们在其中工作流：1）预处理截图的集合，2）识别的脚本presentin图像，3）提取从图像非结构化文本，4）提取的文本的identifiedlanguage，5）提取的关键词从文本，6）的基础上的图像特征识别的标签，7）膨胀通过识别相关的关键字标签集，8）与相关图像插入的图像标签的排名后和索引他们，使其可检索在设备上。我们做了哪些支持多种语言流水线开始执行它的设备，其中涉及隐私问题。我们开发新的架构在管线，优化的性能和内存设备上的计算组件。我们从实验观察到，解决方案开发可降低整体用户的努力和改善最终用户体验，同时搜索，其结果公布。</font>
</div>


<hr>
<div id="paper9"> <b>9. User-in-the-loop Adaptive Intent Detection for Instructable Digital  Assistant</b>  <a href="https://arxiv.org/pdf/2001.06007" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lair%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicolas Lair</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Delgrange%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Clément Delgrange</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mugisha%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Mugisha</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dussoux%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jean-Michel Dussoux</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Oudeyer%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pierre-Yves Oudeyer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dominey%2C+P+F" target="_blank" rel="noopener" style="color:#0000EE;">Peter Ford Dominey</a><br>
<font size="3">
Abstract: People are becoming increasingly comfortable using Digital Assistants (DAs) to interact with services or connected objects. However, for non-programming users, the available possibilities for customizing their DA are limited and do not include the possibility of teaching the assistant new tasks. To make the most of the potential of DAs, users should be able to customize assistants by instructing them through Natural Language (NL). To provide such functionalities, NL interpretation in traditional assistants should be improved: (1) The intent identification system should be able to recognize new forms of known intents, and to acquire new intents as they are expressed by the user. (2) In order to be adaptive to novel intents, the Natural Language Understanding module should be sample efficient, and should not rely on a pretrained model. Rather, the system should continuously collect the training data as it learns new intents from the user. In this work, we propose AidMe (Adaptive Intent Detection in Multi-Domain Environments), a user-in-the-loop adaptive intent detection framework that allows the assistant to adapt to its user by learning his intents as their interaction progresses. AidMe builds its repertoire of intents and collects data to train a model of semantic similarity evaluation that can discriminate between the learned intents and autonomously discover new forms of known intents. AidMe addresses two major issues - intent learning and user adaptation - for instructable digital assistants. We demonstrate the capabilities of AidMe as a standalone system by comparing it with a one-shot learning system and a pretrained NLU module through simulations of interactions with a user. We also show how AidMe can smoothly integrate to an existing instructable digital assistant. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：人们使用数字助理（DAS）与服务或连接的对象进行交互变得越来越舒适。然而，对于非编程的用户，定制自己的DA可用的可能性是有限的，不包括教学助理新任务的可能性。为了充分利用的DA的潜力，用户应该能够通过自然语言（NL），指示他们定制的助手。为了提供这样的功能，在传统的助理NL解释应加以改进：（1）意图识别系统应该能够识别已知的意图的新形式，因为它们是由用户表达了收购意向新。 （2）为了适应新的意图，所述自然语言理解模块应该是样品高效，并且不应该依赖于预训练的模型。相反，因为它学习来自用户的新意图，系统应不断收集训练数据。在这项工作中，我们提出AidMe（在多域环境自适应意图检测），用户在半实物自适应意图检测框架，允许助手通过学习他的意图及其互进步，以适应其用户。 AidMe建立其意图和收集数据的剧目来训练语义相似性评价的模型，可以和所学意图区分自主发现已知意图的新形式。 AidMe地址两大问题 - 意向学习和适应用户 - 对于造说明数字助理。我们通过将其与一次性学习系统，并通过与用户的交互的模拟预训练NLU模块比较表明AidMe的能力，作为一个独立的系统。我们还表明AidMe如何平滑地集成到现有的造说明数字助理。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>python gtts 文本转语音</title>
    <url>/2020/01/19/python-gtts-%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3/</url>
    <content><![CDATA[<h1 id="安装gtts"><a href="#安装gtts" class="headerlink" title="安装gtts"></a>安装gtts</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install gTTS</span><br></pre></td></tr></table></figure><h1 id="文本转语音"><a href="#文本转语音" class="headerlink" title="文本转语音"></a>文本转语音</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gtts <span class="keyword">import</span> gTTS</span><br><span class="line">tts = gTTS(text=<span class="string">"Hello World"</span>, lang=<span class="string">'en'</span>)</span><br><span class="line">tts.save(<span class="string">"helloworld.mp3"</span>)</span><br></pre></td></tr></table></figure><h1 id="播放语音"><a href="#播放语音" class="headerlink" title="播放语音"></a>播放语音</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.system(<span class="string">"start helloworld.mp3"</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>tts</tag>
      </tags>
  </entry>
  <entry>
    <title>python 调用谷歌翻译接口</title>
    <url>/2020/01/19/python-%E8%B0%83%E7%94%A8%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91%E6%8E%A5%E5%8F%A3/</url>
    <content><![CDATA[<p>googletrans 是一个封装了谷歌翻译接口的python代码库，可以通过googletrans实现免费、无限制调用谷歌翻译接口。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install googletrans</span><br></pre></td></tr></table></figure><h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> googletrans <span class="keyword">import</span> Translator</span><br><span class="line">translator = Translator(service_urls=[</span><br><span class="line">      <span class="string">'translate.google.cn'</span>,])<span class="comment"># 如果可以上外网，还可添加 'translate.google.com' 等</span></span><br><span class="line">trans=translator.translate(<span class="string">'Hello World'</span>, src=<span class="string">'en'</span>, dest=<span class="string">'zh-cn'</span>)</span><br><span class="line"><span class="comment"># 原文</span></span><br><span class="line">print(trans.origin)</span><br><span class="line"><span class="comment"># 译文</span></span><br><span class="line">print(trans.text)</span><br></pre></td></tr></table></figure><a id="more"></a>




<h1 id="语种识别"><a href="#语种识别" class="headerlink" title="语种识别"></a>语种识别</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">detection=translator.detect(<span class="string">'All with Love'</span>)</span><br><span class="line">print(detection.lang)</span><br></pre></td></tr></table></figure>

<h1 id="语种缩略表示"><a href="#语种缩略表示" class="headerlink" title="语种缩略表示"></a>语种缩略表示</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">LANGUAGES = &#123;</span><br><span class="line">    <span class="string">'af'</span>: <span class="string">'afrikaans'</span>,</span><br><span class="line">    <span class="string">'sq'</span>: <span class="string">'albanian'</span>,</span><br><span class="line">    <span class="string">'am'</span>: <span class="string">'amharic'</span>,</span><br><span class="line">    <span class="string">'ar'</span>: <span class="string">'arabic'</span>,</span><br><span class="line">    <span class="string">'hy'</span>: <span class="string">'armenian'</span>,</span><br><span class="line">    <span class="string">'az'</span>: <span class="string">'azerbaijani'</span>,</span><br><span class="line">    <span class="string">'eu'</span>: <span class="string">'basque'</span>,</span><br><span class="line">    <span class="string">'be'</span>: <span class="string">'belarusian'</span>,</span><br><span class="line">    <span class="string">'bn'</span>: <span class="string">'bengali'</span>,</span><br><span class="line">    <span class="string">'bs'</span>: <span class="string">'bosnian'</span>,</span><br><span class="line">    <span class="string">'bg'</span>: <span class="string">'bulgarian'</span>,</span><br><span class="line">    <span class="string">'ca'</span>: <span class="string">'catalan'</span>,</span><br><span class="line">    <span class="string">'ceb'</span>: <span class="string">'cebuano'</span>,</span><br><span class="line">    <span class="string">'ny'</span>: <span class="string">'chichewa'</span>,</span><br><span class="line">    <span class="string">'zh-cn'</span>: <span class="string">'chinese (simplified)'</span>,</span><br><span class="line">    <span class="string">'zh-tw'</span>: <span class="string">'chinese (traditional)'</span>,</span><br><span class="line">    <span class="string">'co'</span>: <span class="string">'corsican'</span>,</span><br><span class="line">    <span class="string">'hr'</span>: <span class="string">'croatian'</span>,</span><br><span class="line">    <span class="string">'cs'</span>: <span class="string">'czech'</span>,</span><br><span class="line">    <span class="string">'da'</span>: <span class="string">'danish'</span>,</span><br><span class="line">    <span class="string">'nl'</span>: <span class="string">'dutch'</span>,</span><br><span class="line">    <span class="string">'en'</span>: <span class="string">'english'</span>,</span><br><span class="line">    <span class="string">'eo'</span>: <span class="string">'esperanto'</span>,</span><br><span class="line">    <span class="string">'et'</span>: <span class="string">'estonian'</span>,</span><br><span class="line">    <span class="string">'tl'</span>: <span class="string">'filipino'</span>,</span><br><span class="line">    <span class="string">'fi'</span>: <span class="string">'finnish'</span>,</span><br><span class="line">    <span class="string">'fr'</span>: <span class="string">'french'</span>,</span><br><span class="line">    <span class="string">'fy'</span>: <span class="string">'frisian'</span>,</span><br><span class="line">    <span class="string">'gl'</span>: <span class="string">'galician'</span>,</span><br><span class="line">    <span class="string">'ka'</span>: <span class="string">'georgian'</span>,</span><br><span class="line">    <span class="string">'de'</span>: <span class="string">'german'</span>,</span><br><span class="line">    <span class="string">'el'</span>: <span class="string">'greek'</span>,</span><br><span class="line">    <span class="string">'gu'</span>: <span class="string">'gujarati'</span>,</span><br><span class="line">    <span class="string">'ht'</span>: <span class="string">'haitian creole'</span>,</span><br><span class="line">    <span class="string">'ha'</span>: <span class="string">'hausa'</span>,</span><br><span class="line">    <span class="string">'haw'</span>: <span class="string">'hawaiian'</span>,</span><br><span class="line">    <span class="string">'iw'</span>: <span class="string">'hebrew'</span>,</span><br><span class="line">    <span class="string">'hi'</span>: <span class="string">'hindi'</span>,</span><br><span class="line">    <span class="string">'hmn'</span>: <span class="string">'hmong'</span>,</span><br><span class="line">    <span class="string">'hu'</span>: <span class="string">'hungarian'</span>,</span><br><span class="line">    <span class="string">'is'</span>: <span class="string">'icelandic'</span>,</span><br><span class="line">    <span class="string">'ig'</span>: <span class="string">'igbo'</span>,</span><br><span class="line">    <span class="string">'id'</span>: <span class="string">'indonesian'</span>,</span><br><span class="line">    <span class="string">'ga'</span>: <span class="string">'irish'</span>,</span><br><span class="line">    <span class="string">'it'</span>: <span class="string">'italian'</span>,</span><br><span class="line">    <span class="string">'ja'</span>: <span class="string">'japanese'</span>,</span><br><span class="line">    <span class="string">'jw'</span>: <span class="string">'javanese'</span>,</span><br><span class="line">    <span class="string">'kn'</span>: <span class="string">'kannada'</span>,</span><br><span class="line">    <span class="string">'kk'</span>: <span class="string">'kazakh'</span>,</span><br><span class="line">    <span class="string">'km'</span>: <span class="string">'khmer'</span>,</span><br><span class="line">    <span class="string">'ko'</span>: <span class="string">'korean'</span>,</span><br><span class="line">    <span class="string">'ku'</span>: <span class="string">'kurdish (kurmanji)'</span>,</span><br><span class="line">    <span class="string">'ky'</span>: <span class="string">'kyrgyz'</span>,</span><br><span class="line">    <span class="string">'lo'</span>: <span class="string">'lao'</span>,</span><br><span class="line">    <span class="string">'la'</span>: <span class="string">'latin'</span>,</span><br><span class="line">    <span class="string">'lv'</span>: <span class="string">'latvian'</span>,</span><br><span class="line">    <span class="string">'lt'</span>: <span class="string">'lithuanian'</span>,</span><br><span class="line">    <span class="string">'lb'</span>: <span class="string">'luxembourgish'</span>,</span><br><span class="line">    <span class="string">'mk'</span>: <span class="string">'macedonian'</span>,</span><br><span class="line">    <span class="string">'mg'</span>: <span class="string">'malagasy'</span>,</span><br><span class="line">    <span class="string">'ms'</span>: <span class="string">'malay'</span>,</span><br><span class="line">    <span class="string">'ml'</span>: <span class="string">'malayalam'</span>,</span><br><span class="line">    <span class="string">'mt'</span>: <span class="string">'maltese'</span>,</span><br><span class="line">    <span class="string">'mi'</span>: <span class="string">'maori'</span>,</span><br><span class="line">    <span class="string">'mr'</span>: <span class="string">'marathi'</span>,</span><br><span class="line">    <span class="string">'mn'</span>: <span class="string">'mongolian'</span>,</span><br><span class="line">    <span class="string">'my'</span>: <span class="string">'myanmar (burmese)'</span>,</span><br><span class="line">    <span class="string">'ne'</span>: <span class="string">'nepali'</span>,</span><br><span class="line">    <span class="string">'no'</span>: <span class="string">'norwegian'</span>,</span><br><span class="line">    <span class="string">'ps'</span>: <span class="string">'pashto'</span>,</span><br><span class="line">    <span class="string">'fa'</span>: <span class="string">'persian'</span>,</span><br><span class="line">    <span class="string">'pl'</span>: <span class="string">'polish'</span>,</span><br><span class="line">    <span class="string">'pt'</span>: <span class="string">'portuguese'</span>,</span><br><span class="line">    <span class="string">'pa'</span>: <span class="string">'punjabi'</span>,</span><br><span class="line">    <span class="string">'ro'</span>: <span class="string">'romanian'</span>,</span><br><span class="line">    <span class="string">'ru'</span>: <span class="string">'russian'</span>,</span><br><span class="line">    <span class="string">'sm'</span>: <span class="string">'samoan'</span>,</span><br><span class="line">    <span class="string">'gd'</span>: <span class="string">'scots gaelic'</span>,</span><br><span class="line">    <span class="string">'sr'</span>: <span class="string">'serbian'</span>,</span><br><span class="line">    <span class="string">'st'</span>: <span class="string">'sesotho'</span>,</span><br><span class="line">    <span class="string">'sn'</span>: <span class="string">'shona'</span>,</span><br><span class="line">    <span class="string">'sd'</span>: <span class="string">'sindhi'</span>,</span><br><span class="line">    <span class="string">'si'</span>: <span class="string">'sinhala'</span>,</span><br><span class="line">    <span class="string">'sk'</span>: <span class="string">'slovak'</span>,</span><br><span class="line">    <span class="string">'sl'</span>: <span class="string">'slovenian'</span>,</span><br><span class="line">    <span class="string">'so'</span>: <span class="string">'somali'</span>,</span><br><span class="line">    <span class="string">'es'</span>: <span class="string">'spanish'</span>,</span><br><span class="line">    <span class="string">'su'</span>: <span class="string">'sundanese'</span>,</span><br><span class="line">    <span class="string">'sw'</span>: <span class="string">'swahili'</span>,</span><br><span class="line">    <span class="string">'sv'</span>: <span class="string">'swedish'</span>,</span><br><span class="line">    <span class="string">'tg'</span>: <span class="string">'tajik'</span>,</span><br><span class="line">    <span class="string">'ta'</span>: <span class="string">'tamil'</span>,</span><br><span class="line">    <span class="string">'te'</span>: <span class="string">'telugu'</span>,</span><br><span class="line">    <span class="string">'th'</span>: <span class="string">'thai'</span>,</span><br><span class="line">    <span class="string">'tr'</span>: <span class="string">'turkish'</span>,</span><br><span class="line">    <span class="string">'uk'</span>: <span class="string">'ukrainian'</span>,</span><br><span class="line">    <span class="string">'ur'</span>: <span class="string">'urdu'</span>,</span><br><span class="line">    <span class="string">'uz'</span>: <span class="string">'uzbek'</span>,</span><br><span class="line">    <span class="string">'vi'</span>: <span class="string">'vietnamese'</span>,</span><br><span class="line">    <span class="string">'cy'</span>: <span class="string">'welsh'</span>,</span><br><span class="line">    <span class="string">'xh'</span>: <span class="string">'xhosa'</span>,</span><br><span class="line">    <span class="string">'yi'</span>: <span class="string">'yiddish'</span>,</span><br><span class="line">    <span class="string">'yo'</span>: <span class="string">'yoruba'</span>,</span><br><span class="line">    <span class="string">'zu'</span>: <span class="string">'zulu'</span>,</span><br><span class="line">    <span class="string">'fil'</span>: <span class="string">'Filipino'</span>,</span><br><span class="line">    <span class="string">'he'</span>: <span class="string">'Hebrew'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>官方手册: <a href="https://py-googletrans.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://py-googletrans.readthedocs.io/en/latest/</a></p>
]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>谷歌翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-17</title>
    <url>/2020/01/18/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-17/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Lexical Sememe Prediction using Dictionary Definitions by Capturing  Local Semantic Correspondence <a href="https://arxiv.org/pdf/2001.05954" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Speech Emotion Recognition Based on Multi-feature and Multi-lingual  Fusion <a href="https://arxiv.org/pdf/2001.05908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Comparing Rule-based, Feature-based and Deep Neural Methods for  De-identification of Dutch Medical Records <a href="https://arxiv.org/pdf/2001.05714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> A Pilot Study on Multiple Choice Machine Reading Comprehension for  Vietnamese Texts <a href="https://arxiv.org/pdf/2001.05687" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> AandP: Utilizing Prolog for converting between active sentence and  passive sentence with three-steps conversion <a href="https://arxiv.org/pdf/2001.05672" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Schema2QA: Answering Complex Queries on the Structured Web with a Neural  Model <a href="https://arxiv.org/pdf/2001.05609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Stereotypical Bias Removal for Hate Speech Detection Task using  Knowledge-based Generalizations <a href="https://arxiv.org/pdf/2001.05495" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> AggressionNet: Generalised Multi-Modal Deep Temporal and Sequential  Learning for Aggression Identification <a href="https://arxiv.org/pdf/2001.05493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> #MeToo on Campus: Studying College Sexual Assault at Scale Using Data  Reported on Social Media <a href="https://arxiv.org/pdf/2001.05970" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Show, Recall, and Tell: Image Captioning with Recall Mechanism <a href="https://arxiv.org/pdf/2001.05876" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> "Why is 'Chicago' deceptive?" Towards Building Model-Driven Tutorials  for Humans <a href="https://arxiv.org/pdf/2001.05871" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Ensemble based discriminative models for Visual Dialog Challenge 2018 <a href="https://arxiv.org/pdf/2001.05865" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Discoverability in Satellite Imagery: A Good Sentence is Worth a  Thousand Pictures <a href="https://arxiv.org/pdf/2001.05839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> Document Network Projection in Pretrained Word Embedding Space <a href="https://arxiv.org/pdf/2001.05727" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Delving Deeper into the Decoder for Video Captioning <a href="https://arxiv.org/pdf/2001.05614" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Insertion-Deletion Transformer <a href="https://arxiv.org/pdf/2001.05540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Lexical Sememe Prediction using Dictionary Definitions by Capturing  Local Semantic Correspondence</b>  <a href="https://arxiv.org/pdf/2001.05954" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Du%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiaju Du</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fanchao Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maosong Sun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhiyuan Liu</a><br>
<font size="3">
Abstract: Sememes, defined as the minimum semantic units of human languages in linguistics, have been proven useful in many NLP tasks. Since manual construction and update of sememe knowledge bases (KBs) are costly, the task of automatic sememe prediction has been proposed to assist sememe annotation. In this paper, we explore the approach of applying dictionary definitions to predicting sememes for unannotated words. We find that sememes of each word are usually semantically matched to different words in its dictionary definition, and we name this matching relationship local semantic correspondence. Accordingly, we propose a Sememe Correspondence Pooling (SCorP) model, which is able to capture this kind of matching to predict sememes. We evaluate our model and baseline methods on a famous sememe KB HowNet and find that our model achieves state-of-the-art performance. Moreover, further quantitative analysis shows that our model can properly learn the local semantic correspondence between sememes and words in dictionary definitions, which explains the effectiveness of our model. The source codes of this paper can be obtained from this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：义位，定义为语言学人类语言的语义的最小单位，已在许多自然语言处理的任务被证明是有用的。由于人工建设和义素知识库（KBS）更新是昂贵的，自动义原预测的任务已经提出，以协助义原注释。在本文中，我们将探讨采用字典的定义为预测未注释词义位的方法。我们发现，每个词的义位通常是在语义上在其字典上的定义匹配不同的话，我们命名此匹配关系当地语义对应。因此，我们提出了一个义位对应池（SCORP）模型，它能够捕捉到这种匹配预测义原。我们评估在一个著名的义原KB知网我们的模型和基线的方法和发现，我们的模型实现了国家的最先进的性能。此外，进一步的定量分析表明，我们的模型能够正确地学习字典定义义位与词之间的本地语义对应，这说明我们的模型的有效性。本文的源代码可以从该HTTPS URL来获得。</font>
</div>


<hr>
<div id="paper2"> <b>2. Speech Emotion Recognition Based on Multi-feature and Multi-lingual  Fusion</b>  <a href="https://arxiv.org/pdf/2001.05908" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chunyi Wang</a><br>
<font size="3">
Abstract: A speech emotion recognition algorithm based on multi-feature and Multi-lingual fusion is proposed in order to resolve low recognition accuracy caused by lack of large speech dataset and low robustness of acoustic features in the recognition of speech emotion. First, handcrafted and deep automatic features are extracted from existing data in Chinese and English speech emotions. Then, the various features are fused respectively. Finally, the fused features of different languages are fused again and trained in a classification model. Distinguishing the fused features with the unfused ones, the results manifest that the fused features significantly enhance the accuracy of speech emotion recognition algorithm. The proposed solution is evaluated on the two Chinese corpus and two English corpus, and is shown to provide more accurate predictions compared to original solution. As a result of this study, the multi-feature and Multi-lingual fusion algorithm can significantly improve the speech emotion recognition accuracy when the dataset is small. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于多特征和多语种的融合语音情感识别算法是为了解决由于缺乏大型数据集的讲话和在识别语音情感的声学特征低稳健的低识别精度提出。首先，手工和自动深特征在中国和英语演讲情绪现有的数据中提取。于是，各种功能都融合分别。最后，不同语言的熔断特性再次融合，并在分类模型训练。判定，非融合的，结果清单中的融合功能，融合功能显著增强语音情感识别算法的精度。提出的解决方案是在两名中国语料库和两个英语语料库进行评估，并显示相对于原来的解决方案，以提供更精确的预测。作为这项研究的结果是，多特征和多语种的融合算法可以显著提高语音情感识别的准确性如果数据集小。</font>
</div>


<hr>
<div id="paper3"> <b>3. Comparing Rule-based, Feature-based and Deep Neural Methods for  De-identification of Dutch Medical Records</b>  <a href="https://arxiv.org/pdf/2001.05714" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Trienes%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jan Trienes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Trieschnigg%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dolf Trieschnigg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Seifert%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christin Seifert</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hiemstra%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Djoerd Hiemstra</a><br>
<font size="3">
Abstract: Unstructured information in electronic health records provide an invaluable resource for medical research. To protect the confidentiality of patients and to conform to privacy regulations, de-identification methods automatically remove personally identifying information from these medical records. However, due to the unavailability of labeled data, most existing research is constrained to English medical text and little is known about the generalizability of de-identification methods across languages and domains. In this study, we construct a varied dataset consisting of the medical records of 1260 patients by sampling data from 9 institutes and three domains of Dutch healthcare. We test the generalizability of three de-identification methods across languages and domains. Our experiments show that an existing rule-based method specifically developed for the Dutch language fails to generalize to this new data. Furthermore, a state-of-the-art neural architecture performs strongly across languages and domains, even with limited training data. Compared to feature-based and rule-based methods the neural method requires significantly less configuration effort and domain-knowledge. We make all code and pre-trained de-identification models available to the research community, allowing practitioners to apply them to their datasets and to enable future benchmarks. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在电子健康记录的非结构化信息提供医学研究的宝贵资源。为了保护病人的保密性和符合隐私法规，去识别方法自动删除的个人识别这些医疗记录信息。然而，由于标签的数据，大多数现有的研究被限制在英国的医疗文本和小的不可有人知道的跨语言和领域去识别方法的普遍性。在这项研究中，我们构建了一个不同的数据集从9个院所和荷兰医疗保健的三个域采样数据组成的1260例患者的医疗记录。我们测试的跨语言，跨域三个去识别方法的普遍性。我们的实验表明，专门为荷兰语言开发现有的基于规则的方法不能推广到这个新的数据。此外，一个国家的最先进的神经结构进行强烈跨语言和域，即使在有限的训练数据。相比于基于规则的基于特征和方法，神经方法需要显著较少配置工作和领域的知识。我们让所有的代码和预先训练去标识模型提供给研究界，让从业者将它们应用到自己的数据集，以使未来的基准。</font>
</div>


<hr>
<div id="paper4"> <b>4. A Pilot Study on Multiple Choice Machine Reading Comprehension for  Vietnamese Texts</b>  <a href="https://arxiv.org/pdf/2001.05687" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Van+Nguyen%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kiet Van Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+K+V" target="_blank" rel="noopener" style="color:#0000EE;">Khiem Vinh Tran</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luu%2C+S+T" target="_blank" rel="noopener" style="color:#0000EE;">Son T. Luu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+A+G" target="_blank" rel="noopener" style="color:#0000EE;">Anh Gia-Tuan Nguyen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Nguyen%2C+N+L" target="_blank" rel="noopener" style="color:#0000EE;">Ngan Luu-Thuy Nguyen</a><br>
<font size="3">
Abstract: Machine Reading Comprehension (MRC) is the task of natural language processing which studies the ability to read and understand unstructured texts and then find the correct answers for questions. Until now, we have not yet had any MRC dataset for such a low-resource language as Vietnamese. In this paper, we introduce ViMMRC, a challenging machine comprehension corpus with multiple-choice questions, intended for research on the machine comprehension of Vietnamese text. This corpus includes 2,783 multiple-choice questions and answers based on a set of 417 Vietnamese texts used for teaching reading comprehension for 1st to 5th graders. Answers may be extracted from the contents of single or multiple sentences in the corresponding reading text. A thorough analysis of the corpus and experimental results in this paper illustrate that our corpus ViMMRC demands reasoning abilities beyond simple word matching. We proposed the method of Boosted Sliding Window (BSW) that improves 5.51% in accuracy over the best baseline method. We also measured human performance on the corpus and compared it to our MRC models. The performance gap between humans and our best experimental model indicates that significant progress can be made on Vietnamese machine reading comprehension in further research. The corpus is freely available at our website for research purposes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机阅读理解（MRC）是自然语言处理的哪些研究阅读和理解非结构化的文本，然后找到问题的正确答案的能力的任务。到现在为止，我们还没有过任何MRC数据集这样的低资源语言越南。在本文中，我们介绍ViMMRC，一个具有挑战性的机器理解语料库与多项选择题，供越南文本的机器理解研究。该文集包括基于一套用于教学阅读理解的1日至5年级学生417个越南文2783多项选择题及答案。答案可以从在相应的阅读文本单个或多个句子的内容被提取。本文的语料和实验结果的深入分析表明我们的语料库ViMMRC要求超出了简单的词语匹配的推理能力。我们提出的提振推拉窗（BSW）的，超过最佳基线法提高了精度5.51％的方法。我们还测量了语料库人的表现和它相比，我们的MRC模型。人类和我们最好的实验模型之间的性能差距表明，显著的进展可以在进一步研究越南机器阅读理解进行。该语料库是免费提供的，在我们的网站用于研究目的。</font>
</div>


<hr>
<div id="paper5"> <b>5. AandP: Utilizing Prolog for converting between active sentence and  passive sentence with three-steps conversion</b>  <a href="https://arxiv.org/pdf/2001.05672" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Tran%2C+T+Q" target="_blank" rel="noopener" style="color:#0000EE;">Trung Q. Tran</a><br>
<font size="3">
Abstract: I introduce a simple but efficient method to solve one of the critical aspects of English grammar which is the relationship between active sentence and passive sentence. In fact, an active sentence and its corresponding passive sentence express the same meaning, but their structure is different. I utilized Prolog [4] along with Definite Clause Grammars (DCG) [5] for doing the conversion between active sentence and passive sentence. Some advanced techniques were also used such as Extra Arguments, Extra Goals, Lexicon, etc. I tried to solve a variety of cases of active and passive sentences such as 12 English tenses, modal verbs, negative form, etc. More details and my contributions will be presented in the following sections. The source code is available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我介绍一个简单的，但要解决的英语语法的关键方面是主动句和被动句之间的关系的一个有效的方法。事实上，一个主动句和其对应的被动句表达同一个意思，但它们的结构是不同的。我使用的Prolog [4]与定条款文法（DCG）[5]这样做主动句和被动句子之间的转换沿。一些先进的技术，还使用了诸如额外的参数，额外的目标，词汇，等我试图解决各种主动和被动句等12个英文时态，情态动词，否定形式等更多细节和我的贡献的情况下将在下面的章节中介绍。源代码可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper6"> <b>6. Schema2QA: Answering Complex Queries on the Structured Web with a Neural  Model</b>  <a href="https://arxiv.org/pdf/2001.05609" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Silei Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Campagna%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Giovanni Campagna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lam%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Monica S. Lam</a><br>
<font size="3">
Abstract: Virtual assistants today require every website to submit skills individually into their proprietary repositories. The skill consists of a fixed set of supported commands and the formal representation of each command. The assistants use the contributed data to create a proprietary linguistic interface, typically using an intent classifier. This paper proposes an open-source toolkit, called Schema2QA, that leverages the this http URL markup found in many websites to automatically build skills. Schema2QA has several advantages: (1) Schema2QA handles compositional queries involving multiple fields automatically, such as "find the Italian restaurant around here with the most reviews", or "what W3C employees on LinkedIn went to Oxford"; (2) Schema2QA translates natural language into executable queries on the up-to-date data from the website; (3) natural language training can be applied to one domain at a time to handle multiple websites using the same this http URL representations. We apply Schema2QA to two different domains, showing that the skills we built can answer useful queries with little manual effort. Our skills achieve an overall accuracy between 74% and 78%, and can answer questions that span three or more properties with 65% accuracy. We also show that a new domain can be supported by transferring knowledge. The open-source Schema2QA lets each website create and own its linguistic interface. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：虚拟助理都要求每一个网站提交技巧单独为他们的专利库。技能由一组固定的支持的命令和各命令的正式表示。助手用提供的数据以创建一个专有的语言接口，通常使用的意图分类器。本文提出了一种开放源代码工具包，叫做Schema2QA，即利用了这个在很多网站上找到的自动构建技术HTTP URL标记。 Schema2QA有以下几个优点：（1）Schema2QA自动处理涉及多个领域组成的查询，如“找到意大利餐厅这里与大多数评论围绕”或“去牛津大学在LinkedIn什么W3C员工”; （2）Schema2QA转换自然语言转换为可执行的查询从网站上的最新数据; （3）自然语言培训可以同时被应用到一个域中使用相同的这个HTTP URL交涉处理多个网站。我们应用Schema2QA于两个不同的领域，显示出我们建立了技能可以回答很少的手动工作有用的查询。我们的技能达到74％和78％之间的整体精度，并能回答这个跨越65％的准确率三个或更多的性能问题。我们还表明，一个新的域可以通过知识转移的支持。开源Schema2QA让每个网站创建和拥有自己的语言界面。</font>
</div>


<hr>
<div id="paper7"> <b>7. Stereotypical Bias Removal for Hate Speech Detection Task using  Knowledge-based Generalizations</b>  <a href="https://arxiv.org/pdf/2001.05495" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Badjatiya%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pinkesh Badjatiya</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gupta%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Manish Gupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Varma%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vasudeva Varma</a><br>
<font size="3">
Abstract: With the ever-increasing cases of hate spread on social media platforms, it is critical to design abuse detection mechanisms to proactively avoid and control such incidents. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. Bias removal has been traditionally studied for structured datasets, but we aim at bias mitigation from unstructured text data. In this paper, we make two important contributions. First, we systematically design methods to quantify the bias for any model and propose algorithms for identifying the set of words which the model stereotypes. Second, we propose novel methods leveraging knowledge-based generalizations for bias-free learning. Knowledge-based generalization provides an effective way to encode knowledge because the abstraction they provide not only generalizes content but also facilitates retraction of information from the hate speech detection classifier, thereby reducing the imbalance. We experiment with multiple knowledge generalization policies and analyze their effect on general performance and in mitigating bias. Our experiments with two real-world datasets, a Wikipedia Talk Pages dataset (WikiDetox) of size ~96k and a Twitter dataset of size ~24k, show that the use of knowledge-based generalizations results in better performance by forcing the classifier to learn from generalized content. Our methods utilize existing knowledge-bases and can easily be extended to other tasks </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着不断增加的社会化媒体平台上传播仇恨的情况下，它是设计滥用检测手段，积极主动规避和控制此类事件的关键。虽然存在仇恨言论的检测方法，他们刻板印象的话，因此从本质上偏向训练受到影响。偏置消除历来被研究了结构化数据集，但我们的目标是从非结构化的文本数据缓解偏差。在本文中，我们提出两个重要的贡献。首先，我们系统的设计方法，以量化的任何模型的偏差，提出的算法识别词集该模型定型。第二，我们提出了新的方法利用知识为基础的概括为无偏差的学习。基于知识的推广提供了一个有效的方式来编码知识，因为他们提供的不只是抽象概括的内容，但也有利于信息回缩从仇恨言论检测分类，从而减少不平衡。我们与多个知识推广政策实验和分析整体性能和减轻他们的偏见的影响。我们有两个现实世界的尺寸〜96K的数据集，维基百科对话页数据集（WikiDetox）和大小的Twitter的数据集〜24K，表明通过强制分类在更好的性能使用基于知识的概括的结果，从学习实验广义的内容。我们的方法利用现有的知识基地，可以很容易地扩展到其他任务</font>
</div>


<hr>
<div id="paper8"> <b>8. AggressionNet: Generalised Multi-Modal Deep Temporal and Sequential  Learning for Aggression Identification</b>  <a href="https://arxiv.org/pdf/2001.05493" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Khandelwal%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anant Khandelwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Niraj Kumar</a><br>
<font size="3">
Abstract: Wide usage of social media platforms has increased the risk of aggression, which results in mental stress and affects the lives of people negatively like psychological agony, fighting behavior, and disrespect to others. Majority of such conversations contains code-mixed languages[28]. Additionally, the way used to express thought or communication style also changes from one social media plat-form to another platform (e.g., communication styles are different in twitter and Facebook). These all have increased the complexity of the problem. To solve these problems, we have introduced a unified and robust multi-modal deep learning architecture which works for English code-mixed dataset and uni-lingual English dataset both.The devised system, uses psycho-linguistic features and very ba-sic linguistic features. Our multi-modal deep learning architecture contains, Deep Pyramid CNN, Pooled BiLSTM, and Disconnected RNN(with Glove and FastText embedding, both). Finally, the system takes the decision based on model averaging. We evaluated our system on English Code-Mixed TRAC 2018 dataset and uni-lingual English dataset obtained from Kaggle. Experimental results show that our proposed system outperforms all the previous approaches on English code-mixed dataset and uni-lingual English dataset. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：社会化媒体平台，用途广泛增加侵略的风险，这会导致精神压力和负面影响的人们的生活就像心理上的痛苦，战斗行为，和不尊重他人。这样的对话的大多数包含代码混合语言[28]。此外，该方法用来表达思想或沟通方式也从一个社交媒体平台，改变到另一个平台（例如，沟通方式是在Twitter和Facebook有所不同）。这些都增加了问题的复杂性。为了解决这些问题，我们引入了一个统一和强大的多模态深度学习架构，适用于英语代码混合数据集和单语种英语数据集both.The设计系统，采用心理语言特征和非常BA-SIC语言特征。我们的多模态深度学习架构包含，深金字塔CNN，汇集BiLSTM，并断开RNN（带手套和FastText嵌入，两者）。最后，该系统采用基于模型平均的决定。我们评估了英语代码混合TRAC 2018数据集，并从Kaggle获得单语种英语数据集我们的系统。实验结果表明，该系统优于所有英语代码混合数据集和单语种英语数据集以前的方法。</font>
</div>


<hr>
<div id="paper9"> <b>9. #MeToo on Campus: Studying College Sexual Assault at Scale Using Data  Reported on Social Media</b>  <a href="https://arxiv.org/pdf/2001.05970" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Duong%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Viet Duong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pham%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Phu Pham</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bose%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ritwik Bose</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Luo%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiebo Luo</a><br>
<font size="3">
Abstract: Recently, the emergence of the #MeToo trend on social media has empowered thousands of people to share their own sexual harassment experiences. This viral trend, in conjunction with the massive personal information and content available on Twitter, presents a promising opportunity to extract data driven insights to complement the ongoing survey based studies about sexual harassment in college. In this paper, we analyze the influence of the #MeToo trend on a pool of college followers. The results show that the majority of topics embedded in those #MeToo tweets detail sexual harassment stories, and there exists a significant correlation between the prevalence of this trend and official reports on several major geographical regions. Furthermore, we discover the outstanding sentiments of the #MeToo tweets using deep semantic meaning representations and their implications on the affected users experiencing different types of sexual harassment. We hope this study can raise further awareness regarding sexual misconduct in academia. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，在社会化媒体的#MeToo趋势的出现已经授权成千上万的人分享自己的性骚扰经历。这种病毒发展趋势，结合大量的个人信息，并在Twitter上可用内容，提出了一个有前途的机会抽取数据的深入分析，以补充有关大学性骚扰正在进行的调查为基础的研究。在本文中，我们分析了对高校追随者池#MeToo趋势的影响。结果显示，大部分嵌入在这些#MeToo主题的鸣叫细节性骚扰的故事，并且存在几大地理区域这一趋势，官方报告的患病率之间的相关性显著。此外，我们发现使用深层语义表述及其对受影响的用户体验不同类型的性骚扰影响的#MeToo鸣叫的优秀情绪。我们希望这项研究能提高公众对学术界的性行为不端进一步的认识。</font>
</div>


<hr>
<div id="paper10"> <b>10. Show, Recall, and Tell: Image Captioning with Recall Mechanism</b>  <a href="https://arxiv.org/pdf/2001.05876" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Li Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zechen Bai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yonghua Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongtao Lu</a><br>
<font size="3">
Abstract: Generating natural and accurate descriptions in image cap-tioning has always been a challenge. In this paper, we pro-pose a novel recall mechanism to imitate the way human con-duct captioning. There are three parts in our recall mecha-nism : recall unit, semantic guide (SG) and recalled-wordslot (RWS). Recall unit is a text-retrieval module designedto retrieve recalled words for images. SG and RWS are de-signed for the best use of recalled words. SG branch cangenerate a recalled context, which can guide the process ofgenerating caption. RWS branch is responsible for copyingrecalled words to the caption. Inspired by pointing mecha-nism in text summarization, we adopt a soft switch to balancethe generated-word probabilities between SG and RWS. Inthe CIDEr optimization step, we also introduce an individualrecalled-word reward (WR) to boost training. Our proposedmethods (SG+RWS+WR) achieve BLEU-4 / CIDEr / SPICEscores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 /129.1 / 22.4 with CIDEr optimization on MSCOCO Karpathytest split, which surpass the results of other state-of-the-artmethods. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：生成自然和图像帽tioning准确的描述一直是一个挑战。在本文中，我们亲姿势模仿的方式人类CON-管字幕一种新的召回机制。有三个部分在我们的回忆机甲-NISM：召回单位，语义指南（SG），并回顾-wordslot（RWS）。召回单元是文本的检索模块designedto检索图像召回的话。 SG和RWS被解签订了最好的使用被召回的话。 SG分支cangenerate一个回顾上下文，其可以引导过程ofgenerating字幕。 RWS分公司负责copyingrecalled字标题。通过指向文本摘要机甲-NISM启发，我们采用软切换至SG和RWS之间balancethe产生字概率。在矿井苹果酒优化步骤，我们还引入individualrecalled字奖励（WR），以提升培训。我们的proposedmethods（SG + RWS + WR）实现BLEU-4 /苹果酒/ 36.6 / 116.9 / 21.3与交叉熵损失和38.7 /129.1 /苹果酒优化上MSCOCO Karpathytest分裂22.4，这超越其他状态 - 的结果SPICEscores的最artmethods。</font>
</div>


<hr>
<div id="paper11"> <b>11. "Why is 'Chicago' deceptive?" Towards Building Model-Driven Tutorials  for Humans</b>  <a href="https://arxiv.org/pdf/2001.05871" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lai%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vivian Lai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chenhao Tan</a><br>
<font size="3">
Abstract: To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a training phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically selected examples from training data with explanations. We use deceptive review detection as a testbed and conduct large-scale, randomized human-subject experiments to examine the effectiveness of such tutorials. We find that tutorials indeed improve human performance, with and without real-time assistance. In particular, although deep learning provides superior predictive performance than simple models, tutorials and explanations from simple models are more useful to humans. Our work suggests future directions for human-centered tutorials and explanations towards a synergy between humans and AI. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：为支持与机器学习模型人的决策，我们经常需要嵌入是unsalient的，未知的，或者违反直觉的人类模型阐发模式。虽然现有的方法重点讲解机器的预测具有实时援助，我们探讨模型驱动的教程，以帮助人们了解一个训练阶段这些模式。我们认为，从科学论文，类似于科学传播的现行做法，并从解释训练数据自动选择的例子准则都教程。我们使用欺骗性的审查检测作为测试平台，并进行大规模，随机人体学科实验来检验这种教程的效果。我们发现，确实教程改善人类的性能，使用和不使用实时的援助。特别是，虽然深度学习不是简单的模型，教程和简单模型的解释提供了卓越的预测性能对人体更有益。我们的工作提出了以人为本对人类和人工智能之间的协同作用的教程和说明未来的发展方向。</font>
</div>


<hr>
<div id="paper12"> <b>12. Ensemble based discriminative models for Visual Dialog Challenge 2018</b>  <a href="https://arxiv.org/pdf/2001.05865" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shubham Agarwal</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Raghav Goyal</a><br>
<font size="3">
Abstract: This manuscript describes our approach for the Visual Dialog Challenge 2018. We use an ensemble of three discriminative models with different encoders and decoders for our final submission. Our best performing model on 'test-std' split achieves the NDCG score of 55.46 and the MRR value of 63.77, securing third position in the challenge. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本稿件描述了我们的视觉对话挑战2018年我们使用三种判别模型不同的编码器和解码器为我们最后提交的集成方法。在“测试-STD”分裂我们的最佳表现模型达到NDCG得分55.46和63.77的MRR的价值，确保在挑战第三的位置。</font>
</div>


<hr>
<div id="paper13"> <b>13. Discoverability in Satellite Imagery: A Good Sentence is Worth a  Thousand Pictures</b>  <a href="https://arxiv.org/pdf/2001.05839" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Noever%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Noever</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Regian%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wes Regian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ciolino%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Matt Ciolino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kalin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Josh Kalin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hambrick%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dom Hambrick</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Blankenship%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kaye Blankenship</a><br>
<font size="3">
Abstract: Small satellite constellations provide daily global coverage of the earth's landmass, but image enrichment relies on automating key tasks like change detection or feature searches. For example, to extract text annotations from raw pixels requires two dependent machine learning models, one to analyze the overhead image and the other to generate a descriptive caption. We evaluate seven models on the previously largest benchmark for satellite image captions. We extend the labeled image samples five-fold, then augment, correct and prune the vocabulary to approach a rough min-max (minimum word, maximum description). This outcome compares favorably to previous work with large pre-trained image models but offers a hundred-fold reduction in model size without sacrificing overall accuracy (when measured with log entropy loss). These smaller models provide new deployment opportunities, particularly when pushed to edge processors, on-board satellites, or distributed ground stations. To quantify a caption's descriptiveness, we introduce a novel multi-class confusion or error matrix to score both human-labeled test data and never-labeled images that include bounding box detection but lack full sentence captions. This work suggests future captioning strategies, particularly ones that can enrich the class coverage beyond land use applications and that lessen color-centered and adjacency adjectives ("green", "near", "between", etc.). Many modern language transformers present novel and exploitable models with world knowledge gleaned from training from their vast online corpus. One interesting, but easy example might learn the word association between wind and waves, thus enriching a beach scene with more than just color descriptions that otherwise might be accessed from raw pixels without text annotation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：小卫星星座提供地球陆地面积的每日全球覆盖，但图像富集依赖于自动化样改变检测或功能的搜索关键任务。例如，为了从原始像素提取文本注释需要两个依赖机器学习模型，一个分析俯视图像和其他，以产生描述性标题。我们评估对卫星图片说明以前最大的基准七款车型。我们扩展了标记图像样本五倍，然后扩充的，正确的和修剪词汇接近粗糙最小 - 最大（最小字，最大的描述）。这一结果相比毫不逊色与大预先训练的图像模型，但提供的模型大小百倍还原之前的工作不牺牲整体精度（当日志熵损失测量）。这些小模型提供了新的部署机会，特别是当推到边缘处理器，板载卫星，或分布式地面站。为了量化标题的描述性，我们引入了一个新的多类混淆或错误矩阵得分人类标记的测试数据，而不会标记的图像，包括边框检测，但缺乏完整的句子标题。这项工作表明未来字幕战略，特别是那些能充实类覆盖率超过土地使用的应用程序和减轻色心和邻接的形容词（“绿色”，“近”，“间”等）。许多现代语言的变压器存在新颖性和与世界的知识利用的模型从训练中收集来自其庞大的在线语料库。一个有趣的，但简单的例子可以学习乘风破浪的词语联想，从而丰富海滩场景比，否则可能从原始像素进行访问，而不文本注释只是颜色的描述更多。</font>
</div>


<hr>
<div id="paper14"> <b>14. Document Network Projection in Pretrained Word Embedding Space</b>  <a href="https://arxiv.org/pdf/2001.05727" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gourru%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antoine Gourru</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guille%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrien Guille</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Velcin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Velcin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jacques%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Jacques</a><br>
<font size="3">
Abstract: We present Regularized Linear Embedding (RLE), a novel method that projects a collection of linked documents (e.g. citation network) into a pretrained word embedding space. In addition to the textual content, we leverage a matrix of pairwise similarities providing complementary information (e.g., the network proximity of two documents in a citation graph). We first build a simple word vector average for each document, and we use the similarities to alter this average representation. The document representations can help to solve many information retrieval tasks, such as recommendation, classification and clustering. We demonstrate that our approach outperforms or matches existing document network embedding methods on node classification and link prediction tasks. Furthermore, we show that it helps identifying relevant keywords to describe document classes. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：我们提出正则线性嵌入（RLE），一种新型的方法，其项目链接的文档（例如引网络）的集合到一个预训练的字嵌入空间。除了文本内容，我们利用成对的相似性提供补充信息（例如，在引用图两个文件的网络接近）的基质中。我们首先建立每个文档的简单词汇向量平均，而我们使用的相似改变这种平均表示。该文件表示可以帮助解决许多信息检索任务，如推荐，分类和聚类。我们证明我们的方法比或匹配现有的文档嵌入网络节点上的分类和链接预测任务的方法。此外，我们表明，它可以帮助识别相关关键字来描述文档类。</font>
</div>


<hr>
<div id="paper15"> <b>15. Delving Deeper into the Decoder for Video Captioning</b>  <a href="https://arxiv.org/pdf/2001.05614" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haoran Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jianmin Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaolin Hu</a><br>
<font size="3">
Abstract: Video captioning is an advanced multi-modal task which aims to describe a video clip using a natural language sentence. The encoder-decoder framework is the most popular paradigm for this task in recent years. However, there still exist some non-negligible problems in the decoder of a video captioning model. We make a thorough investigation into the decoder and adopt three techniques to improve the performance of the model. First of all, a combination of variational dropout and layer normalization is embedded into a recurrent unit to alleviate the problem of overfitting. Secondly, a new method is proposed to evaluate the performance of a model on a validation set so as to select the best checkpoint for testing. Finally, a new training strategy called \textit{professional learning} is proposed which develops the strong points of a captioning model and bypasses its weaknesses. It is demonstrated in the experiments on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets that our model has achieved the best results evaluated by BLEU, CIDEr, METEOR and ROUGE-L metrics with significant gains of up to 11.7% on MSVD and 5% on MSR-VTT compared with the previous state-of-the-art models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：视频字幕是一种先进的多模态的任务，目的是描述使用自然语言句子的视频剪辑。编码器，解码器框架是在最近几年，这一任务最流行的范例。但是，仍然存在着一个视频字幕模型的解码器的一些不可忽视的问题。我们做一个彻底的调查，解码器，并采用三种技术来提高模型的性能。首先，变差和层正常化的组合被嵌入到一个重复单元，以减轻过拟合问题。其次，新方法，提出了在验证集评估模型的性能，以便选择最佳的检查点进行测试。最后，新的培训战略称为\ textit {专业学习}，提出了开发一个字幕模型的长处，避开其弱点。它证明了在微软研究院的视频描述语料库（MSVD）和MSR视频的实验文本（MSR-VTT）的数据集，我们的模型已经实现由BLEU，苹果酒，流星和ROUGE-L指标评估了显著收益最好的结果高达11.7％的MSVD和5％的MSR-VTT与以前国家的最先进的机型相比。</font>
</div>


<hr>
<div id="paper16"> <b>16. Insertion-Deletion Transformer</b>  <a href="https://arxiv.org/pdf/2001.05540" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruis%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Laura Ruis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Stern%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mitchell Stern</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Proskurnia%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julia Proskurnia</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">William Chan</a><br>
<font size="3">
Abstract: We propose the Insertion-Deletion Transformer, a novel transformer-based neural architecture and training method for sequence generation. The model consists of two phases that are executed iteratively, 1) an insertion phase and 2) a deletion phase. The insertion phase parameterizes a distribution of insertions on the current output hypothesis, while the deletion phase parameterizes a distribution of deletions over the current output hypothesis. The training method is a principled and simple algorithm, where the deletion model obtains its signal directly on-policy from the insertion model output. We demonstrate the effectiveness of our Insertion-Deletion Transformer on synthetic translation tasks, obtaining significant BLEU score improvement over an insertion-only model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出的插入缺失变压器，一种新型的基于变压器的神经结构和序列生成训练方法。该模型由被迭代执行两个阶段：1）的插入相位和2）的删除相。插入阶段参数化对电流输出假设插入的分布，而删除相位参数化缺失的过电流输出假设的分布。该训练方法是一个原则性和简单的算法，其中该删除模型直接获得关于策略从插入模型输出它的信号。我们证明我们的插入缺失变压器上合成翻译任务的有效性，通过一个只有插入模型取得显著BLEU评分改善。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>一些公司及高校在线翻译系统</title>
    <url>/2020/01/17/%E4%B8%80%E4%BA%9B%E5%85%AC%E5%8F%B8%E5%8F%8A%E9%AB%98%E6%A0%A1%E5%9C%A8%E7%BA%BF%E7%BF%BB%E8%AF%91%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h1 id="公司在线翻译系统"><a href="#公司在线翻译系统" class="headerlink" title="公司在线翻译系统"></a>公司在线翻译系统</h1><ul>
<li><a href="https://fanyi.baidu.com/" target="_blank" rel="noopener">百度翻译</a></li>
<li><a href="https://translate.google.cn/" target="_blank" rel="noopener">谷歌翻译</a></li>
<li><a href="http://fanyi.youdao.com/" target="_blank" rel="noopener">有道翻译</a></li>
<li><a href="https://fanyi.qq.com/" target="_blank" rel="noopener">腾讯翻译君</a></li>
<li><a href="https://fanyi.sogou.com/" target="_blank" rel="noopener">搜狗翻译</a></li>
<li><a href="https://niutrans.vip/trans" target="_blank" rel="noopener">小牛翻译</a></li>
<li><a href="https://cloudtranslation.com/online/" target="_blank" rel="noopener">云译</a></li>
</ul><h1 id="高校在线翻译系统"><a href="#高校在线翻译系统" class="headerlink" title="高校在线翻译系统"></a>高校在线翻译系统</h1><ul>
<li><a href="http://nmt.xmu.edu.cn/" target="_blank" rel="noopener">厦门大学</a></li>
</ul>]]></content>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-16</title>
    <url>/2020/01/17/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-16/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses <a href="https://arxiv.org/pdf/2001.05467" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> A BERT based Sentiment Analysis and Key Entity Detection Approach for  Online Financial Texts <a href="https://arxiv.org/pdf/2001.05326" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Authorship Attribution in Bangla literature using Character-level CNN <a href="https://arxiv.org/pdf/2001.05316" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> A Continuous Space Neural Language Model for Bengali Language <a href="https://arxiv.org/pdf/2001.05315" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Embedding Compression with Isotropic Iterative Quantization <a href="https://arxiv.org/pdf/2001.05314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Tensor Graph Convolutional Networks for Text Classification <a href="https://arxiv.org/pdf/2001.05313" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process  Approach to Linguistic Relationships <a href="https://arxiv.org/pdf/2001.05297" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Urdu-English Machine Transliteration using Neural Networks <a href="https://arxiv.org/pdf/2001.05296" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Language Models Are An Effective Patient Representation Learning  Technique For Electronic Health Record Data <a href="https://arxiv.org/pdf/2001.05295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> The empirical structure of word frequency distributions <a href="https://arxiv.org/pdf/2001.05292" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> Exploring and Improving Robustness of Multi Task Deep Neural Networks  via Domain Agnostic Defenses <a href="https://arxiv.org/pdf/2001.05286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Detecting New Word Meanings: A Comparison of Word Embedding Models in  Spanish <a href="https://arxiv.org/pdf/2001.05285" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Improving Spoken Language Understanding By Exploiting ASR N-best  Hypotheses <a href="https://arxiv.org/pdf/2001.05284" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> FGN: Fusion Glyph Network for Chinese Named Entity Recognition <a href="https://arxiv.org/pdf/2001.05272" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation <a href="https://arxiv.org/pdf/2001.05139" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Parallel Machine Translation with Disentangled Context Transformer <a href="https://arxiv.org/pdf/2001.05136" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Robust Speaker Recognition Using Speech Enhancement And Attention Model <a href="https://arxiv.org/pdf/2001.05031" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> A Tree Adjoining Grammar Representation for Models Of Stochastic  Dynamical Systems <a href="https://arxiv.org/pdf/2001.05320" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Auto Completion of User Interface Layout Design Using Transformer-Based  Tree Decoders <a href="https://arxiv.org/pdf/2001.05308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> Teddy: A System for Interactive Review Analysis <a href="https://arxiv.org/pdf/2001.05171" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Modeling Product Search Relevance in e-Commerce <a href="https://arxiv.org/pdf/2001.04980" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses</b>  <a href="https://arxiv.org/pdf/2001.05467" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Niu%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tong Niu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Bansal</a><br>
<font size="3">
Abstract: Many sequence-to-sequence dialogue models tend to generate safe, uninformative responses. There have been various useful efforts on trying to eliminate them. However, these approaches either improve decoding algorithms during inference, rely on hand-crafted features, or employ complex models. In our work, we build dialogue models that are dynamically aware of what utterances or tokens are dull without any feature-engineering. Specifically, we start with a simple yet effective automatic metric, AvgOut, which calculates the average output probability distribution of all time steps on the decoder side during training. This metric directly estimates which tokens are more likely to be generated, thus making it a faithful evaluation of the model diversity (i.e., for diverse models, the token probabilities should be more evenly distributed rather than peaked at a few dull tokens). We then leverage this novel metric to propose three models that promote diversity without losing relevance. The first model, MinAvgOut, directly maximizes the diversity score through the output distributions of each batch; the second model, Label Fine-Tuning (LFT), prepends to the source sequence a label continuously scaled by the diversity score to control the diversity level; the third model, RL, adopts Reinforcement Learning and treats the diversity score as a reward signal. Moreover, we experiment with a hybrid model by combining the loss terms of MinAvgOut and RL. All four models outperform their base LSTM-RNN model on both diversity and relevance by a large margin, and are comparable to or better than competitive baselines (also verified via human evaluation). Moreover, our approaches are orthogonal to the base model, making them applicable as an add-on to other emerging better dialogue models in the future. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：许多序列对序列的对话模式容易产生安全，无信息的响应。已经有上试图消除他们各种有用的努力。然而，这些方法或者改进的推理过程中的解码算法，依靠手工制作的功能，或采用复杂的模型。在我们的工作，我们建立对话模型是动态意识到什么话语或令牌是没有任何功能的工程平淡。具体而言，我们先从一个简单而有效的自动度量，AvgOut，其在训练期间计算出的解码器侧的所有时间步长的平均输出概率分布。该指标直接估计令牌更容易产生，从而使得它的型号多样的忠实评价（即，对于不同的车型，令牌的概率应该是更均匀地分布，而不是在几个沉闷的令牌见顶）。然后，我们利用这一新的指标，提出促进多样性不失相关性三种模式。第一种模式，MinAvgOut，直接通过最大化每批的输出分布的分集比分;第二个模型，标签微调（LFT），前置到源序列通过分集比分来控制分集电平连续缩放的标签;第三种模式，RL，采用强化学习和对待多样性分数作为奖励信号。此外，我们结合MinAvgOut和RL的损失方面与混合动力模型试验。所有这四种型号跑赢上都多样性和实用性大幅度的基地LSTM-RNN模型，并比竞争基准（也可以通过人工评估验证）相当或更好。此外，我们的方法是正交的示范基地，使它们适用于作为一个附加在未来其他新兴更好的对话模式。</font>
</div>


<hr>
<div id="paper2"> <b>2. A BERT based Sentiment Analysis and Key Entity Detection Approach for  Online Financial Texts</b>  <a href="https://arxiv.org/pdf/2001.05326" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lingyun Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lin Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zheng%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinhao Zheng</a><br>
<font size="3">
Abstract: The emergence and rapid progress of the Internet have brought ever-increasing impact on financial domain. How to rapidly and accurately mine the key information from the massive negative financial texts has become one of the key issues for investors and decision makers. Aiming at the issue, we propose a sentiment analysis and key entity detection approach based on BERT, which is applied in online financial text mining and public opinion analysis in social media. By using pre-train model, we first study sentiment analysis, and then we consider key entity detection as a sentence matching or Machine Reading Comprehension (MRC) task in different granularity. Among them, we mainly focus on negative sentimental information. We detect the specific entity by using our approach, which is different from traditional Named Entity Recognition (NER). In addition, we also use ensemble learning to improve the performance of proposed approach. Experimental results show that the performance of our approach is generally higher than SVM, LR, NBM, and BERT for two financial sentiment analysis and key entity detection datasets. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：出现和互联网的飞速进步也带来了金融领域不断增加的影响。如何快速，准确地矿从大量负面财务文本的关键信息已成为投资者和决策者的关键问题之一。针对这个问题，我们提出了一个情感分析和基于BERT，这是在网上金融文本挖掘和舆情分析社交媒体应用的关键实体检测方法。通过使用预火车模型，我们首先研究情感分析，然后我们考虑的关键实体检测在不同粒度的句子匹配或机器阅读理解（MRC）的任务。其中，我们主要集中在负感伤的信息。我们发现，通过使用我们的方法，这是从传统的命名实体识别（NER）不同的特定实体。此外，我们还可以使用集成学习，以提高该方法的性能。实验结果表明，我们的方法的性能一般比SVM，LR，NBM，和BERT较高的两个财务情绪分析和关键实体检测数据集。</font>
</div>


<hr>
<div id="paper3"> <b>3. Authorship Attribution in Bangla literature using Character-level CNN</b>  <a href="https://arxiv.org/pdf/2001.05316" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Khatun%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aisha Khatun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anisur Rahman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Islam%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Md. Saiful Islam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Marium-E-Jannat" target="_blank" rel="noopener" style="color:#0000EE;">Marium-E-Jannat</a><br>
<font size="3">
Abstract: Characters are the smallest unit of text that can extract stylometric signals to determine the author of a text. In this paper, we investigate the effectiveness of character-level signals in Authorship Attribution of Bangla Literature and show that the results are promising but improvable. The time and memory efficiency of the proposed model is much higher than the word level counterparts but accuracy is 2-5% less than the best performing word-level models. Comparison of various word-based models is performed and shown that the proposed model performs increasingly better with larger datasets. We also analyze the effect of pre-training character embedding of diverse Bangla character set in authorship attribution. It is seen that the performance is improved by up to 10% on pre-training. We used 2 datasets from 6 to 14 authors, balancing them before training and compare the results. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：字符是文本，可以提取stylometric信号来确定文本的作者的最小单位。在本文中，我们研究了字符级信号的孟加拉文学著作权归属的有效性，并表明其结果是有希望的，但改善的。该模型的时间和内存效率比字级同行高得多，但精度比表现最好的字级车型少2-5％。执行并显示各种基于词的模型比较，该模型执行越来越多地与更大的数据集更好。我们还分析了前培训字符著作权归属不同孟加拉字符集的嵌入的效果。可以看出，性能高达10％的预培训提高。我们使用的数据集2的6至14作家，训练前平衡他们并比较结果。</font>
</div>


<hr>
<div id="paper4"> <b>4. A Continuous Space Neural Language Model for Bengali Language</b>  <a href="https://arxiv.org/pdf/2001.05315" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chowdhury%2C+H+A" target="_blank" rel="noopener" style="color:#0000EE;">Hemayet Ahmed Chowdhury</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Imon%2C+M+A+H" target="_blank" rel="noopener" style="color:#0000EE;">Md. Azizul Haque Imon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Rahman%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anisur Rahman</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Khatun%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Aisha Khatun</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Islam%2C+M+S" target="_blank" rel="noopener" style="color:#0000EE;">Md. Saiful Islam</a><br>
<font size="3">
Abstract: Language models are generally employed to estimate the probability distribution of various linguistic units, making them one of the fundamental parts of natural language processing. Applications of language models include a wide spectrum of tasks such as text summarization, translation and classification. For a low resource language like Bengali, the research in this area so far can be considered to be narrow at the very least, with some traditional count based models being proposed. This paper attempts to address the issue and proposes a continuous-space neural language model, or more specifically an ASGD weight dropped LSTM language model, along with techniques to efficiently train it for Bengali Language. The performance analysis with some currently existing count based models illustrated in this paper also shows that the proposed architecture outperforms its counterparts by achieving an inference perplexity as low as 51.2 on the held out data set for Bengali. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言模型通常用来估计各种语言单位的概率分布，使其自然语言处理的基本组成部分之一。语言模型的应用包括任务，如文本摘要，翻译和分类的广泛。对于像孟加拉低资源的语言，在这方面至今也算是研究是起码狭窄，而提出了一些传统的基于计数模式。本文试图解决这个问题，并提出了一个连续的空间神经语言模型，或者更准确地说是ASGD体重也下降LSTM语言模型，用技术来有效地训练它的孟加拉语一起。本文所示还显示了一些目前存在的以计数为基础的模型的性能分析，提出的架构通过实现一个推论困惑低至51.2对孟加拉的伸出数据集优于其同行。</font>
</div>


<hr>
<div id="paper5"> <b>5. Embedding Compression with Isotropic Iterative Quantization</b>  <a href="https://arxiv.org/pdf/2001.05314" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Siyu Liao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jie Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanzhi Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qinru Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Yuan</a><br>
<font size="3">
Abstract: Continuous representation of words is a standard component in deep learning-based NLP models. However, representing a large vocabulary requires significant memory, which can cause problems, particularly on resource-constrained platforms. Therefore, in this paper we propose an isotropic iterative quantization (IIQ) approach for compressing embedding vectors into binary ones, leveraging the iterative quantization technique well established for image retrieval, while satisfying the desired isotropic property of PMI based models. Experiments with pre-trained embeddings (i.e., GloVe and HDC) demonstrate a more than thirty-fold compression ratio with comparable and sometimes even improved performance over the original real-valued embedding vectors. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：词的连续表示是基于深学习NLP车型的标准组件。然而，较大量的词汇需要显著的内存，这可能会导致问题，特别是在资源受限的平台。因此，在本文中，我们提出了压缩嵌入矢量成二进制的，利用图像检索完善的迭代量化技术，同时满足基于PMI模型所需的各向同性各向同性迭代量化（IIQ）的方法。与预训练的嵌入（即，手套和HDC）实验证实与在原始实值嵌入矢量可比有时甚至改善性能超过30倍的压缩比。</font>
</div>


<hr>
<div id="paper6"> <b>6. Tensor Graph Convolutional Networks for Text Classification</b>  <a href="https://arxiv.org/pdf/2001.05313" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xien Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=You%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinxin You</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiao Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Ji Wu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lv%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Ping Lv</a><br>
<font size="3">
Abstract: Compared to sequential learning models, graph-based neural networks exhibit some excellent properties, such as ability capturing global information. In this paper, we investigate graph-based neural networks for text classification problem. A new framework TensorGCN (tensor graph convolutional networks), is presented for this task. A text graph tensor is firstly constructed to describe semantic, syntactic, and sequential contextual information. Then, two kinds of propagation learning perform on the text graph tensor. The first is intra-graph propagation used for aggregating information from neighborhood nodes in a single graph. The second is inter-graph propagation used for harmonizing heterogeneous information between graphs. Extensive experiments are conducted on benchmark datasets, and the results illustrate the effectiveness of our proposed framework. Our proposed TensorGCN presents an effective way to harmonize and integrate heterogeneous information from different kinds of graphs. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：相比于连续的学习模式，基于图形的神经网络表现出一些优异的性能，如能力捕捉全球信息。在本文中，我们研究了文本分类问题，基于图形的神经网络。一个新的框架TensorGCN（图张卷积网络），提出了这一任务。文本图形张量首先被构造来描述语义，语法，和顺序的上下文信息。然后，有两种传播学习上的文字图形张量执行。第一种是用于在单个图表聚集来自邻近节点的信息，图形的帧内传播。第二个是用于协调图之间异构信息曲线图间传播。大量的实验是在基准数据集进行，其结果说明我们提出的框架的有效性。我们提出的TensorGCN礼物协调和异构信息从不同类型的图形整合的有效途径。</font>
</div>


<hr>
<div id="paper7"> <b>7. Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process  Approach to Linguistic Relationships</b>  <a href="https://arxiv.org/pdf/2001.05297" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Cathcart%2C+C+A" target="_blank" rel="noopener" style="color:#0000EE;">Chundra Aroor Cathcart</a><br>
<font size="3">
Abstract: This paper addresses a series of complex and unresolved issues in the historical phonology of West Iranian languages. The West Iranian languages (Persian, Kurdish, Balochi, and other languages) display a high degree of non-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to language contact; we argue, however, that an oversimplified view of the processes at work has prevailed in the literature on West Iranian dialectology, with specialists assuming that deviations from an expected outcome in a given non-Persian language are due to lexical borrowing from some chronological stage of Persian. It is demonstrated that this qualitative approach yields at times problematic conclusions stemming from the lack of explicit probabilistic inferences regarding the distribution of the data: Persian may not be the sole donor language; additionally, borrowing at the lexical level is not always the mechanism that introduces irregularity. In many cases, the possibility that West Iranian languages show different reflexes in different conditioning environments remains under-explored. We employ a novel Bayesian approach designed to overcome these problems and tease apart the different determinants of irregularity in patterns of West Iranian sound change. Our methodology allows us to provisionally resolve a number of outstanding questions in the literature on West Iranian dialectology concerning the dialectal affiliation of certain sound changes. We outline future directions for work of this sort. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文地址了一系列西伊朗语支的历史音韵复杂和悬而未决的问题。西伊朗语支（波斯，库尔德人，俾路支语等语种）表现出高度的非Lautgesetzlich行为。大多数这种不规则的无疑是由于语言接触;我们认为，但是，在工作流程的一个过于简单化的观点在西方的伊朗方言的文学盛行，与由于从一些时间阶段的词汇借用专家假设从给定的非波斯语的预期结果偏差波斯语。据证实，这种定性方法的产量有时有问题的结论，因为缺乏有关数据的分布概率明确推论的词干：波斯可能不是唯一的供体语言;另外，在词汇水平借用并不总是机制引入了不规则性。在许多情况下，西伊朗的语言说明了在不同的环境条件不同反射的可能性仍然充分开发。我们采用设计来克服这些问题，并梳理出不规则的西伊朗声音的变化模式的不同决定一种新的贝叶斯方法。我们的方法可以让我们暂时解决了许多文献对西方的伊朗方言有关的某些声音的变化方言隶属关系悬而未决的问题。我们为这种工作勾勒未来的发展方向。</font>
</div>


<hr>
<div id="paper8"> <b>8. Urdu-English Machine Transliteration using Neural Networks</b>  <a href="https://arxiv.org/pdf/2001.05296" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Din%2C+U+M+u" target="_blank" rel="noopener" style="color:#0000EE;">Usman Mohy ud Din</a><br>
<font size="3">
Abstract: Machine translation has gained much attention in recent years. It is a sub-field of computational linguistic which focus on translating text from one language to other language. Among different translation techniques, neural network currently leading the domain with its capabilities of providing a single large neural network with attention mechanism, sequence-to-sequence and long-short term modelling. Despite significant progress in domain of machine translation, translation of out-of-vocabulary words(OOV) which include technical terms, named-entities, foreign words are still a challenge for current state-of-art translation systems, and this situation becomes even worse while translating between low resource languages or languages having different structures. Due to morphological richness of a language, a word may have different meninges in different context. In such scenarios, translation of word is not only enough in order provide the correct/quality translation. Transliteration is a way to consider the context of word/sentence during translation. For low resource language like Urdu, it is very difficult to have/find parallel corpus for transliteration which is large enough to train the system. In this work, we presented transliteration technique based on Expectation Maximization (EM) which is un-supervised and language independent. Systems learns the pattern and out-of-vocabulary (OOV) words from parallel corpus and there is no need to train it on transliteration corpus explicitly. This approach is tested on three models of statistical machine translation (SMT) which include phrasebased, hierarchical phrase-based and factor based models and two models of neural machine translation which include LSTM and transformer model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：机器翻译已经获得了广泛关注，近年来。这是着眼于从一种语言到另一种语言翻译文本计算语言学的子领域。在不同的翻译技术，目前主导其提供与注意机制，序列对序列和长短期建模一个大的神经网络的功能域的神经网络。尽管在机器翻译，出词汇的词（OOV），其中包括技术术语，命名实体翻译的领域显著进步，外来词仍然是国家的最先进的电流转换系统的挑战，而且这种情况变得更而具有不同结构的低资源语言或语言之间的转换变得更糟。由于语言的形态丰富，一个字可以有不同的上下文不同的脑膜。在这种情况下，文字的翻译不仅足以为了提供正确的/翻译质量。音译是考虑在翻译过程中字/句子的上下文的方式。对于像乌尔都语低资源语言，它是很难有/找到音译平行语料库是足够大的训练系统。在这项工作中，我们提出了基于期望最大化（EM）的音译技术，它是无监督和语言无关。系统学习的模式，从平行语料库超出词汇（OOV）的话，也没有必要训练它音译语料库明确。这种方法是在三个模型的统计机器翻译（SMT），其中包括phrasebased的测试，分层phrasebased和基于因子模型和神经机器翻译的两款车型，其中包括LSTM和变压器模型。</font>
</div>


<hr>
<div id="paper9"> <b>9. Language Models Are An Effective Patient Representation Learning  Technique For Electronic Health Record Data</b>  <a href="https://arxiv.org/pdf/2001.05295" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Steinberg%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Ethan Steinberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jung%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Ken Jung</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Fries%2C+J+A" target="_blank" rel="noopener" style="color:#0000EE;">Jason A. Fries</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Corbin%2C+C+K" target="_blank" rel="noopener" style="color:#0000EE;">Conor K. Corbin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pfohl%2C+S+R" target="_blank" rel="noopener" style="color:#0000EE;">Stephen R. Pfohl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shah%2C+N+H" target="_blank" rel="noopener" style="color:#0000EE;">Nigam H. Shah</a><br>
<font size="3">
Abstract: Widespread adoption of electronic health records (EHRs) has fueled development of clinical outcome models using machine learning. However, patient EHR data are complex, and how to optimally represent them is an open question. This complexity, along with often small training set sizes available to train these clinical outcome models, are two core challenges for training high quality models. In this paper, we demonstrate that learning generic representations from the data of all the patients in the EHR enables better performing prediction models for clinical outcomes, allowing for these challenges to be overcome. We adapt common representation learning techniques used in other domains and find that representations inspired by language models enable a 3.5% mean improvement in AUROC on five clinical outcomes compared to standard baselines, with the average improvement rising to 19% when only a small number of patients are available for training a prediction model for a given clinical outcome. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：广泛采用的电子健康记录（电子病历）具有利用机器学习临床结果模型的燃料的发展。然而，患者的电子病历数据是复杂的，如何最优地表示他们是一个悬而未决的问题。这种复杂性，经常与小的训练集以及尺寸，以训练这些临床结果的模型，是培养高素质模型两个核心挑战。在本文中，我们证明了学习所有的患者在电子病历的数据一般表示为临床结果可以实现更好的进行预测模型，从而不必在克服这些挑战。我们采用通用表示学习其他领域使用的技术，并找到语言模型的启发是表示能够在AUROC五个临床结果3.5％的平均改善比标准的基线，平均提高上升到19％时，只有少数患者可用于训练预测模型对于给定的临床结果。</font>
</div>


<hr>
<div id="paper10"> <b>10. The empirical structure of word frequency distributions</b>  <a href="https://arxiv.org/pdf/2001.05292" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ramscar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Ramscar</a><br>
<font size="3">
Abstract: The frequencies at which individual words occur across languages follow power law distributions, a pattern of findings known as Zipf's law. A vast literature argues over whether this serves to optimize the efficiency of human communication, however this claim is necessarily post hoc, and it has been suggested that Zipf's law may in fact describe mixtures of other distributions. From this perspective, recent findings that Sinosphere first (family) names are geometrically distributed are notable, because this is actually consistent with information theoretic predictions regarding optimal coding. First names form natural communicative distributions in most languages, and I show that when analyzed in relation to the communities in which they are used, first name distributions across a diverse set of languages are both geometric and, historically, remarkably similar, with power law distributions only emerging when empirical distributions are aggregated. I then show this pattern of findings replicates in communicative distributions of English nouns and verbs. These results indicate that if lexical distributions support efficient communication, they do so because their functional structures directly satisfy the constraints described by information theory, and not because of Zipf's law. Understanding the function of these information structures is likely to be key to explaining humankind's remarkable communicative capacities. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在发生个别单词跨语言服从幂律分布的频率，被称为齐普夫定律发现的模式。大量文献论证了，这是否用于优化人力沟通的效率，但这种说法必然是事后，这已经表明齐普夫定律，实际上可能描述的其他分布的混合。从这个角度来看，最近的调查结果Sinosphere第一（家庭）名称几何分布是显着的，因为这是关于最优编码信息理论预测实际上是一致的。名字形成大多数语言自然交际分布，我表明，在关系分析，在一组不同的语言中使用它们的社区，第一个名称发行时都是几何和，从历史上看，非常相似，幂律分布只有当经验分布聚集出现。然后我显示英语名词和动词的交际分布发现重复的这种模式。这些结果表明，如果词汇分布支持有效的沟通，他们这样做是因为他们的功能结构直接满足信息理论中描述的约束，并没有因为齐普夫定律。了解这些信息结构的功能很可能是关键，解释人类的非凡能力，交际。</font>
</div>


<hr>
<div id="paper11"> <b>11. Exploring and Improving Robustness of Multi Task Deep Neural Networks  via Domain Agnostic Defenses</b>  <a href="https://arxiv.org/pdf/2001.05286" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Murali%2C+K+C" target="_blank" rel="noopener" style="color:#0000EE;">Kashyap Coimbatore Murali</a><br>
<font size="3">
Abstract: In this paper, we explore the robustness of the Multi-Task Deep Neural Networks (MT-DNN) against non-targeted adversarial attacks across Natural Language Understanding (NLU) tasks as well as some possible ways to defend against them. Liu et al., have shown that the Multi-Task Deep Neural Network, due to the regularization effect produced when training as a result of its cross task data, is more robust than a vanilla BERT model trained only on one task (1.1%-1.5% absolute difference). We further show that although the MT-DNN has generalized better, making it easily transferable across domains and tasks, it can still be compromised as after only 2 attacks (1-character and 2-character) the accuracy drops by 42.05% and 32.24% for the SNLI and SciTail tasks. Finally, we propose a domain agnostic defense which restores the model's accuracy (36.75% and 25.94% respectively) as opposed to a general-purpose defense or an off-the-shelf spell checker. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们探索了多任务深层神经网络（MT-DNN）的稳健性对整个自然语言理解（NLU）任务以及一些可能的方式来抵御这些非目标对抗性攻击。 Liu等人，已经表明，多任务深层的神经网络中，由于正规化效应产生当训练作为其横任务数据的结果是，比只在一个任务（1.1％培养了香草BERT模型更健壮 - 1.5％的绝对差）。进一步的研究表明，虽然MT-DNN具有更好的推广，使得它很容易跨域和任务转让的，它仍然可以作出妥协，只有2次攻击（1个字符和2个字符）的准确度42.05％和32.24％，下降后对于SNLI和SciTail任务。最后，我们提出了一个未知的领域国防其恢复模型的精确度（36.75％和25.94分别％），而不是通用的防守还是关闭的，现成的拼写检查器。</font>
</div>


<hr>
<div id="paper12"> <b>12. Detecting New Word Meanings: A Comparison of Word Embedding Models in  Spanish</b>  <a href="https://arxiv.org/pdf/2001.05285" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Torres-Rivera%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Andrés Torres-Rivera</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Torres-Moreno%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Juan-Manuel Torres-Moreno</a><br>
<font size="3">
Abstract: Semantic neologisms (SN) are defined as words that acquire a new word meaning while maintaining their form. Given the nature of this kind of neologisms, the task of identifying these new word meanings is currently performed manually by specialists at observatories of neology. To detect SN in a semi-automatic way, we developed a system that implements a combination of the following strategies: topic modeling, keyword extraction, and word sense disambiguation. The role of topic modeling is to detect the themes that are treated in the input text. Themes within a text give clues about the particular meaning of the words that are used, for example: viral has one meaning in the context of computer science (CS) and another when talking about health. To extract keywords, we used TextRank with POS tag filtering. With this method, we can obtain relevant words that are already part of the Spanish lexicon. We use a deep learning model to determine if a given keyword could have a new meaning. Embeddings that are different from all the known meanings (or topics) indicate that a word might be a valid SN candidate. In this study, we examine the following word embedding models: Word2Vec, Sense2Vec, and FastText. The models were trained with equivalent parameters using Wikipedia in Spanish as corpora. Then we used a list of words and their concordances (obtained from our database of neologisms) to show the different embeddings that each model yields. Finally, we present a comparison of these outcomes with the concordances of each word to show how we can determine if a word could be a valid candidate for SN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语义新词（SN）被定义为获得一个新词，同时保持其形式意义的话。鉴于这种新词的性质，目前手动专家在旧词新的天文台进行识别这些新词的意义的任务。为了检测SN以半自动化的方式，我们开发了一个系统，该系统实现了以下策略的组合：主题建模，关键词提取，以及词义消歧。主题建模的作用是检测在输入文本处理的主题。文本中的主题提供有关的被使用，例如词的特殊含义的线索：病毒只有一种含义在计算机科学（CS）和其他的方面讲卫生的时候。要提取的关键字，我们使用TextRank与POS标签过滤。通过这种方法，我们可以得到与已是西班牙词汇的一部分相关的词。我们使用了深刻的学习模式，以确定是否一个给定的关键字可能有新的意义。嵌入物是所有已知的含义（或主题）不同的指示词可能是一个有效的SN候选人。在这项研究中，我们考察以下单词嵌入型号：Word2Vec，Sense2Vec和FastText。模特们在西班牙使用维基百科语料库等效参数训练。然后我们使用的单词的列表和他们的语词（从我们的新词的数据库中获得）来显示不同的嵌入每个型号的产量。最后，我们提出这些结果与每个单词的词汇索引的比较，以显示我们如何确定一个词可能是SN有效候选人。</font>
</div>


<hr>
<div id="paper13"> <b>13. Improving Spoken Language Understanding By Exploiting ASR N-best  Hypotheses</b>  <a href="https://arxiv.org/pdf/2001.05284" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mingda Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ruan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weitong Ruan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xinyue Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soldaini%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luca Soldaini</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hamza%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wael Hamza</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Su%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chengwei Su</a><br>
<font size="3">
Abstract: In a modern spoken language understanding (SLU) system, the natural language understanding (NLU) module takes interpretations of a speech from the automatic speech recognition (ASR) module as the input. The NLU module usually uses the first best interpretation of a given speech in downstream tasks such as domain and intent classification. However, the ASR module might misrecognize some speeches and the first best interpretation could be erroneous and noisy. Solely relying on the first best interpretation could make the performance of downstream tasks non-optimal. To address this issue, we introduce a series of simple yet efficient models for improving the understanding of semantics of the input speeches by collectively exploiting the n-best speech interpretations from the ASR module. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在现代口语理解（SLU）系统，自然语言理解（NLU）模块需要一个讲话的解释从自动语音识别（ASR）模块的输入。该NLU模块通常使用一个给定的讲话在下游任务，如域名和意图分类第一最好的诠释。然而，ASR模块可能误识别的一些讲话和第一最好的诠释可能是错误的和嘈杂。仅仅依靠第一最好的诠释可以使下游任务的性能最优的。为了解决这个问题，我们引入了一系列简单而有效的模型，通过集体从ASR模块利用N条最佳演讲诠释提高输入演讲的语义的理解。</font>
</div>


<hr>
<div id="paper14"> <b>14. FGN: Fusion Glyph Network for Chinese Named Entity Recognition</b>  <a href="https://arxiv.org/pdf/2001.05272" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xuan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhenyu Xuan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bao%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rui Bao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Chuyu Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Jiang%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shengyi Jiang</a><br>
<font size="3">
Abstract: Chinese NER is a challenging task. As pictographs, Chinese characters contain latent glyph information, which is often overlooked. We propose the FGN, Fusion Glyph Network for Chinese NER. This method may offer glyph information for fusion representation learning with BERT. The major innovations of FGN include: (1) a novel CNN structure called CGS-CNN is proposed to capture glyph information from both character graphs and their neighboring graphs. (2) we provide a method with sliding window and Slice-Attention to extract interactive information between BERT representation and glyph representation. Experiments are conducted on four NER datasets, showing that FGN with LSTM-CRF as tagger achieves new state-of-the-arts performance for Chinese NER. Further, more experiments are conducted to investigate the influences of various components and settings in FGN. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：中国NER是一个具有挑战性的任务。作为象形文字，中国字符包含潜在的字形信息，这些信息往往被忽视。我们提出了FGN，融合雕文网中国ER。这种方法可以提供融合表示学习与BERT字形信息。 FGN的主要创新点包括：（1）所谓的CGS-CNN一种新颖的CNN结构，提出从两个字符图和其周边图形捕获字形信息。 （2）我们提供具有滑动窗口和切片-注意提取BERT表示和字形表示之间的交互信息的方法。实验是在四个NER数据集进行，显示为恶搞实现国家的最艺术的新的性能为中国NER与LSTM-CRF是FGN。此外，更多的实验以调查FGN的各种组件和设置的影响。</font>
</div>


<hr>
<div id="paper15"> <b>15. A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation</b>  <a href="https://arxiv.org/pdf/2001.05139" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guan%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jian Guan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fei Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhihao Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiaoyan Zhu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minlie Huang</a><br>
<font size="3">
Abstract: Story generation, namely generating a reasonable story from a leading context, is an important but challenging task. In spite of the success in modeling fluency and local coherence, existing neural language generation models (e.g., GPT-2) still suffer from repetition, logic conflicts, and lack of long-range coherence in generated stories. We conjecture that this is because of the difficulty of associating relevant commonsense knowledge, understanding the causal relationships, and planning entities and events with proper temporal order. In this paper, we devise a knowledge-enhanced pretraining model for commonsense story generation. We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories. To further capture the causal and temporal dependencies between the sentences in a reasonable story, we employ multi-task learning which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：故事的产生，即产生从一个领先的情况下合理的故事，是一个重要而艰巨的任务。尽管在模拟的流畅性和局部连贯，现有的神经语言生成模型的成功（例如，GPT-2）仍从重复，逻辑冲突受到影响，并且缺乏长程的在产生的故事的连贯性。我们推测，这是因为关联相关常识的知识，理解因果关系，并计划实体和事件的适当时间顺序的难度。在本文中，我们设计了常识性的故事，一代知识强化训练前的模式。我们建议利用来自外部的知识基础常识知识产生合理的故事。为了进一步捕获的因果关系，并在合理的故事句子之间的时间相关性，我们采用多任务学习相结合的具有区分客观区分微调过程中真实和虚假的故事。自动和手动评估表明，我们的模型能够产生更合理的故事，比国家的最先进的基线，特别是在逻辑和全球协调方面。</font>
</div>


<hr>
<div id="paper16"> <b>16. Parallel Machine Translation with Disentangled Context Transformer</b>  <a href="https://arxiv.org/pdf/2001.05136" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kasai%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jungo Kasai</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cross%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">James Cross</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ghazvininejad%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marjan Ghazvininejad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiatao Gu</a><br>
<font size="3">
Abstract: State-of-the-art neural machine translation models generate a translation from left to right and every step is conditioned on the previously generated tokens. The sequential nature of this generation process causes fundamental latency in inference since we cannot generate multiple tokens in each sentence in parallel. We propose an attention-masking based model, called Disentangled Context (DisCo) transformer, that simultaneously generates all tokens given different contexts. The DisCo transformer is trained to predict every output token given an arbitrary subset of the other reference tokens. We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：国家的最先进的从左至右和每一步的前提是之前生成的令牌神经机器翻译模型生成翻译。这个生成过程的有序性导致的推论根本延迟，因为我们不能生成并行每个句子多个令牌。我们建议注意的遮蔽基于模型，称为迎刃而解上下文（迪斯科）变压器，能够同时生成给出不同的上下文中的所有令牌。迪斯科变压器训练以预测每个输出令牌给出的其它参考标记的任意子集。我们还开发并行易先推理算法，反复细化每个令牌并行，减少了所需的迭代次数。我们对7点的方向具有不同大小的数据大量的实验证明，如果没有更好的，性能比现有技术中的非自回归机器翻译的状态，而显著减少平均解码时间我们的模型实现了有竞争力的。</font>
</div>


<hr>
<div id="paper17"> <b>17. Robust Speaker Recognition Using Speech Enhancement And Attention Model</b>  <a href="https://arxiv.org/pdf/2001.05031" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanpei Shi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qiang Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hain%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thomas Hain</a><br>
<font size="3">
Abstract: In this paper, a novel architecture for speaker recognition is proposed by cascading speech enhancement and speaker processing. Its aim is to improve speaker recognition performance when speech signals are corrupted by noise. Instead of individually processing speech enhancement and speaker recognition, the two modules are integrated into one framework by a joint optimisation using deep neural networks. Furthermore, to increase robustness against noise, a multi-stage attention mechanism is employed to highlight the speaker related features learned from context information in time and frequency domain. To evaluate speaker identification and verification performance of the proposed approach, we test it on the dataset of VoxCeleb1, one of mostly used benchmark datasets. Moreover, the robustness of our proposed approach is also tested on VoxCeleb1 data when being corrupted by three types of interferences, general noise, music, and babble, at different signal-to-noise ratio (SNR) levels. The obtained results show that the proposed approach using speech enhancement and multi-stage attention models outperforms two strong baselines not using them in most acoustic conditions in our experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文对说话人识别一个新颖的架构通过级联语音增强和扬声器的处理建议。其目的是为了提高说话人识别性能时，语音信号被噪声干扰。而不是单独处理语音增强和说话人识别，这两个模块是通过使用深层神经网络的联合优化集成到一个框架。此外，为了增加可以有效抵抗噪声，采用多级注意机制，突出显示上下文信息在时间和频域学会了说话者相关的功能。为了评价说话人识别和建议的方法验证性能，我们测试它VoxCeleb1，大多采用标准数据集之一的数据集。此外，我们的建议的方法的稳健性上VoxCeleb1数据还测试由三种类型的干扰，一般噪声，音乐和多路重合，在不同的信噪比（SNR）水平被损坏时。得到的结果表明，该方法使用语音增强和多级车型的关注性能优于两周强的基线没有在我们的实验中最声学条件下使用它们。</font>
</div>


<hr>
<div id="paper18"> <b>18. A Tree Adjoining Grammar Representation for Models Of Stochastic  Dynamical Systems</b>  <a href="https://arxiv.org/pdf/2001.05320" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Khandelwal%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dhruv Khandelwal</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Schoukens%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maarten Schoukens</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=T%C3%B3th%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roland Tóth</a><br>
<font size="3">
Abstract: Model structure and complexity selection remains a challenging problem in system identification, especially for parametric non-linear models. Many Evolutionary Algorithm (EA) based methods have been proposed in the literature for estimating model structure and complexity. In most cases, the proposed methods are devised for estimating structure and complexity within a specified model class and hence these methods do not extend to other model structures without significant changes. In this paper, we propose a Tree Adjoining Grammar (TAG) for stochastic parametric models. TAGs can be used to generate models in an EA framework while imposing desirable structural constraints and incorporating prior knowledge. In this paper, we propose a TAG that can systematically generate models ranging from FIRs to polynomial NARMAX models. Furthermore, we demonstrate that TAGs can be easily extended to more general model classes, such as the non-linear Box-Jenkins model class, enabling the realization of flexible and automatic model structure and complexity selection via EA. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：模型的结构和复杂的选择仍然在系统识别一个具有挑战性的问题，尤其是对于参数非线性模型。许多进化算法（EA）为基础的方法在文献中已经提出了用于估计模型结构和复杂性。在大多数情况下，所提出的方法被设计为在指定模型类内估计结构和复杂性，并因此这些方法不延伸到其他模型结构而不显著变化。在本文中，我们提出了一个树连接语法（TAG）为随机参数模型。标签可以用于在EA框架来生成模型，同时施加理想的结构约束和结合先验知识。在本文中，我们提出了一个标记，可以系统地生成模型，从FIR的多项式NARMAX模型。此外，我们证明，标签可以容易地扩展到更一般的模型类，诸如非线性箱Jenkins模型类，可实现灵活和自动模型结构和复杂选择经由EA实现。</font>
</div>


<hr>
<div id="paper19"> <b>19. Auto Completion of User Interface Layout Design Using Transformer-Based  Tree Decoders</b>  <a href="https://arxiv.org/pdf/2001.05308" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Amelot%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Amelot</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Zhou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bengio%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Samy Bengio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Si%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Si Si</a><br>
<font size="3">
Abstract: It has been of increasing interest in the field to develop automatic machineries to facilitate the design process. In this paper, we focus on assisting graphical user interface (UI) layout design, a crucial task in app development. Given a partial layout, which a designer has entered, our model learns to complete the layout by predicting the remaining UI elements with a correct position and dimension as well as the hierarchical structures. Such automation will significantly ease the effort of UI designers and developers. While we focus on interface layout prediction, our model can be generally applicable for other layout prediction problems that involve tree structures and 2-dimensional placements. Particularly, we design two versions of Transformer-based tree decoders: Pointer and Recursive Transformer, and experiment with these models on a public dataset. We also propose several metrics for measuring the accuracy of tree prediction and ground these metrics in the domain of user experience. These contribute a new task and methods to deep learning research. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：一直在该领域越来越多的关注，开发自动机器方便的设计过程。在本文中，我们侧重于帮助图形用户界面（UI）布局设计，在应用发展的重要任务。鉴于部分的布局，设计师已经进入，我们的模型学会通过预测与正确的位置和尺寸，其余的UI元素以及分层结构完成全国布局。这样的自动化将显著缓解UI设计师和开发人员的努力。虽然我们专注于界面布局的预测，我们的模型可以普遍适用于涉及树形结构和二维展示位置等布局预报问题。特别是，我们设计了基于变压器的树解码器的两个版本：指针和递归变压器，并与公共数据集，这些模型进行试验。我们还提出几个指标，用于测量树预测的准确性，并在用户体验领域地这些指标。这些贡献了新的任务和方法，深度学习研究。</font>
</div>


<hr>
<div id="paper20"> <b>20. Teddy: A System for Interactive Review Analysis</b>  <a href="https://arxiv.org/pdf/2001.05171" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xiong Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Engel%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jonathan Engel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Evensen%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sara Evensen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yuliang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Demiralp%2C+%C3%87" target="_blank" rel="noopener" style="color:#0000EE;">Çağatay Demiralp</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tan%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wang-Chiew Tan</a><br>
<font size="3">
Abstract: Reviews are integral to e-commerce services and products. They contain a wealth of information about the opinions and experiences of users, which can help better understand consumer decisions and improve user experience with products and services. Today, data scientists analyze reviews by developing rules and models to extract, aggregate, and understand information embedded in the review text. However, working with thousands of reviews, which are typically noisy incomplete text, can be daunting without proper tools. Here we first contribute results from an interview study that we conducted with fifteen data scientists who work with review text, providing insights into their practices and challenges. Results suggest data scientists need interactive systems for many review analysis tasks. In response we introduce Teddy, an interactive system that enables data scientists to quickly obtain insights from reviews and improve their extraction and modeling pipelines. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：评论是不可或缺的电子商务服务和产品。它们包含了大量关于用户的意见和经验，这有助于更好地了解消费者的决策和提高产品和服务的用户体验信息。如今，科学家数据分析通过制定规则和模型来提取，汇总评价，并了解嵌入在审查文本信息。然而，成千上万的评论，这是典型的吵不完整的文本工作，可没有合适的工具望而生畏。在这里，我们首先从访谈研究，我们具有十五数据科学家谁的工作与评论文本进行，提供洞察到他们的做法和挑战作出贡献的结果。结果表明数据科学家需要对很多的评论分析任务的交互系统。在回应我们介绍泰迪，一个互动系统，使数据科学家能够迅速从审查获得洞察力和改善他们的提取和建模管道。</font>
</div>


<hr>
<div id="paper21"> <b>21. Modeling Product Search Relevance in e-Commerce</b>  <a href="https://arxiv.org/pdf/2001.04980" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Iyer%2C+R+R" target="_blank" rel="noopener" style="color:#0000EE;">Rahul Radhakrishnan Iyer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kohli%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rohan Kohli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Prabhumoye%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Shrimai Prabhumoye</a><br>
<font size="3">
Abstract: With the rapid growth of e-Commerce, online product search has emerged as a popular and effective paradigm for customers to find desired products and engage in online shopping. However, there is still a big gap between the products that customers really desire to purchase and relevance of products that are suggested in response to a query from the customer. In this paper, we propose a robust way of predicting relevance scores given a search query and a product, using techniques involving machine learning, natural language processing and information retrieval. We compare conventional information retrieval models such as BM25 and Indri with deep learning models such as word2vec, sentence2vec and paragraph2vec. We share some of our insights and findings from our experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：随着电子商务的快速发展，在线产品搜索已经成为一种流行和有效的模式，为客户找到所需的产品和从事网上购物。然而，仍然有客户真正渴望的产品的购买和相关性被提出以响应来自客户的查询产品之间有很大的差距。在本文中，我们提出了预测给定的搜索查询和产品的相关性分值，使用涉及机器学习，自然语言处理和信息检索技术的可靠方式。我们比较传统的信息检索模型如BM25和大狐猴与深度学习模式，如word2vec，sentence2vec和paragraph2vec。我们分享我们的一些见解和研究结果，从我们的实验。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-15</title>
    <url>/2020/01/16/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-15/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning <a href="https://arxiv.org/pdf/2001.04935" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Balancing the composition of word embeddings across heterogenous data  sets <a href="https://arxiv.org/pdf/2001.04693" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Bi-Decoder Augmented Network for Neural Machine Translation <a href="https://arxiv.org/pdf/2001.04586" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> On the Replicability of Combining Word Embeddings and Retrieval Models <a href="https://arxiv.org/pdf/2001.04484" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Detecting depression in dyadic conversations with multimodal narratives  and visualizations <a href="https://arxiv.org/pdf/2001.04809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> A (Simplified) Supreme Being Necessarily Exists -- Says the Computer! <a href="https://arxiv.org/pdf/2001.04701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Improved Robust ASR for Social Robots in Public Spaces <a href="https://arxiv.org/pdf/2001.04619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Faster Transformer Decoding: N-gram Masked Self-Attention <a href="https://arxiv.org/pdf/2001.04589" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning</b>  <a href="https://arxiv.org/pdf/2001.04935" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Roei Schuster</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Schuster%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tal Schuster</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Meri%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoav Meri</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shmatikov%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vitaly Shmatikov</a><br>
<font size="3">
Abstract: Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word "meaning" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks. Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the "meaning" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another. An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：字的嵌入，即，低维向量表示，如手套和SGNS，编码字在这个意义上，词语向量之间的距离对应于它们的语义接近“意思是”。这使语义的迁移学习的各种自然语言处理任务。 Word中的嵌入通常是受过训练的大型公共语料库，如维基百科或Twitter。我们表明，攻击者谁可以修改其嵌入训练可以通过改变空间嵌入它们的位置控制的新的和现有的词“意义”的语料库。我们开发了语料库的特点，可作为单词之间距离的代理明确的表达，并建立自己的价值观和嵌入的距离之间的因果关系。然后，我们展示了如何使用两个敌对目标的这种关系：（1）做一个字一个字的世界排名第一的邻居，和（2）从一个语义集群移动一个字到另一个。在嵌入的攻击会影响不同的下游任务，这表明首次数据传输学习情境中毒的力量。我们使用这种攻击来操纵信息检索系统，如简历搜索查询扩展，使某些名字命名实体识别模型或多或少可见，并造成新词被翻译成特定的目标词无论使用什么语言。最后，我们展示了攻击者如何产生语言上可能语料库修改，从而欺骗试图难以置信的句子从使用语言模型的语料库过滤防御。</font>
</div>


<hr>
<div id="paper2"> <b>2. Balancing the composition of word embeddings across heterogenous data  sets</b>  <a href="https://arxiv.org/pdf/2001.04693" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Brandl%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stephanie Brandl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lassner%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Lassner</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Alber%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Maximilian Alber</a><br>
<font size="3">
Abstract: Word embeddings capture semantic relationships based on contextual information and are the basis for a wide variety of natural language processing applications. Notably these relationships are solely learned from the data and subsequently the data composition impacts the semantic of embeddings which arguably can lead to biased word vectors. Given qualitatively different data subsets, we aim to align the influence of single subsets on the resulting word vectors, while retaining their quality. In this regard we propose a criteria to measure the shift towards a single data subset and develop approaches to meet both objectives. We find that a weighted average of the two subset embeddings balances the influence of those subsets while word similarity performance decreases. We further propose a promising optimization approach to balance influences and quality of word embeddings. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：基于上下文信息的嵌入Word中捕捉语义关系，并且是各种各样的自然语言处理应用的基础。值得注意的是这些关系仅由数据并随后将数据组合物影响的语义的嵌入可论证可导致偏置字矢量的教训。考虑到质的不同数据子集，我们的目标是一致的最终的字向量单亚群的影响力，同时保持它们的质量。在这方面，我们提出了一个标准来衡量一个单一的数据子集的转变和发展途径，以满足这两个目标。我们发现，两个子集的嵌入的加权平均余额部分数据的影响，而单词类似性能降低。我们进一步提出了一个有前途的优化方法来平衡影响和字的嵌入质量。</font>
</div>


<hr>
<div id="paper3"> <b>3. Bi-Decoder Augmented Network for Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2001.04586" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Pan%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Boyuan Pan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yazheng Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhao%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhou Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhuang%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yueting Zhuang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cai%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Deng Cai</a><br>
<font size="3">
Abstract: Neural Machine Translation (NMT) has become a popular technology in recent years, and the encoder-decoder framework is the mainstream among all the methods. It's obvious that the quality of the semantic representations from encoding is very crucial and can significantly affect the performance of the model. However, existing unidirectional source-to-target architectures may hardly produce a language-independent representation of the text because they rely heavily on the specific relations of the given language pairs. To alleviate this problem, in this paper, we propose a novel Bi-Decoder Augmented Network (BiDAN) for the neural machine translation task. Besides the original decoder which generates the target language sequence, we add an auxiliary decoder to generate back the source language sequence at the training time. Since each decoder transforms the representations of the input text into its corresponding language, jointly training with two target ends can make the shared encoder has the potential to produce a language-independent semantic space. We conduct extensive experiments on several NMT benchmark datasets and the results demonstrate the effectiveness of our proposed approach. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：神经机器翻译（NMT）已经成为一种流行的技术，近年来，和编码器，解码器的结构与第方法中所有的主流。很明显，从编码语义表示的质量是非常重要的，可以显著影响模型的性能。但是，现有的单向源到目标架构可以几乎不产生文本的语言无关的表示，因为他们在很大程度上依赖于特定的语言对的特定关系。为了缓解这一问题，在本文中，我们提出了一个新颖的双解码器增强网络（毕单）的神经机器翻译的任务。除了生成目标语言序列原有解码器，我们添加辅助解码器，以生成回到了训练时间的源语言序列。因为每个解码器将输入的文本的表示成其相应的语言，共同具有两个靶的端部的训练可以使共享编码器具有以产生独立于语言的语义空间的潜力。我们几个NMT基准数据集进行了广泛的实验，结果证明我们提出的方法的有效性。</font>
</div>


<hr>
<div id="paper4"> <b>4. On the Replicability of Combining Word Embeddings and Retrieval Models</b>  <a href="https://arxiv.org/pdf/2001.04484" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Papariello%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Luca Papariello</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bampoulidis%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Alexandros Bampoulidis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lupu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mihai Lupu</a><br>
<font size="3">
Abstract: We replicate recent experiments attempting to demonstrate an attractive hypothesis about the use of the Fisher kernel framework and mixture models for aggregating word embeddings towards document representations and the use of these representations in document classification, clustering, and retrieval. Specifically, the hypothesis was that the use of a mixture model of von Mises-Fisher (VMF) distributions instead of Gaussian distributions would be beneficial because of the focus on cosine distances of both VMF and the vector space model traditionally used in information retrieval. Previous experiments had validated this hypothesis. Our replication was not able to validate it, despite a large parameter scan space. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近期重复实验，试图证明有关使用费的内核架构和混合模型的聚集对文档表示字的嵌入和使用文档分类，聚类和检索这些表象的一个有吸引力的假说。具体而言，假设是使用冯米塞斯-Fisher分析（VMF）的混合物模型的分布，而不是高斯分布将是因为聚焦在两个VMF和信息检索传统上使用向量空间模型的余弦距离的有益的。以前的实验已经证实了这一假设。我们的复制无法验证它，尽管大的参数扫描空间。</font>
</div>


<hr>
<div id="paper5"> <b>5. Detecting depression in dyadic conversations with multimodal narratives  and visualizations</b>  <a href="https://arxiv.org/pdf/2001.04809" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+J+Y" target="_blank" rel="noopener" style="color:#0000EE;">Joshua Y. Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+G+Y" target="_blank" rel="noopener" style="color:#0000EE;">Greyson Y. Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yacef%2C+K" target="_blank" rel="noopener" style="color:#0000EE;">Kalina Yacef</a><br>
<font size="3">
Abstract: Conversations contain a wide spectrum of multimodal information that gives us hints about the emotions and moods of the speaker. In this paper, we developed a system that supports humans to analyze conversations. Our main contribution is the identification of appropriate multimodal features and the integration of such features into verbatim conversation transcripts. We demonstrate the ability of our system to take in a wide range of multimodal information and automatically generated a prediction score for the depression state of the individual. Our experiments showed that this approach yielded better performance than the baseline model. Furthermore, the multimodal narrative approach makes it easy to integrate learnings from other disciplines, such as conversational analysis and psychology. Lastly, this interdisciplinary and automated approach is a step towards emulating how practitioners record the course of treatment as well as emulating how conversational analysts have been analyzing conversations by hand. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对话包含的多模式信息范围广泛，让我们有预兆说话者的情绪和心情。在本文中，我们开发了一个系统，支持人类分析的对话。我们的主要贡献是适当的多模式特征的识别和整合这些功能集成到逐字谈话笔录。我们证明我们的系统采取广泛的多模式信息，并自动生成个人的抑郁状态的预测得分的能力。我们的实验表明，这种方法取得了比基线模型更好的性能。此外，多模式的叙事方法，可以轻松集成到其他学科，如会话分析和心理学的学习收获。最后，这种跨学科的和自动化的方法是对模拟从业者如何记录治疗过程，以及如何模拟对话分析家一直用手分析对话的一个步骤。</font>
</div>


<hr>
<div id="paper6"> <b>6. A (Simplified) Supreme Being Necessarily Exists -- Says the Computer!</b>  <a href="https://arxiv.org/pdf/2001.04701" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Benzm%C3%BCller%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Christoph Benzmüller</a><br>
<font size="3">
Abstract: A simplified variant of Kurt Gödel's modal ontological argument is presented. Some of Gödel's, resp. Scott's, premises are modified, others are dropped, and modal collapse is avoided. The emended argument is shown valid already in quantified modal logic K. The presented simplifications have been computationally explored utilising latest knowledge representation and reasoning technology based on higher-order logic. The paper thus illustrates how modern symbolic AI technology can contribute new knowledge to formal philosophy and theology. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：哥德尔的模式本体论的简化变体显示。有些哥德尔，RESP的。斯科特的，房屋被修改，其他被丢弃，避免了模态崩溃。在仔细的校勘参数显示有效的已量化模态逻辑K.所提出的简化了计算研究利用最新的知识表示和基于高阶逻辑推理技术。因此，阐述了象征性的AI技术如何现代可以促进新知识的正式哲学和神学。</font>
</div>


<hr>
<div id="paper7"> <b>7. Improved Robust ASR for Social Robots in Public Spaces</b>  <a href="https://arxiv.org/pdf/2001.04619" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Jankowski%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Charles Jankowski</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Mruthyunjaya%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vishwas Mruthyunjaya</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Lin%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruixi Lin</a><br>
<font size="3">
Abstract: Social robots deployed in public spaces present a challenging task for ASR because of a variety of factors, including noise SNR of 20 to 5 dB. Existing ASR models perform well for higher SNRs in this range, but degrade considerably with more noise. This work explores methods for providing improved ASR performance in such conditions. We use the AiShell-1 Chinese speech corpus and the Kaldi ASR toolkit for evaluations. We were able to exceed state-of-the-art ASR performance with SNR lower than 20 dB, demonstrating the feasibility of achieving relatively high performing ASR with open-source toolkits and hundreds of hours of training data, which is commonly available. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：部署在公共场所的社交机器人目前由于多种因素的影响，其中包括20至5分贝的噪音信噪比ASR一项艰巨的任务。现有ASR模型表现良好在这个范围内的较高的信噪比，但更多的噪音大大降低。这项工作探索提供在这样的条件下改善ASR性能的方法。我们使用AiShell-1中国语料库和Kaldi ASR工具包的评估。我们能够超过信噪比国家的最先进的ASR性能大于20dB低，表明达到比较高的用开源工具包和数以百计的训练数据，这是通常可以利用的时间来完成ASR的可行性。</font>
</div>


<hr>
<div id="paper8"> <b>8. Faster Transformer Decoding: N-gram Masked Self-Attention</b>  <a href="https://arxiv.org/pdf/2001.04589" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chelba%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Ciprian Chelba</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mia Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bapna%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Ankur Bapna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Noam Shazeer</a><br>
<font size="3">
Abstract: Motivated by the fact that most of the information relevant to the prediction of target tokens is drawn from the source sentence $S=s_1, \ldots, s_S$, we propose truncating the target-side window used for computing self-attention by making an $N$-gram assumption. Experiments on WMT EnDe and EnFr data sets show that the $N$-gram masked self-attention model loses very little in BLEU score for $N$ values in the range $4, \ldots, 8$, depending on the task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：事实上，大多数的相关目标令牌的预测信息从源句子$ S = S_1，\ ldots，S_S $绘制的启发，我们建议截断用于通过计算自我关注的目标侧窗做一个$ N $ -gram假设。在WMT恩德和EnFr数据集上的实验表明，$ N $ -gram掩盖自我注意模型的BLEU分数$ N $值的范围在$ 4 \ ldots，$ 8，根据任务非常小的损失。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-14</title>
    <url>/2020/01/15/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-14/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Multi-Source Domain Adaptation for Text Classification via  DistanceNet-Bandits <a href="https://arxiv.org/pdf/2001.04362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark  for Chinese <a href="https://arxiv.org/pdf/2001.04351" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural  Architecture Search <a href="https://arxiv.org/pdf/2001.04246" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Mining customer product reviews for product development: A summarization  process <a href="https://arxiv.org/pdf/2001.04200" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> Joint Reasoning for Multi-Faceted Commonsense Knowledge <a href="https://arxiv.org/pdf/2001.04170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> ProphetNet: Predicting Future N-gram for Sequence-to-Sequence  Pre-training <a href="https://arxiv.org/pdf/2001.04063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Stochastic Natural Language Generation Using Dependency Information <a href="https://arxiv.org/pdf/2001.03897" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Rethinking Generalization of Neural Models: A Named Entity Recognition  Case Study <a href="https://arxiv.org/pdf/2001.03844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Revisiting Challenges in Data-to-Text Generation with Fact Grounding <a href="https://arxiv.org/pdf/2001.03830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
<div id="title10">
<b>10.</b> Learning Cross-Context Entity Representations from Text <a href="https://arxiv.org/pdf/2001.03765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper10" style="color:#0000EE;">摘要</a><br></div>
<div id="title11">
<b>11.</b> PatentTransformer-2: Controlling Patent Text Generation by Structural  Metadata <a href="https://arxiv.org/pdf/2001.03708" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper11" style="color:#0000EE;">摘要</a><br></div>
<div id="title12">
<b>12.</b> Does syntax need to grow on trees? Sources of hierarchical inductive  bias in sequence-to-sequence networks <a href="https://arxiv.org/pdf/2001.03632" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper12" style="color:#0000EE;">摘要</a><br></div>
<div id="title13">
<b>13.</b> Reformer: The Efficient Transformer <a href="https://arxiv.org/pdf/2001.04451" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper13" style="color:#0000EE;">摘要</a><br></div>
<div id="title14">
<b>14.</b> LP-SparseMAP: Differentiable Relaxed Optimization for Sparse Structured  Prediction <a href="https://arxiv.org/pdf/2001.04437" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper14" style="color:#0000EE;">摘要</a><br></div>
<div id="title15">
<b>15.</b> Negative Statements Considered Useful <a href="https://arxiv.org/pdf/2001.04425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper15" style="color:#0000EE;">摘要</a><br></div>
<div id="title16">
<b>16.</b> Asymmetrical Hierarchical Networks with Attentive Interactions for  Interpretable Review-Based Recommendation <a href="https://arxiv.org/pdf/2001.04346" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper16" style="color:#0000EE;">摘要</a><br></div>
<div id="title17">
<b>17.</b> Shareable Representations for Search Query Understanding <a href="https://arxiv.org/pdf/2001.04345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper17" style="color:#0000EE;">摘要</a><br></div>
<div id="title18">
<b>18.</b> Improving Dysarthric Speech Intelligibility Using Cycle-consistent  Adversarial Training <a href="https://arxiv.org/pdf/2001.04260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper18" style="color:#0000EE;">摘要</a><br></div>
<div id="title19">
<b>19.</b> Structural Decompositions of Epistemic Logic Programs <a href="https://arxiv.org/pdf/2001.04219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper19" style="color:#0000EE;">摘要</a><br></div>
<div id="title20">
<b>20.</b> A logic-based relational learning approach to relation extraction: The  OntoILPER system <a href="https://arxiv.org/pdf/2001.04192" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper20" style="color:#0000EE;">摘要</a><br></div>
<div id="title21">
<b>21.</b> Retouchdown: Adding Touchdown to StreetLearn as a Shareable Resource for  Language Grounding Tasks in Street View <a href="https://arxiv.org/pdf/2001.03671" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper21" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Multi-Source Domain Adaptation for Text Classification via  DistanceNet-Bandits</b>  <a href="https://arxiv.org/pdf/2001.04362" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Guo%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Han Guo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Pasunuru%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ramakanth Pasunuru</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bansal%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mohit Bansal</a><br>
<font size="3">
Abstract: Domain adaptation performance of a learning algorithm on a target domain is a function of its source domain error and a divergence measure between the data distribution of these two domains. We present a study of various distance-based measures in the context of NLP tasks, that characterize the dissimilarity between domains based on sample estimates. We first conduct analysis experiments to show which of these distance measures can best differentiate samples from same versus different domains, and are correlated with empirical results. Next, we develop a DistanceNet model which uses these distance measures, or a mixture of these distance measures, as an additional loss function to be minimized jointly with the task's loss function, so as to achieve better unsupervised domain adaptation. Finally, we extend this model to a novel DistanceNet-Bandit model, which employs a multi-armed bandit controller to dynamically switch between multiple source domains and allow the model to learn an optimal trajectory and mixture of domains for transfer to the low-resource target domain. We conduct experiments on popular sentiment analysis datasets with several diverse domains and show that our DistanceNet model, as well as its dynamic bandit variant, can outperform competitive baselines in the context of unsupervised domain adaptation. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：对目标域学习算法的域自适应性能是它的源域误差的函数和这两个结构域的数据分布之间的偏差度量。我们提出的在NLP任务范围内各种基于距离的测量，表征根据样本估计域间的差异性进行了研究。我们首先进行分析实验表明其中的这些距离措施最好的分化样本相同与不同的域，并与实验结果是相关的。接下来，我们开发出使用这些距离的措施，或者这些距离测量的混合物DistanceNet模型，作为额外的损失函数要与任务的损失函数共同最小化，从而达到更好的无监督的领域适应性。最后，我们扩展该模型以一种新颖的DistanceNet匪模型，其采用多臂老虎控制器到多个源域之间动态开关和允许模型学习域的最佳轨迹和混合物，然后转移到低资源目标域。我们进行了对流行的情感分析数据集实验与多个不同领域，并表明我们的模型DistanceNet，以及它的动态强盗变种，可以在无人监督的领域适应性的背景下跑赢大市的竞争基准。</font>
</div>


<hr>
<div id="paper2"> <b>2. CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark  for Chinese</b>  <a href="https://arxiv.org/pdf/2001.04351" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Liang Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qianqian Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+C" target="_blank" rel="noopener" style="color:#0000EE;">Cong Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yin Tian</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weitang Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L" target="_blank" rel="noopener" style="color:#0000EE;">Lu Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuanwei Zhang</a><br>
<font size="3">
Abstract: In this paper, we introduce the NER dataset from CLUE organization (CLUENER2020), a well-defined fine-grained dataset for named entity recognition in Chinese. CLUENER2020 contains 10 categories. Apart from common labels like person, organization, and location, it contains more diverse categories. It is more challenging than current other Chinese NER datasets and could better reflect real-world applications. For comparison, we implement several state-of-the-art baselines as sequence labeling tasks and report human performance, as well as its analysis. To facilitate future work on fine-grained NER for Chinese, we release our dataset, baselines, and leader-board. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们将介绍从CLUE组织NER数据集（CLUENER2020），一个明确的细粒度数据集在中国命名实体识别。 CLUENER2020包含10个类别。除了像个人，组织和位置共同的标签，它包含了更多样化的类别。它比目前的其他中国NER数据集更具挑战性，更能反映现实世界的应用。为了便于比较，我们实现国家的最先进的一些基线为序列标注任务和报告人的表现，以及它的分析。为了方便日后对中国细粒度NER的工作，我们发布的数据集，基线和领袖板。</font>
</div>


<hr>
<div id="paper3"> <b>3. AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural  Architecture Search</b>  <a href="https://arxiv.org/pdf/2001.04246" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Daoyuan Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yaliang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qiu%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minghui Qiu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhen Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bofang Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ding%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bolin Ding</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongbo Deng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jun Huang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingren Zhou</a><br>
<font size="3">
Abstract: Large pre-trained language models such as BERT have shown their effectiveness in various natural language processing tasks. However, the huge parameter size makes them difficult to be deployed in real-time applications that require quick inference with limited resources. Existing methods compress BERT into small models while such compression is task-independent, i.e., the same compressed BERT for all different downstream tasks. Motivated by the necessity and benefits of task-oriented BERT compression, we propose a novel compression method, AdaBERT, that leverages differentiable Neural Architecture Search to automatically compress BERT into task-adaptive small models for specific tasks. We incorporate a task-oriented knowledge distillation loss to provide search hints and an efficiency-aware loss as search constraints, which enables a good trade-off between efficiency and effectiveness for task-adaptive BERT compression. We evaluate AdaBERT on several NLP tasks, and the results demonstrate that those task-adaptive compressed models are 12.7x to 29.3x faster than BERT in inference time and 11.5x to 17.0x smaller in terms of parameter size, while comparable performance is maintained. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型预训练的语言模型，如BERT表明它们在不同的自然语言处理任务的有效性。然而，巨大的参数尺寸使得它们很难在需要快速推断资源有限的实时应用进行部署。现有的方法压缩BERT为小机型，而这种压缩是任务无关，即对所有不同的下游任务相同的压缩BERT。由必要性和面向任务的BERT压缩的好处的启发，我们提出了一种新的压缩方法，AdaBERT，它利用微神经结构的搜索自动压缩成BERT任务自适应小型号为特定的任务。我们结合了面向任务的知识蒸馏损失提供搜索提示和效率意识的损失，搜索约束，这使得任务自适应BERT压缩一个很好的权衡效率和效益之间。我们评估几个NLP任务AdaBERT，结果表明，这些任务自适应压缩模型12.7倍至29.3x比推理时间和11.5倍BERT更快17.0x参数规模而言较小，而相当的性能得以维持。</font>
</div>


<hr>
<div id="paper4"> <b>4. Mining customer product reviews for product development: A summarization  process</b>  <a href="https://arxiv.org/pdf/2001.04200" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hou%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tianjun Hou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yannou%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernard Yannou</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Leroy%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yann Leroy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Poirson%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Emilie Poirson</a><br>
<font size="3">
Abstract: This research set out to identify and structure from online reviews the words and expressions related to customers' likes and dislikes to guide product development. Previous methods were mainly focused on product features. However, reviewers express their preference not only on product features. In this paper, based on an extensive literature review in design science, the authors propose a summarization model containing multiples aspects of user preference, such as product affordances, emotions, usage conditions. Meanwhile, the linguistic patterns describing these aspects of preference are discovered and drafted as annotation guidelines. A case study demonstrates that with the proposed model and the annotation guidelines, human annotators can structure the online reviews with high inter-agreement. As high inter-agreement human annotation results are essential for automatizing the online review summarization process with the natural language processing, this study provides materials for the future study of automatization. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本研究着手从网上评论识别和结构关系到客户的好恶词语来指导产品的开发。先前的方法主要集中在产品功能。然而，评论家表达自己的喜好，不仅在产品功能。在本文的基础上，设计科学的全面的文献，作​​者提出了一个包含用户偏好的倍数方面，如产品的可供性，情绪，利用状况的总结模式。同时，描述偏好这些方面的语言模式被发现并起草作为注解的指导方针。案例研究表明，与所提出的模型和注释指引，人工注释就可以构建高之间的协议网上审查。由于采用协议间的人类标注的结果是与自然语言处理automatizing在线审核汇总过程中必不可少的，这项研究提供了自动化的未来学习材料。</font>
</div>


<hr>
<div id="paper5"> <b>5. Joint Reasoning for Multi-Faceted Commonsense Knowledge</b>  <a href="https://arxiv.org/pdf/2001.04170" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Chalier%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yohan Chalier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Razniewski%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Razniewski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weikum%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerhard Weikum</a><br>
<font size="3">
Abstract: Commonsense knowledge (CSK) supports a variety of AI applications, from visual understanding to chatbots. Prior works on acquiring CSK, such as ConceptNet, have compiled statements that associate concepts, like everyday objects or activities, with properties that hold for most or some instances of the concept. Each concept is treated in isolation from other concepts, and the only quantitative measure (or ranking) of properties is a confidence score that the statement is valid. This paper aims to overcome these limitations by introducing a multi-faceted model of CSK statements and methods for joint reasoning over sets of inter-related statements. Our model captures four different dimensions of CSK statements: plausibility, typicality, remarkability and salience, with scoring and ranking along each dimension. For example, hyenas drinking water is typical but not salient, whereas hyenas eating carcasses is salient. For reasoning and ranking, we develop a method with soft constraints, to couple the inference over concepts that are related in in a taxonomic hierarchy. The reasoning is cast into an integer linear programming (ILP), and we leverage the theory of reduction costs of a relaxed LP to compute informative rankings. This methodology is applied to several large CSK collections. Our evaluation shows that we can consolidate these inputs into much cleaner and more expressive knowledge. Results are available at this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：常识知识（CSK）支持多种AI应用，从视觉理解聊天机器人。在获取CSK之前的作品，如ConceptNet，编译语句关联的概念，像日常生活中的物体或活动，性质搁置了大部分或概念的若干实例。每个概念隔离治疗与其他概念和属性的唯一定量测量（或排序）是置信得分的声明是有效的。本文旨在通过引入CSK语句和方法的多面模型在台相互关联的语句联合推理来克服这些限制。我们的模型捕获CSK报表的四个维度：合理性，典型性，remarkability和显着性，与得分和沿每个维度的排名。例如，鬣狗饮用水是典型的但不显着，而鬣狗吃尸体是显着的。推理和排名，我们开发了一个方法与软约束，耦合，而忽视了在一个分类层次结构相关的概念推理。推理铸造成整数线性规划（ILP），和我们利用的轻松LP的降低成本的理论来计算信息排名。这种方法适用于几个大的CSK集合。我们的评估显示，我们可以整合这些投入更清洁，更富有表现力的知识。结果可在此HTTPS URL。</font>
</div>


<hr>
<div id="paper6"> <b>6. ProphetNet: Predicting Future N-gram for Sequence-to-Sequence  Pre-training</b>  <a href="https://arxiv.org/pdf/2001.04063" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yu Yan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Qi%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Weizhen Qi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Gong%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yeyun Gong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dayiheng Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Duan%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nan Duan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jiusheng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Ruofei Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhou%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Ming Zhou</a><br>
<font size="3">
Abstract: In this paper, we present a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism.Instead of the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time step.The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale dataset (160GB) respectively. Experimental results show ProphetNet achieves the best performance on both abstractive summarization and question generation tasks compared to the models using the same base scale pre-training dataset. For the large scale dataset pre-training, ProphetNet achieves new state-of-the-art results on Gigaword and comparable results on CNN/DailyMail using only about 1/5 pre-training epochs of the previous model. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：在本文中，我们提出名为ProphetNet一个新的序列到序列前的训练模式，它引入了一个新的自我监督的目标命名为将来的n-gram预测和建议的N流的自我关注的mechanism.Instead在传统的序列到序列模型中的一个步骤的提前预测的优化，ProphetNet由n-领先一步预测该预测下一个n各自时间step.The将来的n-gram在预测令牌同时基于先前上下文令牌明确地优化鼓励模型来规划未来的令牌，并防止过度拟合强大的本地相关性。我们使用碱规模的数据集（16GB）和分别大规模数据集（160GB）预列车ProphetNet。实验结果表明ProphetNet达到上相比，使用相同的基本预分训练数据集模型既抽象总结和询问生成任务的最佳性能。对于大规模数据集前培训，ProphetNet实现国家的最先进的新的Gigaword和使用CNN /每日邮报以前的型号只有约1/5前的训练时期比较的结果的结果。</font>
</div>


<hr>
<div id="paper7"> <b>7. Stochastic Natural Language Generation Using Dependency Information</b>  <a href="https://arxiv.org/pdf/2001.03897" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Seifossadat%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elham Seifossadat</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Sameti%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hossein Sameti</a><br>
<font size="3">
Abstract: This article presents a stochastic corpus-based model for generating natural language text. Our model first encodes dependency relations from training data through a feature set, then concatenates these features to produce a new dependency tree for a given meaning representation, and finally generates a natural language utterance from the produced dependency tree. We test our model on nine domains from tabular, dialogue act and RDF format. Our model outperforms the corpus-based state-of-the-art methods trained on tabular datasets and also achieves comparable results with neural network-based approaches trained on dialogue act, E2E and WebNLG datasets for BLEU and ERR evaluation metrics. Also, by reporting Human Evaluation results, we show that our model produces high-quality utterances in aspects of informativeness and naturalness as well as quality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文介绍了生成自然语言文本随机基于语料库的模型。我们的模式首先编码的依赖性和通过功能训练数据集的关系，然后连接这些特征来产生一个给定的意思表示一个新的依赖关系树，最后产生从产生依赖关系树的自然语言语句。我们测试我们从表格，对话行为和RDF格式9个域模型。我们的模型优于训练有素的表格数据集基于语料库的国家的最先进的方法和也实现了比较的结果神经网络的基础上对话行为，E2E和WebNLG数据集的BLEU和ERR评价指标办法训练。此外，通过报告人的评价结果​​，我们表明，我们的模型在信息量和自然，以及质量方面的生产高品质的话语。</font>
</div>


<hr>
<div id="paper8"> <b>8. Rethinking Generalization of Neural Models: A Named Entity Recognition  Case Study</b>  <a href="https://arxiv.org/pdf/2001.03844" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Fu%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jinlan Fu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Pengfei Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+Q" target="_blank" rel="noopener" style="color:#0000EE;">Qi Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xuanjing Huang</a><br>
<font size="3">
Abstract: While neural network-based models have achieved impressive performance on a large body of NLP tasks, the generalization behavior of different models remains poorly understood: Does this excellent performance imply a perfect generalization model, or are there still some limitations? In this paper, we take the NER task as a testbed to analyze the generalization behavior of existing models from different perspectives and characterize the differences of their generalization abilities through the lens of our proposed measures, which guides us to better design models and training methods. Experiments with in-depth analyses diagnose the bottleneck of existing neural NER models in terms of breakdown performance analysis, annotation errors, dataset bias, and category relationships, which suggest directions for improvement. We have released the datasets: (ReCoNLL, PLONER) for the future research at our project page: this http URL. As a by-product of this paper, we have open-sourced a project that involves a comprehensive summary of recent NER papers and classifies them into different research topics: this https URL. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：尽管基于神经网络的模型已在大机构的NLP任务，取得了骄人的业绩，不同型号的遗体推广行为知之甚少：这是否出色表现意味着一个完美的泛化模型，还是有仍有一定的局限性？在本文中，我们采取了NER任务作为测试平台，分析从不同的角度现有车型的推广行为，并通过我们的建议措施的镜头，是指导我们更好地设计模型和训练方法表征其泛化能力的差异。在深入分析实验诊断现有的神经NER模型的瓶颈在击穿性能分析，标注错误，数据集偏见和类别的关系，其提出改进方向的术语。我们已经发布了数据集：（ReCoNLL，PLONER）对未来的研究，在我们的项目页面：这个HTTP URL。作为本文的副产品，我们有开源的，涉及到的最近NER文件，并将其分类，全面总结成不同的研究课题项目：该HTTPS URL。</font>
</div>


<hr>
<div id="paper9"> <b>9. Revisiting Challenges in Data-to-Text Generation with Fact Grounding</b>  <a href="https://arxiv.org/pdf/2001.03830" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hongmin Wang</a><br>
<font size="3">
Abstract: Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, hoping to attract more research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：数据到文本代车型面临参照正确的输入源，确保数据的保真度的挑战。在这方面，怀斯曼等激励研究。 （2017）介绍了从箱 - 和线路得分表中生成的NBA比赛的摘要语料库RotoWire。然而，有限的尝试已在这方面取得和挑战依然存在。我们观察到，其中的总结内容只有约60％可以接地的技术统计记录的语料库一个突出的瓶颈。这样的信息不足往往误导了条件语言模型制作无条件随机的事实，从而导致实际的幻觉。在这项工作中，我们恢复信息平衡和改造这个任务专注于事实接地数据到文本生成。我们引进一个纯化和大规模数据集，RotoWire-FG（实况接地），从今年2017年覆盖和丰富的输入表50％以上的数据，希望能吸引更多的研究集中在这个方向。此外，我们通过表重建的一种新形式的积分作为辅助任务，以提高生成质量实现对国家的最先进的模型改进的数据的保真度。</font>
</div>


<hr>
<div id="paper10"> <b>10. Learning Cross-Context Entity Representations from Text</b>  <a href="https://arxiv.org/pdf/2001.03765" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title10" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Ling%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jeffrey Ling</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=FitzGerald%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nicholas FitzGerald</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shan%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zifei Shan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Soares%2C+L+B" target="_blank" rel="noopener" style="color:#0000EE;">Livio Baldini Soares</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=F%C3%A9vry%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Thibault Févry</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weiss%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">David Weiss</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kwiatkowski%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tom Kwiatkowski</a><br>
<font size="3">
Abstract: Language modeling tasks, in which words, or word-pieces, are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases. Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases or human readable encyclopedias tend to be entity-centric, we investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the text contexts in which those entities were mentioned. We show that large scale training of neural models allows us to learn high quality entity representations, and we demonstrate successful results on four domains: (1) existing entity-level typing benchmarks, including a 64% error reduction over previous work on TypeNet (Murty et al., 2018); (2) a novel few-shot category reconstruction task; (3) existing entity linking benchmarks, where we match the state-of-the-art on CoNLL-Aida without linking-specific features and obtain a score of 89.8% on TAC-KBP 2010 without using any alias table, external knowledge base or in domain training data and (4) answering trivia questions, which uniquely identify entities. Our global entity representations encode fine-grained type categories, such as Scottish footballers, and can answer trivia questions such as: Who was the last inmate of Spandau jail in Berlin? </font>
<br>
<font size="2" style="line-height:30px;">
摘要：语言建模任务，其中词或字块，在本地范围内的基础上预测，一直是学习的嵌入词和短语的背景有关的表示是非常有效的。通过观察该努力的代码世界知识转化为机器可读的知识基础或人类可读的百科全书往往是实体为中心的推动下，我们研究使用填充式的空白任务的学习实体的情况下独立表示从文本在这些实体中提到的上下文。我们展示的神经模型的规模大的培训，让我们了解高品质实体交涉，我们证明在四个主要领域成功的结果：（1）现有的实体层面打字的基准，其中包括64％的误差减少了以前的工作在键入net（穆尔蒂。等人，2018）; （2）一种新的几拍重建类别任务; （3）现有的实体连接的基准，在那里我们匹配状态的最先进的上CoNLL-阿依无需关联的特定功能，将获得于2010年05 TAC-KBP得分为89.8％，而无需使用任何别名表，外部知识库或在域训练数据和（4）回答琐事问题，唯一标识实体。我们的全球实体表示编码细粒度类型类别，如苏格兰足球运动员，并且可以回答小问题，如：谁是施潘道监狱在柏林的最后一个犯人？</font>
</div>


<hr>
<div id="paper11"> <b>11. PatentTransformer-2: Controlling Patent Text Generation by Structural  Metadata</b>  <a href="https://arxiv.org/pdf/2001.03708" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title11" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jieh-Sheng Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hsiang%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jieh Hsiang</a><br>
<font size="3">
Abstract: PatentTransformer is our codename for patent text generation based on Transformer-based models. Our goal is "Augmented Inventing." In this second version, we leverage more of the structural metadata in patents. The structural metadata includes patent title, abstract, and dependent claim, in addition to independent claim previously. Metadata controls what kind of patent text for the model to generate. Also, we leverage the relation between metadata to build a text-to-text generation flow, for example, from a few words to a title, the title to an abstract, the abstract to an independent claim, and the independent claim to multiple dependent claims. The text flow can go backward because the relation is trained bidirectionally. We release our GPT-2 models trained from scratch and our code for inference so that readers can verify and generate patent text on their own. As for generation quality, we measure it by both ROUGE and Google Universal Sentence Encoder. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：PatentTransformer是我们基于基于变压器的新型专利文本生成代号。我们的目标是“增强发明了。”在第二个版本中，我们利用更多的结构性元数据的专利。结构元数据包括专利标题，摘要，以及从属权利要求中，除了独立权利要求先前。元数据控制什么样的专利文本为模型来生成。此外，我们利用的元数据之间的关系，以建立一个文本到文本生成流，例如，从几话标题，标题为抽象，抽象到一个独立的权利要求，以及在独立权利要求到多个从属索赔。因为关系是双向训练文本流可以去落后。我们发布我们从头开始训练的GPT-2机型和我们推断代码，使读者可以验证并产生自己的专利文本。至于代的品质，我们双方ROUGE和谷歌万能句子编码器测量。</font>
</div>


<hr>
<div id="paper12"> <b>12. Does syntax need to grow on trees? Sources of hierarchical inductive  bias in sequence-to-sequence networks</b>  <a href="https://arxiv.org/pdf/2001.03632" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title12" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=McCoy%2C+R+T" target="_blank" rel="noopener" style="color:#0000EE;">R. Thomas McCoy</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Frank%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robert Frank</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Linzen%2C+T" target="_blank" rel="noopener" style="color:#0000EE;">Tal Linzen</a><br>
<font size="3">
Abstract: Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：暴露在同样的训练数据学习者可以概括不同，由于不同的感性偏见。在神经网络模型，感性的偏见在理论上可以从模型架构的任何方面引起的。我们调查其建筑因素影响训练的两个句法任务，英语问题的形成和英语时态reinflection神经序列到序列模型的推广行为。对于这两个任务，训练集是基于层次结构和基于线性顺序的推广泛化一致。所有的建筑因素，我们调查定性的影响模型如何推广，其中包括没有明确的连接层次结构的因素。例如，LSTMs越冬和显示本质上不同的感应偏压。然而，持续推动整个任务的分层偏见的唯一因素是使用一个树形结构的模型，而不是连续的复发模型，这表明类似人类的语法概括要求的建筑句法结构。</font>
</div>


<hr>
<div id="paper13"> <b>13. Reformer: The Efficient Transformer</b>  <a href="https://arxiv.org/pdf/2001.04451" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title13" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kitaev%2C+N" target="_blank" rel="noopener" style="color:#0000EE;">Nikita Kitaev</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kaiser%2C+%C5%81" target="_blank" rel="noopener" style="color:#0000EE;">Łukasz Kaiser</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Levskaya%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Anselm Levskaya</a><br>
<font size="3">
Abstract: Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：大型变压器模型通常实现多项任务的国家的最先进的成果，但训练这些模型可能极其昂贵的，特别是在长序列。我们介绍了两种技术来提高变压器的效率。首先，我们通过一个使用局部性敏感散列，从O（$ L ^ 2 $）至O（$ L \材L $），其中$ L $是的长度改变其复杂性替代点积关注顺序。此外，我们使用可逆残渣层而不是标准的残差，其允许在训练过程中，而不是$ N $倍，其中$ N $是层的数目仅一次存储激活。将得到的模型，重整器，而被更内存效率和长序列快得多与Transformer模型看齐进行。</font>
</div>


<hr>
<div id="paper14"> <b>14. LP-SparseMAP: Differentiable Relaxed Optimization for Sparse Structured  Prediction</b>  <a href="https://arxiv.org/pdf/2001.04437" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title14" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Niculae%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Vlad Niculae</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Martins%2C+A+F+T" target="_blank" rel="noopener" style="color:#0000EE;">André F. T. Martins</a><br>
<font size="3">
Abstract: Structured prediction requires manipulating a large number of combinatorial structures, e.g., dependency trees or alignments, either as latent or output variables. Recently, the SparseMAP method has been proposed as a differentiable, sparse alternative to maximum a posteriori (MAP) and marginal inference. SparseMAP returns a combination of a small number of structures, a desirable property in some downstream applications. However, SparseMAP requires a tractable MAP inference oracle. This excludes, e.g., loopy graphical models or factor graphs with logic constraints, which generally require approximate inference. In this paper, we introduce LP-SparseMAP, an extension of SparseMAP that addresses this limitation via a local polytope relaxation. LP-SparseMAP uses the flexible and powerful domain specific language of factor graphs for defining and backpropagating through arbitrary hidden structure, supporting coarse decompositions, hard logic constraints, and higher-order correlations. We derive the forward and backward algorithms needed for using LP-SparseMAP as a hidden or output layer. Experiments in three structured prediction tasks show benefits compared to SparseMAP and Structured SVM. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：结构化预测需要操纵大量组合结构，例如，依赖树木或比对，无论是作为潜在的或输出变量。最近，SparseMAP方法已经被提出作为一个微的，稀疏替代最大后验（MAP）和边际推理。 SparseMAP返回少量的结构，在一些下游应用的期望特性的组合。然而，SparseMAP需要一个听话的地图推断预言。这不包括，例如，多圈图形模型或因子图与逻辑约束，这通常需要近似推断。在本文中，我们介绍了LP-SparseMAP，SparseMAP的扩展，地址通过本地多面体放松这一限制。 LP-SparseMAP使用用于定义和通过任意隐藏结构backpropagating，支撑粗分解，硬逻辑约束，和更高阶的相关性因子图的灵活和强大的域专用语言。我们推导需要使用LP-SparseMAP作为隐藏或输出层中的向前和向后的算法。在三个结构预测任务实验表明相比SparseMAP和结构化SVM的好处。</font>
</div>


<hr>
<div id="paper15"> <b>15. Negative Statements Considered Useful</b>  <a href="https://arxiv.org/pdf/2001.04425" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title15" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Arnaout%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hiba Arnaout</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Razniewski%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Simon Razniewski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Weikum%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerhard Weikum</a><br>
<font size="3">
Abstract: Knowledge bases (KBs), pragmatic collections of knowledge about notable entities, are an important asset in applications such as search, question answering and dialogue. Rooted in a long tradition in knowledge representation, all popular KBs only store positive information, while they abstain from taking any stance towards statements not contained in them. In this paper, we make the case for explicitly stating interesting statements which are not true. Negative statements would be important to overcome current limitations of question answering, yet due to their potential abundance, any effort towards compiling them needs a tight coupling with ranking. We introduce two approaches towards compiling negative statements. (i) In peer-based statistical inferences, we compare entities with highly related entities in order to derive potential negative statements, which we then rank using supervised and unsupervised features. (ii) In query-log-based text extraction, we use a pattern-based approach for harvesting search engine query logs. Experimental results show that both approaches hold promising and complementary potential. Along with this paper, we publish the first datasets on interesting negative information, containing over 1.1M statements for 100K popular Wikidata entities. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：知识库（KBS），约著名的实体知识务实的集合，是在应用程序，如搜索，问答和对话的重要资产。在知识表示有着悠久的传统根深蒂固，所有流行的知识库系统只保存正面信息，而他们从迈出不包含在他们陈述的任何立场弃权。在本文中，我们做出明确说明有趣的声明不属实的情况。克服答疑的电流限制否定陈述将是重要的，但由于其潜在的丰富，对编译他们的任何努力，需要与排名的紧密耦合。我们引入对编译否定陈述两种方法。 （一）在对等的统计推断，我们比较具有高度相关实体的实体，以得出潜在的负面陈述，然后我们使用级监督和无监督的功能。 （二）在查询日志基于文本的提取，我们用收获的搜索引擎查询日志基于模式的方法。实验结果表明，这两种方法保持承诺和互补的潜力。除了本文中，我们公布有趣的负面信息的第一数据集，包含100K流行的维基数据实体超过1.1M的语句。</font>
</div>


<hr>
<div id="paper16"> <b>16. Asymmetrical Hierarchical Networks with Attentive Interactions for  Interpretable Review-Based Recommendation</b>  <a href="https://arxiv.org/pdf/2001.04346" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title16" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dong%2C+X" target="_blank" rel="noopener" style="color:#0000EE;">Xin Dong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ni%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jingchao Ni</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Wei Cheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Z" target="_blank" rel="noopener" style="color:#0000EE;">Zhengzhang Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Zong%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bo Zong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Song%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dongjin Song</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yanchi Liu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Haifeng Chen</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=de+Melo%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gerard de Melo</a><br>
<font size="3">
Abstract: Recently, recommender systems have been able to emit substantially improved recommendations by leveraging user-provided reviews. Existing methods typically merge all reviews of a given user or item into a long document, and then process user and item documents in the same manner. In practice, however, these two sets of reviews are notably different: users' reviews reflect a variety of items that they have bought and are hence very heterogeneous in their topics, while an item's reviews pertain only to that single item and are thus topically homogeneous. In this work, we develop a novel neural network model that properly accounts for this important difference by means of asymmetric attentive modules. The user module learns to attend to only those signals that are relevant with respect to the target item, whereas the item module learns to extract the most salient contents with regard to properties of the item. Our multi-hierarchical paradigm accounts for the fact that neither are all reviews equally useful, nor are all sentences within each review equally pertinent. Extensive experimental results on a variety of real datasets demonstrate the effectiveness of our method. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：近日，推荐系统已经能够通过利用用户提供的评论发出显着改善的建议。现有的方法通常合并给定用户或项目的所有评价为长的文档，然后处理以同样的方式用户和项目的文件。然而在实践中，这两组的评论是显着不同：用户的评价反映的各种物品，他们已经买了，并因此在其主题非常庞杂，而项目的审查只涉及到单个项目，因此是局部均匀。在这项工作中，我们开发了妥善占不对称周到模块的方式这一重要区别一个新的神经网络模型。用户模块学会照顾只有那些相关的相对于目标项目的信号，而项目模块学会了关于该项目的属性提取最突出的内容。我们的多层次模式考虑的事实是，无论是全部评论同样有用，也不是每个评论中的所有语句同样相关。在各种真实数据集的大量实验结果证明了该方法的有效性。</font>
</div>


<hr>
<div id="paper17"> <b>17. Shareable Representations for Search Query Understanding</b>  <a href="https://arxiv.org/pdf/2001.04345" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title17" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mukul Kumar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hu%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Youna Hu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Headden%2C+W" target="_blank" rel="noopener" style="color:#0000EE;">Will Headden</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Goutam%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rahul Goutam</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Lin%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Heran Lin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bing Yin</a><br>
<font size="3">
Abstract: Understanding search queries is critical for shopping search engines to deliver a satisfying customer experience. Popular shopping search engines receive billions of unique queries yearly, each of which can depict any of hundreds of user preferences or intents. In order to get the right results to customers it must be known queries like "inexpensive prom dresses" are intended to not only surface results of a certain product type but also products with a low price. Referred to as query intents, examples also include preferences for author, brand, age group, or simply a need for customer service. Recent works such as BERT have demonstrated the success of a large transformer encoder architecture with language model pre-training on a variety of NLP tasks. We adapt such an architecture to learn intents for search queries and describe methods to account for the noisiness and sparseness of search query data. We also describe cost effective ways of hosting transformer encoder models in context with low latency requirements. With the right domain-specific training we can build a shareable deep learning model whose internal representation can be reused for a variety of query understanding tasks including query intent identification. Model sharing allows for fewer large models needed to be served at inference time and provides a platform to quickly build and roll out new search query classifiers. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：了解搜索查询是购物搜索引擎提供一个满意的客户体验至关重要。流行的购物搜索引擎获得数十亿年唯一的查询，每一个都可以描绘出任何数百个用户的偏好或意图的。为了得到正确的结果，必须知道像“便宜的舞会礼服”旨在不仅是某个产品类型的表面效果，也具有价格低的产品查询客户。称为查询意图，例子还包括作者，品牌，年龄组或只是需要为客户服务的偏好。最近的作品如BERT都展现了大型变压器编码器架构，拥有对各种NLP任务语言模型前培训的成功。我们采用这样的架构，以学习为搜索查询意图和描述的是占搜索查询数据的吵闹和稀疏。我们还描述在低延迟要求的背景下举办的变压器编码器模型的经济有效的方式。有了正确的特定领域的培训，我们可以建立其内部表示可以为多种查询理解任务，包括查询意图识别重复使用一个共享的深度学习模式。模型共享允许在需要推理时间送达较少的大型模型，并提供了一个平台快速构建并推出新的搜索查询的分类。</font>
</div>


<hr>
<div id="paper18"> <b>18. Improving Dysarthric Speech Intelligibility Using Cycle-consistent  Adversarial Training</b>  <a href="https://arxiv.org/pdf/2001.04260" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title18" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/eess?searchtype=author&query=Yang%2C+S+H" target="_blank" rel="noopener" style="color:#0000EE;">Seung Hee Yang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&query=Chung%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Minhwa Chung</a><br>
<font size="3">
Abstract: Dysarthria is a motor speech impairment affecting millions of people. Dysarthric speech can be far less intelligible than those of non-dysarthric speakers, causing significant communication difficulties. The goal of our work is to develop a model for dysarthric to healthy speech conversion using Cycle-consistent GAN. Using 18,700 dysarthric and 8,610 healthy control Korean utterances that were recorded for the purpose of automatic recognition of voice keyboard in a previous study, the generator is trained to transform dysarthric to healthy speech in the spectral domain, which is then converted back to speech. Objective evaluation using automatic speech recognition of the generated utterance on a held-out test set shows that the recognition performance is improved compared with the original dysarthic speech after performing adversarial training, as the absolute WER has been lowered by 33.4%. It demonstrates that the proposed GAN-based conversion method is useful for improving dysarthric speech intelligibility. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：构音障碍是影响数百万人的电机语言障碍。构音障碍的言语可以比那些非构音障碍的扬声器远不如理解，造成显著沟通困难。我们工作的目标是开发用于构音障碍的使用周期一致甘健康语音转换模型。使用18700构音障碍，并且记录在先前的研究中自动识别语音键盘的目的8,610健康控制朝鲜的言论，发电机被训练在频域中，然后将其转换回语音转换构音障碍的健康讲话。客观评价使用上的保持输出测试组示出了识别性能与执行对抗性训练后的原始dysarthic语音相比得到改善，作为绝对WER已经被降低了33.4％的产生的话语的自动语音识别。这表明，所提出的基于GaN的转换方法是提高构音障碍的语音清晰度非常有用。</font>
</div>


<hr>
<div id="paper19"> <b>19. Structural Decompositions of Epistemic Logic Programs</b>  <a href="https://arxiv.org/pdf/2001.04219" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title19" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hecher%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Markus Hecher</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Morak%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Michael Morak</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Woltran%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Stefan Woltran</a><br>
<font size="3">
Abstract: Epistemic logic programs (ELPs) are a popular generalization of standard Answer Set Programming (ASP) providing means for reasoning over answer sets within the language. This richer formalism comes at the price of higher computational complexity reaching up to the fourth level of the polynomial hierarchy. However, in contrast to standard ASP, dedicated investigations towards tractability have not been undertaken yet. In this paper, we give first results in this direction and show that central ELP problems can be solved in linear time for ELPs exhibiting structural properties in terms of bounded treewidth. We also provide a full dynamic programming algorithm that adheres to these bounds. Finally, we show that applying treewidth to a novel dependency structure---given in terms of epistemic literals---allows to bound the number of ASP solver calls in typical ELP solving procedures. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：认知逻辑程序（电子学习）是标准的回答集编程（ASP）提供用于在语言中的推理在结果集的流行推广。这更丰富的形式主义来以较高的计算复杂性达到最高多项式层次的第四级的价格。然而，相对于标准的ASP，朝易处理专用的调查还没有进行呢。在本文中，我们让在这个方向的第一结果和表明中央ELP问题可以在线性时间内解决了在有界树宽的方面表现出结构性质电子学习。我们还提供一个完整的动态规划算法了符合这些界限。最后，我们表明，将树宽以一种新颖的依赖结构---在认识文字的形式给出---允许的ASP求解器的典型ELP解决过程的调用绑定的号码。</font>
</div>


<hr>
<div id="paper20"> <b>20. A logic-based relational learning approach to relation extraction: The  OntoILPER system</b>  <a href="https://arxiv.org/pdf/2001.04192" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title20" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Lima%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Rinaldo Lima</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Espinasse%2C+B" target="_blank" rel="noopener" style="color:#0000EE;">Bernard Espinasse</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Freitas%2C+F" target="_blank" rel="noopener" style="color:#0000EE;">Fred Freitas</a><br>
<font size="3">
Abstract: Relation Extraction (RE), the task of detecting and characterizing semantic relations between entities in text, has gained much importance in the last two decades, mainly in the biomedical domain. Many papers have been published on Relation Extraction using supervised machine learning techniques. Most of these techniques rely on statistical methods, such as feature-based and tree-kernels-based methods. Such statistical learning techniques are usually based on a propositional hypothesis space for representing examples, i.e., they employ an attribute-value representation of features. This kind of representation has some drawbacks, particularly in the extraction of complex relations which demand more contextual information about the involving instances, i.e., it is not able to effectively capture structural information from parse trees without loss of information. In this work, we present OntoILPER, a logic-based relational learning approach to Relation Extraction that uses Inductive Logic Programming for generating extraction models in the form of symbolic extraction rules. OntoILPER takes profit of a rich relational representation of examples, which can alleviate the aforementioned drawbacks. The proposed relational approach seems to be more suitable for Relation Extraction than statistical ones for several reasons that we argue. Moreover, OntoILPER uses a domain ontology that guides the background knowledge generation process and is used for storing the extracted relation instances. The induced extraction rules were evaluated on three protein-protein interaction datasets from the biomedical domain. The performance of OntoILPER extraction models was compared with other state-of-the-art RE systems. The encouraging results seem to demonstrate the effectiveness of the proposed solution. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：关系抽取（RE），检测和文本中的实体之间的表征语义关系的任务，获得了巨大的重要性在过去的二十年中，主要是在生物医学领域。许多论文已使用监督机器学习技术发表了关系抽取。这些技术大部分依赖于统计方法，如基于树的内核基于特征和方法。这样的统计学习的技术通常是基于用于表示实施例中，即一个命题假设空间，他们采用的特征的属性 - 值表示。这种表示法存在一些缺陷，特别是在复杂的关系，其中要求对涉及的情况下，即更多的上下文信息的提取，它不能有效地捕捉解析树结构信息不会丢失信息。在这项工作中，我们目前OntoILPER，一种基于逻辑的关系学习方法关系抽取使用归纳逻辑程序设计中的象征提取规则的形式产生的提取模式。 OntoILPER需要的例子丰富的关系表示，这可以减轻上述缺点的利润。拟议的关系的方式似乎更适合关系抽取比统计的人有几个原因，我们认为。此外，OntoILPER使用领域本体引导的背景知识生成处理，用于存储提取的关系实例。从生物医学域中的三个蛋白质 - 蛋白质相互作用数据集的感应提取规则进行评价。 OntoILPER提取模型的性能与国家的最先进的其它可再生能源系统进行了比较。令人鼓舞的结果似乎证明了该解决方案的有效性。</font>
</div>


<hr>
<div id="paper21"> <b>21. Retouchdown: Adding Touchdown to StreetLearn as a Shareable Resource for  Language Grounding Tasks in Street View</b>  <a href="https://arxiv.org/pdf/2001.03671" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title21" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Mehta%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Harsh Mehta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Artzi%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yoav Artzi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Baldridge%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jason Baldridge</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ie%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Eugene Ie</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Mirowski%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Piotr Mirowski</a><br>
<font size="3">
Abstract: The Touchdown dataset (Chen et al., 2019) provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release (Mirowski et al., 2019) to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both of the Touchdown tasks: vision and language navigation (VLN) and spatial description resolution (SDR). We compare our model results to those given in Chen et al. (2019) and show that the panoramas we have added to StreetLearn fully support both Touchdown tasks and can be used effectively for further research and comparison. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：（Chen等，2019）着陆数据集由通过纽约市的街道导航人工注释，并在给定的位置，解决空间的描述提供了说明。为了使更广泛的研究团体与着陆任务有效地开展工作，我们公开发布的29K原街景全景图所需的触地得分。我们遵循用于StreetLearn数据发布过程（Mirowski等，2019），以检查全景的个人身份信息，模糊它们是必要的。这些已被添加到所述数据集StreetLearn并如前面对StreetLearn使用可以通过相同的过程来获得。我们还为双方的着陆任务提供一个参考实现：视觉和语言导航（VLN）和空间分辨率描述（SDR）。我们比较我们的模型结果与陈等人给出的。 （2019），并表明我们已经添加到StreetLearn全景图完全支持着陆任务，可以进一步研究和比较有效地使用。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【arxiv论文】 Computation and Language 2020-01-13</title>
    <url>/2020/01/14/%E3%80%90arxiv%E8%AE%BA%E6%96%87%E3%80%91%20Computation%20and%20Language%202020-01-13/</url>
    <content><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a><div style="color:red;">目录</div></h1><font size="4">
<div id="title1">
<b>1.</b> Towards Minimal Supervision BERT-based Grammar Error Correction <a href="https://arxiv.org/pdf/2001.03521" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper1" style="color:#0000EE;">摘要</a><br></div>
<div id="title2">
<b>2.</b> Co-evolution of language and agents in referential games <a href="https://arxiv.org/pdf/2001.03361" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper2" style="color:#0000EE;">摘要</a><br></div>
<div id="title3">
<b>3.</b> Machine Learning Approaches for Amharic Parts-of-speech Tagging <a href="https://arxiv.org/pdf/2001.03324" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper3" style="color:#0000EE;">摘要</a><br></div>
<div id="title4">
<b>4.</b> Learning to Multi-Task Learn for Better Neural Machine Translation <a href="https://arxiv.org/pdf/2001.03294" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper4" style="color:#0000EE;">摘要</a><br></div>
<div id="title5">
<b>5.</b> A Scalable Chatbot Platform Leveraging Online Community Posts: A  Proof-of-Concept Study <a href="https://arxiv.org/pdf/2001.03278" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper5" style="color:#0000EE;">摘要</a><br></div>
<div id="title6">
<b>6.</b> Simulating Lexical Semantic Change from Sense-Annotated Data <a href="https://arxiv.org/pdf/2001.03216" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper6" style="color:#0000EE;">摘要</a><br></div>
<div id="title7">
<b>7.</b> Debate Dynamics for Human-comprehensible Fact-checking on Knowledge  Graphs <a href="https://arxiv.org/pdf/2001.03436" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper7" style="color:#0000EE;">摘要</a><br></div>
<div id="title8">
<b>8.</b> Inductive Document Network Embedding with Topic-Word Attention <a href="https://arxiv.org/pdf/2001.03369" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper8" style="color:#0000EE;">摘要</a><br></div>
<div id="title9">
<b>9.</b> Linking Social Media Posts to News with Siamese Transformers <a href="https://arxiv.org/pdf/2001.03303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a> <a href="#paper9" style="color:#0000EE;">摘要</a><br></div>
</font><a id="more"></a>


<hr>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- procjx-wenzhang2 -->
<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-1179774715076800" data-ad-slot="5367332398"></ins></p>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><div style="color:red;">摘要</div></h1><div id="paper1"> <b>1. Towards Minimal Supervision BERT-based Grammar Error Correction</b>  <a href="https://arxiv.org/pdf/2001.03521" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title1" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Li%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yiyuan Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Anastasopoulos%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Antonios Anastasopoulos</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Black%2C+A+W" target="_blank" rel="noopener" style="color:#0000EE;">Alan W Black</a><br>
<font size="3">
Abstract: Current grammatical error correction (GEC) models typically consider the task as sequence generation, which requires large amounts of annotated data and limit the applications in data-limited settings. We try to incorporate contextual information from pre-trained language model to leverage annotation and benefit multilingual scenarios. Results show strong potential of Bidirectional Encoder Representations from Transformers (BERT) in grammatical error correction task. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：当前语法纠错（GEC）模型通常考虑的任务，因为序列产生，这需要大量的注释数据，并限制在数据有限的情况下的应用程序。我们尝试从预先训练语言模型来杠杆注释结合上下文信息，有利于多语言情景。结果表明，从变形金刚（BERT）的语法纠错任务双向编码器交涉的巨大潜力。</font>
</div>


<hr>
<div id="paper2"> <b>2. Co-evolution of language and agents in referential games</b>  <a href="https://arxiv.org/pdf/2001.03361" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title2" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Dagan%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gautier Dagan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Hupkes%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dieuwke Hupkes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Bruni%2C+E" target="_blank" rel="noopener" style="color:#0000EE;">Elia Bruni</a><br>
<font size="3">
Abstract: Referential games offer a grounded learning environment for neural agents, that accounts for the functional aspects of language. However, they fail to account for another fundamental aspect of human language: Because languages are transmitted from generation to generation, they have to be learnable by new language users, which makes them subject to cultural evolution. Recent work has shown that incorporating cultural evolution in referential game results in considerable improvements in the properties of the languages that emerge in the game. In this work, we first substantiate this claim with a different data set and a wider array of evaluation metrics. Then, drawing inspiration from linguistic theories of human language evolution, we consider a scenario in which not only cultural but also genetic evolution is integrated. As our core contribution, we introduce the Language Transmission Engine, in which cultural evolution of the language is combined with genetic evolution of the agents' architecture. We show that this co-evolution scenario leads to across-the-board improvements on all considered metrics. These results stress that cultural evolution is important for language emergence studies, but also the suitability of the architecture itself should be considered. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：参照游戏提供接地的学习环境，为神经剂，这占了语言的功能方面。然而，他们无法解释人类语言的另一个重要方面：由于语言从代代相传，他们必须通过新的语言的用户，这使得他们受到文化的演变可以学习的。最近的研究显示，在纳入参考的比赛结果在在游戏中出现的语言的性质相当大的改善文化的演变。在这项工作中，我们首先证实这一要求与不同的数据集和评价度量的更广泛的阵列。然后，从人类语言进化的语言学理论中汲取灵感，我们认为这不仅是文化，而且基因进化集成的场景。作为我们的核心贡献，我们介绍了语言传输引擎，其中语言文化演进与代理的架构的遗传进化相结合。我们表明，这种协同进化的情况导致对所有考虑的指标，全面的板的改进。这些结果强调的是文化进化是语言出现的研究很重要，而且建筑本身的适用性应予以考虑。</font>
</div>


<hr>
<div id="paper3"> <b>3. Machine Learning Approaches for Amharic Parts-of-speech Tagging</b>  <a href="https://arxiv.org/pdf/2001.03324" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title3" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Gashaw%2C+I" target="_blank" rel="noopener" style="color:#0000EE;">Ibrahim Gashaw</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Shashirekha%2C+H+L" target="_blank" rel="noopener" style="color:#0000EE;">H L. Shashirekha</a><br>
<font size="3">
Abstract: Part-of-speech (POS) tagging is considered as one of the basic but necessary tools which are required for many Natural Language Processing (NLP) applications such as word sense disambiguation, information retrieval, information processing, parsing, question answering, and machine translation. Performance of the current POS taggers in Amharic is not as good as that of the contemporary POS taggers available for English and other European languages. The aim of this work is to improve POS tagging performance for the Amharic language, which was never above 91%. Usage of morphological knowledge, an extension of the existing annotated data, feature extraction, parameter tuning by applying grid search and the tagging algorithms have been examined and obtained significant performance difference from the previous works. We have used three different datasets for POS experiments. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：部分的词类（POS）标记被认为是其所需的许多自然语言处理（NLP）的应用，如词义消歧，信息检索，信息处理，分析，问题解答基本而必要的工具之一，和机器翻译。在阿姆哈拉语当前POS标注器的性能还不如说可用于英语和其他欧洲语言的当代POS标注器的。这项工作的目的是为了改进为阿姆哈拉语，这是从来没有91％以上的词性标注的性能。形态的知识，现有的注解数据的扩展，特征提取，参数整定运用网格搜索和标记算法的使用已经被检查，并从以前的作品获得显著的性能差异。我们使用了三种不同的数据集用于POS实验。</font>
</div>


<hr>
<div id="paper4"> <b>4. Learning to Multi-Task Learn for Better Neural Machine Translation</b>  <a href="https://arxiv.org/pdf/2001.03294" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title4" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Zaremoodi%2C+P" target="_blank" rel="noopener" style="color:#0000EE;">Poorya Zaremoodi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Haffari%2C+G" target="_blank" rel="noopener" style="color:#0000EE;">Gholamreza Haffari</a><br>
<font size="3">
Abstract: Scarcity of parallel sentence pairs is a major challenge for training high quality neural machine translation (NMT) models in bilingually low-resource scenarios, as NMT is data-hungry. Multi-task learning is an elegant approach to inject linguistic-related inductive biases into NMT, using auxiliary syntactic and semantic tasks, to improve generalisation. The challenge, however, is to devise effective training schedules, prescribing when to make use of the auxiliary tasks during the training process to fill the knowledge gaps of the main translation task, a setting referred to as biased-MTL. Current approaches for the training schedule are based on hand-engineering heuristics, whose effectiveness vary in different MTL settings. We propose a novel framework for learning the training schedule, ie learning to multi-task learn, for the MTL setting of interest. We formulate the training schedule as a Markov decision process which paves the way to employ policy learning methods to learn the scheduling policy. We effectively and efficiently learn the training schedule policy within the imitation learning framework using an oracle policy algorithm that dynamically sets the importance weights of auxiliary tasks based on their contributions to the generalisability of the main NMT task. Experiments on low-resource NMT settings show the resulting automatically learned training schedulers are competitive with the best heuristics, and lead to up to +1.1 BLEU score improvements. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：平行句对稀缺是在双语低资源方案培养高素质神经机器翻译（NMT）车型的一大挑战，因为NMT是大量数据的。多任务学习是注入语言相关的感性偏见到NMT，利用辅助句法和语义的任务，以提高泛化一个优雅的方法。我们面临的挑战，但是，是制定有效的培训计划，开处方时，在训练过程中使用的辅助任务，填补了主要翻译任务的知识差距，设定被称为偏压MTL。对于训练计划目前的做法是基于手工工程启发式，其有效性在不同MTL设置而异。我们提出了学习培训计划，即学习多任务学习，感兴趣的MTL设置一个新的框架。我们制定的训练计划为马尔可夫决策过程，铺平了道路雇用政策的学习方法来学习调度策略。我们有效地学习使用Oracle策略算法，动态设置的基础上他们的主要任务NMT的普适性贡献辅助任务的重要性权重模仿学习框架内的培训计划政策。在低资源NMT设置实验表明所产生的自动学习训练调度与最好的启发式竞争力，并导致高达+1.​​1 BLEU得分的改善。</font>
</div>


<hr>
<div id="paper5"> <b>5. A Scalable Chatbot Platform Leveraging Online Community Posts: A  Proof-of-Concept Study</b>  <a href="https://arxiv.org/pdf/2001.03278" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title5" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Jo%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sihyeon Jo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Im%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Sangwon Im</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Han%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">SangWook Han</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S+H" target="_blank" rel="noopener" style="color:#0000EE;">Seung Hee Yang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+H" target="_blank" rel="noopener" style="color:#0000EE;">Hee-Eun Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Kim%2C+S" target="_blank" rel="noopener" style="color:#0000EE;">Seong-Woo Kim</a><br>
<font size="3">
Abstract: The development of natural language processing algorithms and the explosive growth of conversational data are encouraging researches on the human-computer conversation. Still, getting qualified conversational data on a large scale is difficult and expensive. In this paper, we verify the feasibility of constructing a data-driven chatbot with processed online community posts by using them as pseudo-conversational data. We argue that chatbots for various purposes can be built extensively through the pipeline exploiting the common structure of community posts. Our experiment demonstrates that chatbots created along the pipeline can yield the proper responses. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：自然语言处理算法的开发和会话数据的爆炸性增长是令人鼓舞的人机对话的研究。尽管如此，越来越大规模合格的会话数据难，看病贵。在本文中，我们核实使用它们作为伪会话数据建设有处理在线社区的帖子一个数据驱动的聊天机器人的可行性。我们认为，出于各种目的聊天机器人，可以通过管道利用社区帖子的共同结构广泛建立。我们的实验表明，沿管道创建聊天机器人能得到适当的回应。</font>
</div>


<hr>
<div id="paper6"> <b>6. Simulating Lexical Semantic Change from Sense-Annotated Data</b>  <a href="https://arxiv.org/pdf/2001.03216" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title6" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Schlechtweg%2C+D" target="_blank" rel="noopener" style="color:#0000EE;">Dominik Schlechtweg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Walde%2C+S+S+i" target="_blank" rel="noopener" style="color:#0000EE;">Sabine Schulte im Walde</a><br>
<font size="3">
Abstract: We present a novel procedure to simulate lexical semantic change from synchronic sense-annotated data, and demonstrate its usefulness for assessing lexical semantic change detection models. The induced dataset represents a stronger correspondence to empirically observed lexical semantic change than previous synthetic datasets, because it exploits the intimate relationship between synchronic polysemy and diachronic change. We publish the data and provide the first large-scale evaluation gold standard for LSC detection models. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了一种新的方法来模拟从共时性意义标注的数据词汇语义变化，并展示其评估词汇语义变化检测模型有效性。感应数据集表示更强的对应于比以前的合成数据集经验观察词汇语义变化，因为它利用共时多义性和历时变化之间的亲密关系。我们发布的数据，并提供了LSC检测模型的首次大规模评估的黄金标准。</font>
</div>


<hr>
<div id="paper7"> <b>7. Debate Dynamics for Human-comprehensible Fact-checking on Knowledge  Graphs</b>  <a href="https://arxiv.org/pdf/2001.03436" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title7" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Hildebrandt%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Marcel Hildebrandt</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Serna%2C+J+A+Q" target="_blank" rel="noopener" style="color:#0000EE;">Jorge Andres Quintero Serna</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ma%2C+Y" target="_blank" rel="noopener" style="color:#0000EE;">Yunpu Ma</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Ringsquandl%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Martin Ringsquandl</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Joblin%2C+M" target="_blank" rel="noopener" style="color:#0000EE;">Mitchell Joblin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Tresp%2C+V" target="_blank" rel="noopener" style="color:#0000EE;">Volker Tresp</a><br>
<font size="3">
Abstract: We propose a novel method for fact-checking on knowledge graphs based on debate dynamics. The underlying idea is to frame the task of triple classification as a debate game between two reinforcement learning agents which extract arguments -- paths in the knowledge graph -- with the goal to justify the fact being true (thesis) or the fact being false (antithesis), respectively. Based on these arguments, a binary classifier, referred to as the judge, decides whether the fact is true or false. The two agents can be considered as sparse feature extractors that present interpretable evidence for either the thesis or the antithesis. In contrast to black-box methods, the arguments enable the user to gain an understanding for the decision of the judge. Moreover, our method allows for interactive reasoning on knowledge graphs where the users can raise additional arguments or evaluate the debate taking common sense reasoning and external information into account. Such interactive systems can increase the acceptance of various AI applications based on knowledge graphs and can further lead to higher efficiency, robustness, and fairness. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：本文提出了基于辩论动力学知识图其实检查的新方法。其基本思想是将框架三重分类的任务，两个加强学习剂，其提取参数之间辩论的游戏 - 在知识图上的路径 - 用进球来证明的事实是真实的（论文）或事实是假的（对立面），分别。根据这些参数，一个二元分类，简称判断，决定是否其实是真还是假。这两种药剂可以看作是稀疏的特征提取，对于无论是论文或对立面目前可解释的证据。相较于黑箱方法，参数使用户获得了法官的决定的理解。此外，我们的方法允许对知识图，其中用户可以提出额外的参数或评估的辩论采取常识推理和外部信息纳入考虑交互推理。这样的交互系统可以增加接受的基于知识的图表各种AI应用，并进一步导致更高的效率，稳健性和公平性。</font>
</div>


<hr>
<div id="paper8"> <b>8. Inductive Document Network Embedding with Topic-Word Attention</b>  <a href="https://arxiv.org/pdf/2001.03369" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title8" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Brochier%2C+R" target="_blank" rel="noopener" style="color:#0000EE;">Robin Brochier</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Guille%2C+A" target="_blank" rel="noopener" style="color:#0000EE;">Adrien Guille</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&query=Velcin%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Julien Velcin</a><br>
<font size="3">
Abstract: Document network embedding aims at learning representations for a structured text corpus i.e. when documents are linked to each other. Recent algorithms extend network embedding approaches by incorporating the text content associated with the nodes in their formulations. In most cases, it is hard to interpret the learned representations. Moreover, little importance is given to the generalization to new documents that are not observed within the network. In this paper, we propose an interpretable and inductive document network embedding method. We introduce a novel mechanism, the Topic-Word Attention (TWA), that generates document representations based on the interplay between word and topic representations. We train these word and topic vectors through our general model, Inductive Document Network Embedding (IDNE), by leveraging the connections in the document network. Quantitative evaluations show that our approach achieves state-of-the-art performance on various networks and we qualitatively show that our model produces meaningful and interpretable representations of the words, topics and documents. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：文档在网络学习表示了结构化文本语料库即当文档相互链接嵌入目标。最近算法扩展网络嵌入通过将在它们的制剂中的节点相关联的文本内容接近。在大多数情况下，这是很难解释学表示。此外，小的重要性是考虑到泛化到未在网络内观察到新文档。在本文中，我们提出了一个解释和归纳文档网络嵌入方法。我们引入新的机制，主题字注意（TWA），其基于字和主题陈述之间的相互文档表示。我们培养这些词和话题载体通过我们的一般模型，归纳文档网络嵌入（IDNE），通过利用文档网络中的连接。定量评估表明，我们的方法实现了各种网络上的国家的最先进的性能和我们定性地表明，我们的模型产生的话，主题和文件有意义的，可解释的表示。</font>
</div>


<hr>
<div id="paper9"> <b>9. Linking Social Media Posts to News with Siamese Transformers</b>  <a href="https://arxiv.org/pdf/2001.03303" target="_blank" rel="noopener" style="color:#0000EE;">[PDF]</a>  <a href="#title9" style="color:#0000EE;">返回目录</a>
 <br>&nbsp;&nbsp;<a href="https://arxiv.org/search/cs?searchtype=author&query=Danovitch%2C+J" target="_blank" rel="noopener" style="color:#0000EE;">Jacob Danovitch</a><br>
<font size="3">
Abstract: Many computational social science projects examine online discourse surrounding a specific trending topic. These works often involve the acquisition of large-scale corpora relevant to the event in question to analyze aspects of the response to the event. Keyword searches present a precision-recall trade-off and crowd-sourced annotations, while effective, are costly. This work aims to enable automatic and accurate ad-hoc retrieval of comments discussing a trending topic from a large corpus, using only a handful of seed news articles. </font>
<br>
<font size="2" style="line-height:30px;">
摘要：许多计算社会科学的研究项目围绕特定热门话题的在线话语。这些作品往往涉及收购有关问题的情况下大规模语料库的分析应对事件的各个方面。关键字搜索呈现精密召回权衡和人群来源的注解，而有效的，是昂贵的。这项工作的目的在于使的意见，从大语料库讨论一个热门话题自动精确的ad-hoc检索，仅使用种子的新闻报道屈指可数。</font>
</div>


<hr>
<p><font style="color:red;">注：中文为机器翻译结果！</font></p>
]]></content>
      <categories>
        <category>arxiv</category>
        <category>CL</category>
      </categories>
  </entry>
  <entry>
    <title>【论文笔记】Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context</title>
    <url>/2020/01/12/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Task-Oriented-Dialog-Systems-that-Consider-Multiple-Appropriate-Responses-under-the-Same-Context/</url>
    <content><![CDATA[<p><strong>Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context</strong>. Yichi Zhang, Zhijian Ou, Zhou Yu. <a href="https://arxiv.org/abs/1911.10484" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p><img src="/images/DADL1.jpg" alt></p><p><img src="/images/DADL2.jpg" alt></p><p>在对话中，对于同一句话，可以有多种回复。但是，现有模型往往趋于生成出现概率最高的回复，而忽视了概率较低的回复。本文通过数据增强的方法，使得模型具备生成多样化回复的能力。</p><a id="more"></a>



<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p><img src="/images/DADL4.jpg" alt><br>在数据预处理阶段，在整个数据集中，找出所有的dialogue state相同的system actions，作为ground truth的补充增强。</p>
<h2 id="整体方法"><a href="#整体方法" class="headerlink" title="整体方法"></a>整体方法</h2><p><img src="/images/DADL3.jpg" alt><br>训练过程中，所有可能的回复概率都要最大，而不只需要ground truth概率最大。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>1 encoder + 3 decoder<br><img src="/images/DADL5.jpg" alt></p>
<p>作者认为通过这样训练，模型就具备了生成多样性回复的能力，在测试的时候可以通过multi beam search、top-k等方式生成多样性回复。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者本次实验主要在数据集MultiWoZ进行。<br><img src="/images/DADL7.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Dialog System</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Integrating Relation Constraints with Neural Relation Extractors</title>
    <url>/2020/01/08/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Integrating-Relation-Constraints-with-Neural-Relation-Extractors/</url>
    <content><![CDATA[<p><strong>Integrating Relation Constraints with Neural Relation Extractors</strong>. Yuan Ye, Yansong Feng, Bingfeng Luo, Yuxuan Lai, Dongyan Zhao. AAAI 2020. <a href="https://arxiv.org/abs/1911.11493" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>在关系抽取任务中，<strong>某个关系的所有subject或者object属于同一种类型</strong>（如：在“母校”的所有subject都属于“人”），或者<strong>多个关系之间往往存在依赖关系</strong>（如“城市”和“地区”的subject都是地名），但是现有模型都没有考虑这个约束，只是单独考虑每一个关系。本文工作利用这种约束以提升关系抽取任务的效果。</p><a id="more"></a>

<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>本文从Coherent和Semantic两个角度出发，提出两种方法。</p>
<h2 id="Coherent"><a href="#Coherent" class="headerlink" title="Coherent"></a>Coherent</h2><p>一致性：满足约束的两个关系，预测概率要同时高。</p>
<p><img src="/images/IER1.jpg" alt></p>
<p>矩阵v表示关系约束C,如果关系i和关系j满足约束，则v_ij=1。</p>
<h2 id="Semantic"><a href="#Semantic" class="headerlink" title="Semantic"></a>Semantic</h2><p>语义性：符合约束中某个规则的两个实例，至少有一个实例满足规则中的某个关系。</p>
<p><img src="/images/IER2.jpg" alt></p>
<p>矩阵u表示约束C,如果关系j和关系k满足约束i，则v_ij=1,v_ik=1.</p>
<p>最后loss由两部分构成，Lo为原始的loss，Lc为约束loss。<br><img src="/images/IER3.jpg" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在ACNN和APCNN两个模型上进行验证，均获得了提升。<br><img src="/images/IER4.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Neural Relation Extraction</tag>
        <tag>Relation Constraints</tag>
      </tags>
  </entry>
  <entry>
    <title>智源社区2019年大会PPT分享 </title>
    <url>/2020/01/08/%E6%99%BA%E6%BA%90%E7%A4%BE%E5%8C%BA2019%E5%B9%B4%E5%A4%A7%E4%BC%9APPT%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p>获取方式 <a href="https://mp.weixin.qq.com/s/zqqQVwr16EhqA2zxYUQSWQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/zqqQVwr16EhqA2zxYUQSWQ</a></p>
<a id="more"></a>

<p><img src="/images/zhiyuan2019-1.jpg" alt=""></p>
<p><img src="/images/zhiyuan2019-2.jpg" alt=""></p>
]]></content>
  </entry>
  <entry>
    <title>【shell】批量删除除了某个文件外的其他所有文件</title>
    <url>/2020/01/08/%E3%80%90shell%E3%80%91%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E9%99%A4%E4%BA%86%E6%9F%90%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%96%E7%9A%84%E5%85%B6%E4%BB%96%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -f !(no_delete_file1|no_delete_file2)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls |grep -v no_delete_file |xargs rm -f</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>批量删除</tag>
      </tags>
  </entry>
  <entry>
    <title>AAAI2020 预讲会</title>
    <url>/2019/12/22/AAAI2020-%E9%A2%84%E8%AE%B2%E4%BC%9A/</url>
    <content><![CDATA[<p>AAAI2020 预讲会翻译对话与文本生成、文本分析与内容挖掘两个Session比较有意思的论文。</p><ul>
<li><p>Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation <a href="https://arxiv.org/abs/1911.09320" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N2-%E9%82%B5%E6%99%A8%E6%B3%BD-%E4%B8%AD%E7%A7%91%E9%99%A2%E8%AE%A1%E7%AE%97%E6%89%80.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Modeling Fluency and Faithfulness for Diverse Neural Machine Translation <a href="https://arxiv.org/abs/1912.00178" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N1-%E8%B0%A2%E5%A9%89%E8%8E%B9-%E4%B8%AD%E7%A7%91%E9%99%A2%E8%AE%A1%E7%AE%97%E6%89%80.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Task-Oriented Dialog Systems that Consider Multiple Appropriate Response under the Same Context <a href="https://arxiv.org/abs/1911.10484" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N3-%E5%BC%A0%E4%BA%A6%E5%BC%9B-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Neural Machine Translation with Joint Representation <a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S1N8-%E6%9D%8E%E7%82%8E%E6%B4%8B-%E4%B8%9C%E5%8C%97%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Multi-Scale Self-Attention for Text Classification <a href="https://arxiv.org/abs/1912.00544" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S2N1-%E9%83%AD%E7%90%A6%E9%B9%8F-%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Intergrating Relation Constraints with Neural Relation Extractors <a href="https://arxiv.org/abs/1911.11493" target="_blank" rel="noopener">[PDF]</a><a href="https://github.com/procjx/procjx.github.io/blob/master/files/AAAI2020-PRE/%E3%80%90%E7%BB%88%E7%A8%BF%E3%80%91S2N5-%E5%8F%B6%E5%85%83-%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6.pdf" target="_blank" rel="noopener">[Slide]</a></p>
</li>
<li><p>Cross-Lingual Natural Language Generation via Pre-Training <a href="https://arxiv.org/abs/1909.10481" target="_blank" rel="noopener">[PDF]</a></p>
</li>
</ul>]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
      <tags>
        <tag>AAAI</tag>
      </tags>
  </entry>
  <entry>
    <title>国内一些NLP实验室及老师主页</title>
    <url>/2019/12/21/%E5%9B%BD%E5%86%85%E4%B8%80%E4%BA%9BNLP%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%8F%8A%E8%80%81%E5%B8%88%E4%B8%BB%E9%A1%B5/</url>
    <content><![CDATA[<p>以下列表只是我个人整理，随意排序，欢迎大家补充与指正。</p>
<ul>
<li><p><a href="http://nlp.csai.tsinghua.edu.cn/site2/" target="_blank" rel="noopener">清华大学自然语言处理与社会人文计算实验室</a></p>
<p><a href="http://www.cs.tsinghua.edu.cn/publish/cs/4616/2013/20130424103737386785027/20130424103737386785027_.html" target="_blank" rel="noopener">孙茂松</a> <a href="http://nlp.csai.tsinghua.edu.cn/~ly/index_cn.html" target="_blank" rel="noopener">刘洋</a> <a href="http://nlp.csai.tsinghua.edu.cn/~lzy/" target="_blank" rel="noopener">刘知远</a></p>
</li>
<li><p><a href="http://www.wict.pku.edu.cn/" target="_blank" rel="noopener">北京大学王选计算机研究所</a></p>
<p><a href="http://www.icst.pku.edu.cn/xztd/xztd_01/1222625.htm" target="_blank" rel="noopener">万小军</a> <a href="http://www.icst.pku.edu.cn/xztd/xztd_01/1222614.htm" target="_blank" rel="noopener">严睿</a> <a href="http://www.wict.pku.edu.cn/zhaodongyan/" target="_blank" rel="noopener">赵东岩</a> <a href="https://sites.google.com/site/ysfeng/home" target="_blank" rel="noopener">冯岩松</a> <a href="https://www.cs.uic.edu/~liub/" target="_blank" rel="noopener">刘兵</a></p>
</li>
<li><p><a href="http://www.cs.tsinghua.edu.cn/publish/cs/index.html" target="_blank" rel="noopener">清华大学计算机科学与技术系</a></p>
<p><a href="http://coai.cs.tsinghua.edu.cn/hml/" target="_blank" rel="noopener">黄民烈</a></p>
</li>
</ul>
<a id="more"></a>

<ul>
<li><p><a href="http://www.icip.org.cn/zh/homepage/" target="_blank" rel="noopener">中国科学院软件研究所中文信息处理实验室</a></p>
<p><a href="http://www.icip.org.cn/team/sunle/" target="_blank" rel="noopener">孙乐</a> <a href="http://www.icip.org.cn/team/hanxianpei/" target="_blank" rel="noopener">韩先培</a></p>
</li>
<li><p><a href="http://iip.ict.ac.cn/" target="_blank" rel="noopener">中国科学院计算技术研究所智能信息处理重点实验室</a></p>
<p><a href="http://sourcedb.ict.cas.cn/cn/jssrck/201709/t20170910_4857722.html" target="_blank" rel="noopener">冯洋</a></p>
</li>
<li><p><a href="http://www.nlplab.com/niuplan/niutrans.ch.html" target="_blank" rel="noopener">东北大学自然语言处理实验室</a></p>
<p><a href="http://www.nlplab.com/members/zhujingbo.html" target="_blank" rel="noopener">朱靖波</a> <a href="http://www.nlplab.com/members/xiaotong.html" target="_blank" rel="noopener">肖桐</a></p>
</li>
<li><p><a href="http://www.cs.fudan.edu.cn/" target="_blank" rel="noopener">复旦大学计算机科学技术学院</a></p>
<p><a href="https://xpqiu.github.io/" target="_blank" rel="noopener">邱锡鹏</a></p>
</li>
<li><p><a href="http://cs.tju.edu.cn/csweb/" target="_blank" rel="noopener">天津大学计算机科学与技术学院</a></p>
<p><a href="https://zhangmeishan.github.io/chn.html" target="_blank" rel="noopener">张梅山</a> <a href="http://cs.tju.edu.cn/csweb/admin_teacher/view?id=232" target="_blank" rel="noopener">熊德意</a></p>
</li>
<li><p><a href="http://nlp.xmu.edu.cn/index.html" target="_blank" rel="noopener">厦门大学自然语言处理实验室</a></p>
<p><a href="http://121.192.180.171:8080/" target="_blank" rel="noopener">史晓东</a></p>
</li>
<li><p><a href="https://cdmc.xmu.edu.cn/index.htm" target="_blank" rel="noopener">厦门大学数字媒体计算研究中心</a></p>
<p><a href="https://cdmc.xmu.edu.cn/info/1010/1054.htm" target="_blank" rel="noopener">苏劲松</a></p>
</li>
<li><p><a href="http://bcmi.sjtu.edu.cn/index.cn.html" target="_blank" rel="noopener">上海交通大学BCMI实验室</a></p>
<p><a href="http://bcmi.sjtu.edu.cn/~zhaohai/" target="_blank" rel="noopener">赵海</a></p>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>写作助手</title>
    <url>/2019/12/21/%E5%86%99%E4%BD%9C%E5%8A%A9%E6%89%8B/</url>
    <content><![CDATA[<ul>
<li><p><a href="https://www.overleaf.com/" target="_blank" rel="noopener">Overleaf</a><br>在线编辑Latex。</p>
</li>
<li><p><a href="https://www.grammarly.com/" target="_blank" rel="noopener">Grammarly</a><br>自动检测语法。</p>
</li>
<li><p><a href="http://www.esoda.org/" target="_blank" rel="noopener">易搜搭</a><br>词语搭配。</p>
</li>
</ul>
<a id="more"></a>]]></content>
      <tags>
        <tag>写作助手</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Improved Document Modelling with a Neural Discourse Parser</title>
    <url>/2019/12/21/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Improved-Document-Modelling-with-a-Neural-Discourse-Parser/</url>
    <content><![CDATA[<p><strong>Improved Document Modelling with a Neural Discourse Parser</strong>.Fajri Koto, Jey Han Lau, Timothy Baldwin. ArXiv 1911.<a href="https://arxiv.org/abs/1911.06919" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>使用篇章结构信息提高篇章建模。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>文章的关键有两点，篇章结构是什么？如何利用篇章结构？</p><a id="more"></a>


<h2 id="篇章结构是什么？"><a href="#篇章结构是什么？" class="headerlink" title="篇章结构是什么？"></a>篇章结构是什么？</h2><p><img src="/images/DMDP1.jpg" alt></p>
<p>本篇文章的篇章结构由RST分析得到，首先将篇章切分成EDU，然后再EDU基础上建立篇章分析树，树上的叶子结点为EDU，非叶子结点为其两个子节点的篇章关系，树上的边为对应子节点在该关系中的重要性。（具体可以去了解一下RST官网介绍和相关论文）</p>
<h2 id="如何利用篇章结构？"><a href="#如何利用篇章结构？" class="headerlink" title="如何利用篇章结构？"></a>如何利用篇章结构？</h2><p>如何利用篇章结构，首先是如何编码篇章结构，也就是如何抽取篇章分析树的特征。针对每个树根节点到每个叶子结点的路径，作者设计两类特征：Shallow Discourse Features 和 Latent Discourse Features。</p>
<h3 id="Shallow-Discourse-Features"><a href="#Shallow-Discourse-Features" class="headerlink" title="Shallow Discourse Features"></a>Shallow Discourse Features</h3><ul>
<li>叶子结点重要性分数</li>
</ul>
<p><img src="/images/DMDP2.jpg" alt></p>
<p>统计路径上Nucleus的比例，h(root)为根节点高度。</p>
<ul>
<li>关系重要性分数</li>
</ul>
<p><img src="/images/DMDP3.jpg" alt></p>
<p>统计路径上每个关系的加权比例，h(x)为节点x的高度。</p>
<ul>
<li><p>结点类别</p>
<p>  Nucleus or satellite</p>
<ul>
<li>兄弟结点</li>
</ul>
<h3 id="Latent-Discourse-Features"><a href="#Latent-Discourse-Features" class="headerlink" title="Latent Discourse Features"></a>Latent Discourse Features</h3></li>
</ul>
<p><img src="/images/DMDP6.jpg" alt></p>
<p>使用两个Bi-LSTM分别编码词序列和句法特征序列，avg-pool，然后拼接。</p>
<p><img src="/images/DMDP4.jpg" alt></p>
<p>拼接后的序列再过一个Bi-LSTM得到最终特征表示。</p>
<p><img src="/images/DMDP5.jpg" alt></p>
<h3 id="如何利用篇章特征"><a href="#如何利用篇章特征" class="headerlink" title="如何利用篇章特征"></a>如何利用篇章特征</h3><p> 得到两类特征后，要如何利用呢？本文提出了三种方法。</p>
<ul>
<li><p>拼接word embedding</p>
<p><img src="/images/DMDP7.jpg" alt></p>
</li>
<li><p>加一层Bi-LSTM</p>
<p><img src="/images/DMDP8.jpg" alt></p>
</li>
<li><p>作为解码attention的一个额外输入</p>
<p><img src="/images/DMDP9.jpg" alt></p>
</li>
</ul>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="Document-Summarizatoin"><a href="#Document-Summarizatoin" class="headerlink" title="Document Summarizatoin"></a>Document Summarizatoin</h2><p>  <img src="/images/DMDP10.jpg" alt></p>
<p>  第一种和第二种方法较好。</p>
<h2 id="Petition-Popularity-Prediction"><a href="#Petition-Popularity-Prediction" class="headerlink" title="Petition Popularity Prediction"></a>Petition Popularity Prediction</h2><p>  <img src="/images/DMDP11.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Summarization</tag>
        <tag>Discourse</tag>
        <tag>RST</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】linux批量杀死进程</title>
    <url>/2019/12/21/%E3%80%90shell%E3%80%91linux%E6%89%B9%E9%87%8F%E6%9D%80%E6%AD%BB%E8%BF%9B%E7%A8%8B/</url>
    <content><![CDATA[<p>批量杀死包含关键字“keyword1”但不包含“keyword2”的进程。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef|grep keyword1|grep -v keyword2|cut -c 9-15|xargs kill -9</span><br></pre></td></tr></table></figure><ul>
<li><strong>“ps -ef”</strong> ——查看所有进程</li>
<li><strong>“grep keyword1”</strong> ——列出所有含有关键字”keyword1”的进程</li>
<li><strong>“grep -v keyword2”</strong> ——在列出的进程中去除含有关键字”keyword2”的进程</li>
<li><strong>“cut -c 9-15″</strong> ——截取输入行的第9个字符到第15个字符，而这正好是进程号PID</li>
<li><strong>“xargs kill -9″</strong> ——xargs 命令是用来把前面命令的输出结果(PID)作为”kill -9″命令的参数，并执行该命令。”kill -9″会强行杀掉指定进程。</li>
</ul>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>批量</tag>
        <tag>杀死进程</tag>
      </tags>
  </entry>
  <entry>
    <title>好的研究想法从哪里来</title>
    <url>/2019/12/03/%E5%A5%BD%E7%9A%84%E7%A0%94%E7%A9%B6%E6%83%B3%E6%B3%95%E4%BB%8E%E5%93%AA%E9%87%8C%E6%9D%A5/</url>
    <content><![CDATA[<p>转载自：知乎专栏 NLP日知录 <a href="https://zhuanlan.zhihu.com/p/93765082" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/93765082</a></p><p>作者：刘知远 </p><p>背景说明：临近ACL 2020投稿截止时间，跟同学密集讨论，争论哪些研究想法适合投到ACL有机会命中。从自己十多年研究经历来看，如何判断一个研究想法好不好，以及这些研究想法从哪里来，对于初学者而言的确是个难题。所以，简单攒了这篇小短文，分享一些经验和想法，希望对刚进入NLP领域的新同学有用。多有舛误请指正。</p><a id="more"></a>


<p>王家卫的电影《一代宗师》中有段经典的比武桥段，宫会长对叶问说“今天我们不比武术，比想法”。其实，好的点子或者想法（idea），也是一篇优秀研究成果的灵魂。而计算机领域流行着一句话“IDEA is cheap, show me the code”，也说明对于重视实践的计算机学科而言，想法的好坏还取决于它的实际效能。这里就来谈下好的研究想法从哪里来。</p>
<h1 id="什么算是好的想法"><a href="#什么算是好的想法" class="headerlink" title="什么算是好的想法"></a>什么算是好的想法</h1><p>2015年，我在微博上写过一个调侃的小段子：</p>
<blockquote>
<p>ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。</p>
</blockquote>
<p>到了2018年，我又续了一小段：</p>
<blockquote>
<p>不期数年，北方DL神教异军突起，内修表示学习，外练神经网络，心法众多，曰门，曰注意，曰记忆，曰对抗，曰增强。经ImageNet一役威震武林，豢Alpha犬一匹无人可近。一时家家筑丹炉，人人炼丹忙，门徒云集，依附者众，有一统江湖之势。有童谣为证：左手大数据，右手英伟达，每逢顶会炼丹忙。</p>
</blockquote>
<p>这里面提到的图模型加圈、神经网络加层、优化目标加正则，神经网络中的门、注意、记忆等，都是一些改进模型性能的创新思路，被各大NLP任务广泛使用并发表论文，也许就是因为被不同NLP任务的重复使用和发表，多少有些审美疲劳而缺少更深的创新思想，被有些网友和学者诟为“灌水”，似乎都不算好的想法。</p>
<p>那么什么才是好的想法呢？我理解这个”好“字，至少有两个层面的意义。</p>
<h2 id="学科发展角度的“好”"><a href="#学科发展角度的“好”" class="headerlink" title="学科发展角度的“好”"></a>学科发展角度的“好”</h2><p>学术研究本质是对未知领域的探索，是对开放问题的答案的追寻。所以从推动学科发展的角度，评判什么是好的研究想法的标准，首先就在一个<strong>“新”</strong>字。</p>
<p>过去有个说法，人工智能学科有个魔咒，凡是人工智能被解决（或者有解决方案）的部分，就不再被认为代表“人类智能”。计算机视觉、自然语言处理、机器学习、机器人之所以还被列为人工智能主要方向，也许正是因为它们尚未被解决，尚能代表“人类智能”的尊严。而我们要开展创新研究，就是要提出新的想法解决这些问题。这其中的”新“字，可以体现在提出新的问题和任务，探索新的解决思路，提出新的算法技术，实现新的工具系统等。</p>
<p>在保证”新“的基础上，研究想法好不好，那就看它<strong>对推动学科发展的助力有多大</strong>。深度学习之所以拥有如此显赫的影响力，就在于它对于人工智能自然语言处理、语音识别、计算机视觉等各重要方向都产生了革命性的影响，彻底改变了对无结构信号（语音、图像、文本）的语义表示的技术路线。</p>
<h2 id="研究实践角度的”好“"><a href="#研究实践角度的”好“" class="headerlink" title="研究实践角度的”好“"></a>研究实践角度的”好“</h2><p>那是不是想法只要够”新“就好呢？是不是越新越好呢？我认为应该还不是。因为，只有<strong>能做得出来的想法</strong>才有资格被分析好不好。所以，从研究实践角度，还需要考虑研究想法的<strong>可实现性</strong>和<strong>可验证性</strong>。</p>
<p>可实现性，体现在该想法是否有足够的数学或机器学习工具支持实现。可验证性，体现在该想法是否有合适的数据集合和广泛接受的评价标准。很多民间科学家的想法之所以得不到学术界的认同，就是因为这些想法往往缺乏可实现性和可验证性，只停留在天马行空的纸面，只是些虚无缥缈的理念。</p>
<h1 id="好的研究想法从哪里来"><a href="#好的研究想法从哪里来" class="headerlink" title="好的研究想法从哪里来"></a>好的研究想法从哪里来</h1><p>想法好还是不好，并不是非黑即白的二分问题，而是像光谱一样呈连续分布，因时而异，因人而宜。计算机科技领域的发展既有积累的过程，也有跃迁的奇点，积累量变才会产生质变，吃第三个馒头饱了，也是因为前面两个馒头打底。</p>
<p>现在的学术研究已经成为高度专业化的职业，有庞大的研究者群体。”Publish or Perish“，是从事学术职业（如教授、研究员、研究生）的人必须做好平衡的事情，不能要求研究者的每份工作都是“诺贝尔奖”或“图灵奖”级的才值得发表。只要对研究领域的发展有所助力，就值得发表出来，帮助同行前进。鲁迅说：天才并不是自生自长在深林荒野里的怪物，是由可以使天才生长的民众产生，长育出来的，所以没有这种民众，就没有天才。这个庞大研究者群体正是天才成长的群众基础。同时，学术新人也是在开展创新研究训练中，不断磨砺寻找好想法能力，鲁迅也说：即使天才，在生下来的时候的第一声啼哭，也和平常的儿童的一样，决不会就是一首好诗。</p>
<p>那么，好的研究想法从哪里来呢？我总结，首先要有区分研究想法好与不好的能力，这需要<strong>深入全面了解所在研究方向的历史与现状</strong>，具体就是对学科文献的全面掌握。人是最善于学习的动物，完全可以将既有文献中不同时期研究工作的想法作为学习对象，通过了解它们提出后对学科发展的影响——具体体现在论文引用、学术评价情况等各方面——建立对研究想法好与不好的评价模型。我们很难条分缕析完美地列出区分好与不好想法的所有特征向量，但人脑强大的学习能力，只要给予足够的输入数据，就可以在神经网络中自动学习建立判别的模型，鉴古知今，见微知著，这也许就是常说的学术洞察力。</p>
<p>做过一些研究的同学会有感受，仅阅读自己研究方向的文献，新想法还是不会特别多。这是因为，读到的都是该研究问题已经完成时的想法，它们本身无法启发新的想法。如何产生新的想法呢？我总结有三种可行的基本途径：</p>
<p><strong>实践法</strong>。即在研究任务上实现已有最好的算法，通过分析实验结果，例如发现这些算法计算复杂度特别高、训练收敛特别慢，或者发现该算法的错误样例呈现明显的规律，都可以启发你改进已有算法的思路。现在很多自然语言处理任务的Leaderboard上的最新算法，就是通过分析错误样例来有针对性改进算法的 [1]。</p>
<p><strong>类比法</strong>。即将研究问题与其他任务建立类比联系，调研其他相似任务上最新的有效思想、算法或工具，通过合理的转换迁移，运用到当前的研究问题上来。例如，当初注意力机制在神经网络机器翻译中大获成功，当时主要是在词级别建立注意力，后来我们课题组的林衍凯和沈世奇提出建立句子级别的注意力解决关系抽取的远程监督训练数据的标注噪音问题 [2]，这就是一种类比的做法。</p>
<p><strong>组合法</strong>。即将新的研究问题分解为若干已被较好解决的子问题，通过有机地组合这些子问题上的最好做法，建立对新的研究问题的解决方案。例如，我们提出的融合知识图谱的预训练语言模型，就是将BERT和TransE等已有算法融合起来建立的新模型 [3]。</p>
<p>正如武侠中的最高境界是无招胜有招，好的研究想法并不拘泥于以上的路径，很多时候是在研究者对研究问题深刻认知的基础上，综合丰富的研究阅历和聪明才智产生”顿悟“的结果。这对初学者而言恐怕还很难一窥门径，需要从基本功做起，经过大量科研实践训练后，才能有登堂入室之感。</p>
<p>在科研实践过程中，除了通过大量文献阅读了解历史，通过深入思考总结产生洞察力外，还有一项必不可少的工作，那就是主动开放的学术交流和合作意识。不同研究领域思想和成果交流碰撞，既为创新思想提供了新的来源，也为”类比“和”顿悟“提供了机会。了解一下历史就可以知晓，人工智能的提出，就是数学、计算机科学、控制论、信息论、脑科学等学科交叉融合的产物。而当红的深度学习的起源，1980年代的Parallel Distributed Processing （PDP），也是计算机科学、脑认知科学、心理学、生物学等领域研究者通力合作的产物。下面是1986年出版的名著《Parallel Distributed Processing: Explorations in the Microstructure of Cognition》第一卷的封面。</p>
<p><img src="https://pic2.zhimg.com/80/v2-a8d3f6e553f9f279cdafea5a3e218701_hd.jpg" alt></p>
<p>作者在前言中是这么讲他们的合作过程的，在最初长达六个月的时间里，它们每周见面交流两次讨论研究进展。</p>
<blockquote>
<p>We expected the project to take about six months. We began in January 1982 by bringing a number of our colleagues together to form a discussion group on these topics. During the first six months we met twice weekly and laid the foundation for most of the work presented in these volumes.</p>
</blockquote>
<p>而书中提供的PDP研究组的成员名单，40年后的今天仍让我惊叹其高度的跨机构、跨学科的交叉特点。所以，特别建议同学们在科研训练中，在专注研究问题的前提下，保持主动的学术交流意识，无论是听讲座报告，参加学术会议，还是选修课程，都有意识地扩宽学术交流的广度，不仅与小同行打成一片，更有看似八竿子打不着的研究领域的学术伙伴。随着研究经历的丰富，会越来越强烈地感受到，越是大跨度交叉的学术报告，越让你受到更大的启发，产生更多让自己兴奋的研究想法。</p>
<p><img src="https://pic2.zhimg.com/80/v2-404a752001300a69baabd40fb3d78b99_hd.jpg" alt></p>
<h1 id="初学者应该怎么做"><a href="#初学者应该怎么做" class="headerlink" title="初学者应该怎么做"></a>初学者应该怎么做</h1><p>与阅读论文、撰写论文、设计实验等环节相比，如何产生好的研究想法，是一个不太有章可循的环节，很难总结出固定的范式可供遵循。像小马过河，需要通过大量训练实践，来积累自己的研究经验。不过，对于初学者而言，仍然有几个简单可行的原则可以参考。</p>
<p><strong>一篇论文的可发表价值，取决于它与已有最直接相关工作间的Delta</strong>。我们大部分研究工作都是站在前人工作的基础上推进的。牛顿说：如果说我看得比别人更远些，那是因为我站在巨人的肩膀上。在我看来，评判一篇论文研究想法的价值，就是看它站在了哪个或哪些巨人的肩膀上，以及在此基础上又向上走了多远。反过来，在准备开始一项研究工作之前，在形成研究想法的时候，也许要首先明确准备站在哪个巨人的肩膀上，以及计划通过什么方式走得更远。与已有最直接相关工作之间的Delta，决定了这个研究想法的价值有多大。</p>
<p><strong>兼顾摘果子和啃骨头</strong>。人们一般把比较容易想到的研究想法，叫做Low Hanging Fruit（低垂果实）。低垂果实容易摘，但同时摘的人也多，选择摘果子就容易受到想法撞车的困扰。例如，2018年以BERT为首的预训练语言模型取得重大突破，2019年中就出现大量改进工作，其中以跨模态预训练模型为例，短短几个月里<a href="http://arxiv.org上挂出了超过六个来自不同团队的图像与文本融合的预训练模型" target="_blank" rel="noopener">http://arxiv.org上挂出了超过六个来自不同团队的图像与文本融合的预训练模型</a> [4]。设身处地去想，进行跨模态预训练模型研究，就是一个比较容易想到的方向，你一定需要有预判能力，知道世界上肯定会有很多团队也同时开展这方面研究，这时你如果选择入场，就一定要做得更深入更有特色，有自己独特的贡献才行。相对而言，那些困难的问题，愿意碰的人就少，潜下心来啃硬骨头，也是不错的选择，当然同时就会面临做不出来的风险，或者做出来也得不到太多关注的风险。同学需要根据自身特点、经验和需求，兼顾摘果子和啃骨头两种类型的研究想法。</p>
<p><img src="https://pic1.zhimg.com/80/v2-d71aaf2b86116e3ea1e891bf9230a2c4_hd.jpg" alt></p>
<p><strong>注意多项研究工作的主题连贯性</strong>。同学的研究训练往往持续数年，需要注意前后多项研究工作的主题连贯性，保证内在逻辑统一。需要考虑，在个人简历上，在出国申请Personal Statement中，或者在各类评奖展示中，能够将这些研究成果汇总在一起，讲出自己开展这些研究工作的总目标、总设想。客观上讲，人工智能领域研究节奏很快，技术更新换代快，所以成果发表也倾向于小型化、短平快。我有商学院、社科的朋友，他们一项研究工作往往需要持续一年甚至数年以上；高性能计算、计算机网络方向的研究周期也相对较长。人工智能这种小步快跑的特点，决定了很多同学即使本科毕业时，也会有多篇论文发表，更不用说硕士生、博士生。在这种情况下，就格外需要在研究选题时，注意前后工作的连贯性和照应关系。几项研究工作放在一起，到底是互相割裂说不上话，还是在为一个统一的大目标而努力，格外反映研究的大局意识和布局能力。例如，下图是我们课题组涂存超博士2018年毕业时博士论文《面向社会计算的网络表示学习》的章节设置，整体来看就比《社会计算的若干重要问题研究》等没有内在关联的写法要更让人信服一些。当然，对于初学者而言，一开始就想清楚五年的研究计划，根本不可能。但想，还是不去想，结果还是不同的。</p>
<p><img src="https://pic1.zhimg.com/80/v2-9fbee2d16f9c05fa1cb1ec86a27d265c_hd.jpg" alt></p>
<p><strong>注意总结和把握研究动态和趋势，因时而动</strong>。2019年在知乎上有这样一个问题：”2019年在NLP领域，资源有限的个人/团队能做哪些有价值有希望的工作？“ 我当时的回答如下：</p>
<blockquote>
<p>我感觉，产业界开始集团化搞的问题，说明其中主要的开放性难题已经被解决得差不多了，如语言识别、人脸识别等，在过去20年里面都陆续被广泛商业应用。看最近的BERT、GPT-2，我理解更多的是将深度学习对大规模数据拟合的能力发挥到极致，在深度学习技术路线基本成熟的前提下，大公司有强大计算能力支持，自然可以数据用得更多，模型做得更大，效果拟合更好。<br>成熟高新技术进入商用竞争，就大致会符合摩尔定律的发展规律。现在BERT等训练看似遥不可及，但随着计算能力等因素的发展普及，说不定再过几年，人人都能轻易训练BERT和GPT-2，大家又会在同一个起跑线上，把目光转移到下一个挑战性难题上。<br>所以不如提前考虑，哪些问题是纯数据驱动技术无法解决的。NLP和AI中的困难任务，如常识和知识推理，复杂语境和跨模态理解，可解释智能，都还没有可行的解决方案，我个人也不看好数据驱动方法能够彻底解决。更高层次的联想、创造、顿悟等认知能力，更是连边还没碰到。这些正是有远见的研究者们应该开始关注的方向。</p>
</blockquote>
<p>需要看到，不同时期的研究动态和趋势不同。把握这些动态和趋势，就能够做出研究社区感兴趣的成果。不然的话，即使研究成果没有变化，只是简单早几年或晚几年投稿，结果也会大不相同。例如，2013年word2vec发表，在2014-2016年之间开展词表示学习研究，就相对比较容易得到ACL、EMNLP等会议的录用；但到了2017-2018年，ACL等会议上的词表示学习的相关工作就比较少见了。</p>
<h1 id="最后的补充"><a href="#最后的补充" class="headerlink" title="最后的补充"></a>最后的补充</h1><p>这篇短文，主要是希望面向初学者，介绍一些求新过程中的经验和注意事项，希望大家少走一些弯路。但阅读文献，深入思考，接收拒稿不断改进的苦，该吃的还是要吃。学术研究和论文发表，对个人而言也许意味着高薪资和奖学金，但其最终的目的还是真正的推动学科的发展。所以，要做经得起考验的学术研究，关键就在”真“与”新“，需要我们始终恪守和孜孜以求。</p>
<p>著名历史学家、清华校友何炳棣先生曾在自传《读史阅世六十年》中提及著名数学家林家翘的一句嘱咐：“要紧的是不管搞哪一行，千万不要做第二等的题目。” 具体到每个领域，什么是一等的题目本身见仁见智，其实更指向内心“求真”的态度。</p>
<p>参考文献<br>[1] <a href="https://paperswithcode.com/" target="_blank" rel="noopener">https://paperswithcode.com/</a> &amp; <a href="http://nlpprogress.com/" target="_blank" rel="noopener">http://nlpprogress.com/</a></p>
<p>[2] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, Maosong Sun. Neural Relation Extraction with Selective Attention over Instances. The 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016).</p>
<p>[3] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu. ERNIE: Enhanced Language Representation with Informative Entities. The 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019).</p>
<p>[4] <a href="https://github.com/thunlp/PLMpapers" target="_blank" rel="noopener">https://github.com/thunlp/PLMpapers</a></p>
]]></content>
  </entry>
  <entry>
    <title>Keyphrase Generation任务综述</title>
    <url>/2019/12/02/Keyphrase-Generation%E4%BB%BB%E5%8A%A1%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="任务简介"><a href="#任务简介" class="headerlink" title="任务简介"></a>任务简介</h1><p>A <strong>keyphrase</strong> or keyword is a piece of short, summative content that expresses the main semantic meaning of a longer text. The typical use of a keyphrase or keyword is in scientific publications to provide the core information of a paper. </p><a id="more"></a>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><ul>
<li>F1@5</li>
<li>F1@10</li>
</ul>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><ul>
<li>KP20K</li>
<li>SemEval</li>
<li>NUS</li>
<li>Krapivin</li>
<li>Inspec</li>
</ul>
<h1 id="SOTA"><a href="#SOTA" class="headerlink" title="SOTA"></a>SOTA</h1><ul>
<li>2019-06-13 <strong>Title-Guided Encoding for Keyphrase Generation</strong></li>
</ul>
<table>
<thead>
<tr>
<th align="center">Dataset</th>
<th align="center">F1@5</th>
<th align="center">F1@10</th>
</tr>
</thead>
<tbody><tr>
<td align="center">KP20K</td>
<td align="center">0.372</td>
<td align="center">0.315</td>
</tr>
</tbody></table>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><ul>
<li><p><strong>Deep Keyphrase Generation</strong>. Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, Yu Chi. ACL 2017. <a href="https://aclweb.org/anthology/papers/P/P17/P17-1054/" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/memray/seq2seq-keyphrase" target="_blank" rel="noopener">[code]</a><br>Keyphrase Generation的第一篇paper，主要框架是 seq2seq + copy.</p>
</li>
<li><p><strong>Semi-Supervised Learning for Neural Keyphrase Generation</strong>. Hai Ye, Lu Wang. EMNLP 2018. <a href="https://aclweb.org/anthology/papers/D/D18/D18-1447/" target="_blank" rel="noopener">[pdf]</a><br>解决资源不足问题，提出两个策略：<br>（1）微调，通过keyphrase extraction方式人为构造大量数据预训练模型，再通过已有数据微调；<br>（2）多任务框架，生成keyphrase的同时生成title。</p>
</li>
<li><p><strong>Title-Guided Encoding for Keyphrase Generation</strong>. Wang Chen, Yifan Gao, Jiani Zhang, Irwin King, Michael R. Lyu1. AAAI 2019. <a href="https://arxiv.org/abs/1808.08575" target="_blank" rel="noopener">[pdf]</a><br>本文认为标题包含了文章的主要信息，通过标题来引导摘要的建模已提升模型性能。</p>
</li>
<li><p><strong>Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</strong>. Hou Pong Chan, Wang Chen, Lu Wang, Irwin King. ACL 2019. <a href="https://arxiv.org/abs/1906.04106" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/kenchan0226/keyphrase-generation-rl" target="_blank" rel="noopener">[code]</a></p>
</li>
</ul>
<ul>
<li><strong>Topic-Aware Neural Keyphrase Generation for Social Media Language. Yue Wang</strong>. Jing Li, Hou Pong Chan, Irwin King, Michael R. Lyu, Shuming Shi. ACL 2019. <a href="https://arxiv.org/abs/1906.03889" target="_blank" rel="noopener">[pdf]</a> <a href="https://github.com/yuewang-cuhk/TAKG" target="_blank" rel="noopener">[code]</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>【shell】截断字符串</title>
    <url>/2019/11/29/%E3%80%90shell%E3%80%91%E6%88%AA%E6%96%AD%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
    <content><![CDATA[<p><a href="https://www.cnblogs.com/fengbohello/p/5954895.html" target="_blank" rel="noopener">https://www.cnblogs.com/fengbohello/p/5954895.html</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">line='abcd&lt;SEG&gt;efg'</span><br><span class="line">newline=$&#123;line#*&lt;SEG&gt;&#125;</span><br><span class="line">echo $newline # efg</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】判断字符串是否包含子串</title>
    <url>/2019/11/29/%E3%80%90shell%E3%80%91%E5%88%A4%E6%96%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%98%AF%E5%90%A6%E5%8C%85%E5%90%AB%E5%AD%90%E4%B8%B2/</url>
    <content><![CDATA[<p><a href="https://blog.csdn.net/iamlihongwei/article/details/59484029" target="_blank" rel="noopener">https://blog.csdn.net/iamlihongwei/article/details/59484029</a></p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [[ $line =~ "&lt;BEGIN&gt;" ]] </span><br><span class="line">then</span><br><span class="line">	echo "包含&lt;BEGIN&gt;"</span><br><span class="line">elif [[ $line =~ "&lt;SEG&gt;" ]]</span><br><span class="line">then</span><br><span class="line">	echo "包含&lt;SEG&gt;"</span><br><span class="line">else</span><br><span class="line">	echo "都不包含"</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>使用python发送免费短信</title>
    <url>/2019/11/27/%E4%BD%BF%E7%94%A8python%E5%8F%91%E9%80%81%E5%85%8D%E8%B4%B9%E7%9F%AD%E4%BF%A1/</url>
    <content><![CDATA[<p>首先在 <a href="https://www.twilio.com/" target="_blank" rel="noopener">twilio</a> 上注册帐号，并申请一个 twilio 手机号，并认证自己的手机号，twilio只能给认证过的手机号发送短信。</p><p>使用 python 发送短信</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> twilio.rest <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line">account_sid = &lt;your account sid&gt;</span><br><span class="line">auth_token = &lt;your auth token&gt;</span><br><span class="line">client = Client(account_sid, auth_token)</span><br><span class="line"></span><br><span class="line">message=client.messages.create(</span><br><span class="line">	from_=&lt;your twilio phone num&gt;,</span><br><span class="line">	body=&lt;your message&gt;,</span><br><span class="line">	to=&lt;your phone num&gt;</span><br><span class="line">)</span><br><span class="line">print(message.sid)</span><br></pre></td></tr></table></figure><a id="more"></a>



<p>注：试用版免费次数有限。</p>
]]></content>
      <categories>
        <category>技术杂谈</category>
      </categories>
  </entry>
  <entry>
    <title>【论文笔记】Context-Aware Learning for Neural Machine Translation</title>
    <url>/2019/11/22/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Context-Aware-Learning-for-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Context-Aware Learning for Neural Machine Translation</strong>. Sébastien Jean, Kyunghyun Cho. ArXiv 1903. <a href="https://arxiv.org/pdf/1903.04715.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文提出一个正则化项，鼓励模型利用上下文信息，从而提高篇章翻译结果。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>人们认为使用上下文可以提高篇章翻译，也就是使用上下文信息后译文翻译概率会更高。</p><a id="more"></a>


<p><img src="/images/context-aware1.png" alt></p>
<p>这个不等式在token、sentence、data三个层次上都成立</p>
<p><img src="/images/context-aware2.png" alt></p>
<p>本文在损失函数中加入一个正则化项，正则化项由三个max margin组成。</p>
<p><img src="/images/context-aware3.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><img src="/images/context-aware4.png" alt></p>
<p>（a）句子级别翻译，不利用上下信息</p>
<p>（b）利用随机上下文，随机上下文的期望跟不利用上下文的期望一样，所以使用上下文没有提升</p>
<p>（c）利用前文上下文，对比随机上下文有提升，并且跟没有利用上下文相差0.4</p>
<p>（d）鼓励利用前文上下文，跟没有利用上下文相差3.74，并且比（c）有提升</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
        <tag>Context</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Pretrained Language Models for Document-Level Neural Machine Translation</title>
    <url>/2019/11/21/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Pretrained-Language-Models-for-Document-Level-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Pretrained Language Models for Document-Level Neural Machine Translation</strong>. Liangyou Li, Xin Jiang, Qun Liu. ArXiv. <a href="https://arxiv.org/pdf/1911.03110.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>现有篇章翻译工作大都只能有限的上下文（前面3句话），当利用更长上下文时，由于训练不稳定模型效果反而下降。理论上来说更长的上下文可以提供更多信息，更有助于翻译。本文就是希望能够在篇章翻译中利用更长的上下文。</p><a id="more"></a>

<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="BERT初始化"><a href="#BERT初始化" class="headerlink" title="BERT初始化"></a>BERT初始化</h2><p>现有有些篇章翻译工作先利用大量平行句对预训练（有些只有利用篇章语料的平行句对预训练），然后再利用平行篇章语料微调。本文不再使用平行句对预训练，而是使用别人训练好的BERT来初始化模型参数。（BERT是在大量单语篇章语料上训练得到的）</p>
<h2 id="利用上下文"><a href="#利用上下文" class="headerlink" title="利用上下文"></a>利用上下文</h2><p>本文使用的上下文为前面512个词。将上下文和当前句子拼接起来，中间有个分隔符，但是如果直接使用Encoder对拼接后的句子进行编码，生成的译文反而更差（训练不稳定）。</p>
<p><img src="/images/mlmdoc.png" alt></p>
<p>本文提出了三个改进方法：</p>
<ul>
<li><p><strong>Segment Embeddings</strong>:<br>用来标记每个词是属于当前要翻译句子，还是属于上下文。</p>
</li>
<li><p><strong>Reverse Position Embeddings</strong>:<br>先对当前要翻译句子进行编号，再对上下文进行编号。</p>
</li>
<li><p><strong>Context Masks</strong>:<br>经过编码器后，当前句子已经包含了上下文信息，对上下文的隐状态加 mask，使得解码器更加关注当前句子。（不加mask，训练不稳定）</p>
</li>
</ul>
<h2 id="多任务"><a href="#多任务" class="headerlink" title="多任务"></a>多任务</h2><p>本文引入了Mask Language Model</p>
<p><img src="/images/mlmdoc1.png" alt></p>
<p>X: 源端当前句子</p>
<p>C: 源端上下文</p>
<p>Y: 目标端当前句子</p>
<p>S: X 和 C 的拼接</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p><img src="/images/mlmdoc2.png" alt></p>
<p>可以训12层encoder还是比较 NB 的。<br><font color="#FF0000">我认为作者可以补充一个只加BERT的实验结果。</font></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
      </tags>
  </entry>
  <entry>
    <title>【深度学习基础】Dropout</title>
    <url>/2019/11/15/%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91Dropout/</url>
    <content><![CDATA[<p>** Improving neural networks by preventing co-adaptation of feature detectors**. Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov. arXiv 1207.0580. <a href="https://arxiv.org/abs/1207.0580" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="一些博客"><a href="#一些博客" class="headerlink" title="一些博客"></a>一些博客</h1><ul>
<li><a href="https://www.zhihu.com/question/61751133" target="_blank" rel="noopener">神经网络Dropout层中为什么dropout后还需要进行rescale？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/66337970" target="_blank" rel="noopener">Dropout的前世与今生</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>【git 使用】分支合并</title>
    <url>/2019/11/12/%E3%80%90git-%E4%BD%BF%E7%94%A8%E3%80%91%E5%88%86%E6%94%AF%E5%90%88%E5%B9%B6/</url>
    <content><![CDATA[<ul>
<li>master发生改变，同步到feature branch</li>
</ul><p>merge master into feature branch</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git checkout &lt;feature branch&gt;</span><br><span class="line">git merge master</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>ArXiv 论文 2019/11/2-2019/11/8</title>
    <url>/2019/11/10/ArXiv-%E8%AE%BA%E6%96%87-2019-11-2-2019-11-8/</url>
    <content><![CDATA[<ul>
<li><a href="https://arxiv.org/abs/1911.00492" target="_blank" rel="noopener">Reasoning Over Paths via Knowledge Base Completion</a></li>
<li><a href="https://arxiv.org/abs/1911.00069" target="_blank" rel="noopener">Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping</a></li>
<li><a href="https://arxiv.org/abs/1911.00133" target="_blank" rel="noopener">Dreaddit: A Reddit Dataset for Stress Analysis in Social Media</a></li>
<li><a href="https://arxiv.org/abs/1911.00176" target="_blank" rel="noopener"><strong>Sequence Modeling with Unconstrained Generation Order</strong></a></li>
<li><a href="https://arxiv.org/abs/1911.00317" target="_blank" rel="noopener">On the Linguistic Representational Power of Neural Machine Translation Models</a></li>
<li><a href="https://arxiv.org/abs/1911.00473" target="_blank" rel="noopener">BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding</a><a id="more"></a></li>
<li><a href="https://arxiv.org/abs/1911.00484" target="_blank" rel="noopener">Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents</a></li>
<li><a href="https://arxiv.org/abs/1911.00225" target="_blank" rel="noopener">When Choosing Plausible Alternatives, Clever Hans can be Clever</a></li>
<li><a href="https://arxiv.org/abs/1911.00269" target="_blank" rel="noopener">A Robust Data-Driven Approach for Dialogue State Tracking of Unseen Slot Values</a></li>
<li><a href="https://arxiv.org/abs/1911.00274" target="_blank" rel="noopener">Kernelized Bayesian Softmax for Text Generation</a></li>
<li><a href="https://arxiv.org/abs/1911.00359" target="_blank" rel="noopener">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data</a></li>
</ul>
]]></content>
      <categories>
        <category>arxiv</category>
      </categories>
      <tags>
        <tag>ArXiv</tag>
      </tags>
  </entry>
  <entry>
    <title>【git 使用】clone、branch、add、commit、push</title>
    <url>/2019/11/08/%E3%80%90git-%E4%BD%BF%E7%94%A8%E3%80%91clone%E3%80%81branch%E3%80%81add%E3%80%81commit%E3%80%81push/</url>
    <content><![CDATA[<h1 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h1><ul>
<li>普通</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@xxx.git</span><br></pre></td></tr></table></figure><ul>
<li>指定branch</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@xxx.git -b &lt;branchname&gt;</span><br></pre></td></tr></table></figure><h1 id="提交修改"><a href="#提交修改" class="headerlink" title="提交修改"></a>提交修改</h1><ul>
<li>提交某个文件</li>
</ul><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add &lt;path to file&gt;</span><br><span class="line">git commit -m '&lt;message&gt;'</span><br><span class="line">git push origin &lt;branchname&gt;</span><br></pre></td></tr></table></figure><a id="more"></a>








<ul>
<li>提交多个文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git add --all</span><br><span class="line">git commit -m '&lt;message&gt;'</span><br><span class="line">git push origin &lt;branchname&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>【shell】alias设置指令别名</title>
    <url>/2019/11/08/%E3%80%90shell%E3%80%91alias%E8%AE%BE%E7%BD%AE%E6%8C%87%E4%BB%A4%E5%88%AB%E5%90%8D/</url>
    <content><![CDATA[<p>alias 可以用来将一些较长的指令进行简化，使用alias时，用户必须使用单引号’’将原来的命令引起来，防止特殊字符导致错误。</p><h1 id="alias基本使用"><a href="#alias基本使用" class="headerlink" title="alias基本使用"></a>alias基本使用</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias 新指令=‘原指令 -选项/参数’</span><br></pre></td></tr></table></figure><p>如：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias myscp='scp admin@192.168.72.77'</span><br></pre></td></tr></table></figure><h1 id="查看永久已设置别名"><a href="#查看永久已设置别名" class="headerlink" title="查看永久已设置别名"></a>查看永久已设置别名</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">alias -p</span><br></pre></td></tr></table></figure><a id="more"></a>







<h1 id="设置永久别名"><a href="#设置永久别名" class="headerlink" title="设置永久别名"></a>设置永久别名</h1><p>修改<code>~/.bashrc</code>文件</p>
<p>参考：<a href="https://man.linuxde.net/alias" target="_blank" rel="noopener">https://man.linuxde.net/alias</a></p>
]]></content>
      <categories>
        <category>技术杂谈</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>alias</tag>
      </tags>
  </entry>
  <entry>
    <title>ArXiv 论文 2019/10/28-2019/11/1</title>
    <url>/2019/11/02/ArXiv-%E8%AE%BA%E6%96%87-2019-10-28-2019-11-1/</url>
    <content><![CDATA[<ul>
<li><a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
<li><a href="https://arxiv.org/abs/1910.14659" target="_blank" rel="noopener">Pseudolikelihood Reranking with Masked Language Models</a></li>
<li><a href="https://arxiv.org/abs/1910.14549" target="_blank" rel="noopener">Positional Attention-based Frame Identification with BERT: A Deep Learning Approach to Target Disambiguation and Semantic Frame Selection</a></li>
<li><a href="https://arxiv.org/abs/1910.14192" target="_blank" rel="noopener">Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning</a></li>
<li><a href="https://arxiv.org/abs/1910.14176" target="_blank" rel="noopener">Predicting Discourse Structure using Distant Supervision from Sentiment</a><a id="more"></a></li>
<li><a href="https://arxiv.org/abs/1910.14142" target="_blank" rel="noopener">Discourse-Aware Neural Extractive Model for Text Summarization</a></li>
<li><a href="https://arxiv.org/abs/1910.14075" target="_blank" rel="noopener">Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.14613" target="_blank" rel="noopener">Neural Assistant: Joint Action Prediction, Response Generation, and Latent Knowledge Reasoning</a></li>
<li><a href="https://arxiv.org/abs/1910.14208" target="_blank" rel="noopener">Hidden State Guidance: Improving Image Captioning using An Image Conditioned Autoencoder</a></li>
<li><a href="https://arxiv.org/abs/1910.13890" target="_blank" rel="noopener">A Latent Morphology Model for Open-Vocabulary Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.13794" target="_blank" rel="noopener">Let Me Know What to Ask: Interrogative-Word-Aware Question Generation</a></li>
<li><a href="https://arxiv.org/abs/1910.13466" target="_blank" rel="noopener">Ordered Memory</a></li>
<li><a href="https://arxiv.org/abs/1910.13106" target="_blank" rel="noopener">Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots</a></li>
<li><a href="https://arxiv.org/abs/1910.13267" target="_blank" rel="noopener">BPE-Dropout: Simple and Effective Subword Regularization</a></li>
<li><a href="https://arxiv.org/abs/1910.13294" target="_blank" rel="noopener">Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control</a></li>
<li><a href="https://arxiv.org/abs/1910.13437" target="_blank" rel="noopener">An Empirical Study of Generation Order for Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1910.12708" target="_blank" rel="noopener">Evaluating Lottery Tickets Under Distributional Shifts</a></li>
<li><a href="https://arxiv.org/abs/1910.12702" target="_blank" rel="noopener">Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling</a></li>
<li><a href="https://arxiv.org/abs/1910.12698" target="_blank" rel="noopener">Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis</a></li>
<li><a href="https://arxiv.org/abs/1910.12527" target="_blank" rel="noopener">RPM-Oriented Query Rewriting Framework for E-commerce Keyword-Based Sponsored Search</a></li>
<li><a href="https://arxiv.org/abs/1910.12391" target="_blank" rel="noopener">What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?</a></li>
<li><a href="https://arxiv.org/abs/1910.12197" target="_blank" rel="noopener">Look-up and Adapt: A One-shot Semantic Parser</a></li>
<li><a href="https://arxiv.org/abs/1910.12196" target="_blank" rel="noopener">Open the Boxes of Words: Incorporating Sememes into Textual Adversarial Attack</a></li>
<li><a href="https://arxiv.org/abs/1910.11966" target="_blank" rel="noopener">Yall should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts</a></li>
<li><a href="https://arxiv.org/abs/1910.12038" target="_blank" rel="noopener">Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention</a></li>
<li><a href="https://arxiv.org/abs/1910.11959" target="_blank" rel="noopener">FineText: Text Classification via Attention-based Language Model Fine-tuning</a></li>
<li><a href="https://arxiv.org/abs/1910.12094" target="_blank" rel="noopener">Meta Learning for End-to-End Low-Resource Speech Recognition</a></li>
<li><a href="https://arxiv.org/abs/1910.11491" target="_blank" rel="noopener">Attention Optimization for Abstractive Document Summarization</a></li>
<li><a href="https://arxiv.org/abs/1910.11471" target="_blank" rel="noopener">Machine Translation from Natural Language to Code using Long-Short Term Memory</a></li>
<li><a href="https://arxiv.org/abs/1910.11470" target="_blank" rel="noopener">A Survey on Recent Advances in Named Entity Recognition from Deep Learning models</a></li>
<li><a href="https://arxiv.org/abs/1910.11411" target="_blank" rel="noopener">Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations</a></li>
<li><a href="https://arxiv.org/abs/1910.11399" target="_blank" rel="noopener">Comparison of Quality Indicators in User-generated Content Using Social Media and Scholarly Text</a></li>
<li><a href="https://arxiv.org/abs/1910.11494" target="_blank" rel="noopener">Fast and Accurate Knowledge-Aware Document Representation Enhancement for News Recommendations</a></li>
<li><a href="https://arxiv.org/abs/1910.11455" target="_blank" rel="noopener">Recognizing long-form speech using streaming end-to-end models</a></li>
</ul>
]]></content>
      <categories>
        <category>arxiv</category>
      </categories>
      <tags>
        <tag>ArXiv</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Discourse-Aware Neural Extractive Model for Text Summarization</title>
    <url>/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Discourse-Aware-Neural-Extractive-Model-for-Text-Summarization/</url>
    <content><![CDATA[<p><strong>Discourse-Aware Neural Extractive Model for Text Summarization</strong>. Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu. ArXiv 1910.14142.<a href="https://arxiv.org/pdf/1910.14142.pdf" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>作者分析认为现有抽取式文档摘要存在以下两个不足：</p><a id="more"></a>

<ul>
<li>抽取式文档摘要都是以句子级别进行抽取，导致结果包含冗余或者没有用的信息。</li>
<li>BERT常被SOTA文档摘要模型用在文档编码器，但是BERT是再句对上预训练的，不能很好捕捉长距离的句间依赖关系。</li>
</ul>
<p>针对以上两个不足，作者提出了两个解决方法：</p>
<ul>
<li>按EDU进行抽取 （EDU是RST中的基本单元，具体可以去了解discourse parsing）</li>
<li>构造RST Graph和Coreference Graph建模长距离句间依赖关系。</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>Discourse Segmentation: sequence to EDU</p>
<p>Discourse Parsing: EDU to RST tree</p>
<h2 id="RST-Graph"><a href="#RST-Graph" class="headerlink" title="RST Graph"></a>RST Graph</h2><p>通过篇章分析，可以在篇章上构造得到一棵树，树的叶子节点是EDU，树上的边代表的是对应子节点的重要性程度，N代表主要，S代表次要，可以认为S是N的补充。相邻两个子节点可以有三种关系，N-N,N-S,S-N。</p>
<p>作者提出假设：S依赖N,所以存在一条路径从S指向N；如果两个节点都是N，就认为是右N依赖做N。</p>
<p>根据这个假设，可以将RST discourse tree转成成RST dependence graph。</p>
<p><img src="/images/discbert1.jpg" alt></p>
<p>注：论文原图中没有标N和S，为了好理解我标了N和S。</p>
<p>如果存在一条从第i个EDU指向第j个EDU的路径，则设GR[i][j]=1，否则为0,这样就可以将RST Graph转化成GR矩阵。</p>
<p><img src="/images/discbert2.jpg" alt></p>
<h2 id="Coreference-Graph"><a href="#Coreference-Graph" class="headerlink" title="Coreference Graph"></a>Coreference Graph</h2><p>通过斯坦福的CoreNLP工具，可以得到多个共指簇（coreference clusters），每个簇中的EDU都指向同一个实体。指向同一个实体的EDU存在联系，所以同一个簇中的所有EDU之间（包括自己跟自己）存在一条边。基于这个原则，作者设计一个构造coreference graph的算法，遍历所有簇，簇中每个EDU之间存在一个边。也就得到了共指矩阵GC。</p>
<p><img src="/images/discbert3.jpg" alt></p>
<h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p><img src="/images/discbert4.jpg" alt></p>
<p>首先使用BERT编码整个篇章，使用BERT得到的隐状态表示，每个EDU内部做self-attention得到EDU的表示，由得到的EDU表示和两个矩阵表示GR和GC，做GCN得到EDU新的表示，通过MLP预测EDU是否被抽取出来做EDU（0-1序列标注）。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在两个数据集上进行验证，并得到了SOTA结果。</p>
<p><img src="/images/discbert5.jpg" alt></p>
<p><img src="/images/discbert6.jpg" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Discourse Structure</tag>
        <tag>Extractive</tag>
        <tag>Summarization</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Document-level Neural Machine Translation with Inter-Sentence Attention</title>
    <url>/2019/11/02/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Document-level-Neural-Machine-Translation-with-Inter-Sentence-Attention/</url>
    <content><![CDATA[<p><strong>Document-level Neural Machine Translation with Inter-Sentence Attention</strong>. Shu Jiang, Rui Wang, Zuchao Li, Masao Utiyama, Kehai Chen, Eiichiro Sumita, Hai Zhao, Bao-liang Lu. ArXiv 1910.14528. <a href="https://arxiv.org/pdf/1910.14528.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文认为大部分篇章翻译只是引入大体的篇章上下文信息，但不是所有的上下文信息都对当前句子翻译有效，本文希望对上下文信息进行筛选。于是本文提出一个associated memory network（AMN）考虑句间关系，建模更加相关的上下文。(<em>其实 SAN 和 QCN 都有对上下文进行筛选</em>)</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p><img src="/images/camn.jpg" alt></p>
<p>（1）使用RNN对previous sentences（cj）进行编码，得到每个词的隐状态表示<font color="#FF0000">(<em>不是很懂为什么要用RNN，不直接使用transformer，并且当前句子x也不用像c一样使用RNN编码</em>)</font></p>
<p>（2）MultiHead Self-Attention更新每个句子的表示<br><img src="/images/camn1.jpg" alt><br><img src="/images/camn2.jpg" alt></p>
<p>（3）当前句子x的每个词和前面每个句子cj中的每个词算一个相似度分数<br><img src="/images/camn3.jpg" alt></p>
<p>（4）对相似性分数按行做softmax作为最终的相似性分数<br><img src="/images/camn4.jpg" alt></p>
<p>（5）得到句子级别上下文表示<br><img src="/images/camn5.jpg" alt></p>
<p>（6）建模每个句子的权重<br><img src="/images/camn6.jpg" alt></p>
<p>（7）得到篇章级别上下文<br><img src="/images/camn7.jpg" alt></p>
<p>（8）在transformer encoder中融入篇章级别上下文信息<br><img src="/images/camn8.jpg" alt><br><img src="/images/camn9.jpg" alt></p>
<p><font color="#FF0000">整体上来说，这种方法略显粗暴。</font></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>作者在TED Talks, Subtitles, News三个数据集上验证了自己的模型有效性。</p>
<p><img src="/images/camn10.jpg" alt></p>
<font color="#FF0000">
  我认为实验还是存在一些不足：（1）没有跟SAN、QCN等工作进行对比（2）按照HAN公开代码，HAN是没有做BPE的，但是本文有做BPE，而本文中报的结果是HAN中报的没有做BPE的结果。
</font>]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NMT</tag>
        <tag>Document NMT</tag>
        <tag>Inter-Sentence</tag>
      </tags>
  </entry>
  <entry>
    <title>Accepted Papers List</title>
    <url>/2019/11/01/Accepted-Papers-List/</url>
    <content><![CDATA[<h1 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h1><ul>
<li><a href="https://aaai.org/Conferences/AAAI-20/wp-content/uploads/2020/01/AAAI-20-Accepted-Paper-List.pdf" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://openreview.net/group?id=ICLR.cc/2020/Conference" target="_blank" rel="noopener">ICLR</a></li>
</ul>
<h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><ul>
<li><a href="https://dblp.org/db/conf/aaai/aaai2019" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://aclweb.org/anthology/events/acl-2019/" target="_blank" rel="noopener">ACL</a></li>
<li><a href="http://openaccess.thecvf.com/CVPR2019.py" target="_blank" rel="noopener">CVPR</a></li>
<li><a href="https://www.aclweb.org/anthology/events/emnlp-2019/" target="_blank" rel="noopener">EMNLP</a></li>
<li><a href="https://openreview.net/group?id=ICLR.cc/2019/Conference" target="_blank" rel="noopener">ICLR</a></li>
<li><a href="https://icml.cc/Conferences/2019/Schedule?type=Poster" target="_blank" rel="noopener">ICML</a></li>
<li><a href="https://www.ijcai19.org/accepted-papers.html" target="_blank" rel="noopener">IJCAI</a></li>
<li><a href="https://aclweb.org/anthology/events/naacl-2019/" target="_blank" rel="noopener">NAACL</a></li>
<li><a href="https://nips.cc/Conferences/2019/Schedule?type=Poster" target="_blank" rel="noopener">NeurIPS</a></li>
</ul>
<a id="more"></a>

<h1 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h1><ul>
<li><a href="https://dblp.org/db/conf/aaai/aaai2018" target="_blank" rel="noopener">AAAI</a></li>
<li><a href="https://aclweb.org/anthology/events/acl-2018/" target="_blank" rel="noopener">ACL</a></li>
<li><a href="http://openaccess.thecvf.com/CVPR2018.py" target="_blank" rel="noopener">CVPR</a></li>
<li><a href="https://aclweb.org/anthology/events/emnlp-2018/" target="_blank" rel="noopener">EMNLP</a></li>
<li><a href="https://iclr.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">ICLR</a></li>
<li><a href="https://icml.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">ICML</a></li>
<li><a href="https://www.ijcai-18.org/accepted-papers/index.html" target="_blank" rel="noopener">IJCAI</a></li>
<li><a href="https://aclweb.org/anthology/events/naacl-2018/" target="_blank" rel="noopener">NAACL</a></li>
<li><a href="https://nips.cc/Conferences/2018/Schedule?type=Poster" target="_blank" rel="noopener">NeurIPS</a></li>
</ul>
]]></content>
      <categories>
        <category>论文列表</category>
      </categories>
      <tags>
        <tag>AAAI</tag>
        <tag>Accepted Papers</tag>
        <tag>ACL</tag>
        <tag>CVPR</tag>
        <tag>EMNLP</tag>
        <tag>ICLR</tag>
        <tag>ICML</tag>
        <tag>IJCAI</tag>
        <tag>NAACL</tag>
        <tag>NIPS</tag>
      </tags>
  </entry>
  <entry>
    <title>公开课推荐</title>
    <url>/2019/10/31/%E5%85%AC%E5%BC%80%E8%AF%BE%E6%8E%A8%E8%8D%90/</url>
    <content><![CDATA[<h1 id="机器学习（斯坦福大学）"><a href="#机器学习（斯坦福大学）" class="headerlink" title="机器学习（斯坦福大学）"></a>机器学习（斯坦福大学）</h1><p>机器学习是一门研究在非特定编程条件下让计算机采取行动的学科。最近二十年，机器学习为我们带来了自动驾驶汽车、实用的语音识别、高效的网络搜索，让我们对人类基因的解读能力大大提高。当今机器学习技术已经非常普遍，您很可能在毫无察觉情况下每天使用几十次。许多研究者还认为机器学习是人工智能（AI）取得进展的最有效途径。</p><a id="more"></a>
<p>本课程将广泛介绍机器学习、数据挖掘和统计模式识别。相关主题包括：(i) 监督式学习（参数和非参数算法、支持向量机、核函数和神经网络）。(ii) 无监督学习（集群、降维、推荐系统和深度学习）。(iii) 机器学习实例（偏见/方差理论；机器学习和AI领域的创新）。课程将引用很多案例和应用，您还需要学习如何在不同领域应用学习算法，例如智能机器人（感知和控制）、文本理解（网络搜索和垃圾邮件过滤）、计算机视觉、医学信息学、音频、数据库挖掘等领域。</p>
<h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a></li>
<li><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="noopener">网易公开课</a></li>
</ul>
<hr>
<h1 id="CS231n（斯坦福大学）"><a href="#CS231n（斯坦福大学）" class="headerlink" title="CS231n（斯坦福大学）"></a>CS231n（斯坦福大学）</h1><p>计算机视觉已经在我们的社会中无处不在，在搜索，图像理解，应用程序，测绘，医药，无人驾驶飞机和自动驾驶汽车中的应用。许多这些应用程序的核心是视觉识别任务，如图像分类，定位和检测。神经网络（又名“深度学习”）方法的最新发展极大地提高了这些最先进的视觉识别系统的性能。本课程深入探讨深度学习架构的细节，重点是学习这些任务的端到端模型，尤其是图像分类。在为期10周的课程中，学生将学习实施，训练和调试自己的神经网络，并获得对计算机视觉尖端研究的详细了解。最后的任务将涉及培训一个数百万参数卷积神经网络，并将其应用于最大的图像分类数据集（ImageNet）。我们将着重教授如何设置图像识别问题，学习算法（例如反向传播），用于训练和微调网络的实际工程技巧，并引导学生通过实践任务和最终课程项目。本课程的大部分背景和材料都将从ImageNet挑战中吸取。</p>
<h2 id="链接-1"><a href="#链接-1" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="noopener">YouTube</a></li>
</ul>
<hr>
<h1 id="CS20SI（斯坦福大学）"><a href="#CS20SI（斯坦福大学）" class="headerlink" title="CS20SI（斯坦福大学）"></a>CS20SI（斯坦福大学）</h1><p>Tensorflow是Google Brain研究人员开发的一个功能强大的机器学习开源软件库。它具有许多预建功能，可以简化构建不同神经网络的任务。 Tensorflow允许在不同计算机之间分配计算，以及在一台机器中分配多个CPU和GPU。 TensorFlow提供了一个Python API，以及一个较少记录的C ++ API。对于本课程，我们将使用Python。</p>
<p>本课程将涵盖深入学习研究的Tensorflow图书馆的基本原理和当代用法。帮助学生理解Tensorflow的图形计算模型，探索其提供的功能，并学习如何构建和构建最适合深度学习项目的模型。通过本课程，学生将使用Tensorflow建立不同复杂度的模型，从简单的线性/逻辑回归到卷积神经网络和带有LSTM的递归神经网络，以解决词嵌入，翻译，光学字符识别等任务。学生还将学习最佳实践来构建模型并管理研究实验。</p>
<h2 id="链接-2"><a href="#链接-2" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://web.stanford.edu/class/cs20si/" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/watch?v=g-EvyKpZjmQ&list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="noopener">YouTube</a></li>
</ul>
<hr>
<h1 id="CS224d（斯坦福大学）"><a href="#CS224d（斯坦福大学）" class="headerlink" title="CS224d（斯坦福大学）"></a>CS224d（斯坦福大学）</h1><p>自然语言处理（NLP）是信息时代最重要的技术之一。理解复杂的语言也是人工智能的重要组成部分。 NLP的应用无处不在，因为人们用语言沟通大多数事物：网络搜索，广告，电子邮件，客户服务，语言翻译，放射学报告等等。NLP应用背后有大量的基础任务和机器学习模型。最近，深度学习方法在许多不同的NLP任务中获得了非常高的性能。这些模型通常可以通过单一的端到端模型进行培训，而且不需要传统的，特定于任务的特征工程。在这个冬季的季度课程中，学生将学习实施，培训，调试，可视化和创造自己的神经网络模型。本课程深入介绍了深入学习NLP的前沿研究。在模型方面，我们将介绍词向量表示，基于窗口的神经网络，递归神经网络，长期 - 短期记忆模型，递归神经网络，卷积神经网络以及一些涉及存储器组件的最新模型。通过讲座和编程作业，学生将学习使神经网络适应实际问题的必要工程技巧。</p>
<h2 id="链接-3"><a href="#链接-3" class="headerlink" title="链接"></a>链接</h2><ul>
<li><a href="https://www.youtube.com/watch?v=g-EvyKpZjmQ&list=PLQ0sVbIj3URf94DQtGPJV629ctn2c1zN-" target="_blank" rel="noopener">课程主页</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6" target="_blank" rel="noopener">YouTube</a></li>
</ul>
]]></content>
      <categories>
        <category>公开课</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>CS231n</tag>
        <tag>CS20SI</tag>
        <tag>CS224d</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Hierarchical-Modeling-of-Global-Context-for-Document-Level-Neural-Machine-Translation/</url>
    <content><![CDATA[<p><strong>Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation</strong>. Xin Tan, Longyin Zhang, Deyi Xiong, Guodong Zhou. EMNLP 2019. <a href="https://www.aclweb.org/anthology/D19-1168.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文觉得现有篇章翻译工作基于pre-context的方法存在两个不足：</p>
<p>（1）只利用一边（one-sidedness）的上下文可能还不够</p>
<p>（2）不正确的pre-context（translation bias propagation caused by improper pre-context）可能会导致翻译错误，所以本文想要利用整个篇章建模全局上下文（global context）来提升篇章翻译。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="使用层次结构建模全局上下文"><a href="#使用层次结构建模全局上下文" class="headerlink" title="使用层次结构建模全局上下文"></a>使用层次结构建模全局上下文</h2><p><img src="/images/HM-GDC.png" alt></p>
<p>A. Sentence Encoder</p>
<p>首先对句子进行编码得到每个词的隐状态表示，</p>
<p><img src="/images/h1.png" alt></p>
<p>求和得到整个句子的表示，</p>
<p><img src="/images/h21.png" alt></p>
<p>B. Document Encoder</p>
<p>对篇章所有句子进行编码，得到拥有篇章信息的句子表示（sentence-level global context）</p>
<p><img src="/images/h22.png" alt></p>
<p>C. Backpropagation of global context</p>
<p>由sentence-level global context得到word-level global context</p>
<p><img src="/images/h3.png" alt></p>
<h2 id="将全局上下文结合到NMT中"><a href="#将全局上下文结合到NMT中" class="headerlink" title="将全局上下文结合到NMT中"></a>将全局上下文结合到NMT中</h2><p>像其他工作一样，这个global context既结合在编码阶段，也可以结合在解码阶段。</p>
<p>A. 结合在编码阶段</p>
<p>使用word-level global context更新每个词的表示，P表示残差dropout，这里为0.1。</p>
<p><img src="/images/h5.png" alt></p>
<p>B. 结合在解码阶段</p>
<p><img src="/images/h4.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>本文实验在中英和德英两个数据集上进行。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>A. 中英</p>
<p>句子级别数据（用于预训练）：2.8M news corpora (LDC 2003E14, LDC2004T07, LDC2005T06, LDC2005T10, LDC2004T08)</p>
<p>篇章级别数据: IWSLT 2017 TED (1906个文档，226K个句对 )</p>
<p>B. 德英</p>
<p>(不进行预训练，没有句子级别数据)</p>
<p>篇章级别数据：IWSLT 2014 TED (1361个文档，172个句对)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>EMNLP</tag>
        <tag>NMT</tag>
        <tag>Context</tag>
        <tag>Document NMT</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Cross-Lingual-BERT-Transformation-for-Zero-Shot-Dependency-Parsing/</url>
    <content><![CDATA[<p><strong>Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</strong>. Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, Ting Liu. EMNLP 2019 <a href="https://arxiv.org/abs/1909.06775" target="_blank" rel="noopener">[PDF]</a>（短文）</p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本篇论文主要解决目前大部分cross-lingual word embedding技术存在的问题：</p><a id="more"></a>

<p>（1）依赖大量跨语言数据</p>
<p>（2）需要大量计算资源和训练时间</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>本文提出一种简单快捷的离线cross-lingual BERT线性映射方法：</p>
<p>（1）通过无监督词对齐方法获得上下文对齐次对（context-level，非词典）</p>
<p>（2）通过预训练好的BERT模型得到上下文对齐次对（x,y）中x,y的上下文表示</p>
<p>（3）通过SVD(奇异值分解)、GD(梯度下降)的方式求得两个表示的线性映射</p>
<p><img src="https://img-blog.csdnimg.cn/2019100320530798.png" alt></p>
<p>作者将获得的跨语言上下文词向量应用到zero-shot依存分析任务上，并获得了目前最好结果。并与XLM(利用跨语言数据重新训练BERT的方法)进行了对比，实验表明该方法在取得与XLM相近结果的情况下，需要的计算资源更少，训练速度也更快。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>EMNLP</tag>
        <tag>Cross Lingual</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Neural-Keyphrase-Generation-via-Reinforcement-Learning-with-Adaptive-Rewards/</url>
    <content><![CDATA[<p><strong>Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards</strong>. Hou Pong Chan, Wang Chen, Lu Wang, Irwin King. ACL 2019. <a href="https://arxiv.org/abs/1906.04106" target="_blank" rel="noopener">[PDF]</a></p><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本篇论文主要解决目前keyphrase generation任务中存在的两个不足：</p><a id="more"></a>

<p>（1）模型生成的keyphrase比真实的keyphrase个数少</p>
<p>（2）已有评价标准依赖词的完成匹配（不完全匹配就算错，如真实keyphrase为SVM，模型生成的keyphrase为support vector machine也算错）</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>keyphrase根据是否在原文中是否出现分present和absent，这里将一个document的所有keyphrase拼接成一个序列，present在前absent在后，并通过利用seq2seq编码document来生成所有的keyphrase。<br><img src="https://img-blog.csdnimg.cn/20190620092517159.png" width="55%" height="55%"></p>
<p>针对第一个不足，作者使用了reinforcement learning，</p>
<p>sample policy：<br><img src="https://img-blog.csdnimg.cn/20190620092800754.png" alt></p>
<p>reward function: RF1<br><img src="https://img-blog.csdnimg.cn/20190620091404860.png" alt></p>
<p>N为目前已生成的keyphrase个数，G为真实keyphrase个数。作者认为当生成keyphrase的个数还少于真实keyphrase个数时，应该鼓励模型去生成更多的keyphrase，所以用recall作为reward；当个数足够时，除了要求个数也要要求正确性，所以用的F1。看到这里可能也有人会有疑问，为什么前面只重视个数而忽视正确性呢？为什么不改变个数和正确性的权重呢（可以认为是F1的变形）？在这里我个人认为作者可能是实验驱动，只用recall就有效果了；如果没有效果作者可能会去设计吧。。。</p>
<p>presen keyphrase 和 absent keyphrase分别计算reward:<br><img src="https://img-blog.csdnimg.cn/20190620093050519.png" width="55%" height="55%"></p>
<p>针对第二个不足，思路也很容易理解，就是找keyphrase的各种形式，作者这里主要有三个方法</p>
<p>（1）Acronyms in the ground-truths</p>
<p>（2）Wikipedia entity titles</p>
<p>（3）Wikipedia disambiguation pages</p>
<p>然后作者在四个baseline基础上分别验证了方法的有效性，并且对生成的keyphrase的个数、RF1进行了分析。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>ACL</tag>
        <tag>Keyphrase Generation</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 教程</title>
    <url>/2019/10/31/Hexo-%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<ul>
<li><a href="https://hexo-guide.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">hexo指南</a></li>
<li><a href="https://ahh666.com/posts/blog_gitalk_about.html" target="_blank" rel="noopener">添加gitalk评论</a></li>
<li><a href="https://blog.yleao.com/2018/0901/hexo-next%E4%B8%BB%E9%A2%98%E4%B8%8B%E7%9A%84%E7%BE%8E%E5%8C%96.html" target="_blank" rel="noopener">hexo-next主题下的美化</a></li>
<li><a href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/zh-CN/MATH.md" target="_blank" rel="noopener">hexo-next使用公式</a></li>
<li><a href="https://io-oi.me/tech/hexo-next-optimization/" target="_blank" rel="noopener">打造个性超赞博客 Hexo + NexT + GitHub Pages 的超深度优化</a></li>
<li><a href="https://muse.theme-next.org/" target="_blank" rel="noopener">Next官方文档</a></li>
<li><a href="https://fontawesome.com/v4.7.0/icons/" target="_blank" rel="noopener">图标</a></li>
</ul>]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文笔记】Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</title>
    <url>/2019/10/31/%E3%80%90%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%91Unsupervised%20Neural%20Single-Document%20Summarization%20of%20Reviews%20via/</url>
    <content><![CDATA[<p><strong>Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking</strong>. Masaru Isonuma, Junichiro Mori, Ichiro Sakata. ACL 2019. <a href="https://arxiv.org/pdf/1906.05691.pdf" target="_blank" rel="noopener">[PDF]</a></p><a id="more"></a>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>本文认为，评论（review）可以当作一个棵篇章树，树的根节点是其摘要，表示该评论的整体意思; 树的其他节点是对其父节点的细化。 也就是说这棵篇章树由摘要（根节点）与评论中所有句子（非根节点，每个非根节点代表一个句子）组成。于是本文通过学习构造这个隐式篇章树来建模得到评论摘要，并提出一种排序（rank）算法选择对生成摘要更加重要的句子。</p>
<p><img src="/images/strsum.png" alt></p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="模型整体方法"><a href="#模型整体方法" class="headerlink" title="模型整体方法"></a>模型整体方法</h2><p>（1）双向GRU+maxpooling 建模得到每个句子表示</p>
<p>（2）建模 父节点-子节点 对应关系权重（权重代表了树的关系）</p>
<p>（3）加权求和所有子节点表示，生成父节点（本文假设：子节点能够还原父节点，因为子节点包含了比父节点更多的信息。）</p>
<p>目标函数就是所有父节点生成概率最大。</p>
<p><img src="/images/strsum2.png" alt></p>
<h2 id="父节点-子节点-对应关系权重建模方法"><a href="#父节点-子节点-对应关系权重建模方法" class="headerlink" title="父节点-子节点 对应关系权重建模方法"></a>父节点-子节点 对应关系权重建模方法</h2><p>初始建模：边界概率（Marginal Probability of Dependency）</p>
<p><img src="/images/strsum3.png" alt></p>
<p>归一化（公式推导不是很懂）</p>
<p><img src="/images/strsum4.png" alt></p>
<p>调整：篇章排序（DiscourseRank）</p>
<p>受PageRank算法启发，更重要的句子有更多后代，迭代更新权重矩阵。<br><img src="/images/strsum5.png" alt></p>
<p><img src="/images/strsum6.png" alt></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>ACL</tag>
        <tag>Discourse Structure</tag>
        <tag>Discourse Ranking</tag>
      </tags>
  </entry>
</search>
